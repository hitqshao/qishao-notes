---
title: Memory Compression
date: 2023-06-06 
permalink: /pages/2476bf/
---

1. Compresso: Pragmatic Main Memory Compression
2. Translation-optimized Memory Compression for Capacity

---
#### 1. Compresso: Pragmatic Main Memory Compression

Cite from the paper:
We propose Compresso, with optimizations to reduce compressed data movement in a hardware compressed memory, while maintaining high compression ratio by repacking data at the right time.

Compresso uses the modified BPC compression algorithm, achieving 1.85x average compression on a wide range of applications.

Compresso uses the compression granularity of 64B.

Compresso uses LinePack with 4 possible cache line sizes.

We compare variable-sized chunks (512B, 1KB, 2KB and 4KB) with 512B fixed-sized chunks. Compresso uses incremental allocation in 512B chunks,thereby allowing 8 page sizes (512B, 1KB, 1.5KB and so on).

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/d8d83212-ff2b-4f84-85c1-454255ee5e98)

Additional Data Movement:
1. split-access cachelines
2. changes in compressibility(overflows)
3. metadata access

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/d04d18a4-5678-428c-a2cf-d1bc43ca2f3f)

Difference in exception in LCP compression
Instead, Compresso allows some number of such inflated cachelines to be stored uncompressed in the inflation room at the end of an MPA page, provided that there is space in that page (Fig. 5a). This is similar to the
exception region in LCP, but is used for an entirely different reason—to reduce compression-related data movement, rather than to support a specific packing scheme.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/c97e0467-88e3-43a9-b25f-3d43477744d5)

We present the first main-memory compression architecture that is designed to run an unmodified operating system.

---
#### 2. Translation-optimized Memory Compression for Capacity

prior workscompress and pack/migrate data at a small - memory block-level - granularity; this introduces an additional block-level translation after the page-level virtual address translation. In general, the smaller the granularity of address translation, the higher the translation overhead.

A promising solution is to only save memory from cold (i.e.,less recently accessed) pages without saving memory from hot (i.e., more recently accessed) pages (e.g., keep the hot pages
uncompressed).

Two challenges:
1. after a compressed cold page becomes hot again, migrating the page to a full 4KB DRAM location still adds another level (albeit page-level, instead of block-level) of translation on top of existing virtual address translation.
**Solution**
we propose compressing page table blocks in hardware to opportunistically embed compression translations into them in a software-transparent manner to effectively prefetch compression translations during a page walk, instead of serially fetching them after the walk.

First, CTE misses typically occur after PTE misses in TLB because CTEs, especially the page-level CTEs under an OS-inspired approach, have similar translation reach as PTEs.
Second, we observe page table blocks (PTBs) are highly compressible because adjacent virtual pages often have identical status bits and the most significant bits in physical page numbers are unused. 
As such, to hide the latency of CTE misses, TMCC transparently compresses each PTB in hardware to free up space in the PTB to embed the CTEs of the 4KB pages (i.e., either data pages or page table pages) that the PTB points to; this enables each page walk to also prefetch the matching CTE required for fetching from DRAM either the end data or the next PTB.

2. only compressing cold data require compressing them very aggressively to achieve high overall memory savings.
**Solution**
we perform a large design space exploration across many hardware configurations and diverse workloads to derive and implement in HDL an ASIC Deflate that is specialized for memory.


Prior new hardware managed translation entries as Compression Translation Entries (CTEs), as they are similar to OS page table entries (PTEs). Prior works cache CTEs in the memory controller via a dedicated CTE cache, similar to the TLBs dedicated to caching PTEs.

let hardware take on an OS-inspired approach: only save memory from cold (i.e., less recently accessed) pages without saving memory from hot (i.e., recently accessed) pages (e.g., keep the hot pages uncompressed), like OS memory compression.
Solves the problem of 
1) translation overheads that large and/or irregular workloads suffer from high PTE miss under hardware memory compression.
2) Fine-grained address translation

Accesses to a compressed virtual page in ML2 incurs a page fault to wake up OS to pop a free physical page from ML1’s free list and migrate the virtual page to the page.

ML2 also keeps many free lists, each tracking sub-physical pages of a different size, to store any compressed virtual page in a practically ideal matching sub-physical page.

ML2 gracefully grows and shrinks relative to ML1 with increasing and decreasing memory usage. When everything can fit in memory uncompressed, ML2 shrinks to zero bytes in physical size so ML1 can have every physical page. Specifically, when ML2’s free list(s) get large (e.g., due to reducing memory usage), ML2 donates free physical pages from its free list(s) to ML1.
OS also grows ML1 free list, when it gets small, by migrating cold virtual pages to ML2.
Migrating a virtual page to ML2 shrinks one of ML2’s free lists. If a ML2 free list gets empty, ML1 gives cold victim physical pages to ML2 (i.e., track them in ML2 instead of ML1), so that ML2 can compress the virtual pages currently in the victim pages to free space in the victims to grow ML2’s free list(s).

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/a8a3f90b-4334-4d5b-a7ad-3a158b9f0966)

Key Idea: Based on our observations, we propose transparently compressing each PTB in hardware to free up space in the PTB to embed the CTEs of the 4KB pages (i.e., either data pages or page table pages) that the PTB points to; this enables each page walk access to also prefetch the matching CTE required either for the next page walk access (i.e., to the next PTB) or for the actual data (or instruction) access after the walk.
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/58f6c74a-d910-4c85-9811-2f74a15aec07)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/744c2c42-6a02-42df-8d19-ccd707abe9ad)


A practical challenge is that after migrating a page (e.g., from ML1 to ML2 after the page becomes cold), the corresponding CTE embedded in the page’s PTB should be updated. However, hardware has no easy way to use the PPN of the migrating page to find/access the page’s PTB(s).
TMCC addresses this challenge by lazily updating the CTE in the PTB later around when the PTB is naturally accessed by the page walker, instead of updating it at the time of migrating the page. 
However, this means that for the first page walker access to the PTB after migrating one of the pages that the PTB points to, the corresponding CTE is out-of-date.
To ensure correctness, TMCC also accesses the correct CTE in DRAM (or in CTE cache) in parallel to verify the correctness of the DRAM access. 
Figure 8 compares and contrasts how TMCC serves an LLC miss that also misses in CTE cache with the baseline approach.
Figure 9 provides n architectural overview of TMCC.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/d351dbe2-dc03-4069-8977-0c17dc3300e0)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/60eeac47-a9c5-4589-8ecb-e23578097575)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/6c31c55f-6120-42fe-882a-a707129eadcf)

**My Comment**
When os access compressed page, that page is migrated from ML2 to ML1. Hardware cannot update the PTB easily. Thus it utilizes lazily update. 
During the page table walk, it will buffer the piggybacked CTE into CTE buffer. And when data miss req happens,  L2 extracts the PPN from the received request to lookup the CTE Buffer to obtain the CTE for MC to translate the PPN.


