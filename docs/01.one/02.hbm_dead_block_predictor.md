---
title: HBM Dead Block Predictor
date: 2023-05-15 
permalink: /pages/2476af/
---

1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory
2. Die-Stacked DRAM: Memory, Cache, or MemCache?
3. A Survey Of Techniques for Architecting DRAM Caches
4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)
5. BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM
6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches
7. To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches
8. ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction
---
9. A Survey of Cache Bypassing Techniques
10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel
11. Bypass and Insertion Algorithms for Exclusive Last-level Caches
12. Counter-Based Cache Replacement and Bypassing Algorithms
13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)
---
### 1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory
- Software manage DRAM and NVM
1. First touch policy<br>
   Alloc all pages in DRAM
2. Static profile-based policy
3. Spill Migration<br>
   LRU spill policy keeps track of last access time for each page in DRAM, and in case of eviction selects one that is least recently used.
   Spill migration policy first allocates a page in fast memory (in our case DRAM), and later evicts it to PCM.
   Spill profile-based policy can either spare a page from eviction if its future traffic is high, or victimize it if it is low, regardless of its previous access count.
4. Dynamic page migration

   ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/f32475d8-b0ce-4066-a416-d36d1f44c86f)
   
   When a page is first brought to the PCM we reset its access counter, regardless of how many times it was accessed in the DRAM. At the same time we keep track of   the number of accesses for every page in the DRAM, as well as the average for all the pages (nDRAMavg). When a page in PCM is accessed, we compare its access counter
(naccesses) with the average number of accesses to pages in DRAM.
   Back migration threshold (BMT) is a value that controls the aggressiveness of migration triggering.
   If it is set to zero, a page is migrated as soon as it is touched in PCM, so the DRAM acts as a typical cache. In this case we expect good performance as the system tends to always move active pages to DRAM, but due to a large number of migrations, number of writes to PCM may go high.
   On the other hand, if BMT is set to infinity the page never gets migrated back, and then the policy is equivalent to LRU spill. In between those extremes we would like to search for values that give good performace and low number of PCM writes.

---   
### 2. Die-Stacked DRAM: Memory, Cache, or MemCache?
- Part as Memory ans Part as Cache
- Discuss and compared with Alloy Cache, Unison Cache, Banshee Cache, HMA
- Hot Data Sets pages in memory HBM and transient pages in cache HBM

Cited from org paper:
In this proposal, a software procedure pre-processes the application and determines hot pages,then asks the OS to map them to the memory portion of the die-stacked DRAM. The cache portion of the die-stacked DRAM is managed by hardware, caching data allocated in the off-chip memory.

To identify hot pages, we use a static profile-based approach before the execution of an application. A software procedure, incorporated into the compiler, pre-processes the application and sorts the pages based on their access frequency. Then it picks the top pages and asks the OS to map them to the memory portion of the die-stacked DRAM.

After detection of hot pages, their details are coded into the program binary. Whenever the program gets executed, the Loader passes the required information of hot pages to the OS. Then, the OS tries to map such hot pages to physical locations that belong to the memory portion of the die-stacked DRAM. For the OS, allocating pages in the die-stacked and off-chip memory is similar to the same operations in Non-Uniform Memory Architecture (NUMA) [50] systems.

In this paper, **they raised the issue** that when process switches, previous hbm space allocated to a process might left inadequate space for the following process. Other orthogonal research  " Various proposals (e.g., [13, 47, 54, 62, 78]) have suggested to optimize memory management in such situations typically by gradually or periodically migrating application pages between different types of memories based on factors like programming model, application’s criticality, sharing degree, and so on".

They identify the portion of hbm memory(the hot pages that need to be allocated to HBM) for hot pages by trying to allocate the maximum number of pages into hbm memory without worsening the cacheAHF.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/b6fac2f3-c4df-4747-8588-d96b22f317fd)

---
### 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)
#### Me
- Hybrid Memory
- Blk/Page Size 2KB/64KB<br>
  64KB page size is due to the fact that it maps all memory in dram and hbm.<br>
  Every request will cam PRT and BLE. <br>
  In multi core simulation env,it have to support multi core read and write the SRAM.<br>
- Distinguish Spacial Locality and Temporal Locality.<br>
  cacheHBM (cHBM) for temporal locality<br>
  memoryHBM (mHBM) for spacial locality<br>
- Page Allocation.<br>
  Different from previous design that allocate all memory in HBM or DRAM. It allocate page according to its neighbour pages.
  But it does not mention how it interact with page table. If page is deallocate or written back to disk, the PRT should also be updated.
- The ratio between cHBM and mHBM is flexible.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/6a4417fe-70f7-4620-bcb9-014503a7b6d1)

- If memory footprint is high, all used by OS, all the HBM will be served as flat memory.

Cited from org paper:
Program Statistics

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/5937f36c-c347-4be5-be5c-fbf446cde4c4)

In each remapping set, the hotness tracker includes a hot table and five parameters: the HBM occupied ratio (Rh), a hotness threshold (T) to decide if an off-chip DRAM page should be brought in HBM for high Rh condition, the number of cHBM pages (Nc), and the number of mHBM pages in which most blocks have/have not been accessed (Na/Nn).

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/a3206fc5-49f2-412e-8001-9694c4d78047)

For SL>0 (strong spatial locality), more hot data should be brought in mHBM to better exploit the spatial locality and utilize the memory bandwidth. For SL ≤ 0 (weak spatial locality), hot data should be cached in cHBM to reduce over-fetching.

The threshold T in the hotness tracker can alleviate this issue. If Rh is high, for SL>0, only pages whose hotness value is larger than T are permitted to be migrated to mHBM and for SL ≤ 0, only blocks in a page whose hotness value is larger than T are permitted to be cached in
cHBM.
**Me**
From this aspect, SL means that number of mHBM pages that most blocks have been accessed is far larger than not been accessed. This means strong spatial locality.

---
### 5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM
**Insights**
- bandwidth distribution
- dram and hbm similar latency

As the NM simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. We observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.

We leverage our key insight on controlling data movement and propose Bandwidth-Aware Tiered-Memory Management (BATMAN), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the NM and the FM. 
We define the desired access rate of the NM as the target access rate (TAR). TAR is the fraction of memory accesses serviced by the NM when memory accesses to both memories are proportional to the respective bandwidth.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/a307ab63-9871-4e66-bf4c-792a391022ae)

2X 2/3
4X 4/5
8X 8/9

**Me**
Bandwidth-Aware Tired-Memory Management tries to distritube memory according to HBM and DRAM bandwidth ratio. And also treat it as a threshold to refuse page migration.

---
### 6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches
Year:2015

Ideally, we want the bandwidth consumed for such secondary operations to be negligible, and have almost all the bandwidth be available for transfer of useful data from the DRAM cache to the processor.
BEAR integrates three components, one each for reducing the bandwidth consumed by miss detection, miss fill, and writeback probes.

1. Miss Probe (to detect a miss, we need to look up the tag store in the DRAM cache)
2. Miss Fill (on a cache miss the missed line is obtained from memory and filled in the cache)
3. Write back Probe (on a dirty eviction from the on-chip LLC identifying if that line is present in the DRAM cache)
4. Writeback Update (if writeback probe gives a hit, updating the content of the line in DRAM cache)
5. Writeback Fill (filling the writeback data in the cache, if a writeback probe gives a miss)

They define BloatFactor, the ratio of the total bandwidth consumed by the DRAM cache to the bandwidth required for transferring only the data lines to the processor chip.

1. Bandwidth Efficient Cache Fills
   We propose Bandwidth Aware **Bypass** (BAB) to reduce the bandwidth consumed by fill operations while limiting the loss in cache hit rate to a desired level.
   
   ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/f0f82689-5971-49c9-b816-6dd29895d217)

2. Bandwidth Efficient Writeback Probe
   DRAM Cache Presence (DCP), reduces Writeback Probe by introducing state information in the on-chip Last Level Cache (LLC) to track if the line exists in the DRAM cache.
   **Inclusive Cache**
   
   ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/8c440d58-845f-45ef-b464-7bed63f2d72c)

3. Bandwidth Efficient Miss Probe
   We reduce the bandwidth consumed by Miss Probe by leveraging the property of DRAM caches to streams multiple tags on each access. We buffer the tags of recently accessed adjacent cache line's tags in the Neighboring Tag Cache (NTC).
   **Neighboring Tag Cache**
   
   ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/b985c20d-3da0-4e6c-9370-f02f0ab363db)

**Comment from To Update or Not to update**
Along the same lines, Chou et. al [6] propose a policy that bypasses the cache with 90% probability (we call this policy 90%-Bypass).
**Me**
This comment from to update or not to update is not accurate, the bear paper mentioned that "Overall, the speed up from probabilistic bypass is negligible, and we may deem PB to be ineffective at improving performance." Then it prefers set-duleling.

---
### 7.To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches
Year: 2019

**Me**
Previous dram cache is stateless, due to the fact that maintaining state of cache would require significant bandwidth.

**Cite from org paper**
We propose a stateful replacement/bypass policy called RRIP Age-On-Bypass (RRIP-AOB), that tracks reuse state for high-reuse lines, protects such lines by bypassing other lines, and Ages the state On cache Bypass.

**The DRAM cache in KNL [4, 5],for example, employs an Always-Install policy.** The DRAM cache places each tag information in the unused bits in the ECC space and streams out the data and tag (contained in ECC) on each access.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/979e9ae0-0138-4259-9ffc-ede35c44c056)


Our goal is to increase the hit-rate of such DRAM caches. In fact, the DRAM cache only uses about 8-10 bits from the unused 28 bits in the ECC space, so we have 18-20 bits per line available for managing the DRAM cache intelligently.

To reduce significant bandwidth to update state,  we propose Efficient Tracking of Reuse (ETR). ETR makes state tracking efficient by accurately tracking the state of only one line from a region, and using the state of that line to guide the replacement decisions for other lines in that region.

1.  We propose a bypass version of RRIP (RRIP-AOB) suitable for caches with limited associativity. However, we find an effective replacement policy for DRAM caches must optimize not only hit-rate but also state update cost. 
    We introduce two properties, **coresidency and eviction-locality**, that can be exploited to reduce state update cost for implementing intelligent replacement.
    
    ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/a1545088-5b17-4983-bc49-c4b21e7d2cf6)

    Coresidency indicates that at any given time if a line is present, then several other line belonging to that 4KB region are also present in the cache.
    Eviction-Locality indicates that when a line gets evicted from the cache, the replacement-state of the other coresident lines belonging to that region tend to have similar replacement state as the line being evicted. 
    **Me**
    Just a synonym for spacial locality. This granularity is 4KB. Doubt about its authenticity.
2.  We propose Efficient Tracking of Reuse (ETR), a design that performs updates for only a subset of lines and uses their state to guide the replacement decisions of
other lines.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/2033799b-814f-495f-b9ef-93ad9c33b411)

**Me**
This is similar to set dueling.

The design of ETR consists of three parts: 
(1) Selecting a Representative-Line in the region.
(2) Keeping accurate RRPV for only the Representative-Line.
(3) Using the representative’s RRPV to infer coresident lines’ RRPV to make bypass decisions.

---
### 8.ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction
A method to optimize prediction way of dram.

---
### 9. A Survey of Cache Bypassing Techniques
---
### 10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel
---
### 11. Bypass and Insertion Algorithms for Exclusive Last-level Caches

---
### 12. Counter-Based Cache Replacement and Bypassing Algorithms
---
### 13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)

    




