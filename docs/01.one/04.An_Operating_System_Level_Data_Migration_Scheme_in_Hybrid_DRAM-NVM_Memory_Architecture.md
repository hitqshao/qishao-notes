---
title: An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture
date: 2023-05-12 
permalink: /pages/2476ae/
---

1. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures
2. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture
3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method

---
#### 1. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures
**LRU**:
Even though it requires only constant time and space overhead, LRU has a critical weakness in virtual memory environments. On every memory hit, LRU needs to move a page to the most recently used (MRU) position in the list. This involves list manipulations that cannot be handled by the paging unit hardware.
**CLOCK**: 
Specifically, on a hit to a page, the paging unit hardware sets the reference bit of the page to 1 when a read or a write reference for that page occurs, and sets the dirty bit to 1 when a write reference occurs. Then, pages are maintained in a circular list.
In the course of the scan, for every page with reference bit 1, CLOCK clears it to zero, without removing the page from the list.

The reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. Even though CLOCK does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.

LRU maintains the temporal locality. 
Frequency of Write Reference collect statistics of reference cnter.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/bb52315d-df24-4598-84e9-2d7b05db5683)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/3743af17-f2fa-46d2-b527-54aedf9ff09a)

The shape of the curves in these figures can be modeled as a monotonic decreasing function, implying that a more recently referenced page is more likely to be written in the near future.

Specifically, we can observe ranking inversion of temporal locality, i.e., a more recently used page shows a smaller fraction of writes for some ranking ranges.

<img src="https://github.com/hitqshao/qishao-notes/assets/23403286/a3859015-7e6c-4064-a3b8-30091626880f" alt="Your image title" width="600">

In Fig. 3, x axis is the ranking by LRU.
y axis represents the number of write references of the page ranking in x-axis.

In Fig. 4, the x-axis represents the ranking of pages based on their past write counts (black plot) and read/write counts (gray plot).
The y-axis represents the number of writes occurring on that ranking.

The reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. Even though CLOCK does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.

This indicates that frequency based estimations are more accurate compared to temporal locality based estimations for most cases. Specifically, frequency based stimations indicate that a wide range of top ranking pages, that is, pages that have been written to frequently in the past, are likely to be written to again in the future.

In summary, write frequency is generally a better estimator than temporal locality in predicting the re-reference likelihood of write references, but the very recent past write history is also a strong indicator of future writes.

**Me**
In pic4, the axis x is also ranked by number of reference. That's why its x axis can correlates with y axis.
Maybe 80% rule can also explain this.

---
#### 2. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/83bec501-7aa0-4c08-a8dc-5cdb375ef834)

Contrary to CLOCK-DWF that places page faults issued by read requests on NVM, the proposed scheme moves all pages from disk to DRAM area. This is motivated by the fact that moving to either NVM or DRAM will result in a page write in NVM since the DRAM is always full and moving a data page to DRAM will issue an eviction to NVM. Therefore, the cost of moving to NVM or DRAM is the same in terms of writes in NVM. The newly accessed data pages have higher probability of access compared to the older data pages and moving this new page to DRAM will result in increase in DRAM hit ratio instead of NVM hit ratio.

First,it requires an ordering scheme in order to identify data pages that are cold but will be accessed once in a long time. These data pages will reside long enough in NVM to have a high counter values and therefore will be moved to DRAM where they cannot compete with hot data pages and will return to NVM which makes their migration to DRAM without any benefits. Second, there is no difference between pages that are frequently accessed and typically reside near the head of the NVM LRU queue for the entire time and data pages which go back and forth in the queue.

The housekeeping information will be only stored for a few percentage of top positions in the NVM LRU queue. Once a data page moves to the end of this selected percentage of LRU, the corresponding counter will be reset to zero. This will handle both ordering scheme and identifying burst data accesses.

Finding the data page in DRAM will result in a normal LRU housekeeping. Otherwise, the extra housekeeping information in NVM will be updated based on the request type. The read and write counters will be stored for readperc and writeperc top data pages in the NVM, respectively. 
[Still confused why they have readperc and writeperc]

The values of read threshold and write threshold determine how aggressive we plan to prevent the migrations with low probability of being useful.
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/1e29cb59-437f-4499-adf9-c43d1388bc43)

To this end, we use two Least Recently Used (LRU) queues (one for DRAM and one for NVM) and optimize the LRU queue for NVM to prevent nonbeneficial migrations to DRAM.

#### 3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method
Comments on the previous paper:
CLOCK-DWF [19]. CLOCK-DWF first proposes to load the write-request pages into DRAM. For new pages, if it is for a write request, it will be swapped into DRAM. Otherwise, it will be placed in NVRAM. For pages stored in NVRAM, if one is hit by a write request, it will be migrated from NVRAM to DRAM. At this time when DRAM is full, CLOCK-DWF will select a victim page that has the lowest number of writes or that has not been accessed for the longest period of time in DRAM to be evicted.
Double LRU [20]. Double LRU recognizes the high migration cost between NVRAM and DRAM, and tries to restrict the number of page migrations by setting some threshold. It uses two separate LRU linked lists to manage pages in DRAM and NVRAM. For each page in NVRAM, it maintains a read/write request count. When a page is accessed, Double LRU checks its read/write request count, and if the count reaches a certain threshold, it will be migrated from NVRAM to DRAM; otherwise, it will remain in NVRAM. In DRAM, the page at the end of the LRU list is always selected as the victim. For new pages, Double LRU stores them directly in DRAM, assuming new pages will be accessed frequently in the near future. regardless of the read or write requests.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/ce0ed6ce-164f-4d40-b088-0060b4c3de7a)

UIMigrate consists of three parts: unified hot page identification, page migration, and self-adaptive adjustment.

To consider both the number of accesses and access time,we add the attenuation factor to quantify the hotness of each page, hoping to quickly reduce the access counts for pages that are accessed a long time ago. Thus, while updating page hotness upon each access, UIMigrate also uses an attenuation coefficient to lower the page popularity of old accesses.

If all DRAM pages should be accessed in one cycle, (acc_count.global-acc.countpage)/DRAMsize denotes the number of cycles the page that has not been accessed.

the attenuation coefficient d is closely related to the value of hotold and the number of cycles the page that has not been accessed since last time. 

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/81ee2ea8-9abb-43b0-a69f-dccebe4cbc3e)

UIMigrate sets a threshold, called new page threshold, to measure the hotness of each victim page. When the quantified hotness of a selected victim is larger than the preset threshold, it means that this victim page is too hot to be evicted, and so UIMigrate will store the new page in NVRAM. Otherwise, it will be migrated to DRAM.

In order to effectively adapt to the change of access patterns, UIMigrate adjusts migration thresholds (new page threshold, hot page threshold and cold page threshold) automatically to promote or suppress the page migrations, according to real-time migration revenue.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/d31942fa-876d-4eec-a2db-69d0d55ef0cb)

When they are evicted from DRAM, UIMigrate calculates the migration revenue based on Equations (3) and (4). 
If the migration revenue is below zero, it means that the migration cost is greater than the benefit. In this case, UIMigrate will increase hot page threshold to prevent certain pages from getting hot in NVRAM and decrease cold page threshold to prevent some pages from becoming cold in DRAM, thus retaining more pages in NVRAM. For new pages, UIMigrate will also reduce new page threshold, so that more new pages will go to NVRAM instead of DRAM. When the calculated migration benefit is larger than the migration cost, UIMigare will reduce hot page threshold and increase cold page threshold to make migrate more pages to DRAM, and increase new page threshold to keep more new pages in DRAM.

