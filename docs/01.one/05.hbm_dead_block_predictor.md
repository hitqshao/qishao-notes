---
title: hbm dead block predictor
date: 2023-05-15 
permalink: /pages/2476af/
---

1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory
2. Die-Stacked DRAM: Memory, Cache, or MemCache?
3. A Survey Of Techniques for Architecting DRAM Caches
4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)
5. BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM
6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches
7. To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches

---

9. A Survey of Cache Bypassing Techniques
10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel
11. Bypass and Insertion Algorithms for Exclusive Last-level Caches
12. Counter-Based Cache Replacement and Bypassing Algorithms
13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)
---
### 1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory
- Software manage DRAM and NVM
1. First touch policy<br>
   Alloc all pages in DRAM
2. Static profile-based policy
3. Spill Migration<br>
   LRU spill policy keeps track of last access time for each page in DRAM, and in case of eviction selects one that is least recently used.
   Spill migration policy first allocates a page in fast memory (in our case DRAM), and later evicts it to PCM.
   Spill profile-based policy can either spare a page from eviction if its future traffic is high, or victimize it if it is low, regardless of its previous access count.
4. Dynamic page migration

   ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/f32475d8-b0ce-4066-a416-d36d1f44c86f)
   
   When a page is first brought to the PCM we reset its access counter, regardless of how many times it was accessed in the DRAM. At the same time we keep track of   the number of accesses for every page in the DRAM, as well as the average for all the pages (nDRAMavg). When a page in PCM is accessed, we compare its access counter
(naccesses) with the average number of accesses to pages in DRAM.
   Back migration threshold (BMT) is a value that controls the aggressiveness of migration triggering.
   If it is set to zero, a page is migrated as soon as it is touched in PCM, so the DRAM acts as a typical cache. In this case we expect good performance as the system tends to always move active pages to DRAM, but due to a large number of migrations, number of writes to PCM may go high.
   On the other hand, if BMT is set to infinity the page never gets migrated back, and then the policy is equivalent to LRU spill. In between those extremes we would like to search for values that give good performace and low number of PCM writes.

---   
### 2. Die-Stacked DRAM: Memory, Cache, or MemCache?
- Part as Memory ans Part as Cache
- Discuss and compared with Alloy Cache, Unison Cache, Banshee Cache, HMA
- Hot Data Sets pages in memory HBM and transient pages in cache HBM

Cited from org paper:
In this proposal, a software procedure pre-processes the application and determines hot pages,then asks the OS to map them to the memory portion of the die-stacked DRAM. The cache portion of the die-stacked DRAM is managed by hardware, caching data allocated in the off-chip memory.

To identify hot pages, we use a static profile-based approach before the execution of an application. A software procedure, incorporated into the compiler, pre-processes the application and sorts the pages based on their access frequency. Then it picks the top pages and asks the OS to map them to the memory portion of the die-stacked DRAM.

After detection of hot pages, their details are coded into the program binary. Whenever the program gets executed, the Loader passes the required information of hot pages to the OS. Then, the OS tries to map such hot pages to physical locations that belong to the memory portion of the die-stacked DRAM. For the OS, allocating pages in the die-stacked and off-chip memory is similar to the same operations in Non-Uniform Memory Architecture (NUMA) [50] systems.

In this paper, **they raised the issue** that when process switches, previous hbm space allocated to a process might left inadequate space for the following process. Other orthogonal research  " Various proposals (e.g., [13, 47, 54, 62, 78]) have suggested to optimize memory management in such situations typically by gradually or periodically migrating application pages between different types of memories based on factors like programming model, application’s criticality, sharing degree, and so on".

They identify the portion of hbm memory(the hot pages that need to be allocated to HBM) for hot pages by trying to allocate the maximum number of pages into hbm memory without worsening the cacheAHF.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/b6fac2f3-c4df-4747-8588-d96b22f317fd)

---
### 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)
#### Me
- Hybrid Memory
- Blk/Page Size 2KB/64KB<br>
  64KB page size is due to the fact that it maps all memory in dram and hbm.<br>
  Every request will cam PRT and BLE. <br>
  In multi core simulation env,it have to support multi core read and write the SRAM.<br>
- Distinguish Spacial Locality and Temporal Locality.<br>
  cacheHBM (cHBM) for temporal locality<br>
  memoryHBM (mHBM) for spacial locality<br>
- Page Allocation.<br>
  Different from previous design that allocate all memory in HBM or DRAM. It allocate page according to its neighbour pages.
  But it does not mention how it interact with page table. If page is deallocate or written back to disk, the PRT should also be updated.
- The ratio between cHBM and mHBM is flexible.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/6a4417fe-70f7-4620-bcb9-014503a7b6d1)

- If memory footprint is high, all used by OS, all the HBM will be served as flat memory.

Cited from org paper:
Program Statistics

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/5937f36c-c347-4be5-be5c-fbf446cde4c4)

In each remapping set, the hotness tracker includes a hot table and five parameters: the HBM occupied ratio (Rh), a hotness threshold (T) to decide if an off-chip DRAM page should be brought in HBM for high Rh condition, the number of cHBM pages (Nc), and the number of mHBM pages in which most blocks have/have not been accessed (Na/Nn).
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/a3206fc5-49f2-412e-8001-9694c4d78047)
For SL>0 (strong spatial locality), more hot data should be brought in mHBM to better exploit the spatial locality and utilize the memory bandwidth. For SL ≤ 0 (weak spatial locality), hot data should be cached in cHBM to reduce over-fetching.

The threshold T in the hotness tracker can alleviate this issue. If Rh is high, for SL>0, only pages whose hotness value is larger than T are permitted to be migrated to mHBM and for SL ≤ 0, only blocks in a page whose hotness value is larger than T are permitted to be cached in
cHBM.
**Me**
From this aspect, SL means that number of mHBM pages that most blocks have been accessed is far larger than not been accessed. This means strong spatial locality.

---
### 5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM
**Insights**
- bandwidth distribution
- dram and hbm similar latency

As the NM simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. We observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.

We leverage our key insight on controlling data movement and propose Bandwidth-Aware Tiered-Memory Management (BATMAN), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the NM and the FM. 
We define the desired access rate of the NM as the target access rate (TAR). TAR is the fraction of memory accesses serviced by the NM when memory accesses to both memories are proportional to the respective bandwidth.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/a307ab63-9871-4e66-bf4c-792a391022ae)

2X 2/3
4X 4/5
8X 8/9

**Me**
Bandwidth-Aware Tired-Memory Management tries to distritube memory according to HBM and DRAM bandwidth ratio. And also treat it as a threshold to refuse page migration.



