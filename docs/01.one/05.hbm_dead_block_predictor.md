---
title: hbm dead block predictor
date: 2023-05-15 
permalink: /pages/2476af/
---

1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory
2. Die-Stacked DRAM: Memory, Cache, or MemCache?
3. A Survey Of Techniques for Architecting DRAM Caches
4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)
5. BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM
6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches
7. To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches

---

9. A Survey of Cache Bypassing Techniques
10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel
11. Bypass and Insertion Algorithms for Exclusive Last-level Caches
12. Counter-Based Cache Replacement and Bypassing Algorithms
13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)
---
### 1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory
- Software manage DRAM and NVM
1. First touch policy
  Alloc all pages in DRAM
2. Static profile-based policy
3. Spill Migration
   LRU spill policy keeps track of last access time for each page in DRAM, and in case of eviction selects one that is least recently used.
   Spill migration policy first allocates a page in fast memory (in our case DRAM), and later evicts it to PCM.
   Spill profile-based policy can either spare a page from eviction if its future traffic is high, or victimize it if it is low, regardless of its previous access count.
4. Dynamic page migration
   ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/f32475d8-b0ce-4066-a416-d36d1f44c86f)
   When a page is first brought to the PCM we reset its access counter, regardless of how many times it was accessed in the DRAM. At the same time we keep track of   the number of accesses for every page in the DRAM, as well as the average for all the pages (nDRAMavg). When a page in PCM is accessed, we compare its access counter
(naccesses) with the average number of accesses to pages in DRAM.
   Back migration threshold (BMT) is a value that controls the aggressiveness of migration triggering.
   If it is set to zero, a page is migrated as soon as it is touched in PCM, so the DRAM acts as a typical cache. In this case we expect good performance as the system tends to always move active pages to DRAM, but due to a large number of migrations, number of writes to PCM may go high.
   On the other hand, if BMT is set to infinity the page never gets migrated back, and then the policy is equivalent to LRU spill. In between those extremes we would like to search for values that give good performace and low number of PCM writes.

---
### 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)
#### Me
- Hybrid Memory
- Blk/Page Size 2KB/64KB
  64KB page size is due to the fact that it maps all memory in dram and hbm.
  Every request will cam PRT and BLE. 
  In multi core simulation env,it have to support multi core read and write the SRAM.
- Distinguish Spacial Locality and Temporal Locality.
  cacheHBM (cHBM) for temporal locality
  memoryHBM (mHBM) for spacial locality
- Page Allocation.
  Different from previous design that allocate all memory in HBM or DRAM. It allocate page according to its neighbour pages.
  But it does not mention how it interact with page table. If page is deallocate or written back to disk, the PRT should also be updated.
- The ratio between cHBM and mHBM is flexible.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/6a4417fe-70f7-4620-bcb9-014503a7b6d1)

- If memory footprint is high, all used by OS, all the HBM will be served as flat memory.

Cited from org paper:
Program Statistics

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/5937f36c-c347-4be5-be5c-fbf446cde4c4)

In each remapping set, the hotness tracker includes a hot table and five parameters: the HBM occupied ratio (Rh), a hotness threshold (T) to decide if an off-chip DRAM page should be brought in HBM for high Rh condition, the number of cHBM pages (Nc), and the number of mHBM pages in which most blocks have/have not been accessed (Na/Nn).
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/a3206fc5-49f2-412e-8001-9694c4d78047)
For SL>0 (strong spatial locality), more hot data should be brought in mHBM to better exploit the spatial locality and utilize the memory bandwidth. For SL ≤ 0 (weak spatial locality), hot data should be cached in cHBM to reduce over-fetching.

The threshold T in the hotness tracker can alleviate this issue. If Rh is high, for SL>0, only pages whose hotness value is larger than T are permitted to be migrated to mHBM and for SL ≤ 0, only blocks in a page whose hotness value is larger than T are permitted to be cached in
cHBM.
#### Me
From this aspect, SL means that number of mHBM pages that most blocks have been accessed is far larger than not been accessed. This means strong spatial locality.




