---
title: HBM Paper List
date: 2023-05-08 
permalink: /pages/24769e/
---

1. Baryon: Efficient Hybrid Memory Management with Compression and Sub-Blocking
2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache
3. Hybrid2: Combining Caching and Migration in Hybrid Memory Systems
4. SILC-FM: Subblocked InterLeaved Cache-Like Flat Memory Organization
5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories
6. Transparent Hardware Management of Stacked DRAM as Part of Memory
7. CHAMELEON: A Dynamically Reconfigurable Heterogeneous Memory System
8. BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM
9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories
10. Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems
11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation
12. Die-Stacked DRAM: Memory, Cache, or MemCache?
13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache
14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors
15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems

DRAM NVM
1. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture
2. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures
3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method
4. Page Placement in Hybrid Memory Systems

---
#### 2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache
##### MemPod:

CAMEO [13] proposes a cache-like flat address space memory management scheme in an attempt to close the gap between cache and flat memory organizations. CAMEO operates similarly to THM, however it does so at the granularity of cache lines (64B). Migrations are restricted within segments with one fast line location per segment. Its bookkeeping structures are entirely stored in memory, while a “Line Location Predictor” attempts to save some bookkeeping-related accesses by predicting the location of a line. 

CAMEO initiates a line migration upon every access to slow memory.

CAMEO can incur high migration traffic as every access could induce a migration.

---
#### 5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories

MemPod uses MEA counters to track page access activity and identify hot pages. They are dramatically smaller than prior tracking mechanisms while capturing activity counts and temporal recency in a way that provides more effective prediction of future page access.

What makes MEA most useful, though, is its failure mode – when it fails to find the most-accessed pages, it does so by favoring recency over quantity. That is, a page accessed several times near the end of an interval can easily knock out a page accessed many more times early in the interval. As a result, it combines both access counting and temporal locality, at a fraction of the cost of access counting alone.

![image](https://user-images.githubusercontent.com/23403286/237034027-921f15ba-0cf5-4a55-9221-deffea837202.png)

Three triggers are most commonly used whenever state must be updated based on tracking information (MC scheduling, migrations, dynamic voltage and frequency scaling etc.). Interval-based (or epoch-based) triggers occur with a set frequency, while threshold-based solutions trigger whenever a predetermined criterion is met. Finally, event-based triggers react to predefined events. Both interval-based and threshold-based approaches face the same challenge of identifying the optimal interval or threshold value.

MemPod achieves the best performance (lower AMMAT) with 50us intervals and 64 counters per Pod. MemPod’s lightweight operation allows for such small intervals. For comparison purposes, HMA [14] identi-
fied the best epoch length to be 100ms (2000x larger) in order to support all the lengthy processes that take place during a migration event for that method.

Based on these results, we use 64 MEA 2-bit counters over 50us intervals for subsequent results in this paper. Each one of the 64 MEA entries needs 21 bits for addressing the 1.1M pages per Pod and 2 bits for its counter, leading to an area cost of only 184B per Pod and 736B total. Compared to the state of the art, MemPod’s activity tracking requirement is ∼712x smaller than THM’s (512KB) and ∼12800x smaller than HMA’s (9MB).

![image](https://user-images.githubusercontent.com/23403286/237031049-f4cbb08e-5ee3-4ab5-9ffd-d4abc43306db.png)

---
#### 6. Transparent Hardware Management of Stacked DRAM as Part of Memory
##### MemPod:
Sim, et al. proposed a technique for transparent hardware management of a hybrid memory system [17],which we will refer to as “THM”. THM does not require OS intervention while managing migrations. In order to keep bookkeeping costs manageable, THM allows migrations only within sets of pages (called segments). Each segment includes one fast memory page and a set of slow memory pages. The slow pages of each segment can only migrate to the one fast page location, and any such migration results in the eviction of the currently-residing page. THM monitors memory accesses with one “competing counter” per segment resulting in a low cost profiling solution. Finally, THM supports caching part of its structures on chip while the rest is stored in memory.

THM’s competing counters can lead to false positives, allowing a cold page to migrate to fast memory.

THM offers significantly limited flexibility by restricting migrations withing segments, however this decision reduces bookkeeping costs significantly. Competing counters in each segment are used for activity tracking, occasionally leading to false (threshold-based) migration triggering if a cold page gets accessed at the right time. Identifying migration candidates incurs very little overhead since there is exactly one fast memory location for each slow memory page that triggers migration.

---
#### 9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories
##### MemPod:
HMA [14] is a HW/SW mechanism that attempts to predict frequently accessed pages in memory and, at predefined intervals, migrate those pages to fast memory. HW support is required for profiling memory accesses using counters for each memory page, while the migration is handled by the OS. Due to the costly OS involvement, HMA’s intervals are kept large. Additionally, the hardware cost of its profiling counters is high. However, HMA is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages as the OS can update page tables and TLBs to reflect migrations.

HMA does not require a remap table due to the OS updating the existing system’s structures. For activity tracking it uses Full Counters. The costly OS involvement and the high penalty for sorting all its
counters force HMA to operate at very large intervals, weakening its adaptability to phase changes. However, HMA offers full flexibility for migrations.

---
#### 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache

The state-of-the-art block-based design, called Alloy Cache, colocates a tag with each data block (e.g., 64B) in the stacked DRAM to provide fast access to data in a single DRAM access. However, such a design suffers from low hit rates due to poor temporal locality in the DRAM cache. In contrast, the state-of-the-art page-based design, called Footprint Cache, organizes the DRAM cache at page granularity (e.g., 4KB), but fetches only the blocks that will likely be touched within a page. In doing so, the Footprint Cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. However, multi-gigabyte stacked DRAM caches will soon be practical and needed by server applications, thereby mandating tens of MBs of tag storage even for page-based DRAM caches.

We introduce a novel stacked-DRAM cache design, Unison Cache. Similar to Alloy Cache’s approach, Unison Cache incorporates the tag metadata directly into the stacked DRAM to enable scalability to arbitrary stacked-DRAM capacities. Then, leveraging the insights from the Footprint Cache design, Unison Cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. Our evaluation using server workloads and caches of up to 8GB reveals that Unison cache improves performance by 14% compared to Alloy Cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical SRAM-based tags of around 50MB.

#### Me
Block-based high miss ratio, page-based high migration penalty when fetching useless data.
If footprint meta data is adopted to collect data trace, FootCache meta table cannot scale with increasing capactiy of HBM.

![image](https://user-images.githubusercontent.com/23403286/237041324-e1d78ea0-4ff7-4b2c-8870-03b5b3cb9c4a.png)

---
#### 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors

[Link to notes for Dynamically Adapting...](https://hitqshao.github.io/qishao-notes/pages/24769f/ "Link to notes")

#### 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems

“on-the-fly” migration performs better than epoch-based page migration techniques, since we migrate recent hot pages.

Instead of relying completely on OS to perform AR for the evicted entries, as done in [Ramoset al. 2011], we propose a hardware-based AR, where the MigC hardware initiates TLB shootdown and cache flushing without explicitly stopping the user program.

**Like previous studies [Meswani et al. 2015; Prodromou et al. 2017; Su et al. 2015], we observe that not all applications benefit from page migration, since page migration incurs performance overheads due to extra data movement.**

The model works with the principle that, to get performance benefit, one should migrate the smallest set of pages from slow to fast memory that yields in the largest increase in memory accesses to fast memory to amortize the migration overhead.

**Me**
Try to use 80% principle.
In our study, we look at the 80-percentile accesses and identify the “set of top-accessed pages” that contribute to more than 80% of all memory accesses.

In our proposal, we migrate a page immediately when it receives sufficient number of memory accesses, unlike any epoch-based schemes. We allow full flexibility in page relocation like HMAHS [Meswani et al. 2015] and keep a remap table for address redirection. We keep this table small by periodically evicting entries and it is placed on-chip.

The particular access count, which can separate such top-accessed pages from other pages, is referred to as “filter count.” Note that, filter count indicates an upper bound for hotness threshold

**Me**
This paper explain the address reconciliation AR in detail.

**Flow of AR**
-First, all cache lines from these pages, which are currently residing in the cache hierarchies and tagged with OS-visible PA, must be invalidated (and dirty lines written back), since the current OS-visible PA will be replaced with the new PA. All future accesses to these pages will only have access to the new PA.
-Next,corresponding page table entries (PTEs) for A and B need to be updated with new PAs. 
-The TLB entries in all cores using the old PA must also be invalidated (as well as any other OS structures that contain the physical page addresses).

(i) flush_cache_page()
(ii) change PTE,
(iii) flush_tlb_page()

We use a **Migration Benefit Quotient (MBQ)**, by calculating the difference between the total number of accesses to any page and the filter count used in classifying applications’ memory locality, as described. 
MBQ indicates how many of the future accesses of a page may go to fast memory if the page were migrated from slow to fast memory using the filter count as a hotness
threshold.

**Sacturation account**
the sum of access counts of all pages with 32,909 accesses or less (the bars corresponding to x-axis 0 to 3,290) accounts for 98% of all accesses. For our purposes, we use this access count as a saturation count (or assume that the maximum number of accesses any page can receive). 

**For each workload, we use the difference between the saturation count and the filter count to determine MBQ.**

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/e942285a-16d6-4b39-9b75-662513ac34f8)

1. Low MBQ, difference is less than 1K. For example, in Figure 5(b) xalanc, pages with memory access count 609 or less (the bars corresponding to x-axis 0 to 60) provide 98% of the memory accesses. Hence, the difference between saturation count (609) and filter count (70) is 539. These workloads may not achieve significant increase in accesses to fast memory after page migration.
2. Medium MBQ, difference is in between 1K to K (e.g., Figure 5(c) omnetpp). These workloads may receive moderate benefits, depending the migration overheads.
3. High MBQ, difference is more than 8K (e.g., Figure 5(a) mcf). These workloads are likely to receive higher hits in faster memory as a result of page migration.

