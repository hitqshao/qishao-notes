---
title: cache timing
date: 2024-07-16 15:32:49
permalink: /pages/cc7039/
---

Non-Uniform Cache Architecture (NUCA). The idea is to split the cache into a large number of banks and employ an on-chip network for communication. Now the access time of the cache is a function of distance between the bank and
cache controller rather than the latency of the slowest subbank.

* The access latency of a cache depends on delays in the decoders, word-lines, bit-lines, and drivers.
* Decoder and driver delay components of a cache go up with increase in the number of sub-banks.
* On the other hand, word line or bit line delay components reduce with decrease in subbank size in the vertical and horizontal directions, respectively.
* large caches in future processors will have an additional overhead of network delay. Due to the growing disparity between wire and transistor delay, this factor will continue to dominate with technology improvements. T


[[1]](#1) 
![image](https://github.com/user-attachments/assets/c2eec81c-7cde-4818-9843-780584f10eca)


CACTI divides the total access time of the cache into seven main components (decoder, wordline, bitline, senseamp, comparator, multiplexer-driver, and output driver). 

##### senseamp and comparator delay
Of these,  **senseamp and comparator delay** is almost constant across different cache sizes and its contribution to the total access time reduces with increase in cache size.


##### multiplexer
The mux-driver delay component consists of delay due to **multiplexer logic (to select the appropriate line) and driver delay** to route the control signal to the output driver.

**The latter part is proportional to the size of the cache.**

##### Decoder
The decoder part of the cache consists of a single centralized pre-decoder and a separate decoder for each subarray.

The output from the pre-decoder is fed to the final decoder stage to drive the appropriate wordline.

**Thus, the decoder delay component is the sum of time to route address bits to the central predecoder, time to route the output of the pre-decoder to the finalstage decoder, and the logic delay associated with predecoder, decoder, and driver circuits. Thus, decoder delay
depends on both cache size and subarray count.**

#### wordline bit delay 
The wordline delay of data/tag array is proportional to the length of the array and the bit line delay is proportional to the height of the array.

These two delay values can be tweaked by adjusting the aspect ratio of the sub-array.

To bring down the delay values of both these components the cache is split into a number of sub-arrays.

**But, with an increased number of sub-arrays, the latency to send signals to and from the central pre-decoder increases.**

**Thus, there exists a trade-off between the sub-array size and the wire length.**


### Analysis
![image](https://github.com/user-attachments/assets/080bd0ea-e9d3-4859-9746-3ee6e76e3bd2)

bank access latency increases exponentially with a decrease in bank count value. This is because as the size of the cache increases, the decoder delay component (that includes wiring delay) increases significantly. Also, the
bank access time saturates beyond a bank count value of 512 (bank size of 64KB).

Beyond this point, the latency is primarily due to logic delay associated with each stage, which is constant across different cache sizes.

The average (uncontended) network latency plotted in the graph is obtained by calculating the access time to each indvidual bank and averaging them against the total bank
count value. **This value depends on both the bank size and total number of banks.**


> Please notice, in the following statement, hop latency means the hop latency for each hop. There is another variable hop counts.
> The network latency is determined by hop latency * hop count.

It can be observed that the average latency first goes down with an increase in bank count and then starts increasing for large bank count values.
If the bank size is extremely large, **the hop latency dominates the total access time** and hence the network latency is very high.

Ideally, the network latency should go down with an increase in bank count. But, dividing the bank into half only reduces the area of data and tag arrays.

Other constant area overheads associated with each bank will remain the same and **hence the reduction in hop latency is less than half its original value**.

For very large bank count values, **the reduction in hop latency is usually less than the increase in hop count to reach a destination**, leading to high average network latencies. The only exception is a
change in bank count value from 1024 to 2048 â€“ because hop latencies are rounded up to the next integer value, a doubling in bank count results in halving the vertical hop
latency.

Thus, finding the optimal bank count value is critical to achieving the least possible access latency.

### An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches [[2]](#2) 
**Citation over 1000.**

Data residing in the part of a large cache close to the processor could be accessed much faster than data that reside physically farther from the processor. For example, the closest bank in a 16-megabyte, on-chip L2 cache built in a 50-nanometer process technology could be accessed in 4 cycles, while an access to the farthest bank might take 47 cycles. 

![image](https://github.com/user-attachments/assets/b8bc95d0-41f3-4463-a4e1-c90477ac3507)

![image](https://github.com/user-attachments/assets/d157df40-ac2a-4de8-8dc3-b1ec73601655)

![image](https://github.com/user-attachments/assets/d74e3a9c-cc1d-4fb6-9aa2-7bfead0cb6c5)

![image](https://github.com/user-attachments/assets/d7350e84-faf7-4c48-8583-c22d873b4dc8)


* Simple mapping: each column of banks in the cache becomes a bank set, and all banks within that column comprise the set-associative ways.
Thus, the cache may be searched for a line by first selecting the bank column, selecting the set within the column, and finally performing a tag match on banks within that column of the cache.
The two drawbacks of this scheme are that
  - the number of rows may not correspond to the number of desired ways in each bank set
  - latencies to access all bank sets are not the same; some bank sets will be faster than others, since some rows are closer to the cache controller than others.

* fair mapping policy, which addresses both problems of the simple mapping policy at the cost of additional complexity.
* shared mapping policy, attempts to provide fastest-bank access to all bank sets by sharing the closest banks among multiple bank sets.

The policies we explore for D-NUCA consist of four major components:
(1) Mapping: simple or shared.

(2) Search: multicast, incremental, or combination. We restrict the combined policies such that a block set is partitioned into just two groups, which may then each vary in size (number of blocks) and the method of access (incremental or multicast).

(3) Promotion: described by promotion distance, measured in cache banks, and promotion trigger, measured in number of hits to a bank before a promotion occurs.

(4) Insertion: identifies the location to place an incoming block and what to do with the block it replaces (zero copy or one copy policies).

### References
<a id="1">[1]</a> 
The Effect of Interconnect Design on the Performance of Large L2 Caches

<a id="2">[2]</a>
An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches
