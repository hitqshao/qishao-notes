---
title: Learn TPU_MLIR (1)
date: 2024-12-09
permalink: /pages/000010/
---

<details>
<summary>Code</summary>
       
```
```
</details>



## Lowering
### 1. Declaration of oprator and conversion pass


<details>
<summary>Code</summary>
       
```
       
// include/tpu_mlir/Dialect/Tpu/IR/TpuOps.td
class Tpu_ConvOp<string mnemonic, list<Trait> traits = []> : Tpu_Op<mnemonic,
    !listconcat(traits, [SupportFuseRelu,
    DeclareOpInterfaceMethods<TypeInterface>,
    DeclareOpInterfaceMethods<LocalGenInterface, ["BackwardH"]>])> {
  let summary = "convolution operator";

  let description = [{
  }];

  let arguments = (ins
    AnyTensor:$input,
    AnyTensor:$filter,
    AnyTensorOrNone:$bias,
    I64ArrayAttr:$kernel_shape,
    I64ArrayAttr:$strides,
    I64ArrayAttr:$pads, // top,left,bottom,right
    DefaultValuedAttr<I64Attr, "1">:$group,
    OptionalAttr<I64ArrayAttr>:$dilations,
    OptionalAttr<I64ArrayAttr>:$inserts,
    DefaultValuedAttr<BoolAttr, "false">:$do_relu,
    DefaultValuedAttr<F64Attr, "-1.0">:$relu_limit,
    //new param
    BoolAttr:$with_bias,
    DefaultValuedAttr<BoolAttr, "false">:$coeff_merged,
    DefaultValuedAttr<I64Attr, "0">:$use_3ic_optimize,
    DefaultValuedAttr<I64Attr, "0">:$kernel_zp,
    OptionalAttr<I64ArrayAttr>:$multiplier,
    OptionalAttr<I64ArrayAttr>:$rshift,
    DefaultValuedAttr<Tpu_RequantModeAttr, "tpu::RequantMode::Normal">:$quant_mode,
    OptionalAttr<Tpu_LayerGroupAttr>:$ginfo
  );

  let results = (outs AnyTensor:$output);
  let extraClassDeclaration = [{
    conv_attr_t parseParam();
  }];
}


```
</details>


<details>
<summary>Code</summary>
```
// include/tpu_mlir/Conversion/Passes.td
def ConvertTopToTpu : Pass<"convert-top-to-tpu", "ModuleOp"> {
  let summary = "Convert top-level Top Ops to Tpu Ops";
  let constructor = "tpu_mlir::createConvertTopToTpu()";
  let dependentDialects = ["tpu_mlir::top::TopDialect", "tpu_mlir::tpu::TpuDialect"];
  let options = [
    Option<"mode", "mode", "std::string", /*default=*/"",
           "default quantization mode: INT8/BF16/F32">,
    Option<"qtable", "qtable", "std::string", /*default=*/"",
           "a table of Ops that quantized to specific mode">,
    Option<"chip", "chip", "std::string", /*default=*/"",
           "chip: cv183x/cv182x/bm1684/bm1684x">,
    Option<"isAsymmetric", "asymmetric", "bool", /*default=*/"false",
           "true for asymmetric quantization, or false for symmetric">,
  ];
}
```
</details>

### 2. Implementation of lowering

TopLowering is derived from mlir::OpRewritePattern

Each OPLowering will be declared.

include/tpu_mlir/Conversion/TopToTpu

<details>
<summary>Code</summary>
       
```
// TopLowering.h
template <typename OpTy> class TopLowering : public OpRewritePattern<OpTy> {
public:
  using OpRewritePattern<OpTy>::OpRewritePattern;

  LogicalResult matchAndRewrite(OpTy opTy,
                                PatternRewriter &rewriter) const override {
...
  virtual void LoweringINT8(PatternRewriter &rewriter, OpTy opTy,
                            bool asymmetric) const {
    llvm_unreachable("Not Implemented");
  }
  virtual void LoweringINT4(PatternRewriter &rewriter, OpTy opTy,
                            bool asymmetric) const {
    LoweringINT8(rewriter, opTy, asymmetric);
  }
  virtual void LoweringBF16(PatternRewriter &rewriter, OpTy opTy) const {
    llvm_unreachable("Not Implemented");
  }
  virtual void LoweringF16(PatternRewriter &rewriter, OpTy opTy) const {
    llvm_unreachable("Not Implemented");
  }
  virtual void LoweringF32(PatternRewriter &rewriter, OpTy opTy) const {
    llvm_unreachable("Not Implemented");
  }
  virtual void LoweringQuantized(PatternRewriter &rewriter, OpTy opTy) const {
    llvm_unreachable("Not Implemented");
  }
                                
}

// LoweringBM1684X.h
#define LOWERING_BM1684X(OP)                                                   \
  struct OP##Lowering : public TopLowering<top::OP##Op> {                      \
    OP##Lowering(MLIRContext *ctx) : TopLowering<top::OP##Op>(ctx) {}          \
    void LoweringINT8(PatternRewriter &rewriter, top::OP##Op op,               \
                      bool asymmetric) const override;                         \
    void LoweringINT4(PatternRewriter &rewriter, top::OP##Op op,               \
                      bool asymmetric) const override;                         \
    void LoweringBF16(PatternRewriter &rewriter,                               \
                      top::OP##Op op) const override;                          \
    void LoweringF16(PatternRewriter &rewriter,                                \
                     top::OP##Op op) const override;                           \
    void LoweringF32(PatternRewriter &rewriter,                                \
                     top::OP##Op op) const override;                           \
    void LoweringQuantized(PatternRewriter &rewriter,                          \
                           top::OP##Op op) const override;                     \
  };

LOWERING_BM1684X(Abs)
LOWERING_BM1684X(Add)
LOWERING_BM1684X(AddConst)
LOWERING_BM1684X(AvgPool)
LOWERING_BM1684X(Cast)
LOWERING_BM1684X(Concat)
LOWERING_BM1684X(Conv)
LOWERING_BM1684X(Deconv)
LOWERING_BM1684X(Depth2Space)
LOWERING_BM1684X(Div)
LOWERING_BM1684X(Exp)
LOWERING_BM1684X(Gather)
LOWERING_BM1684X(GRU)

...

```
</details>



Then in ./lib/Conversion/TopToTpu/BM1684X/
each operator will include a *.cpp that implement LoweringFunction.
![image](https://github.com/user-attachments/assets/40cfb7b3-84e3-453f-8907-ee3237a1aeca)

<details>
<summary>Code</summary>
       
```
void ConvLowering::LoweringF32(PatternRewriter &rewriter,
                               top::ConvOp op) const {
  rewriter.setInsertionPointAfter(op);
  std::vector<Value> operands;
  const int nInputs = op->getNumOperands();
  for (auto i = 0; i < nInputs; ++i) {
    operands.push_back(op->getOperand(i));
  }
  std::vector<NamedAttribute> attrs;
  for (auto &attr : op->getAttrs()) {
    attrs.push_back(attr);
  }
  bool with_bias = !op.getBias().getType().isa<mlir::NoneType>();
  attrs.push_back(
      rewriter.getNamedAttr("with_bias", rewriter.getBoolAttr(with_bias)));
  auto newValue =
      CreateConvOp(rewriter, op.getKernelShape().size(), op->getLoc(),
                   op.getOutput().getType(), operands, attrs);
  rewriter.replaceOp(op, {newValue});
}
```
</details>


### 3. Lowering in ConvertTopToTPU

**bm1684x::populateTopToTpuConversionPatterns(&patterns);**

**Reuse applyPatternsAndFoldGreedily**


lib/Conversion/TopToTPU/TopToTPUPass.cpp

<details>
<summary>Code</summary>
       
```
struct ConvertTopToTpu : public ::impl::ConvertTopToTpuBase<ConvertTopToTpu> {
public:
  void runOnOperation() override {
    RewritePatternSet patterns(ctx_);
    ConversionTarget target(*ctx_);
    target.addLegalDialect<tpu::TpuDialect, func::FuncDialect>();
    // no need to lowering:
    // Qi Note, add legal op, only NoneOp, InputOp, weightOp
    target.addLegalOp<top::InputOp, top::WeightOp, top::NoneOp>();
    if (module::isBM1684XFamily()) {
      // Qi Note: Call LoweringBM1684X.cpp
      // AddLowering to the patterns
      bm1684x::populateTopToTpuConversionPatterns(&patterns);
    } else if (module::isBM1684Family()) {
      bm1684::populateTopToTpuConversionPatterns(&patterns);
    } else if (module::isCV18xx()) {
      cv18xx::populateTopToTpuConversionPatterns(&patterns);
    } else {
      llvm_unreachable("Not Implemented");
    }
    auto config = GreedyRewriteConfig();
    config.maxIterations = 0; // apply each pattern only once.
    // Qi Node: Apply pattern into module
    // applyPatternsAndFoldGreedily is provided by MLIR
    applyPatternsAndFoldGreedily(module_, std::move(patterns), config);
    patterns.clear();
    // 
    patterns.add<ForwardTypePattern<tpu::ReshapeOp>>(ctx_);
    applyPatternsAndFoldGreedily(module_, std::move(patterns));
    cast_process();
    relu_process();
    module::updateModuleTypes();
    module::setState(module::State::TPU_LOWERED);
  }
}

// Qi Node: this function will be called by
// include/tpu_mlir/Conversion/Passes.td constructor
// it will create the pass of converting top operator to tpu operator
std::unique_ptr<Pass> createConvertTopToTpu() {
  return std::make_unique<ConvertTopToTpu>();
}

```
</details>



<details>
<summary>Code</summary>
       
```
void populateTopToTpuConversionPatterns(RewritePatternSet *patterns) {
  patterns->add<
      // clang-format off
      AbsLowering,
      AddLowering,
      AddConstLowering,
      AvgPoolLowering,
      CastLowering,
      ConcatLowering,
      ConvLowering,
      DeconvLowering,
      Depth2SpaceLowering,
      DivLowering,
      ExpLowering,
      GatherLowering,
...
```
</details>
This process is similar to LLVM pass.


