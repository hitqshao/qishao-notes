---
title: Learn TVM (1)
date: 2024-12-08
permalink: /pages/000009/
---

---
## 1. Model Parsing and Relay IR Construction

In TVM, high-level optimization in the Relay IR phase includes several graph-level optimizations, data layout transformations, and other functional passes.
These optimizations are implemented in various source files under src/relay/transform and src/relay/op directories.
Below is a categorized list of these optimizations along with their corresponding source code files and functions:

### 1.1 Graph-Level Optimizations

Graph-level optimizations restructure or simplify the computation graph for better performance.

|Optimization	Source |File	| Key Functions/Classes|
|---|---|---|
|Constant Folding|	src/relay/transform/fold_constant.cc	|FoldConstant, ConstantFolder
|Operator Fusion|	src/relay/transform/fuse_ops.cc	|FuseOps, FuseMutator, PatternMatcher
|Dead Code Elimination (DCE)	|src/relay/transform/eliminate_common_subexpr.cc|	EliminateCommonSubexpr
|Common Subexpression Elimination	| src/relay/transform/eliminate_common_subexpr.cc	|EliminateCommonSubexpr
|Simplify Inference	|src/relay/transform/simplify_inference.cc	|SimplifyInference, SimplifyInferenceMutator
|Call Folding	|src/relay/transform/fold_call.cc	|FoldCall
|Inline Functions	|src/relay/transform/inline.cc	|Inline, InlineMutator
|Prune Unused Functions|	src/relay/transform/prune_unused_functions.cc	|PruneUnusedFunctions

### 1.2 Data Layout Transformations
These optimizations adjust the layout of tensors for better memory access patterns and compatibility with target hardware.
|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Alter Layout|src/relay/transform/alter_op_layout.cc|AlterOpLayout, AlterOpLayoutRewriter|
|Convert Layout|s	src/relay/transform/convert_layout.cc|ConvertLayout|
|Fold Scale Axis|src/relay/transform/fold_scale_axis.cc|FoldScaleAxis, ScaleAxisSimplifier|
|Layout Optimization|src/relay/transform/layout_rewrite.cc|LayoutRewrite|

### 1.3 Quantization and Precision Management
TVM supports quantization optimizations for reduced precision operations.

|Optimization |File	| Key Functions/Classes|
|---|---|---|
|Quantize|src/relay/quantize/quantize.cc|Quantize, CreateQuantizePass|
|Dequantize|src/relay/quantize/dequantize.cc|Dequantize|
|SimplifyQuantize|src/relay/transform/simplify_quantize.cc|SimplifyQuantize, SimplifyQuantizeRewriter|

### 1.4 Automatic Differentiation
TVM includes an autodiff system for neural networks.

|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Reverse Mode Autodiff|src/relay/transforms/gradient.cc	|AutomaticDifferentiation, ReverseAD|

### 1.5 High-Level Hardware-Aware Optimizations
These optimizations modify operations based on the target hardware.

|Optimization |File	| Key Functions/Classes|
|---|---|---|
|Annotate Target|src/relay/transform/annotate_target.cc|AnnotateTarget|
|Partition Graph|src/relay/transform/partition_graph.cc|PartitionGraph|
|Merge Compiler Regions|src/relay/transform/merge_compiler_regions.cc|MergeCompilerRegions|

### 1.6 Device Placement
These passes assign operations to devices for heterogeneous execution.
|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Rewrite Annotated Ops|src/relay/transform/rewrite_annotated_ops.cc|	RewriteAnnotatedOps|
|Device Annotation	|src/relay/transform/device_annotation.cc	|DeviceAnnotation|

### 1.7 Meta-Pass Management
Relay provides a meta-pass system to manage and sequence passes.

|Meta-Pass |File	| Key Functions/Classes|
|---|---|---|
|Sequential Pass Manager	|src/relay/transform/sequential.cc|	Sequential, PassManager
|Pass Context	|src/relay/transform/pass.cc|	PassContext, WithPassContext

---

## 2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR

The lowering process from Relay IR to Tensor Expression (TE) and Tensor IR (TIR) in TVM involves multiple phases.
These include converting Relay IR to TE, applying tensor computation abstractions, performing scheduling, and constructing device-specific low-level TIR.
Hereâ€™s a detailed breakdown of the corresponding TVM source code files and functions for these stages:

### 2.1 Converting Relay IR to Tensor Expression (TE)
This phase converts high-level Relay IR into the computation abstractions provided by TE.

|Process |File	| Key Functions/Classes|
|---|---|---|
|Relay to TE Lowering|	src/relay/backend/te_compiler.cc|	LowerToTE, CreateSchedule, ScheduleGetter|
|Operator Strategy|	src/relay/op/strategy/generic.cc|	GenericFunc, OpStrategy|
|Relay to TE Bridge|	src/relay/backend/te_compiler_cache.cc|	TECompiler, LowerTE|
|Shape Function Lowering|	src/relay/backend/te_compiler.cc|	LowerShapeFunc|

Explanation:
- The Relay IR graph is analyzed, and for each operator, TVM retrieves a corresponding TE function using OpStrategy.
- TE functions define high-level operations like matrix multiplication, element-wise addition, etc.

### 2.2 Abstraction of Computation in Tensor Expression (TE)
TE provides a declarative way to express computation. This includes operations like tiling, unrolling, and vectorizing.
|Process |File	| Key Functions/Classes|
|---|---|---|
|Tensor Expression Build|	src/te/operation/create_primfunc.cc|	CreatePrimFunc, ComputeBody, ScheduleOps|
|Compute Definition|	src/te/operation/compute_op.cc|	ComputeOpNode, ComputeOp|
|Tensor Compute Intrinsics|	src/te/operation/tensorize.cc|	Tensorize, CreateIntrinBody|

Explanation:
- High-level computations are abstracted into a declarative format using ComputeOp.
- Intrinsic support for tensorization is added for specialized hardware operations.

### 2.3 Scheduling in Tensor Expression
Scheduling is where TVM optimizes how computations are performed on the target device.
|Process |File	| Key Functions/Classes|
|---|---|---|
|Tile, Unroll, Vectorize	|src/te/schedule/schedule_dataflow_rewrite.cc	|ScheduleDataFlowRewrite, Tile, Unroll, Vectorize|
|Thread and Block Mapping	|src/te/schedule/schedule_lang.cc	|bind, split, reorder, fuse|
|AutoScheduler Interface	|src/auto_scheduler/compute_dag.cc|	ComputeDAG, ApplySteps|
|Lowering Schedule to TIR	|src/te/schedule/graph.cc|	ScheduleGraph, LowerSchedule|

Explanation:
- This phase defines how computations should be split into smaller tiles, mapped to hardware threads and blocks, and optimized using unrolling and vectorization.
- Tensor schedules are converted into lower-level forms through ScheduleGraph.

### 2.4 Constructing Low-Level Tensor IR (TIR)
TIR represents a low-level, device-specific IR used to generate target-specific code.

|Process |File	| Key Functions/Classes|
|---|---|---|
|TIR Construction	|src/tir/stmt_functor.cc|	StmtFunctor, VisitStmt, MakeStmt
|Lowering to TIR	|src/tir/transforms/lower_tir.cc|	LowerTIR, TransformTIR
|Memory Planning	|src/tir/transforms/storage_rewrite.cc|	StorageRewrite, PlanMemory
|Device-Specific TIR	|src/target/codegen.cc|	Build, BuildIRModule

Explanation:
- TE schedules are converted into TIR, which provides explicit control over memory accesses and device-specific optimizations.
- StorageRewrite optimizes memory allocation and reuse.

### 2.5 Device-Specific Optimizations
Device-specific optimizations tailor the generated code for the target platform (e.g., CUDA).

|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Thread/Block Mapping	|src/tir/transforms/thread_storage_sync.cc|	ThreadStorageSync|
|Loop Partitioning	|src/tir/transforms/loop_partition.cc|	LoopPartition|
|Device Codegen	|src/target/source/codegen_cuda.cc|	CodeGenCUDA, PrintKernel|

High-Level Summary of the Workflow
- Relay to TE:\
Converts high-level operations into Tensor Expression (TE) definitions using strategies (src/relay/backend/te_compiler.cc).
- Computation Abstraction:
Defines computations in TE with ComputeOp (src/te/operation/compute_op.cc).
- Scheduling:\
Applies optimizations like tiling, unrolling, and mapping computations to threads/blocks (src/te/schedule/schedule_lang.cc).
- Lowering to TIR:\
Translates the schedule into TIR, which explicitly handles device memory and control flow (src/tir/transforms/lower_tir.cc).
- Device-Specific Codegen:\
Emits target-specific code (e.g., CUDA) via CodeGenCUDA (src/target/source/codegen_cuda.cc).

---
## 3. Code Generation

### 3.1 GPU Code Generation
This phase translates Tensor IR (TIR) into GPU-compatible low-level code, generating CUDA kernels and API calls.

|Process|File	| Key Functions/Classes|
|---|---|---|
|TIR to CUDA Kernel	|src/target/source/codegen_cuda.cc	|CodeGenCUDA, GenerateKernel, PrintStmt|
|CodeGen Base Class	|src/target/source/codegen_c.cc	|CodeGenC, PrintExpr|
|Shared Memory Handling	|src/target/source/codegen_cuda.cc	|PrintStorageScope, PrintStorageSync|
|Thread/Block Synchronization	|src/tir/transforms/thread_storage_sync.cc|	ThreadStorageSync|

**Explanation:**
CodeGenCUDA translates TIR to CUDA kernels, emitting device-side code and managing constructs like thread/block mappings, shared memory, and synchronization.
Synchronization points are inserted using PrintStorageSync.


### 3.2. Kernel Construction
Kernel construction involves creating CUDA device kernels and host-side launcher code to invoke them.

|Process|File	| Key Functions/Classes|
|---|---|---|
|Kernel Emission	|src/target/source/codegen_cuda.cc	|PrintFuncBody, EmitFunction|
|Kernel Launch Code	|src/runtime/cuda/cuda_module.cc	|CUDAWrappedFunc, LaunchKernel|
|Kernel Metadata Management|	src/runtime/module.cc|	PackImports, ExportModule|

Explanation:
The EmitFunction generates kernel function declarations and definitions for execution on the GPU.
Host-side kernel launchers are defined in cuda_module.cc.

### 3.3. cuBLAS/CUTLASS Integration
When using cuBLAS or CUTLASS for tensor computations (e.g., GEMM), TVM generates calls to these libraries instead of writing explicit CUDA kernels.

|Process|File	| Key Functions/Classes|
|---|---|---|
|cuBLAS Integration	|src/runtime/contrib/cublas/cublas.cc|	CUBLASCall, InitCUBLASHandle, GemmOp|
|CUTLASS Integration	|src/contrib/cutlass/gen_cutlass_gemm.cc|GenerateCutlassGemm, EmitCutlassCode|
|External Code Generation	|src/relay/backend/contrib/cublas_codegen.cc|	CUBLASFunction, CodegenCUBLAS|

Explanation:
cublas.cc provides wrappers for cuBLAS API calls like cublasSgemm, with TVM handling data layout transformations as needed.
CUTLASS integration uses template-based code generation in gen_cutlass_gemm.cc, emitting optimized kernels for matrix operations.

### 3.4. Target-Specific Optimizations
Target-specific optimizations fine-tune the generated CUDA code based on the GPU architecture and memory hierarchy.

|Process|File	| Key Functions/Classes|
|---|---|---|
|Thread/Block Mapping	|src/tir/transforms/thread_storage_sync.cc|	ThreadStorageSync, OptimizeThreads|
|Loop Partitioning	|src/tir/transforms/loop_partition.cc|	LoopPartition|
|Memory Planning	|src/tir/transforms/storage_rewrite.cc|	StorageRewrite, PlanMemory|
|Warp-Level Optimization	|src/tir/transforms/vectorize_loop.cc|	VectorizeLoop, Vectorizer|

Explanation:
Thread and block mapping ensures optimal utilization of GPU threads and memory.
Loop partitioning and vectorization optimize data access patterns for warp-level efficiency.
StorageRewrite minimizes memory usage by analyzing reuse patterns and adjusting allocation.

### 3.5. Memory Management
Efficient memory management involves optimizing shared/global memory usage and enabling memory reuse.

|Process|File	| Key Functions/Classes|
|---|---|---|
|Shared Memory Usage	|src/target/source/codegen_cuda.cc	|PrintStorageScope, EmitSharedMemory|
|Memory Allocation	|src/tir/transforms/storage_rewrite.cc	|PlanMemory, ReuseMemory|
|Memory Alignment	|src/target/source/codegen_cuda.cc	|PrintStorageAlloc|

Explanation:
Shared memory scopes are explicitly emitted during CUDA codegen (EmitSharedMemory).
PlanMemory optimizes allocation to minimize fragmentation and overhead.

### 3.6. Overall Codegen Workflow
Key Stages and Their Files

- TIR Lowering:\
File: src/tir/transforms/lower_tir.cc\
Function: LowerTIR, TransformTIR

- CUDA Kernel Emission:\
File: src/target/source/codegen_cuda.cc\
Function: EmitFunction, GenerateKernel

- cuBLAS Integration:\
File: src/runtime/contrib/cublas/cublas.cc\
Function: CUBLASCall, InitCUBLASHandle

- CUTLASS Integration:\
File: src/contrib/cutlass/gen_cutlass_gemm.cc\
Function: GenerateCutlassGemm, EmitCutlassCode

- Target-Specific Optimizations:\
File: src/tir/transforms/thread_storage_sync.cc\
Function: ThreadStorageSync, OptimizeThreads
