---
title: Learn TVM (1)
date: 2024-12-08
permalink: /pages/000009/
---


In TVM, high-level optimization in the Relay IR phase includes several graph-level optimizations, data layout transformations, and other functional passes.
These optimizations are implemented in various source files under src/relay/transform and src/relay/op directories.
Below is a categorized list of these optimizations along with their corresponding source code files and functions:

### 1.1 Graph-Level Optimizations

Graph-level optimizations restructure or simplify the computation graph for better performance.

|Optimization	Source |File	| Key Functions/Classes|
|---|---|---|
|Constant Folding|	src/relay/transform/fold_constant.cc	|FoldConstant, ConstantFolder
|Operator Fusion|	src/relay/transform/fuse_ops.cc	|FuseOps, FuseMutator, PatternMatcher
|Dead Code Elimination (DCE)	|src/relay/transform/eliminate_common_subexpr.cc|	EliminateCommonSubexpr
|Common Subexpression Elimination	| src/relay/transform/eliminate_common_subexpr.cc	|EliminateCommonSubexpr
|Simplify Inference	|src/relay/transform/simplify_inference.cc	|SimplifyInference, SimplifyInferenceMutator
|Call Folding	|src/relay/transform/fold_call.cc	|FoldCall
|Inline Functions	|src/relay/transform/inline.cc	|Inline, InlineMutator
|Prune Unused Functions|	src/relay/transform/prune_unused_functions.cc	PruneUnusedFunctions

### 1.2 Data Layout Transformations
These optimizations adjust the layout of tensors for better memory access patterns and compatibility with target hardware.
|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Alter Layout|src/relay/transform/alter_op_layout.cc|AlterOpLayout, AlterOpLayoutRewriter|
|Convert Layout|s	src/relay/transform/convert_layout.cc|ConvertLayout|
|Fold Scale Axis|src/relay/transform/fold_scale_axis.cc|FoldScaleAxis, ScaleAxisSimplifier|
|Layout Optimization|src/relay/transform/layout_rewrite.cc|LayoutRewrite|

### 1.3 Quantization and Precision Management
TVM supports quantization optimizations for reduced precision operations.

|Optimization |File	| Key Functions/Classes|
|---|---|---|
|Quantize|src/relay/quantize/quantize.cc|Quantize, CreateQuantizePass|
|Dequantize|src/relay/quantize/dequantize.cc|Dequantize|
|SimplifyQuantize|src/relay/transform/simplify_quantize.cc|SimplifyQuantize, SimplifyQuantizeRewriter|

### 1.4 Automatic Differentiation
TVM includes an autodiff system for neural networks.

|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Reverse Mode Autodiff|src/relay/transforms/gradient.cc	|AutomaticDifferentiation, ReverseAD|

### 1.5 High-Level Hardware-Aware Optimizations
These optimizations modify operations based on the target hardware.

|Optimization |File	| Key Functions/Classes|
|---|---|---|
|Annotate Target|src/relay/transform/annotate_target.cc|AnnotateTarget|
|Partition Graph|src/relay/transform/partition_graph.cc|PartitionGraph|
|Merge Compiler Regions|src/relay/transform/merge_compiler_regions.cc|MergeCompilerRegions|

### 1.6 Device Placement
These passes assign operations to devices for heterogeneous execution.
|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Rewrite Annotated Ops|src/relay/transform/rewrite_annotated_ops.cc|	RewriteAnnotatedOps|
|Device Annotation	|src/relay/transform/device_annotation.cc	|DeviceAnnotation|

### 1.7 Meta-Pass Management
Relay provides a meta-pass system to manage and sequence passes.

|Meta-Pass |File	| Key Functions/Classes|
|---|---|---|
|Sequential Pass Manager	|src/relay/transform/sequential.cc|	Sequential, PassManager
|Pass Context	|src/relay/transform/pass.cc|	PassContext, WithPassContext

---

## 2
The lowering process from Relay IR to Tensor Expression (TE) and Tensor IR (TIR) in TVM involves multiple phases.
These include converting Relay IR to TE, applying tensor computation abstractions, performing scheduling, and constructing device-specific low-level TIR.
Hereâ€™s a detailed breakdown of the corresponding TVM source code files and functions for these stages:

### 2.1 Converting Relay IR to Tensor Expression (TE)
This phase converts high-level Relay IR into the computation abstractions provided by TE.

|Process |File	| Key Functions/Classes|
|---|---|---|
|Relay to TE Lowering|	src/relay/backend/te_compiler.cc|	LowerToTE, CreateSchedule, ScheduleGetter|
|Operator Strategy|	src/relay/op/strategy/generic.cc|	GenericFunc, OpStrategy|
|Relay to TE Bridge|	src/relay/backend/te_compiler_cache.cc|	TECompiler, LowerTE|
|Shape Function Lowering|	src/relay/backend/te_compiler.cc|	LowerShapeFunc|

Explanation:
- The Relay IR graph is analyzed, and for each operator, TVM retrieves a corresponding TE function using OpStrategy.
- TE functions define high-level operations like matrix multiplication, element-wise addition, etc.

### 2.2 Abstraction of Computation in Tensor Expression (TE)
TE provides a declarative way to express computation. This includes operations like tiling, unrolling, and vectorizing.
|Process |File	| Key Functions/Classes|
|---|---|---|
|Tensor Expression Build|	src/te/operation/create_primfunc.cc|	CreatePrimFunc, ComputeBody, ScheduleOps|
|Compute Definition|	src/te/operation/compute_op.cc|	ComputeOpNode, ComputeOp|
|Tensor Compute Intrinsics|	src/te/operation/tensorize.cc|	Tensorize, CreateIntrinBody|

Explanation:
- High-level computations are abstracted into a declarative format using ComputeOp.
- Intrinsic support for tensorization is added for specialized hardware operations.

### 2.3 Scheduling in Tensor Expression
Scheduling is where TVM optimizes how computations are performed on the target device.
|Process |File	| Key Functions/Classes|
|---|---|---|
|Tile, Unroll, Vectorize	|src/te/schedule/schedule_dataflow_rewrite.cc	|ScheduleDataFlowRewrite, Tile, Unroll, Vectorize|
|Thread and Block Mapping	|src/te/schedule/schedule_lang.cc	|bind, split, reorder, fuse|
|AutoScheduler Interface	|src/auto_scheduler/compute_dag.cc|	ComputeDAG, ApplySteps|
|Lowering Schedule to TIR	|src/te/schedule/graph.cc|	ScheduleGraph, LowerSchedule|

Explanation:
- This phase defines how computations should be split into smaller tiles, mapped to hardware threads and blocks, and optimized using unrolling and vectorization.
- Tensor schedules are converted into lower-level forms through ScheduleGraph.

### 2.4 Constructing Low-Level Tensor IR (TIR)
TIR represents a low-level, device-specific IR used to generate target-specific code.

|Process |File	| Key Functions/Classes|
|---|---|---|
|TIR Construction	|src/tir/stmt_functor.cc|	StmtFunctor, VisitStmt, MakeStmt
|Lowering to TIR	|src/tir/transforms/lower_tir.cc|	LowerTIR, TransformTIR
|Memory Planning	|src/tir/transforms/storage_rewrite.cc|	StorageRewrite, PlanMemory
|Device-Specific TIR	|src/target/codegen.cc|	Build, BuildIRModule

Explanation:
- TE schedules are converted into TIR, which provides explicit control over memory accesses and device-specific optimizations.
- StorageRewrite optimizes memory allocation and reuse.

### 2.5 Device-Specific Optimizations
Device-specific optimizations tailor the generated code for the target platform (e.g., CUDA).
|Transformation |File	| Key Functions/Classes|
|---|---|---|
|Thread/Block Mapping	|src/tir/transforms/thread_storage_sync.cc|	ThreadStorageSync|
|Loop Partitioning	|src/tir/transforms/loop_partition.cc|	LoopPartition|
|Device Codegen	|src/target/source/codegen_cuda.cc|	CodeGenCUDA, PrintKernel|



|Transformation |File	| Key Functions/Classes|
|---|---|---|
||||
||||
||||
||||
||||

|Transformation |File	| Key Functions/Classes|
|---|---|---|
||||
||||
||||
||||
||||


|Transformation |File	| Key Functions/Classes|
|---|---|---|
||||
||||
||||
||||
||||

|Transformation |File	| Key Functions/Classes|
|---|---|---|
||||
||||
||||
||||
||||

|Transformation |File	| Key Functions/Classes|
|---|---|---|
||||
||||
||||
||||
||||

|Transformation |File	| Key Functions/Classes|
|---|---|---|
||||
||||
||||
||||
||||
