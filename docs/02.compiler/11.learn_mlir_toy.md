---
title: MLIR TOY Tutorial
date: 2024-12-10
permalink: /pages/000011/
---

### Chapter 1. Toy Language and AST
Lexer and recursive descent parser construt AST

### Chapter 2. Emit Basic MLIR

Operations: instructions, globals(functions), modules, in LLVM

![image](https://github.com/user-attachments/assets/1547200e-1e13-4075-b202-77be37cf0b74)
From [link](https://medium.com/sniper-ai/mlir-tutorial-create-your-custom-dialect-lowering-to-llvm-ir-dialect-system-1-1f125a6a3008)

Transpose Operation

```
%t_tensor = "toy.transpose"(%tensor) {inplace = true} : (tensor<2x3xf64>) ->
tensor<3x2xf64> loc("example/file/path":12:1)
```

meaning of each part

```
result  = name of operation (input operands) dictionary of traits :
type of operations (input and output), location
```

- A name for the operation.
- A list of SSA operand values.
- A list of attributes.
- A list of types for result values.
- A source location for debugging purposes.
- A list of successors blocks (for branches, mostly).
- A list of regions (for structural operations like functions).

**Opaque API**


**Define a Toy Dialect**

dialect could be defined by c++ or tablegen(declarative specification).

after the definiation, it could be loaded to MLIR Context.
context.loadDialect<ToyDialect>();

**Defining Toy Operations**

```
class ConstantOp : public mlir::Op<
                     /// `mlir::Op` is a CRTP class, meaning that we provide the
                     /// derived class as a template parameter.
                     ConstantOp,
                     /// The ConstantOp takes zero input operands.
                     mlir::OpTrait::ZeroOperands,
                     /// The ConstantOp returns a single result.
                     mlir::OpTrait::OneResult,
                     /// We also provide a utility `getType` accessor that
                     /// returns the TensorType of the single result.
                     mlir::OpTraits::OneTypedResult<TensorType>::Impl> {

 public:
  /// Inherit the constructors from the base Op class.
  using Op::Op;
  ...
  static void build(mlir::OpBuilder &builder, mlir::OperationState &state,
                    mlir::Type result, mlir::DenseElementsAttr value);
```

register operation:

```
void ToyDialect::initialize() {
  addOperations<ConstantOp>();
}
```

**Op vs Operation: Using MLIR Operations**

**Using Operation Definition Specification Framwork**

base Toy_Op

```
class Toy_Op<string mnemonic, list<Trait> traits = []> :
    Op<Toy_Dialect, mnemonic, traits>;
```

ConstantOp

```
def ConstantOp : Toy_Op<"constant"> {
}
```

**Attaching build Methods**

In ConstantOp, it declared a list of build. ODS will generate the first build.\
As to other builds, we have to atttach.
```
def ConstantOp : Toy_Op<"constant"> {
  ...

  // Add custom build methods for the constant operation. These methods populate
  // the `state` that MLIR uses to create operations, i.e. these are used when
  // using `builder.create<ConstantOp>(...)`.
  let builders = [
    // Build a constant with a given constant tensor value.
    OpBuilder<(ins "DenseElementsAttr":$value), [{
      // Call into an autogenerated `build` method.
      build(builder, result, value.getType(), value);
    }]>,

    // Build a constant with a given constant floating-point value. This builder
    // creates a declaration for `ConstantOp::build` with the given parameters.
    OpBuilder<(ins "double":$value)>
  ];
}
```

**Specifying a Custom Assembly Format**

The printout version of IR has too much information.

We can strip out by implementing our owne oversion of print and parse function.

Take PrintOp as example.

```
void PrintOp::print(mlir::OpAsmPrinter &printer) {
  printer << "toy.print " << op.input();
  printer.printOptionalAttrDict(op.getAttrs());
  printer << " : " << op.input().getType();
}

mlir::ParseResult PrintOp::parse(mlir::OpAsmParser &parser,
                                 mlir::OperationState &result) {
...
}
```

### Chapter 3. High-level Language-Specific Analysis and Transformation

- Imperative, C++ Pattern match and Rewrite
- Decalrative, rule-based pattern-match and rewrite using table-driven

**C++**

```
/// Fold transpose(transpose(x)) -> x
struct SimplifyRedundantTranspose : public mlir::OpRewritePattern<TransposeOp> {
  SimplifyRedundantTranspose(mlir::MLIRContext *context)
      : OpRewritePattern<TransposeOp>(context, /*benefit=*/1) {}
  llvm::LogicalResult
  matchAndRewrite(TransposeOp op,
                  mlir::PatternRewriter &rewriter) const override {
    // Look through the input of the current transpose.
  ....
  }
};

// Register our patterns for rewrite by the Canonicalization framework.
void TransposeOp::getCanonicalizationPatterns(
    RewritePatternSet &results, MLIRContext *context) {
  results.add<SimplifyRedundantTranspose>(context);
}

// Add into Pass Manager
mlir::PassManager pm(module->getName());
pm.addNestedPass<mlir::toy::FuncOp>(mlir::createCanonicalizerPass());
```

**rule-based pattern-match and rewrite (DRR)**

```
def TypesAreIdentical : Constraint<CPred<"$0.getType() == $1.getType()">>;
def RedundantReshapeOptPattern : Pat<
  (ReshapeOp:$res $arg), (replaceWithValue $arg),
  [(TypesAreIdentical $res, $arg)]>;
```

### Chapter 4. Enabling Generic Transformation with Interfaces

If you still remember correctly, in last chapter, we register *SimplifyRedundantTranspose* into getCanonicalizationPatterns\
which *applies transformations defined by operations in a greedy, iterative manner*. 

```
void TransposeOp::getCanonicalizationPatterns(
    RewritePatternSet &results, MLIRContext *context) {
  results.add<SimplifyRedundantTranspose>(context);
}
```

This is not scalable.

**Shape Inference: Preparing for Code Generation**

This starts from an example of inlining all function calls to perform intraprocedural shape propagation.

**Inlining**

MLIR provide a generic inliner algorithm that dialects can plug into.

In toy, we need to provide interfaces so the inliner could hook into.

1. The first, we need to define constraints on inlining operations.

Define a interface, which is a class containing a set of virtual hooks which the dialects can override.

```
struct ToyInlinerInterface : public DialectInlinerInterface {
  using DialectInlinerInterface::DialectInlinerInterface;
  ...
  // check whether the region is able to inline
  isLegalToInline();
  ...
  void handleTerminator(Operation *op,
                        MutableArrayRef<Value> valuesToRepl)
  ...
}
```

2. Register dialect interface on to TOY dialect.

```
void ToyDialect::initialize() {
  addInterfaces<ToyInlinerInterface>();
}
```

3. We need a way to inform inliner that toy.generic_call is a *call* and toy.func is *function*.

We achieve this goal by operation interface, marking they are callable or callop.

```
include "mlir/Interfaces/CallInterfaces.td"

def FuncOp : Toy_Op<"func",
    [DeclareOpInterfaceMethods<CallableOpInterface>]> {
  ...
}

def GenericCallOp : Toy_Op<"generic_call",
    [DeclareOpInterfaceMethods<CallOpInterface>]> {
  ...
}
```

The above *DeclareOpInterfaceMethods* directive auto-declares all of the interface methods in the class declaration of GenericCallOp.

Then we need to fill in the definitions:

```
...
Region *FuncOp::getCallableRegion()
...
CallInterfaceCallable GenericCallOp::getCallableForCallee()
...
GenericCallOp::setCalleeFromCallable(CallInterfaceCallable callee)
```

4. Now the inliner has been informed about the Toy dialect.

add the inlinerpasser to pass manager
```
pm.addPass(mlir::createInlinerPass());
```

5. Considering the input of the transpose function is different from argument.

Then they define a cast Op to cast between different shapes.

```
def CastOp : Toy_Op<"cast", [
    DeclareOpInterfaceMethods<CastOpInterface>,
    Pure,
    SameOperandsAndResultShape]

 > {
...
}
```

Notice that CastOp add CastOpInterface into traits lists.


```
/// Returns true if the given set of input and result types are compatible with
/// this cast operation. This is required by the `CastOpInterface` to verify
/// this operation and provide other additional utilities.
bool CastOp::areCastCompatible(TypeRange inputs, TypeRange outputs) {
...
}
```

```
struct ToyInlinerInterface : public DialectInlinerInterface {
  ...

  /// Attempts to materialize a conversion for a type mismatch between a call
  /// from this dialect, and a callable region. This method should generate an
  /// operation that takes 'input' as the only operand, and produces a single
  /// result of 'resultType'. If a conversion can not be generated, nullptr
  /// should be returned.
  Operation *materializeCallConversion(OpBuilder &builder, Value input,
                                       Type resultType,
                                       Location conversionLoc) const final {
    return builder.create<CastOp>(conversionLoc, resultType, input);
  }
};
```

Then, the output is as expected.

```
toy.func @main() {
  %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>
  %1 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>
  %2 = toy.cast %1 : tensor<2x3xf64> to tensor<*xf64>
  %3 = toy.cast %0 : tensor<2x3xf64> to tensor<*xf64>
  %4 = toy.transpose(%2 : tensor<*xf64>) to tensor<*xf64>
  %5 = toy.transpose(%3 : tensor<*xf64>) to tensor<*xf64>
  %6 = toy.mul %4, %5 : tensor<*xf64>
  toy.print %6 : tensor<*xf64>
  toy.return
}
```

**Intraprocedural Shape Inference**

Current state: all function has been inlined and mixed with static and dynamic shaped operations.

The shape propagation should be generic to many dialects.

*Operation Interface*

We could define this by:

```
def ShapeInferenceOpInterface : OpInterface<"ShapeInference"> {
  let description = [{
    Interface to access a registered method to infer the return types for an
    operation that can be used during type inference.
  }];

  let methods = [
    InterfaceMethod<"Infer and set the output shape for the current operation.",
                    "void", "inferShapes">
  ];

}
```

Then we add interface to necessary Toy Operations.

This is simple to add *CallOpInterface* to GenericCallOp.

```
def MulOp : Toy_Op<"mul",
    [..., DeclareOpInterfaceMethods<ShapeInferenceOpInterface>]> {
  ...
}
```

As to the operations has been added the interface, we need to provide definition of the method.

```
void MulOp::inferShapes() { getResult().setType(getLhs().getType()); }
```

```
class ShapeInferencePass
    : public mlir::PassWrapper<ShapeInferencePass, OperationPass<FuncOp>> {
  void runOnOperation() override {
    FuncOp function = getOperation();
    ...
  }
};
```

*Helper method*
```
std::unique_ptr<mlir::Pass> mlir::toy::createShapeInferencePass() {
  return std::make_unique<ShapeInferencePass>();
}
```

```
pm.addPass(mlir::createShapeInferencePass());
```

Then the output will be like this:

```
toy.func @main() {
  %0 = toy.constant dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>
  %1 = toy.transpose(%0 : tensor<2x3xf64>) to tensor<3x2xf64>
  %2 = toy.mul %1, %1 : tensor<3x2xf64>
  toy.print %2 : tensor<3x2xf64>
  toy.return
}
```
