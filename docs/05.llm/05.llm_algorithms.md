---
title: llm algorthms
date: 2024-11-03 23:32:49
permalink: /pages/dc7039/
---

1. [1900] Mixed Precision Training
2. 
3. [142] Performance, Design, and Autotuning of Batched GEMM for GPUs
4. [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization

---
### 1. [1900] Mixed Precision Training

![image](https://github.com/user-attachments/assets/015ccb67-ef34-4c76-ad52-10618c578ad7)

![image](https://github.com/user-attachments/assets/1a564342-e231-42a4-99d5-e9fef4500a1a)


**Loss Scaling**

Note that much of the FP16 representable range was left unused, while many values were below the minimum representable range and became zeros.\
Scaling up the gradients will shift them to occupy more of the representable range and preserve values that are otherwise lost to zeros.\
This particular network diverges when gradients are not scaled, but scaling them by a factor of 8 (increasing the exponents by 3) is sufficient to match the accuracy achieved with FP32 training.

activation gradient values below 2^−27 in magnitude were irrelevant to the training of this model, but values in the [2^−27, 2^−24) range were important to preserve.

One efficient way to shift the gradient values into FP16-representable range is to scale the loss value computed in the forward pass, prior to starting back-propagation.

By chain rule back-propagation ensures that all the gradient values are scaled by the same amount.

The gradients need to be unscaled before the final weight update.


---
### 2. [142] Performance, Design, and Autotuning of Batched GEMM for GPUs

![image](https://github.com/user-attachments/assets/ad7a256c-91eb-4087-a8de-ccfeb05ea8a3)

![image](https://github.com/user-attachments/assets/f205bad1-19a3-4843-8fca-cc12cfa7298a)

---
### 4. [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization

FlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in LLMs, with the remaining layers using 8 bits.

The 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by large matrix calculation.\
Our work achieves up to 2× speedup and 2.3× memory reduction for LLMs with negligible loss in accuracy.

![image](https://github.com/user-attachments/assets/dee83087-aa07-4441-b6f6-31a8eafedaed)

**Framework**
![image](https://github.com/user-attachments/assets/d386534e-97ad-4ac6-a34e-83545e1e68b2)

![image](https://github.com/user-attachments/assets/1924a296-1948-46c1-b835-a99bfa229b03)

