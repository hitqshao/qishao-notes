---
title: llm algorthms
date: 2024-11-03 23:32:49
permalink: /pages/dc7039/
---

1. [1900] Mixed Precision Training
2. [1519] Training Compute-Optimal Large Language Models
3. [440] Measuring the Effects of Data Parallelism on Neural Network Training
4. [255] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU
5. [142] Performance, Design, and Autotuning of Batched GEMM for GPUs
6. [13] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving
7. [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization

---
### [1900] Mixed Precision Training

![image](https://github.com/user-attachments/assets/015ccb67-ef34-4c76-ad52-10618c578ad7)

![image](https://github.com/user-attachments/assets/1a564342-e231-42a4-99d5-e9fef4500a1a)


**Loss Scaling**

Note that much of the FP16 representable range was left unused, while many values were below the minimum representable range and became zeros.\
Scaling up the gradients will shift them to occupy more of the representable range and preserve values that are otherwise lost to zeros.\
This particular network diverges when gradients are not scaled, but scaling them by a factor of 8 (increasing the exponents by 3) is sufficient to match the accuracy achieved with FP32 training.

activation gradient values below 2^−27 in magnitude were irrelevant to the training of this model, but values in the [2^−27, 2^−24) range were important to preserve.

One efficient way to shift the gradient values into FP16-representable range is to scale the loss value computed in the forward pass, prior to starting back-propagation.

By chain rule back-propagation ensures that all the gradient values are scaled by the same amount.

The gradients need to be unscaled before the final weight update.


---
### [1519] Training Compute-Optimal Large Language Models

**Fix model sizes and vary number of training tokens**
![image](https://github.com/user-attachments/assets/7c6f0cec-cc66-4b28-a1eb-035aee0ef342)

On the left we show all of our different runs. We launched a range of model sizes going from 70M to 10B, each for four different cosine cycle lengths.\
From these curves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate the optimal model size (center) for a given compute budget and the optimal number of training tokens (right).\
In green, we show projections of optimal model size and training token count based on the number of FLOPs used to train Gopher (5.76 × 1023).

**IsoFLOP profiles**
![image](https://github.com/user-attachments/assets/bf7aa98a-ca33-474e-bdfe-706a95f20a94)

For various model sizes, we choose the number of training tokens such that the final FLOPs is a constant.\
The cosine cycle length is set to match the target FLOP count.\
We find a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train (left).\
Using the location of these valleys, we project optimal model size and number of tokens for larger models (center and right).\
In green, we show the estimated number of parameters and tokens for an optimal model trained with the compute budget of Gopher.

---
### [440] Measuring the Effects of Data Parallelism on Neural Network Training [Google]

#### Steps to Result Depends on Batch Size in a Similar Way Across Problems.

In all cases, as the batch size grows, there is an initial period of perfect scaling (indicated with a dashed line) where the steps needed to achieve the error goal
halves for each doubling of the batch size.\
Then there is a region of diminishing returns that eventually leads to a region of maximal data parallelism where additional parallelism provides no benefit whatsoever.

![image](https://github.com/user-attachments/assets/afb695c3-1503-45cc-a123-37cd8110880e)

![image](https://github.com/user-attachments/assets/a132a4fc-f469-4605-b522-558345f0b9c3)

If the curves in Figure 1 were very sensitive to the goal validation error, then measuring the steps needed to reach our particular choice of the goal would not be a meaningful proxy for training speed.\

For small changes in the goal validation error, we do not care about vertical shifts as long as the transition points between the three scaling regions remain relatively unchanged.\

Figure 2 shows that varying the error goal only vertically shifts the stepsto-result curve, at least for modest variations centered around a good absolute validation error.

Furthermore, although we ultimately care about out-of-sample error, if our plots looked very different when measuring the steps needed to reach a particular training error, then we would need to include both curves when presenting our results.


#### Validating Our Measurement Protocol

#### Some Models Can Exploit Much Larger Batch Sizes Than Others

![image](https://github.com/user-attachments/assets/d4e1c07a-5e03-42f4-94fc-e750175b1396)

**This might be the begining of the scale law.**

#### Momentum Extends Perfect Scaling to Larger Batch Sizes, but Matches Plain SGD at Small Batch Sizes

#### The Data Set Matters, at Least Somewaht

#### Regularization Can Be More Helpful at Some Batch Sizes than Others

#### The Best Learning Rate and Momentrum Vary with Batch Size

#### Solution Quality Depends on Compute Budget More Than Batch Size

Taken together, these observations suggest that in practice the relevant question is not which batch size leads to the best performance, but rather how compute budget varies as a function of batch size. 

![image](https://github.com/user-attachments/assets/a7164eb8-4ff2-4e9f-a68f-8538d4ce54b4)

---
### [142] Performance, Design, and Autotuning of Batched GEMM for GPUs

![image](https://github.com/user-attachments/assets/ad7a256c-91eb-4087-a8de-ccfeb05ea8a3)

![image](https://github.com/user-attachments/assets/f205bad1-19a3-4843-8fca-cc12cfa7298a)

---
### [255] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU

- We formally define a search space of possible offloading strategies by considering computation schedule, tensor placement, and computation delegation.\
We prove that our search space captures a computation order with I/O complexity within 2× of optimality.\
We then develop a linear programming-based search algorithm to optimize the throughput within the search space.

- We show that it is possible to compress both the weights and KV cache for LLMs like OPT-175B to 4 bits without retraining or calibration, all with negligible
accuracy loss.\
This is achieved through fine-grained groupwise quantization (Shen et al., 2020), which is suitable for reducing I/O costs and memory usage during offloading.

![image](https://github.com/user-attachments/assets/f82b1417-22f4-42a3-9206-d09994df5307)

![image](https://github.com/user-attachments/assets/d7d16c50-4988-4daf-8e16-fa97a5788407)

**All existing systems (Aminabadi et al., 2022; HuggingFace, 2022) traverse the graph row-by-row, as shown in Fig. 3(a).** \
This is reasonable because it is the fastest way to finish the generation for one batch and the KV cache can be freed immediately after a row.\
However, because every two contiguous squares do not share weights, this schedule has to repeatedly load the weights and incurs huge I/O costs.

---
### [13] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving

cost-performance trade-offs for inference strategies:
- greedy search
- majority voting
- best-of-n
- weighted voting 
- two different tree search algorithms, using different model sizes and compute budgets.

Smaller models (e.g., Llemma-7B) can outperform larger models given the same computation budgets \
Smaller models paired with advanced inference algorithms yield Pareto-optimal cost-performance tradeoffs. For instance, the Llemma-7B model, equipped with our novel tree search algorithm, consistently outperforms Llemma-34B with standard majority voting on the MATH benchmark across all FLOPs budgets.

![image](https://github.com/user-attachments/assets/2e085a00-2755-48b9-8d23-bf1f8dba1488)

the accuracy of the language model will ultimately saturate to a fixed limit which is determined by the output probabilities assigned by the model, exhibiting exponential convergence speed through sampling and voting.\
this highlights the necessity for more sophisticated inference algorithms.

the commonly-used MCTS method does not perform well with weighted voting, as it often yields many unfinished solutions, hence having less effective votes.

To address this issue, we propose a novel tree search algorithm, **REward BAlanced SEarch (REBASE)**, which pairs well with weighted voting and achieves a Pareto-optimal trade-off between accuracy and inference
compute.\
The key idea of REBASE is to use a node-quality reward to control node expansion, which eliminates the need for explicit rollouts while ensuring enough candidate solutions for voting.

![image](https://github.com/user-attachments/assets/d3214e44-c71a-4f18-9da4-4e4e2f543fd3)

#### Inference Strategy
Greedy search. This strategy generates tokens one at a time by selecting the highest probability token at each step. It is computationally efficient but often suboptimal in terms of diversity.
- Best-of-n. This strategy, also known as rejection sampling, generates a set of candidates and chooses the one with the highest score given by the reward model.
- Majority voting. In this strategy, a set of candidates are generated, and the final answer to the problem is determined by the most frequently occurring answer in all the outputs.
- Weighted majority voting. This strategy is a variant of majority voting in which the candidates are weighted based on the scores given by the reward model.

Sampling-based if it uses a standard autoregressive sampling algorithm (e.g., temperature sampling) to generate the candidate set (greedy search is separate, in that it only has a single deterministic candidate).

A tree-search variant uses a tree search to generate the candidate set.

**Informally, as long as the reward model is “better than random”, i.e., assigning higher rewards to correct solutions on average, the accuracy limit of weighted majority voting is higher than that of majority voting.**

- Monte Carlo Tree Search(MCTS)

MCTS consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods.

a significant portion of the paths in the search tree are used to estimate and select nodes, and **these paths do not necessarily become a part of the final candidate solution**, although MCTS ensures that the sampled solutions comprise high-quality intermediate steps.

sampling methods generate multiple solutions in parallel and independently, and **all the generated sequences are included in the candidate solutions**.\
However, the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no mechanism for pruning poor steps or exploiting promising ones.





- 


---
### [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization

FlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in LLMs, with the remaining layers using 8 bits.

The 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by large matrix calculation.\
Our work achieves up to 2× speedup and 2.3× memory reduction for LLMs with negligible loss in accuracy.

![image](https://github.com/user-attachments/assets/dee83087-aa07-4441-b6f6-31a8eafedaed)

**Framework**
![image](https://github.com/user-attachments/assets/d386534e-97ad-4ac6-a34e-83545e1e68b2)

![image](https://github.com/user-attachments/assets/1924a296-1948-46c1-b835-a99bfa229b03)

