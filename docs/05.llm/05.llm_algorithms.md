---
title: llm algorthms
date: 2024-11-03 23:32:49
permalink: /pages/dc7039/
---

1. [1900] Mixed Precision Training
2. [142] Performance, Design, and Autotuning of Batched GEMM for GPUs
3. 

---
### 1. Mixed Precision Training

![image](https://github.com/user-attachments/assets/015ccb67-ef34-4c76-ad52-10618c578ad7)

![image](https://github.com/user-attachments/assets/1a564342-e231-42a4-99d5-e9fef4500a1a)


**Loss Scaling**

Note that much of the FP16 representable range was left unused, while many values were below the minimum representable range and became zeros.
Scaling up the gradients will shift them to occupy more of the representable range and preserve values that are otherwise lost to zeros.
This particular network diverges when gradients are not scaled, but scaling them by a factor of 8 (increasing the exponents by 3) is sufficient to match the accuracy achieved with FP32 training.

activation gradient values below 2^−27 in magnitude were irrelevant to the training of this model, but values in the [2^−27, 2^−24) range were important to preserve.

One efficient way to shift the gradient values into FP16-representable range is to scale the loss value computed in the forward pass, prior to starting back-propagation.

By chain rule back-propagation ensures that all the gradient values are scaled by the same amount.


---
### 2. Performance, Design, and Autotuning of Batched GEMM for GPUs

![image](https://github.com/user-attachments/assets/ad7a256c-91eb-4087-a8de-ccfeb05ea8a3)

![image](https://github.com/user-attachments/assets/f205bad1-19a3-4843-8fca-cc12cfa7298a)
