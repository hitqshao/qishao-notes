---
title: llm algorthms
date: 2024-11-03 23:32:49
permalink: /pages/dc7039/
---

1. [1900] Mixed Precision Training
2. [440] Measuring the Effects of Data Parallelism on Neural Network Training
3. [142] Performance, Design, and Autotuning of Batched GEMM for GPUs
4. [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization

---
### 1. [1900] Mixed Precision Training

![image](https://github.com/user-attachments/assets/015ccb67-ef34-4c76-ad52-10618c578ad7)

![image](https://github.com/user-attachments/assets/1a564342-e231-42a4-99d5-e9fef4500a1a)


**Loss Scaling**

Note that much of the FP16 representable range was left unused, while many values were below the minimum representable range and became zeros.\
Scaling up the gradients will shift them to occupy more of the representable range and preserve values that are otherwise lost to zeros.\
This particular network diverges when gradients are not scaled, but scaling them by a factor of 8 (increasing the exponents by 3) is sufficient to match the accuracy achieved with FP32 training.

activation gradient values below 2^−27 in magnitude were irrelevant to the training of this model, but values in the [2^−27, 2^−24) range were important to preserve.

One efficient way to shift the gradient values into FP16-representable range is to scale the loss value computed in the forward pass, prior to starting back-propagation.

By chain rule back-propagation ensures that all the gradient values are scaled by the same amount.

The gradients need to be unscaled before the final weight update.


---
### 2. [440] Measuring the Effects of Data Parallelism on Neural Network Training [Google]

#### Steps to Result Depends on Batch Size in a Similar Way Across Problems.

In all cases, as the batch size grows, there is an initial period of perfect scaling (indicated with a dashed line) where the steps needed to achieve the error goal
halves for each doubling of the batch size.\
Then there is a region of diminishing returns that eventually leads to a region of maximal data parallelism where additional parallelism provides no benefit whatsoever.

![image](https://github.com/user-attachments/assets/afb695c3-1503-45cc-a123-37cd8110880e)

![image](https://github.com/user-attachments/assets/a132a4fc-f469-4605-b522-558345f0b9c3)

If the curves in Figure 1 were very sensitive to the goal validation error, then measuring the steps needed to reach our particular choice of the goal would not be a meaningful proxy for training speed.\

For small changes in the goal validation error, we do not care about vertical shifts as long as the transition points between the three scaling regions remain relatively unchanged.\

Figure 2 shows that varying the error goal only vertically shifts the stepsto-result curve, at least for modest variations centered around a good absolute validation error.

Furthermore, although we ultimately care about out-of-sample error, if our plots looked very different when measuring the steps needed to reach a particular training error, then we would need to include both curves when presenting our results.


#### Validating Our Measurement Protocol

#### Some Models Can Exploit Much Larger Batch Sizes Than Others

![image](https://github.com/user-attachments/assets/d4e1c07a-5e03-42f4-94fc-e750175b1396)

**This might be the begining of the scale law.**

#### Momentum Extends Perfect Scaling to Larger Batch Sizes, but Matches Plain SGD at Small Batch Sizes

#### The Data Set Matters, at Least Somewaht

#### Regularization Can Be More Helpful at Some Batch Sizes than Others

#### The Best Learning Rate and Momentrum Vary with Batch Size

#### Solution Quality Depends on Compute Budget More Than Batch Size

Taken together, these observations suggest that in practice the relevant question is not which batch size leads to the best performance, but rather how compute budget varies as a function of batch size. 

![image](https://github.com/user-attachments/assets/a7164eb8-4ff2-4e9f-a68f-8538d4ce54b4)

---
### 2. [142] Performance, Design, and Autotuning of Batched GEMM for GPUs

![image](https://github.com/user-attachments/assets/ad7a256c-91eb-4087-a8de-ccfeb05ea8a3)

![image](https://github.com/user-attachments/assets/f205bad1-19a3-4843-8fca-cc12cfa7298a)

---
### 4. [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization

FlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in LLMs, with the remaining layers using 8 bits.

The 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by large matrix calculation.\
Our work achieves up to 2× speedup and 2.3× memory reduction for LLMs with negligible loss in accuracy.

![image](https://github.com/user-attachments/assets/dee83087-aa07-4441-b6f6-31a8eafedaed)

**Framework**
![image](https://github.com/user-attachments/assets/d386534e-97ad-4ac6-a34e-83545e1e68b2)

![image](https://github.com/user-attachments/assets/1924a296-1948-46c1-b835-a99bfa229b03)

