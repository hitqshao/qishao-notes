---
title: llm compute & memory bound
date: 2024-12-05 23:32:49
permalink: /pages/dc7041/
---

1.[148] Data Movement is All You Need: A Case Study on Optimizing Transformers
2.[70 2024] Splitwise: Efficient Generative LLM Inference Using Phase Splitting



---
### 1. Data Movement is All You Need: A Case Study on Optimizing Transformers

Contributions:

- We find transformer training to be memory-bound and significantly underperforming on GPUs.
- We develop a generic recipe for optimizing training using dataflow analyses.

![image](https://github.com/user-attachments/assets/26739a63-f1c3-46f6-b0d1-b2aea2e953fa)

**Tensor Constraction**: matrix-matrix multiplication

We consider only MMMs and batched MMMs for simplicity, as these are efficiently supported by cuBLAS.\
In transformers, these are linear layers and components of MHA.\
These operations are the most compute-intensive part of training a transformer.\
For good performance, data layout and algorithm selection (e.g., tiling strategy) are critical.

**Statistical Normalization**: softmax and layer normalization\
Less compute-intensive than tensors\
This compute pattern means that data layout and vectorization is important for operator performance.

**Element-wise Operators**: biases, dropout, activations, and residual connections\
These are the least compute-intensive operations.

![image](https://github.com/user-attachments/assets/791880f1-a512-4778-a0d4-20d29282f898)


---
### 2. Splitwise: Efficient Generative LLM Inference Using Phase Splitting

First, **the prompt computation phase**, in which all the input prompt tokens run through the forward pass of the model in parallel to generate the first output token.\
This phase tends to be computationally intensive and requires the high FLOPs (floating point operations per second) of the latest GPUs today.\
Second, **the token generation phase**, in which subsequent output tokens are generated sequentially based on the forward pass of the last token and all the cached context from previous tokens in the sequence.\
Given the lack of compute parallelism, this phase tends to be more memory bandwidth and capacity bound, despite state-of-the-art batching.

The context generated from the attention layers during the prompt computation is saved in the key-value (KV) cache, since it is needed for all the future token generation iterations.\
After the first token is generated, the following tokens only use the last generated token and the KV-cache as inputs to the forward pass of the model.\
This makes the subsequent token generation more memory bandwidth and capacity intensive than the computationally heavy prompt phase.

![image](https://github.com/user-attachments/assets/6fcf7ec7-9dc5-460a-8a64-0c85965f15ef)
