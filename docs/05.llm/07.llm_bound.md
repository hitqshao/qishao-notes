---
title: llm compute & memory bound
date: 2024-12-05 23:32:49
permalink: /pages/dc7041/
---

1.[148] Data Movement is All You Need: A Case Study on Optimizing Transformers



---
### 1. Data Movement is All You Need: A Case Study on Optimizing Transformers

Contributions:

- We find transformer training to be memory-bound and significantly underperforming on GPUs.
- We develop a generic recipe for optimizing training using dataflow analyses.

![image](https://github.com/user-attachments/assets/26739a63-f1c3-46f6-b0d1-b2aea2e953fa)

**Tensor Constraction**: matrix-matrix multiplication

We consider only MMMs and batched MMMs for simplicity, as these are efficiently supported by cuBLAS.\
In transformers, these are linear layers and components of MHA.\
These operations are the most compute-intensive part of training a transformer.\
For good performance, data layout and algorithm selection (e.g., tiling strategy) are critical.

**Statistical Normalization**: softmax and layer normalization\
Less compute-intensive than tensors\
This compute pattern means that data layout and vectorization is important for operator performance.

**Element-wise Operators**: biases, dropout, activations, and residual connections\
These are the least compute-intensive operations.

![image](https://github.com/user-attachments/assets/791880f1-a512-4778-a0d4-20d29282f898)
