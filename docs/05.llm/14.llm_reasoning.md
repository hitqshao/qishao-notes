---
title: Reasoning in LLM
date: 2025-01-27 23:32:49
permalink: /pages/dc7049/
---

1. [331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting
2. [9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
3. [641] Towards Reasoning in Large Language Models: A Survey

---
## [331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting

- Key Findings:
  - Unfaithful Explanations: CoT explanations can be plausible but systematically unfaithful, failing to reflect the true reasoning process.
  - Biasing Features: Models are heavily influenced by biasing features (e.g., reordering multiple-choice options), which are not mentioned in explanations.
  - Accuracy Drop: Biasing models toward incorrect answers leads to a 36% drop in accuracy on BIG-Bench Hard tasks.
  - Social Bias: Models justify stereotype-aligned answers without acknowledging the influence of social biases.
  - Counterfactual Simulatability: Models rarely acknowledge biasing features, making explanations systematically unfaithful.

They instruct llm with bias：

![image](https://github.com/user-attachments/assets/9c02c120-64d7-488c-b1af-bebeb28e8582)

![image](https://github.com/user-attachments/assets/c11dfc4e-35af-4ce2-b88f-fd51f8980805)

---

## [9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

Key findings:
- Chain-of-thought prompting significantly improves the performance of large language models on a variety of reasoning tasks.
- Chain-of-thought prompting is an emergent property of model scale, meaning that it only provides significant performance gains when used with very large language models (around 100 billion parameters).
- The improvements from chain-of-thought prompting are robust across different language models, datasets, and annotators

Usage cases:
- Arithmetic reasoning: CoT prompting can help language models solve math word problems that require multiple steps, such as the GSM8K benchmark.    
- Commonsense reasoning: CoT prompting can also improve the performance of language models on tasks that require commonsense reasoning, such as the StrategyQA dataset, which requires models to infer a multi-hop strategy to answer questions.    
- Symbolic reasoning: CoT prompting has also been shown to be effective for symbolic reasoning tasks, such as last letter concatenation, which requires the model to concatenate the last letters of words in a name.

---

## [641] Towards Reasoning in Large Language Models: A Survey

![image](https://github.com/user-attachments/assets/a54b39f5-4293-4d16-ad88-ce289ed787a9)

Large language models (LLMs) have made impressive strides in natural language processing, but their ability to reason remains a hot topic. This blog post delves into the fascinating world of reasoning in LLMs, exploring the techniques, evaluations, and key findings that are shaping this field.

### What is Reasoning?
Reasoning is the process of using evidence, logic, and past experiences to form conclusions or make decisions. It's a fundamental aspect of human intelligence, allowing us to solve problems, think critically, and understand the world around us.   There are different types of reasoning, including:   

- Deductive reasoning: Drawing a conclusion based on the truth of the premises (e.g., if all mammals have kidneys and all whales are mammals, then all whales have kidneys).    
- Inductive reasoning: Drawing a conclusion based on observations or evidence (e.g., if every winged creature we've seen is a bird, then a new winged creature is likely a bird).    
- Abductive reasoning: Drawing a conclusion based on the best explanation for a set of observations (e.g., if the car won't start and there's a puddle under it, then the car probably has a leak).    

### Techniques for Enhancing Reasoning in LLMs
Researchers are constantly developing new techniques to improve or elicit reasoning in LLMs. Some of the most promising methods include:

- Fully supervised fine-tuning: This involves fine-tuning a pre-trained LLM on a dataset containing explicit reasoning examples. For instance, a model could be trained to generate rationales explaining its predictions.    
- Prompting and in-context learning: This approach involves prompting LLMs with a question and a few examples of how to solve similar questions. Chain-of-thought prompting is a popular technique where the examples include intermediate reasoning steps, guiding the LLM to generate its own reasoning process.
  - Prompting & In-Context Learning: in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ tripples
    - manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation
  -  Rationale Engieering： creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs.
    -  Rationale refinement
      -  complexity-based prompting to create rationales with more reasoning steps. Their experiments show that the performance of LLMs improves with the increased rationale complexity
      -  algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations
    - Rationale exploration: decoding strategy, sampling a divese set of rationale, instead of the greedy one
    - Rationale verification

![image](https://github.com/user-attachments/assets/3cda04b5-8ce4-4149-910e-920c0113efa0)
    
- Hybrid methods: These methods combine techniques like pre-training or fine-tuning LLMs on datasets that include reasoning, along with prompting techniques to elicit reasoning.
  - LLMs trained on datasets containing scientific and mathematical data can achieve better performance on reasoning tasks like quantitative reasoning problems when using CoT prompting.
  - bootstrapping & self improving: using LLMs to self-improve their reasoning abilities through a process known  as bootstrapping.
      - Specifically, with CoT prompting, the model first generates initial rationales. And then, the model is finetuned on rationales that lead to correct answers. This process can be repeated, with each iteration resulting in an improved model that can generate better training data.

### Evaluating Reasoning in LLMs
Evaluating the reasoning abilities of LLMs is crucial. Researchers use various methods and benchmarks to assess their performance, including:

- End task performance: This involves measuring the accuracy of LLMs on **tasks requiring reasoning, such as arithmetic, commonsense, and symbolic reasoning benchmarks.**    
- Analysis of reasoning: This approach focuses on directly assessing the reasoning steps taken by LLMs, rather than just the final answer. This can involve analyzing the quality of the generated rationales or using formal metrics to evaluate the reasoning process.

### Key Findings and Implications
Research in reasoning in LLMs has yielded some interesting findings:

- Emergent ability: Reasoning seems to be an emergent ability of LLMs, **becoming more pronounced as the models get larger (around 100 billion parameters or more).**    
- Chain-of-thought prompting: This technique has been shown to significantly improve the performance of LLMs on various reasoning tasks.    
- Complex reasoning challenges: Despite progress, LLMs still struggle with complex reasoning tasks, suggesting that current benchmarks might be too simple.

### Open Questions and Future Directions
The field of reasoning in LLMs is still evolving, with many open questions and exciting avenues for future research:

- True reasoning or mimicry?: Are LLMs truly capable of reasoning, or are they simply learning to mimic human reasoning through pattern recognition?    
- Improving reasoning capabilities: How can we further enhance the reasoning capabilities of LLMs? This could involve developing new training methods, model architectures, or prompting techniques.

By addressing these questions and continuing to explore the intricacies of reasoning in LLMs, we can unlock their full potential and pave the way for more intelligent and reliable language-based AI systems.
