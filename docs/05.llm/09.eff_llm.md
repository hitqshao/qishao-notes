---
title: Efficient LLM
date: 2025-01-20 23:32:49
permalink: /pages/dc7043/
---

[1] Resource-efficient Algorithms and Systems of Foundation Models: A Survey

---

![image](https://github.com/user-attachments/assets/539f3a36-1e67-486a-a1af-d7b637cfaf8e)
*Source: Resource-efficient*

Computation complexity of attention is O(T 2D), whereas that of the FFN is O(TD2), where T represents the sequence length and D the hidden state dimension of the model.\
The FFN layer is the most computationally intensive component.


## 1. Resource-Efficient Architectures
### 1.1 Efficient Attention

![image](https://github.com/user-attachments/assets/d87f3dae-6a64-46ed-af28-51b4f2c16f3d)

- **Sparse Attention**: Reduces complexity (e.g., Longformer, BigBird).\
Motivated by graph sparsification, sparse attention aims to build a sparse attention matrix.\
This approach aims to retain the empirical advantages of a fully quadratic self-attention scheme while employing a reduced number of inner products.

- **Approximate Attention**: Low-rank approximations (e.g., Linformer, Reformer).
Approximate attention mainly includes low-rank approximations of the self-attention matrix and innovative reformulations of the self-attention.

- **Attention-Free Approaches**: Alternatives like Hyena, Mamba.\
Despite the dominance of attention-based transformer architectures in large FMs, several works have put forth innovative architectures that hold the potential to replace the traditional transformer model.

### 1.2 Dynamic Neural Network

![image](https://github.com/user-attachments/assets/baefb07e-0c3b-46c2-a728-5cb2e94a21b5)

- **Mixture of Experts (MoE)**: (e.g., Switch Transformer, GLaM, MoEfication, FFF).

- **Early Exiting**: Premature termination based on confidence (e.g., FREE, SkipDecode,DeeBERT, PABEE).
early-exiting optimization is a strategy that allows a model to terminate its computational process prematurely when it attains high confidence in the prediction or encounters resource constraints.

### 1.3 Diffusion-Specific Optimization
- **Efficient Sampling**
- **Diffusion in Latent Space**
- **Diffusion Architecture Variants**

### 1.4 ViT-Specific Optimizations
- **Efficient ViT Variants**: MobileViT, EfficientFormer, EdgeViT.

## 2. Resource-Efficient Algorithms
### 2.1 Pre-Training Algorithms
- **Training Data Quality Control**: DataComp, DFN.\
A portion of work focus on controlling the quality of training data.
- **Training Data Reduction**: Deduplication, image patch removal.\
Pre-training for large FMs needs a dataset at the trillion scale, exemplified by 0.3 trillion tokens for GPT-3-175B [25] and 2 trillion tokens for LLaMa-2-70B [238].\
prior literature resorts to reduce vast training data through two aspects: deduplicating text datasets and image patch removal.
- **Progressive Learning**: StackingBERT, CompoundGrow.
Progressive learning is a training strategy that begins by training a small model and then gradually increases the model size, throughout the training process.
- **Mixed Precision Training**: Mesa, GACT.
Mixed precision training often utilizes half-precision floating-point data representation instead of single precision. This approach significantly reduces memory
requirements, approximately halving the storage space needed for weights, activations, and gradients.

### 2.2 Fine-Tuning Algorithms

![image](https://github.com/user-attachments/assets/1ed489c1-3451-4874-8f36-fa99fc65e0ca)

* **Additive Tuning**:
  - *Adapter tuning* aims to reduce training costs by introducing adapter modules to specific layers (or all layers) of pre-trained large FMs.
    During tuning, the backbone of the pre-trained model remains frozen, and adapter modules are utilized to acquire task-specific knowledge.
  - *prompt tuning* involves designing a task-specific prompt for each task, with the aim of replacing the traditional fine-tuning of pre-trained large FMs parameters.\
    By tuning the input prompts instead, this method significantly reduces the resources and time required for the fine-tuning.
  - *prefix tuning* introduces a trainable, task-specific prefix part to each layer of large FMs.
    This technique aims to reduce the tuning cost by limiting the updates to the parameters in this prefix.
* **Selective Tuning**: Freezing most parameters, selective updates.
* **Re-parameter Tuning**: Low-rank adaptation (e.g., LoRA, Delta-LoRA).

### 2.3 Inference Algorithms
- **Opportunistic Decoding**: Speculative decoding, look-ahead decoding.
- **Input Filtering and Compression**: Prompt compression, token pruning.
- **KV Cache Optimization**: Memory-efficient sparse attention.
- **Long Context Handling**: LM-Infinite, StreamingLLM.

### 2.4 Model Compression
- **Pruning**: Structured, unstructured, and contextual pruning.
- **Knowledge Distillation**: Black-box and white-box KD.
- **Quantization**: Quantization-aware training (QAT), post-training quantization (PTQ).
- **Low-Rank Decomposition (LoRD)**: TensorGPT, LoSparse.

## 3. Resource-Efficient Systems
### 3.1 Distributed Training
- **Resilience**: Checkpointing, redundant computation.
- **Parallelism**: Data, model, and sequence parallelism.
- **Communication**: Compression, overlapping with computation.
- **Storage**: Offloading, heterogeneous GPUs.
- **MoE Optimization**: MegaBlocks, Tutel.

### 3.2 Hardware-Aware Optimizations
- **EdgeBERT**: Latency-aware energy optimization.
- **FlightLLM**: FPGA-based LLM inference.
- **SpAtten**: Sparse attention with cascade token pruning.

### 3.3 Serving on Cloud
- **Inference Accelerating**: Kernel optimization, request batching.
- **Memory Saving**: KV cache management, offloading.
- **Emerging Platforms**: Spot instances, heterogeneous GPUs.

### 3.4 Serving on Edge
- **Edge-Cloud Collaboration**: EdgeFM.
- **On-Device MoE**: EdgeMoE, PC-MoE.
- **Memory Optimization**: LLMCad, PowerInfer.
- **I/O Optimization**: STI, LLM in a flash.
- **Kernel Optimization**: Integer-based edge kernels.
