---
title: Efficient LLM
date: 2025-01-20 23:32:49
permalink: /pages/dc7043/
---

[1] Resource-efficient Algorithms and Systems of Foundation Models: A Survey

---

![image](https://github.com/user-attachments/assets/539f3a36-1e67-486a-a1af-d7b637cfaf8e)

*Source: Resource-efficient*

Computation complexity of attention is O(T 2D), whereas that of the FFN is O(TD2), where T represents the sequence length and D the hidden state dimension of the model.\
The FFN layer is the most computationally intensive component.


## 1. Resource-Efficient Architectures
### 1.1 Efficient Attention

![image](https://github.com/user-attachments/assets/d87f3dae-6a64-46ed-af28-51b4f2c16f3d)

- **Sparse Attention**: Reduces complexity (e.g., **Longformer[C4522 2020]**, BigBird).\
Motivated by graph sparsification, sparse attention aims to build a sparse attention matrix.\
This approach aims to retain the empirical advantages of a fully quadratic self-attention scheme while employing a reduced number of inner products.

[C2016 2019] Generating Long Sequences with Sparse Transformers\
[C602 2020] Efficient Content-Based Sparse Attention with Routing Transformers
[C136 2021] Scatterbrain: Unifying Sparse and Low-rank Attention Approximation
[C55 2021] Is Sparse Attention more Interpretable?

- **Approximate Attention**: Low-rank approximations (e.g., Linformer, Reformer).
Approximate attention mainly includes low-rank approximations of the self-attention matrix and innovative reformulations of the self-attention.

- **Attention-Free Approaches**: Alternatives like Hyena, Mamba.\
Despite the dominance of attention-based transformer architectures in large FMs, several works have put forth innovative architectures that hold the potential to replace the traditional transformer model.

### 1.2 Dynamic Neural Network

![image](https://github.com/user-attachments/assets/baefb07e-0c3b-46c2-a728-5cb2e94a21b5)

- **Mixture of Experts (MoE)**: (e.g., Switch Transformer, GLaM, MoEfication, FFF).
  - *C1950 2021* Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
  - *Janus[C20 2023]* Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models
  - *C124 2018* Deep Mixture of Experts via Shallow Embedding
  - *C364 2013* Learning Factored Representations in a Deep Mixture of Experts
  - *C264 2022* Mixture-of-Experts with Expert Choice Routing
  - *C594 2022* GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
  - *C113 2021* MoEfication: Transformer Feed-forward Layers are Mixtures of Experts
- **Early Exiting**: Premature termination based on confidence (e.g., FREE, SkipDecode,DeeBERT, PABEE).
early-exiting optimization is a strategy that allows a model to terminate its computational process prematurely when it attains high confidence in the prediction or encounters resource constraints.
[C342 2020] BERT Loses Patience: Fast and Robust Inference with Early Exit
[C383 2020] DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference
[C46 2023] SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference

### 1.3 Diffusion-Specific Optimization
- **Efficient Sampling**
- **Diffusion in Latent Space**
- **Diffusion Architecture Variants**

### 1.4 ViT-Specific Optimizations
- **Efficient ViT Variants**: MobileViT, EfficientFormer, EdgeViT.

## 2. Resource-Efficient Algorithms
### 2.1 Pre-Training Algorithms
- **Training Data Quality Control**: DataComp, DFN.\
A portion of work focus on controlling the quality of training data.
- **Training Data Reduction**: Deduplication, image patch removal.\
Pre-training for large FMs needs a dataset at the trillion scale, exemplified by 0.3 trillion tokens for GPT-3-175B [25] and 2 trillion tokens for LLaMa-2-70B [238].\
prior literature resorts to reduce vast training data through two aspects: deduplicating text datasets and image patch removal.
- **Progressive Learning**: StackingBERT, CompoundGrow.
Progressive learning is a training strategy that begins by training a small model and then gradually increases the model size, throughout the training process.
- **Mixed Precision Training**: Mesa, GACT.
Mixed precision training often utilizes half-precision floating-point data representation instead of single precision. This approach significantly reduces memory
requirements, approximately halving the storage space needed for weights, activations, and gradients.

### 2.2 Fine-Tuning Algorithms

![image](https://github.com/user-attachments/assets/1ed489c1-3451-4874-8f36-fa99fc65e0ca)

* **Additive Tuning**:
  - *Adapter tuning* aims to reduce training costs by introducing adapter modules to specific layers (or all layers) of pre-trained large FMs.
    During tuning, the backbone of the pre-trained model remains frozen, and adapter modules are utilized to acquire task-specific knowledge.
  - *prompt tuning* involves designing a task-specific prompt for each task, with the aim of replacing the traditional fine-tuning of pre-trained large FMs parameters.\
    By tuning the input prompts instead, this method significantly reduces the resources and time required for the fine-tuning.
  - *prefix tuning* introduces a trainable, task-specific prefix part to each layer of large FMs.
    This technique aims to reduce the tuning cost by limiting the updates to the parameters in this prefix.
* **Selective Tuning**: Freezing most parameters, selective updates.
  Selective tuning aims to maintain high performance on new tasks with low training costs by freezing the majority of parameters in large FMs and selectively updating only a small portion of the parameters.
* **Re-parameter Tuning**: Low-rank adaptation (e.g., **LoRA**, Delta-LoRA).
  ![image](https://github.com/user-attachments/assets/920fd758-54f1-492c-bf1e-0a5b4209f2b4)

  Re-parameter tuning adapts large FMs by targeting a significantly smaller subspace than the original, expansive training space.\
  This approach involves fine-tuning low-rank matrix parameters, a technique that effectively reduces the overall training cost.

### 2.3 Inference Algorithms
- **Opportunistic Decoding**:
  - Speculative decoding (*SpecInfer, LLMCad*) generating sequences autoregressively with a cost-efficient small model, followed by parallel token verification using a larger model.
  - Look-ahead decoding accelerates inference in large FMs without relying on a draft model or data store, reducing decoding steps in proportion to log(FLOPs).
- **Input Filtering and Compression**:
  - Prompt compression(LLMLingua,LLMZip,ICAE,COT-Max)
    LLMZip [241] employs LLaMA-7B for compressing natural language. Experimental results demonstrate that LLMZip outperforms cutting-edge text compression methods, including BSC, ZPAQ, and paq8h.
  - Token pruning Pruning of input sequences for transformers, often involving the incremental removal of less important tokens during inference.
    
- **KV Cache Optimization**: Memory-efficient sparse attention.
  most sparse attention designs, which primarily target the reduction of computational complexity [24, 294], do not necessarily lead to a reduction in KV cache memory consumption.\
  This is because achieving a reduced memory footprint for the KV cache necessitates a more stringent sparsity pattern.
  - *H2O* KV cache eviction stragegy: employs attention scores to identify and select the least important KV cache tokens in the current state for eviction
  - *Dynamic Context Pruning* learns a memory-efficient KV cache eviction strategy during the pre-training phase.
  - *Scissorhands*: innovative compact KV cache
  - *Landmark Attention* enables the storage of most KV caches in a slower but larger capacity memory
- **Long Context Handling**: LM-Infinite, StreamingLLM. Due to the quadratic computational cost associated with attention mechanisms, various resource-efficient optimizations have been proposed to handle long inputs.
  - LM-Infinite introduces a Λ-shaped attention mechanism to handle long contexts efficiently.
  - StreamingLLM facilitates large FMs trained with a finite-length attention window to generalize to infinite stream decoding without the need for any fine-tuning.

### 2.4 Model Compression

![image](https://github.com/user-attachments/assets/d3d1214d-8f83-4553-97bd-467a2b914dd4)

- **Pruning**
  - Structured pruning eliminates entire structural components, such as groups of consecutive parameters or hierarchical structures\
    LLM Pruner[C372 2023] selectively removes non-essential model structures based on gradient information and incorporates LoRA to recover the model’s accuracy after pruning.\
    Structured pruning is also employed in training.\
    *Sheared LLaMA* adopts an end-to-end to remove channels, encompassing layers, attention heads, intermediate layers, and hidden layers.\
    *AdaPrune* accelerates neural network training using transposable masks.\
    *GUM* considers neuron specificity and introduces pruning through network component-based global mobility and local uniqueness scores.\
    *PLATON* tackles the uncertainty in importance scores during pruning by employing the upper confidence bound of importance estimation.
  - Unstructred pruning It removes neurons with weights below a threshold, thereby compressing the model.
    *SparseGPT[C497 2023]* sparse regression solver\
    *Wanda[C346 2023]* prunes weights with smallest magnitude multiplied by corresponding input activations.\
    *SIGE* converts computation reduction into latency reduction.
  - **Contextual pruning** selects the sparse state of each layer.
    *Deja Vu* dynamically predicts the sparsity of the next layer using the activations of the previous layer. It determines which neurons of MLP blocks and the heads of attention blocks need to be retained. To mitigate the overhead of this predictor, Deja Vu asynchronously predicts the next layer.\
    *PowerInfer* utilizes the sparsity of activation to dynamically predict the hotactivated neurons of the next layer and computes them on the GPU, whereas other cold-activated neurons are computed on the CPU.
- **Knowledge Distillation**: Black-box and white-box KD.
- **Quantization**: Quantization-aware training (QAT), post-training quantization (PTQ).
  - Quantization-aware training
    *LLM-QAT obtains training data for LLMs by leveraging pre-trained models to generate samples through data-free distillation. it quantizes weights, activations, and KV cache, thereby improving training throughput.\
    QuantGPT incorporats contrastive distillation from a full-precision teacher model and distilling logit information to a quantized student model during autoregressive pre-training.\
    BitNet pioneers QAT for 1-bit language models, training the language model with 1-bit weights and activations.*
  - Post-training quantization
    * Weights-only Quantization
      
      *identification of outliers and important weights in weights that significantly contribute to accuracy and treating these outliers specially.*
      
      *SpQR identifies outlier weights and maintains them with high precision while quantizing the rest of the weights.*
        
      *LLM.int8() employs vectorized quantization and mixed-precision decomposition to handle outlier values for efficient inference.*
        
      *AWQ[C523 2023] reduces quantization error by protecting the top 1% important weights in the model, utilizing per-channel scaling to determine the optimal scaling factor.*
        
      *OWQ[C37 2023] analysis suggests that abnormal activations amplify quantization errors, and it employs a mixed-precision scheme, applying higher-precision quantization to weights with a significant impact from activated outlier values.*
        
      *SqueezeLLM[C160 2023] observes that sensitive weights determine the final model’s quantization performance and proposes a non-uniform quantization approach to minimize quantization errors in these sensitive weights.*
      
    * Weights-Activation Coquantization
   
      *SmoothQuant[C770 2022] takes advantage of the similarity in the channel-wise activations of different tokens and performs quantization on both weight and activation using per-channel scaling transforms.*
      
       *RPTQ recognizes the substantial range differences across different channels, reordering the channels for quantization and integrating them into layer normalization and linear layer weights.*
      
       *OliVe adopts outlier-victim pair quantization and locally processes outliers.*
      
       *Outlier Suppression+ builds upon Outlier Suppression, discovering that harmful outliers exhibit an asymmetric distribution mainly concentrated in specific channels.\
        Considering the asymmetry of outliers and quantization errors from the weights of the next layer, this approach performs channel-level translation and scaling operations.*
      
       *QLLM addresses the issue of activation outliers through an adaptive channel reassembly method and mitigates the information loss caused by quantization using calibration data.*
      
       *LLM-FP4 quantizes weights into 4-bit float points, proposes per-channel activation quantization, and reparameters additional scaling factors as exponential biases of weights.*
      
       *ZeroQuant combines layer-wise KD and optimized quantization support to achieve 8-bit quantization.*
      
       *FlexRound updates the quantization scale of weights and activations by minimizing the error between the quantized values and the full-precision values.*
      
       *ATOM significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.*
      
- **Low-Rank Decomposition (LoRD)**: TensorGPT, LoSparse.

## 3. Resource-Efficient Systems
### 3.1 Distributed Training
- **Resilience**: Checkpointing, redundant computation.
- **Parallelism**: Data, model, and sequence parallelism.
- **Communication**: Compression, overlapping with computation.
- **Storage**: Offloading, heterogeneous GPUs.\
  ZeRO-Offload offloads data and computations to CPU to train large models on a single GPU.\
  FlashNeuron[C52 2021] offloads selective data to the SSD for higher throughput.
- **MoE Optimization**: optimizes the dynamism-related mechanisms, parallelism, and communication in MoE training.
    *MegaBlocks[C74 2022] leverages sparse primitives to handle dynamic routing and load-imbalanced computation.*
      
    *FlexMoE focuses on the dynamic expert management and device placement problem.*
  
    *Tutel designs dynamic adaptive parallelism and pipelining strategies.*

### 3.2 Hardware-Aware Optimizations
- **EdgeBERT**: Latency-aware energy optimization.
- **FlightLLM**: FPGA-based LLM inference.
- **SpAtten**: Sparse attention with cascade token pruning.
- **A3[C227 2020]**

### 3.3 Serving on Cloud
- **Inference Accelerating**:
    - *FlashAttention, FlashAttention-2*
    - *Flash-Decoding, FlashDecoding++*
    - *DeepSpeed-Inference*
    - *ByteTransformer*
    - *Google PaLM*
  
    **Batching and scheduling**
  
    *Orca proposes selective batching and iteration-level scheduling to batch requests of different lengths at the granularity of iterations to increase the maximum batch size.*

    *FlexGen proposes a request scheduling algorithm to mitigate the impact of offloading on the performance of latencyinsensitive FM serving in a single GPU.*
  
    *FastServe proposes an iteration-level preemptive scheduling and proactive KV cache swapping to mitigate the impact of head-of-line blocking on the performance of distributed FM serving.*
  
    *SARATHI and DeepSpeed-FastGen split the computation of the prefill phase into small chunks and schedule these chunks with the decoding phase to mitigate the impact of the prefill phase on the performance of large FMs serving.*
  
    *Splitwise splits the prefill phase and the decoding phase onto different machines according to their different computation and memory requirements.*
  
    *Sarathi-Serve introduces a chunked-prefills scheduler which splits a prefill request into near equal sized chunks and creates stall-free schedules that adds new requests in a batch without pausing ongoing decodes.*
  
    *dLoRA dynamically merges/unmerges adapters with the base model and migrating requests/adapters between worker replicas, significantly improving the serving throughput.*
  
- **Memory Saving**
    - DeepSpeed-Inference and FlexGen offload activations or model parameters to the DRAM or NVMe memories
    - KV cache managing
      - **vLLM** adopts block-level on-demand memory allocation mechanism.
      - S-LoRA extends this idea to Unified Paging to manage multiple LoRA adapters at the same time.
- **Emerging Platforms**: Spot instances, heterogeneous GPUs.

### 3.4 Serving on Edge
- **Edge-Cloud Collaboration**
  
  *EdgeFM* queries and adapts the large FMs to the specific edge models with customized knowledge and architectures so that the dynamic edge model can ensure both low latency and close accuracy to the original large FMs.
- **On-Device MoE**
  
  *EdgeMoe identifies the problem that experts have to be dynamically loaded into memory during inference. To tackle this issue, this approach proposes expert-wise bit-width adaptation to reduce the size of expert parameters with acceptable accuracy loss, saving parameters’ loading time.*

  *PC-MoE is based on a crucial observation that expert activations are subject to temporal locality. Based on this observation, PC-MoE proposes Parameter Committee, which intelligently maintains a subset of crucial experts in use to reduce resource consumption.*
  
  [C21 2024] Mobile foundation model as firmware
  
- **Memory Optimization**: LLMCad, **PowerInfer**.
  
  *LLMCad utilizes speculative decoding, which can offload most workloads to a smaller memory-resident draft model.*

  *PowerInfer relies on large FMs runtime sparsity (i.e., only hot neurons are consistently activated across inputs). PowerInfer pre-loads hot-activated neurons onto the GPU for fast access, whereas cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPUGPU data transfers.*
  
- **I/O Optimization**: STI, LLM in a flash.
  *STI [C16] identifies that loading parameters time is highly longer than computation time. To address this problem, STI proposes dynamically adapting weights bit-width during the loading procedure according to parameters importance, minimizing loading overhead under maximum inference accuracy.*
- **Kernel Optimization**: Integer-based edge kernels.
