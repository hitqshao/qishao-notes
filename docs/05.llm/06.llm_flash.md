---
title: llm flash algorthms
date: 2024-11-05 23:32:49
permalink: /pages/dc7040/
---

1. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective


---
### 1. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective

#### Optimizations on Hardware Platforms
##### Quantization

Data Format
- Uniform Quantization
- Non-uniform Quantization

Granularity
- group-wise: Group-wise granularity is a coarser approach where multiple channels or layers are quantized with the same parameters.
- channel-wise: Channel-wise granularity involves quantizing each channel individually within the model.
- tensor-wise: Tensor-wise granularity is the most detailed approach, where each tensor (such as weight tensors or activation tensors) is quantized separately.

**Weight-Only Quantization**

Uniform & Norn Uniform

Matrix decomposition quantization is a specialized method where a large matrix is approximated by the product of several smaller matrices.

**Weight-Activation Quantization**

Quantization include the activations generated during model inference.

In this method, both the weights and the activations at each layer are quantized to lower precision formats.

This reduces memory bandwidth requirements and enhances inference speed.

The challenge with weight-activation quantization is to manage the trade-off between quantization errors and model accuracy.

Techniques such as dynamic range quantization or specific quantization schemes are used to balance precision and computational efficiency.

![image](https://github.com/user-attachments/assets/c7bae637-2223-4d24-88de-c89d3252e34a)

#### Weight-Only Quantization

Shen et al. [97] leverage Intel Neural Compressor to automate the INT4 quantization process with negligible accuracy loss, supporting various quantization recipes such as **GPTQ**, **AWQ** and **TEQ**.

Due to the overheads of weight dequantization from integer to floating, T-MAC leverages lookup tables (LUTs) for efficient low-bit LLM inference on edge CPUs, circumventing the need for dequantization and mixed precision matrix multiplication.




 
