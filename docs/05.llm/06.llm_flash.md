---
title: llm flash algorthms
date: 2024-11-05 23:32:49
permalink: /pages/dc7040/
---

1. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective
2. Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward
3. [51] A Systematic Survey of Resource-Efficient Large Language Models :+1: :+1: :+1: :+1: :+1:


---
### 1. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective

#### Optimizations on Hardware Platforms
##### Quantization

Data Format
- Uniform Quantization
- Non-uniform Quantization

Granularity
- group-wise: Group-wise granularity is a coarser approach where multiple channels or layers are quantized with the same parameters.
- channel-wise: Channel-wise granularity involves quantizing each channel individually within the model.
- tensor-wise: Tensor-wise granularity is the most detailed approach, where each tensor (such as weight tensors or activation tensors) is quantized separately.

**Weight-Only Quantization**

Uniform & Norn Uniform

Matrix decomposition quantization is a specialized method where a large matrix is approximated by the product of several smaller matrices.

**Weight-Activation Quantization**

Quantization include the activations generated during model inference.

In this method, both the weights and the activations at each layer are quantized to lower precision formats.

This reduces memory bandwidth requirements and enhances inference speed.

The challenge with weight-activation quantization is to manage the trade-off between quantization errors and model accuracy.

Techniques such as dynamic range quantization or specific quantization schemes are used to balance precision and computational efficiency.

![image](https://github.com/user-attachments/assets/c7bae637-2223-4d24-88de-c89d3252e34a)

#### Weight-Only Quantization

Shen et al. [97] leverage Intel Neural Compressor to automate the INT4 quantization process with negligible accuracy loss, supporting various quantization recipes such as **GPTQ**, **AWQ** and **TEQ**.

Due to the overheads of weight dequantization from integer to floating, T-MAC leverages lookup tables (LUTs) for efficient low-bit LLM inference on edge CPUs, circumventing the need for dequantization and mixed precision matrix multiplication.


**GPTQ**
GPTQ is an one-shot weight quantization method based on approximate second-order information and error compensation, that is both highly-accurate and highly-efficient.\
It can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3-bit or 4-bit per weight, with negligible accuracy degradation relative to the uncompressed baseline.

**AWQ**
AWQ is based on the observation that protecting 1% of salient weights whose activations are extremely large can greatly reduce quantization error.\
It first searches for the optimal per-channel scaling and then multiplies the salient weights with the per-channel scalings.\
It also reduces the bitwidth down to 3 or 4 bits per weight.

**SpQR**
To further reduce the accuracy loss for smaller models in the 1-10B parameter range, SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision like half data type (16-bit), while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs.

**SqueezeLLM**
SqueezeLLM proposes a sensitivity-based non-uniform quantization method, which searches for the optimal bit precision assignment based on second-order information.\
It also applies dense and sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format.

**LLM-MQ**
LLM-MQ proposes sensitivity-based precision allocation to assign the proper bitwidth for each layer within the given budget for weight memory based on their first-order information and quantization error.
It also develops an efficient CUDA core kernels to accelerate LLMs by fusing the dequantization and general matrix-vector multiplication (GEMV).

**APTQ**
APTQ proposes an attention-aware 2/4-bit mixed-precision quantization for LLMs, which considers not only the second-order information of each layer’s weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model.

**LUT-GEMM**
LUT-GEMM proposes an efficient LUT-based GPU kernel for quantized matrix multiplication, which not only eliminates the resource-intensive dequantization process but also reduces computational costs compared to previous kernels for weight-only quantization.

**FLUTE**
FLUTE is a flexible lookup table engine for LUT-quantized LLMs, which uses offline restructuring of the quantized weight matrix to minimize bit manipulations associated with unpacking, and vectorization and duplication of the lookup table to mitigate shared memory bandwidth constraints.

#### Weight-Activation Quantization

In addition to hardware units that support FP16 computations, NVIDIA GPUs also provide hardware units that support INT4, INT8, and FP8 computations.\
The number of these computation units can be 2× and 4× greater than FP16 on each chip.\
Compared to weight-only quantization, weight-activation quantization can utilize INT4, INT8, and FP8 computations, thereby maximizing the peak computational performance of the GPU.\

**Since the prefill phase in LLM inference is compute-bound, weight-activation quantization can significantly enhance performance during this stage.**

**LLM.int8**
LLM.int8 uses vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features.

**SmmothQuant**
SmoothQuant enables 8-bit weight and 8-bit activation (W8A8) quantization for LLMs.

**QUIK**
QUIK is for the first time, that the majority of inference computations for LLMs can be performed with both weights and activations being cast to 4 bits.

Prevalent quantization schemes (e.g., W8A8) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.

Atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.

#### Quantitative Comparison

For CPUs, power consumption ranges from 3W to 385W, with inference speeds between 3 tokens/s and 50 tokens/s, located in the bottom part of the figure.

For GPUs, power consumption ranges from 40W to 450W, with inference speeds between 18 tokens/s and 194 tokens/s, situated in the upper right part of the figure.

![image](https://github.com/user-attachments/assets/65194934-51e3-4bcc-a3ca-1c86fdd6ea23)

#### Sparsity
Sparsity patterns can be categorized into random and structured sparsity as shown in Figure 5.

![image](https://github.com/user-attachments/assets/4cae57a4-8489-43c3-96f3-7f279f959787)

Random pattern involves a random distribution of zero elements within the matrix, achieving higher accuracy but potentially lower speed for computation.\
Structured pattern applies a specific pattern to the sparsity, improving computational efficiency by aligning with hardware optimizations.\
Within structured sparsity, common patterns include block-wise sparsity, N:M sparsity, channel-wise sparsity and some combinations of structured pattern sparsity.

#### GPU
##### Weight Sparsity
##### Attention Sparsity

**Static Sparisty**

- Sparse Transformer
- StreamingLLM
- Bigbird
- Longformer

Above schemes use the naumal combination of global and local patterns to replace the full attention patterns.

**Dynamic Sparsity*

- Adaptive Sparse Attention
- Reformer
- Sparse Flash Attention
- Sparse Sinkhorn Attention
- H2O Heavy Hitters

#### Speculative Decoding

Speculative decoding is proposed to overcome the inherently sequential process in the autoregressive decoding of LLM.\
The essential decoding mechanism is to make predictions (i.e., draft tokens) parallelly for multiple time steps and then select the longest prefix verified by a scoring model as the final output.

- Lookahead decoding
- Medusa
- Eagle
- Ouroboros
- Sequoia
- Draft&Verify
- Kangaroo
- LayerSkip
- LLAMA

#### Skip Layer

- AdaInfer
  AdaInfer statistically analyzes the activated layers across tasks and proposes a simple algorithm to determine the inference termination moment based on the input instance adaptively.
- RAEE
  RAEE proposes to build the retrieval database to store the token information offline and leverages the information of the retrieved similar token by searching the pre-built retrieval database to guide the backbone model to exit at the layer.
- MOD
  MOD decides whether to skip the current layer or not by pretraining the model to add a router in each layer like Mixture-of-Experts.

---
### 2. Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward

#### Compression of LLMs

- Architecture Pruning
- Quantization
- Knowledge distillation
- Low-rank decomposition

#### System Level Approaches

- **Paged Attention** inspired by the classical virtual memory and paging techniques in operating systems, it allows storage of continuous keys and values cached in non-contiguous memory.
- **Tensor Parallelisim** entails dividing a tensor into shards distributed across various GPUs, processing each shard independently and in parallel, and subsequently synchronizing the
results at the end of the step. 
- **Pipeline Parallelism** allows a model to be vertically split across multiple GPUs at the layer level, where each GPU handles one or several layers, enabling parallel processing of distinct stages in the pipeline.
- **GPU/CPU Offloadding** [Song et al., 2023]- involves transferring specific weight layers to GPU devices for matrix multiplication, subsequently transmitting the computed results back to the secondary device (RAM), thus optimizing parallel processing capabilities while allowing the secondary device to handle the remaining memory intensive computations
- **Flash Attention** optimizes attention computation by employing incremental softmax reduction through input block tiling, avoiding the need for whole-input access, and expedites the backward pass by storing the softmax normalization factor from the forward pass, eliminating the requirement to read the large attention matrix from high bandwidth memory (HBM). 
- **Fused Operations** involves consolidating multiple computational tasks, such as combining existing kernels or creating new ones, to minimize the overhead associated with multiple kernel API invocations.
- **Speculative Decoding** efficiently generates multiple future tokens from a chosen smaller model and verifies them in parallel using the larger model, enabling the simultaneous decoding of multiple tokens per step.


 
