---
title: LLM Sparsity
date: 2025-02-03 23:32:49
permalink: /pages/dc7051/
---

1. [C93 2021] Sparse is Enough in Scaling Transformers
2. [C78 2023] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers

---

## 1. [C93 2021] Sparse is Enough in Scaling Transformers

pretrained on C4.

**Key Contributions:**

1. **Sparse Feedforward Layers:** 
    - Uses dynamic sparsity by activating only a subset of weights during inference.
    - Reduces computational cost while maintaining similar perplexity to dense models.

![image](https://github.com/user-attachments/assets/18175da6-5e32-4c01-88d7-36fcb8342a04)
 
2. **Sparse Attention (QKV) Layers:**  
    - Uses a **multiplicative layer**  to efficiently represent any permutation of input tokens.
    - Introduces convolution-based sparsity to further reduce the computational burden.



Following figure (a) shows the multiplicative dense layer:

![image](https://github.com/user-attachments/assets/69fde5c5-a906-429e-b6f0-073691afdf3b)

![image](https://github.com/user-attachments/assets/669ddf38-eaad-4f86-b4c6-27e47f80206e)

Please notice the "S" in formula and the output of the blue block is S*M dimension.

![image](https://github.com/user-attachments/assets/90a4879f-1c9d-4a89-89a5-cf7e5f96f02c)

> We process this tensor with a two-dimensional convolutional layer, treating the length dimension and number of modules S like height and width of an image.
> This layer uses M filters and a kernel size of F × F so that each filter looks at F modules (‘S’ axis) of the last F tokens
(‘length’ axis).
 
3. **Speed and Efficiency Gains:**  
    - Sparse feedforward and QKV layers achieve a **3.05× speedup**  in decoding for an 800M parameter model and **20× speedup**  for a 17B parameter model. 
    - The full **Terraformer**  model (with sparse layers and reversible architecture) achieves **37× speedup**  in inference for large models.

---

## 2. [C78 2023] The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers

**Summary of "The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers"** This paper explores a surprising phenomenon in Transformer models: **activation sparsity** . The authors observe that in trained Transformers, only a **small fraction of neurons**  in the feedforward layers are activated for any given input. This **emergent sparsity**  increases as the model size grows, raising questions about computational efficiency, robustness, and generalization.

**Key Findings**  
1. **Activation Sparsity is Ubiquitous**  
  - In trained Transformer models like **T5 and ViT** , on average, only **3.0% of neurons in T5-Base**  and **6.3% in ViT-B16**  are activated after the ReLU function.
  - Sparsity **increases**  as the model **gets deeper and wider** .
  - This phenomenon holds across **NLP and vision models** , for both **training and evaluation data** .

![image](https://github.com/user-attachments/assets/11ee0496-3cf9-4b36-8e11-5c56876e34d9)


2. **Sparsity is Not Explicitly Enforced—It Emerges Naturally**  
  - Unlike traditional sparsity techniques that use explicit regularization, **this sparsity arises naturally**  through training without any special constraints. 
  - Similar to **biological brains** , where only a subset of neurons fire at any given time.
 
3. **Sparsity Can Reduce Computation Costs**  
  - Since most activations are **zero** , many FLOPs (floating-point operations) in MLP layers can be **skipped** , improving efficiency. 
  - A **Top-k Transformer**  (where only the **k most active neurons**  are used) further reduces computational overhead **without degrading performance** .
 
4. **Sparsity Improves Robustness and Calibration**  
Models with enforced sparsity (e.g., Top-k Transformers) **perform better**  on: 
    - **Noisy training data**  (label corruption)
    - **Adversarial input perturbations**
    - **Confidence calibration**  (better uncertainty estimation)

![image](https://github.com/user-attachments/assets/80a351d3-cabe-4ce1-adb0-6cb1938c3a3a)

![image](https://github.com/user-attachments/assets/d870f16b-64b9-4877-ba0c-9f1934cb0d37)

![image](https://github.com/user-attachments/assets/e86428ea-d02e-4119-b95b-da8ccdc48c21)


**Implications**  
- **Efficiency Gains** : Exploiting this natural sparsity could reduce the computational cost of Transformer inference and training.
- **Robustness** : Sparse activations help the model generalize better and resist overfitting.
- **Theoretical Insights** : Suggests a link between biological sparsity and deep learning efficiency.

> we emphasize that the sparsity in Top-k Transformers is unstructured and data-dependent, which is not well supported on existing computation hardwares such as TPUs and GPUs.
> Hence, the results in Figure 6 are for proof-of-concept purposes, and are far from fully realizing the benefit of FLOPs reduction via sparsity.
> We leave a study of better implementation of sparse computation for obtaining higher wall time reduction to future work.

**The first MLP layer FLOP in the first MLP layer may be reduced to have sublinear complexity in dff.**

![image](https://github.com/user-attachments/assets/672e3686-1813-4fb0-a154-972efe98c42a)
