---
title: LLM Sparsity
date: 2025-02-03 23:32:49
permalink: /pages/dc7051/
---

1. [C93 2021] Sparse is Enough in Scaling Transformers

---

## [C93 2021] Sparse is Enough in Scaling Transformers

pretrained on C4.

**Key Contributions:**

1. **Sparse Feedforward Layers:** 
    - Uses dynamic sparsity by activating only a subset of weights during inference.
    - Reduces computational cost while maintaining similar perplexity to dense models.

![image](https://github.com/user-attachments/assets/18175da6-5e32-4c01-88d7-36fcb8342a04)
 
2. **Sparse Attention (QKV) Layers:**  
    - Uses a **multiplicative layer**  to efficiently represent any permutation of input tokens.
    - Introduces convolution-based sparsity to further reduce the computational burden.



Following figure (a) shows the multiplicative dense layer:

![image](https://github.com/user-attachments/assets/69fde5c5-a906-429e-b6f0-073691afdf3b)

![image](https://github.com/user-attachments/assets/669ddf38-eaad-4f86-b4c6-27e47f80206e)

Please notice the "S" in formula and the output of the blue block is S*M dimension.

![image](https://github.com/user-attachments/assets/90a4879f-1c9d-4a89-89a5-cf7e5f96f02c)

> We process this tensor with a two-dimensional convolutional layer, treating the length dimension and number of modules S like height and width of an image.
> This layer uses M filters and a kernel size of F × F so that each filter looks at F modules (‘S’ axis) of the last F tokens
(‘length’ axis).
 
3. **Speed and Efficiency Gains:**  
    - Sparse feedforward and QKV layers achieve a **3.05× speedup**  in decoding for an 800M parameter model and **20× speedup**  for a 17B parameter model. 
    - The full **Terraformer**  model (with sparse layers and reversible architecture) achieves **37× speedup**  in inference for large models.
