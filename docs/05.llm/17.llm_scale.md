---
title: Scaling Law
date: 2025-02-03 23:32:49
permalink: /pages/dc7051/
---

1. [A 2025] s1: Simple test-time scaling :+1:

---

## [A 2025] s1: Simple test-time scaling :+1:

- First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality.

- Second, we develop budget forcing to control test-time compute by forcefully terminating the model’s thinking process or lengthening it by appending “Wait” multiple times to the model’s generation when it tries to end.

This can lead the model to double-check its answer, often fixing incorrect reasoning steps.

After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questionsby up to 27% (MATH and AIME24).

![image](https://github.com/user-attachments/assets/b24f8dba-b210-4b4a-abd4-93fa1f8ac594)

Finetune train 26 mins on 1,000 carefully curated questions.

(I) If the model generates more thinking tokens than a desired limit, we forcefully end the thinking process by appending an end-of-thinking token delimiter.
Ending the thinking this way makes the model transition to generating its answer.\
(II) If we want the model to spend more test-time compute on a problem, we suppress the generation of the end-of-thinking token delimiter and instead append “Wait” to the model’s current reasoning trace to encourage more exploration.

![image](https://github.com/user-attachments/assets/ad80b3f2-81d0-4909-82d1-881af722529e)

### Budget Forcing

**Maximum token**

we enforce a maximum token count by simply appending the end-of-thinking token delimiter and “Final Answer: to early exit the thinking stage and make the model provide its current best answer.

**Minimum token**

we suppress the generation of the end-of-thinking token delimiter and optionally append the string “Wait” to the model’s current reasoning trace to encourage the model to reflect on its current generation.


#### benchmark

(I) Conditional length-control methods, which rely on telling the model in the prompt how long it should generate for.\
We group them by granularity into\
  - Token-conditional control: We specify an upper bound of thinking tokens in the prompt;\
  - Step-conditional control: We specify an upper bound of thinking steps, where each step is around 100 tokens;\
  - Class-conditional control: We write two generic prompts that tell the model to either think for a short or long\
amount of time.

(II) Rejection sampling, which samples until a generation fits a predetermined compute budget.

![image](https://github.com/user-attachments/assets/de618a69-a8c5-4ef0-98b2-34a8968a7340)

Sequential scaling via Forcing with S1 is better than Parallel scaling.


- Token-conditional control fails without budget forcing, as our model cannot reliably count tokens - even when trained to do so.
- Under step-conditional control, the model generates a similar total number of tokens when given different step targets, as the model goes from few steps with many tokens per step, to many steps with few tokens in each step.
  Thus, the model learns to hack its way around the compute constraint making the controllability of this method mediocre.
- **Class-conditional control can work** - telling a model to simply think longer can increase its test-time compute and performance, which leads good scaling.

Why does supervised finetuning on just 1,000 samples lead to such performance gains?\
We hypothesize that the model is already exposed to large amounts of reasoning data during pretraining which spans trillions of tokens.\
Thus, the ability to perform reasoning *is already present in our model*.\
Our **sample-efficient finetuning stage just activates it** and we scale it further at test time with budget forcing. This is similar to the "Superficial Alignment Hypothesis"

