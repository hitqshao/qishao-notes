---
title: LLM Attention
date: 2025-02-05 23:32:49
permalink: /pages/dc7055/
---

1. [742] A Multiscale Visualization of Attention in the Transformer Model
2. [1910] What Does BERT Look At? An Analysis of BERT’s Attention
3. [626] Transformer Feed-Forward Layers Are Key-Value Memories :+1:

---

### 1.[742] A Multiscale Visualization of Attention in the Transformer Model

The paper, "A Multiscale Visualization of Attention in the Transformer Model" by Jesse Vig, introduces an open-source tool designed to visualize attention mechanisms in Transformer-based models like BERT and GPT-2.\
The tool provides three distinct views: Attention-head view, Model view, and Neuron view, each offering a unique perspective on how attention operates at different scales within the model.\
The tool aims to make the multi-layer, multi-head attention mechanism more interpretable, helping researchers and practitioners understand how the model assigns weights to different input elements.

#### Key Findings:
##### Attention-head View:

- Function: Visualizes attention patterns produced by one or more attention heads in a given layer.
- Key Insights:
  - Different heads learn unique attention mechanisms.
    For example, some heads focus on **positional patterns (e.g., attending to the previous word), while others capture lexical patterns (e.g., named entities, subject-verb pairs)**.
  - Attention heads can detect **syntactic and semantic relations, such as dependency relations and part-of-speech tags**.
  - Use Case: Detecting model bias, such as gender bias in coreference resolution.
    For instance, the model may associate "She" with "nurse" and "He" with "doctor," indicating potential gender bias.

![image](https://github.com/user-attachments/assets/fb797584-2b55-427b-8198-77b00f3de17e)


##### Model View:

- Function: Provides a high-level overview of attention across all layers and heads for a given input.
- Key Insights:
  - Attention patterns evolve across layers, with some heads focusing on within-sentence patterns and others on between-sentence patterns.
  - The model view helps identify relevant heads for specific tasks, such as paraphrase detection, where heads that draw connections between sentences are particularly useful.
  - Use Case: Locating relevant attention heads for tasks like paraphrase detection, where inter-sentence attention patterns are crucial.

##### Neuron View:

- Function: Visualizes how individual neurons in the query and key vectors interact to produce attention.
- Key Insights:
  - The neuron view reveals how specific neurons contribute to attention patterns, such as distance-decaying attention (where attention decreases with distance from the source token).
  - Certain neurons are responsible for specific attention behaviors, and modifying these neurons could control attention patterns (e.g., adjusting the decay rate for different text complexities).
  - Use Case: Linking neurons to model behavior, allowing for potential interventions to control attention mechanisms.

#### Additional Findings:
##### Attention Patterns:
Attention heads capture a wide range of behaviors, from coarse positional patterns to specific lexical and syntactic patterns.
Some heads are specialized for tasks like coreference resolution, while others focus on syntactic structures or semantic relationships.

##### Model Bias:
The tool can help detect and analyze biases in the model, such as gender bias in coreference resolution, by visualizing attention patterns associated with biased predictions.

##### Intervention Opportunities:
The neuron view provides insights into how specific neurons influence attention, offering opportunities for model intervention.\
For example, modifying neurons responsible for attention decay could adjust the context window size, which might be useful for generating texts of varying complexity.

#### Future Work:
The authors plan to develop a unified interface for navigating all three views within the tool.\
They aim to expose other components of the model, such as value vectors and state activations.\
The tool could be extended to allow users to manipulate the model by modifying attention or editing individual neurons.

#### Conclusion:
The paper presents a powerful visualization tool that enhances the interpretability of Transformer models by providing multiscale views of attention mechanisms.\
The tool not only helps in understanding how attention works in these models but also aids in detecting biases, locating relevant attention heads, and linking neurons to specific model behaviors.\
This work opens up new possibilities for model analysis and intervention, contributing to the broader goal of making complex neural models more transparent and controllable.

---

## 2.[1910] What Does BERT Look At? An Analysis of BERT’s Attention

The paper "What Does BERT Look At? An Analysis of BERT's Attention" by Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning investigates the attention mechanisms within BERT, a large pre-trained Transformer model, to understand what linguistic features it learns from unlabeled data. The authors propose methods for analyzing BERT's attention maps and apply them to uncover patterns and behaviors of its attention heads.

### Key Findings:
#### Attention Patterns:
BERT's attention heads exhibit common patterns such as attending to specific positional offsets, delimiter tokens (e.g., [SEP]), or broadly attending over the entire sentence.

A significant amount of attention is focused on the [SEP] token, which the authors hypothesize acts as a "no-op" (no operation) when the attention head's function is not applicable.

> qualitative analysis (see Figure 5) shows that heads with specific functions attend to [SEP] when the function is not called for.
> For example, in head 8-10 direct objects attend to their verbs.
> For this head, non-nouns mostly attend to [SEP].
> Therefore, we speculate that attention over these special tokens might be used as a sort of “no-op” when the attention head’s function is not applicable.

#### Linguistic Phenomena:
Certain attention heads correspond well to linguistic notions of syntax and coreference. For example, some heads accurately attend to direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions.

The authors propose an attention-based probing classifier, which achieves 77% Unlabeled Attachment Score (UAS) in dependency parsing, indicating that BERT's attention captures substantial syntactic information.

#### Individual Attention Heads:

Individual attention heads specialize in specific syntactic relations, such as prepositions attending to their objects or nouns attending to their determiners, with high accuracy.

One attention head performs well in coreference resolution, particularly for nominal mentions, suggesting that BERT learns some aspects of coreference without explicit supervision.

#### Attention Head Combinations:

The authors propose probing classifiers that combine attention maps from multiple heads, showing that BERT's attention maps collectively capture a thorough representation of English syntax.

#### Clustering of Attention Heads:

Attention heads within the same layer tend to behave similarly, and some heads form clear clusters based on their behavior. This redundancy might be due to attention dropout during training.

### Contributions:
The paper provides a detailed analysis of BERT's attention mechanisms, revealing that BERT learns significant syntactic and coreference information through self-supervised training.

The authors introduce novel methods for probing attention maps, demonstrating that attention-based analysis complements other model analysis techniques.

### Conclusion:
The study shows that BERT's attention mechanisms capture a substantial amount of linguistic knowledge, particularly syntax and coreference, even though the model is not explicitly trained for these tasks.\
The findings suggest that pre-training on large corpora enables BERT to learn complex linguistic structures indirectly, which contributes to its success in various NLP tasks.\
The paper also highlights the importance of analyzing attention maps to better understand what neural networks learn about language.

![image](https://github.com/user-attachments/assets/973c45be-0592-48c5-a8f7-1bf6638e0fe2)

---

## 3. [626] Transformer Feed-Forward Layers Are Key-Value Memories

:+1: *This is a good paper that deserves further reading.*

The authors argue that these layers, which constitute two-thirds of a transformer's parameters, function as key-value memories, where keys detect specific input patterns and values induce distributions over the output vocabulary.

The paper provides empirical evidence to support this claim and explores how these memories contribute to the model's predictions.

### Key Findings:
#### Feed-Forward Layers as Key-Value Memories:
The feed-forward layers in transformers can be viewed as key-value memories, where the first matrix (keys) detects input patterns, and the second matrix (values) induces distributions over the output vocabulary.

Each key correlates with specific textual patterns in the training data, and the corresponding value represents the distribution of tokens likely to follow that pattern.

#### Interpretable Patterns in Keys:
The keys in feed-forward layers capture human-interpretable patterns in the input. Lower layers tend to detect shallow patterns (e.g., n-grams), while upper layers capture more semantic patterns (e.g., topics or contexts).

Experiments show that the top trigger examples for each key share clear, recognizable patterns, and these patterns become more semantic as the layer depth increases.

#### Values as Output Distributions:

The values in feed-forward layers can be interpreted as distributions over the output vocabulary.

In upper layers, these distributions often align with the next-token predictions for the patterns detected by the corresponding keys.

The agreement between the value's top prediction and the next token in the trigger example increases significantly in the upper layers, indicating that these layers are more predictive of the output.

![image](https://github.com/user-attachments/assets/218bede0-9617-447c-ac8b-4b69bf26abee)


#### Memory Composition and Refinement:

The output of a feed-forward layer is a composition of multiple memories, where hundreds of active memories contribute to the final distribution.

The layer's prediction is often different from the predictions of individual memories, suggesting a complex aggregation process.

The model refines its predictions through residual connections across layers. Lower layers often determine the final prediction, while upper layers fine-tune the distribution.

#### Layer-Wise Behavior:

Lower layers focus on shallow patterns and have a higher number of active memories, while upper layers focus on semantic patterns and exhibit fewer active memories.

![image](https://github.com/user-attachments/assets/8cb44c86-6a6e-4082-8a28-99f5f7a26003)


The model's predictions are refined sequentially, with the residual connections playing a crucial role in combining and refining the predictions from each layer.

![image](https://github.com/user-attachments/assets/d78fa44d-6f49-44e4-aed2-62514fa49fc8)

![image](https://github.com/user-attachments/assets/b26d8b9e-bd48-470f-8109-3bb84d9bd0e0)


### Contributions:
The paper provides a novel interpretation of feed-forward layers in transformers as key-value memories, shedding light on their role in the model's predictions.

It demonstrates that feed-forward layers capture interpretable patterns in the input and that these patterns evolve from shallow to semantic as the layer depth increases.

The study shows how the model composes and refines its predictions through the aggregation of memories and residual connections.

### Implications:
The findings open new research directions, including understanding the embedding space transformation across layers, extending the analysis to other transformer models (e.g., BERT), and exploring practical implications for interpretability and data privacy.

The paper suggests that feed-forward layers play a crucial role in the model's ability to capture and predict linguistic patterns, contributing to the overall success of transformer-based language models.

### Conclusion:
The paper advances our understanding of the inner workings of transformers by revealing that feed-forward layers act as key-value memories that detect input patterns and induce output distributions.

This work highlights the importance of feed-forward layers in the model's predictions and provides a foundation for further research into the mechanisms of transformer-based models.


---
