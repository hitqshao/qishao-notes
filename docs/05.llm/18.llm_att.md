---
title: LLM Attention
date: 2025-02-05 23:32:49
permalink: /pages/dc7055/
---

1.[742] A Multiscale Visualization of Attention in the Transformer Model
2.[1910] What Does BERT Look At? An Analysis of BERT’s Attention

---

### 1.[742] A Multiscale Visualization of Attention in the Transformer Model

The paper, "A Multiscale Visualization of Attention in the Transformer Model" by Jesse Vig, introduces an open-source tool designed to visualize attention mechanisms in Transformer-based models like BERT and GPT-2.\
The tool provides three distinct views: Attention-head view, Model view, and Neuron view, each offering a unique perspective on how attention operates at different scales within the model.\
The tool aims to make the multi-layer, multi-head attention mechanism more interpretable, helping researchers and practitioners understand how the model assigns weights to different input elements.

#### Key Findings:
##### Attention-head View:

- Function: Visualizes attention patterns produced by one or more attention heads in a given layer.
- Key Insights:
  - Different heads learn unique attention mechanisms.
    For example, some heads focus on **positional patterns (e.g., attending to the previous word), while others capture lexical patterns (e.g., named entities, subject-verb pairs)**.
  - Attention heads can detect **syntactic and semantic relations, such as dependency relations and part-of-speech tags**.
  - Use Case: Detecting model bias, such as gender bias in coreference resolution.
    For instance, the model may associate "She" with "nurse" and "He" with "doctor," indicating potential gender bias.

![image](https://github.com/user-attachments/assets/fb797584-2b55-427b-8198-77b00f3de17e)


##### Model View:

- Function: Provides a high-level overview of attention across all layers and heads for a given input.
- Key Insights:
  - Attention patterns evolve across layers, with some heads focusing on within-sentence patterns and others on between-sentence patterns.
  - The model view helps identify relevant heads for specific tasks, such as paraphrase detection, where heads that draw connections between sentences are particularly useful.
  - Use Case: Locating relevant attention heads for tasks like paraphrase detection, where inter-sentence attention patterns are crucial.

##### Neuron View:

- Function: Visualizes how individual neurons in the query and key vectors interact to produce attention.
- Key Insights:
  - The neuron view reveals how specific neurons contribute to attention patterns, such as distance-decaying attention (where attention decreases with distance from the source token).
  - Certain neurons are responsible for specific attention behaviors, and modifying these neurons could control attention patterns (e.g., adjusting the decay rate for different text complexities).
  - Use Case: Linking neurons to model behavior, allowing for potential interventions to control attention mechanisms.

#### Additional Findings:
##### Attention Patterns:
Attention heads capture a wide range of behaviors, from coarse positional patterns to specific lexical and syntactic patterns.
Some heads are specialized for tasks like coreference resolution, while others focus on syntactic structures or semantic relationships.

##### Model Bias:
The tool can help detect and analyze biases in the model, such as gender bias in coreference resolution, by visualizing attention patterns associated with biased predictions.

##### Intervention Opportunities:
The neuron view provides insights into how specific neurons influence attention, offering opportunities for model intervention.\
For example, modifying neurons responsible for attention decay could adjust the context window size, which might be useful for generating texts of varying complexity.

#### Future Work:
The authors plan to develop a unified interface for navigating all three views within the tool.\
They aim to expose other components of the model, such as value vectors and state activations.\
The tool could be extended to allow users to manipulate the model by modifying attention or editing individual neurons.

#### Conclusion:
The paper presents a powerful visualization tool that enhances the interpretability of Transformer models by providing multiscale views of attention mechanisms.\
The tool not only helps in understanding how attention works in these models but also aids in detecting biases, locating relevant attention heads, and linking neurons to specific model behaviors.\
This work opens up new possibilities for model analysis and intervention, contributing to the broader goal of making complex neural models more transparent and controllable.

---

## 2.[1910] What Does BERT Look At? An Analysis of BERT’s Attention

The paper "What Does BERT Look At? An Analysis of BERT's Attention" by Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning investigates the attention mechanisms within BERT, a large pre-trained Transformer model, to understand what linguistic features it learns from unlabeled data. The authors propose methods for analyzing BERT's attention maps and apply them to uncover patterns and behaviors of its attention heads.

### Key Findings:
#### Attention Patterns:
BERT's attention heads exhibit common patterns such as attending to specific positional offsets, delimiter tokens (e.g., [SEP]), or broadly attending over the entire sentence.

A significant amount of attention is focused on the [SEP] token, which the authors hypothesize acts as a "no-op" (no operation) when the attention head's function is not applicable.

> qualitative analysis (see Figure 5) shows that heads with specific functions attend to [SEP] when the function is not called for.
> For example, in head 8-10 direct objects attend to their verbs.
> For this head, non-nouns mostly attend to [SEP].
> Therefore, we speculate that attention over these special tokens might be used as a sort of “no-op” when the attention head’s function is not applicable.

#### Linguistic Phenomena:
Certain attention heads correspond well to linguistic notions of syntax and coreference. For example, some heads accurately attend to direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions.

The authors propose an attention-based probing classifier, which achieves 77% Unlabeled Attachment Score (UAS) in dependency parsing, indicating that BERT's attention captures substantial syntactic information.

#### Individual Attention Heads:

Individual attention heads specialize in specific syntactic relations, such as prepositions attending to their objects or nouns attending to their determiners, with high accuracy.

One attention head performs well in coreference resolution, particularly for nominal mentions, suggesting that BERT learns some aspects of coreference without explicit supervision.

#### Attention Head Combinations:

The authors propose probing classifiers that combine attention maps from multiple heads, showing that BERT's attention maps collectively capture a thorough representation of English syntax.

#### Clustering of Attention Heads:

Attention heads within the same layer tend to behave similarly, and some heads form clear clusters based on their behavior. This redundancy might be due to attention dropout during training.

### Contributions:
The paper provides a detailed analysis of BERT's attention mechanisms, revealing that BERT learns significant syntactic and coreference information through self-supervised training.

The authors introduce novel methods for probing attention maps, demonstrating that attention-based analysis complements other model analysis techniques.

### Conclusion:
The study shows that BERT's attention mechanisms capture a substantial amount of linguistic knowledge, particularly syntax and coreference, even though the model is not explicitly trained for these tasks. The findings suggest that pre-training on large corpora enables BERT to learn complex linguistic structures indirectly, which contributes to its success in various NLP tasks. The paper also highlights the importance of analyzing attention maps to better understand what neural networks learn about language.


![image](https://github.com/user-attachments/assets/973c45be-0592-48c5-a8f7-1bf6638e0fe2)



---
