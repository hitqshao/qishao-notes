---
title: LLM Internals
date: 2024-12-15 23:32:49
permalink: /pages/dc7042/
---

Too many paper on llms...

**Survey**
1. [C60 2024] Understanding LLMs: A Comprehensive Overview from Training to Inference 
2. [C20 2024] Mobile Edge Intelligence for Large Language Models: A Contemporary Survey
3. [P 58] A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms
4. [P 30] A Comprehensive Evaluation of Quantization Strategies for Large Language Models
5. [P 8]  A Comprehensive Study on Quantization Techniques for Large Language Models
6. [C24] Model Compression and Efficient Inference for Large Language Models: A Survey

**KV Cache**
1. [C60 2024 USENIX] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management  :+1:
2. [C1 2024]LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management

**Quantization**
1. [C8 Y2024] An Empirical Study of LLaMA3 Quantization: From LLMs to MLLMs

**Cross-layer Attention**
1. [C25 Y2024] **Reducing Transformer Key-Value Cache Size** with Cross-Layer Attention
2. [C4 2024] Cross-layer Attention Sharing for Large Language Models


**Attention**
1. [C573 2018] Efficient Attention: Attention With Linear Complexities
2. [C24 2024] Tensor Attention Training: Provably Efficient Learning of Higher-order Transformers

**Is All You Need**
1. [2025] Element-wise Attention Is All You Need
3. [2025] Tensor Product Attention Is All You Need


**Attatch Memory**
1. [C2 2024] Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU
2. [C63 2023] LLM in a flash: Efficient Large Language Model Inference with Limited Memory
 
**Novel LLM**

1. [C2 2024] Larimar: Large Language Models with Episodic Memory Control

**Batch**
1. [C59 2023] Batch Prompting: Efficient Inference with Large Language Model APIs

**Pruning**
1. [C25 2024] ZipLM: Inference-Aware Structured Pruning of Language Models

**Speculative decoding**
1. [C25 2024] Efficient Inference for Large Language Model-based Generative Recommendation
2. [C39 2024] Enhancing Inference Efficiency and Accuracy in Large Language Models through Next-Phrase Prediction  :+1:
3. [C5 2023] Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy
4. [C64 2024] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding

**Interesting**
1. [C60 2024] SnapKV: LLM Knows What You Are Looking for before Generation  :+1:
2. [C6 2024] Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training
3. [C9 ISCA 2024] LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference
4. [C6 2024] Efficient Large Foundation Models Design: A Perspective From Model and System Co-Design
5. [C83 2023] Compressing Context to Enhance Inference Efficiency of Large Language Models  :+1:
6. [C13 2024 OSDI] ServerlessLLM: Low-Latency Serverless Inference for Large Language Models
7. [C12 2024] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models
8. [C418 2022] Transformers Learn In-Context by Gradient Descent :+1:
9. [C51 2024] Massive Activations in Large Language Models :+1:
10. [C2007 2019] Generating Long Sequences with Sparse Transformers :+1:
11. [C1890 2019] What does BERT look at? An Analysis of BERTâ€™s Attention
12. [C402 2019] Analyzing the Structure of Attention in a Transformer Language Model
13. [C38517 2020] Language Models are Few-Shot Learners :+1:
14. [C24 2024] What can a Single Attention Layer Learn? A Study Through the Random Features Lens
15. [C9 2023] Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps
16. [C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories :+1:
17. [Blog] The Feedforward Demystified: A Core Operation of Transformers

**Why Infer?**
1. [C78 2024] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU  :+1:
2. [C25 2024] Powerinfer-2: fast large language model inference on a smartphone
3. [C6 2024] InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference

**General Efficient**
1. [C44 2023] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity
2. [C4 2024] Efficient Training and Inference: Techniques for Large Language Models Using Llama
3. [C226 2023] H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models

