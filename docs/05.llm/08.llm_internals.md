---
title: llm internals
date: 2024-12-15 23:32:49
permalink: /pages/dc7042/
---

Too many paper on llms...

Survey
1. [C60 2024] Understanding LLMs: A Comprehensive Overview from Training to Inference
2. [P 58] A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms
3. [P 30] A Comprehensive Evaluation of Quantization Strategies for Large Language Models
4. [P 8]  A Comprehensive Study on Quantization Techniques for Large Language Models
5. 

KV Cache
1. [C60 2024 USENIX] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management

Quantization 
1. [C8 Y2024] An Empirical Study of LLaMA3 Quantization: From LLMs to MLLMs

Cross-layer Attention
1. [C25 Y2024] **Reducing Transformer Key-Value Cache Size** with Cross-Layer Attention
2. [C4 2024] Cross-layer Attention Sharing for Large Language Models


Attatch Memory
1. [C2 2024] Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU

Novel LLM

1. [C2 2024] Larimar: Large Language Models with Episodic Memory Control

Batch
1. [C59 2023] Batch Prompting: Efficient Inference with Large Language Model APIs

Pruning
1. [C25 2024] ZipLM: Inference-Aware Structured Pruning of Language Models

Speculative decoding
1. [C25 2024] Efficient Inference for Large Language Model-based Generative Recommendation
2. [C39 2024] Enhancing Inference Efficiency and Accuracy in Large Language Models through Next-Phrase Prediction
3. [C5 2023] Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy

Interesting

1. [C60 2024] SnapKV: LLM Knows What You Are Looking for before Generation
2. [C6 2024] Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training
3. [C9 ISCA 2024] LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference
4. [C6 2024] Efficient Large Foundation Models Design: A Perspective From Model and System Co-Design
5. [C83 2023] Compressing Context to Enhance Inference Efficiency of Large Language Models
6. [C13 2024 OSDI] ServerlessLLM: Low-Latency Serverless Inference for Large Language Models
7. [C12 2024] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models


