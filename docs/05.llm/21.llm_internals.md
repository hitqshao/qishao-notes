---
title: LLM Internals
date: 2025-02-11 23:32:49
permalink: /pages/dc7059/
---

1. [C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories

---

## 1. [C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories


### **Overview**
The paper investigates the under-explored role of feed-forward layers in transformer-based language models.

While self-attention has been extensively studied, feed-forward layers contain two-thirds of a transformer's parameters.

The authors propose that these layers function as **key-value (KV) memories** , where: 

- **Keys**  capture **textual patterns**  in training examples.
- **Values**  induce **distributions over the output vocabulary** .

Through experiments, the paper demonstrates that:
- Lower-layer keys capture **shallow syntactic patterns** , while upper layers learn **semantic patterns** .
- Values **predict next-token distributions** , aligning more strongly in upper layers.
- The final model prediction is an **aggregation of memories** , refined by **residual connections** .

### **Key Contributions**  
1. **Feed-forward layers operate as key-value memories**  
  - The first parameter matrix (**The first parameter matrix (keys, $K$** ) interacts with inputs to compute a weighted sum of the second parameter matrix (**The first parameter matrix (**The first parameter matrix (keys, $K$** ) interacts with inputs to compute a weighted sum of the second parameter matrix (values, $V$** ).
  - This mirrors neural memory models like those from **Sukhbaatar et al. (2015, 2019)** .
 
2. **Keys capture human-interpretable textual patterns**  
  - Keys correspond to **n-grams, phrase structures, and semantic themes** .
  - Patterns vary across layers: 
    - **Lower layers → Shallow linguistic patterns (e.g., common words, syntactic structures)**
    - **Upper layers → Semantic relationships (e.g., “a part of,” “military base”)**
  - Removing the **last word**  in a sentence affects activations more than removing the **first word** , indicating that later words are more salient.

3. **Values store next-token probability distributions**  
  - Values represent **un-normalized probability distributions**  over the output vocabulary. 
  - Agreement between **keys and values increases in upper layers** , meaning values encode likely next words based on key patterns.
 
4. **Prediction refinement across layers**  
  - The final output results from an **aggregation of multiple memories**  at each layer.
  - Residual connections act as a **refinement mechanism** , tuning predictions layer-by-layer.
  - In lower layers, residuals dominate the prediction, while in **upper layers** , feed-forward layers play a bigger role.


### **Key Insights and Findings**

#### **1. Feed-Forward Layers as Unnormalized Key-Value Memories**  
- Each **feed-forward layer resembles a key-value memory network** .
- The equation governing a feed-forward layer:
$$FF(x) = f(x \cdot K^T) \cdot V$$

closely resembles memory-based networks:
$$MN(x) = \text{softmax}(x \cdot K^T) \cdot V$$

- The primary difference is the absence of **softmax normalization**  in transformer feed-forward layers.
#### **2. Keys Detect Patterns in Training Data**  
- Experiments show that **keys correlate with specific input patterns** .
- **Methodology** : 
  - **Trained a 16-layer transformer (Baevski & Auli, 2019) on WikiText-103** .
  - **Extracted the most activating input sentences for each key** .
  - **Human annotators**  identified patterns.
- **Findings** : 
  - **Lower-layer keys**  detect surface-level syntax (e.g., "words ending in -ing"). 
  - **Upper-layer keys**  capture deeper semantics (e.g., “military bases,” “a part of” relationships).
  - **Removing the last word in a sentence impacts activations more than removing the first** .
#### **3. Values Represent Next-Token Distributions** 
- Values store distributions over output words.
- **In lower layers, values are uncorrelated with key patterns** .
- **In upper layers, values strongly correlate with likely next tokens** .
- Agreement rate between **key activations and value-predicted tokens rises in deeper layers** .
#### **4. Aggregation of Memories Produces Final Prediction**  
- Each layer **combines multiple key-value pairs** , producing a **distribution different from any individual memory** .
- Prediction **refinement occurs layer-by-layer**  via residual connections.
- **At least 30% of predictions are already determined in the lower layers** .
- **Upper layers refine predictions, sometimes vetoing earlier predictions** .

### **Unintuitive Findings**  
1. **Lower layers do not predict tokens well, but upper layers do**  
  - Unlike self-attention, which is useful throughout the model, **feed-forward layers do not contribute much to token prediction in early layers** .
  - Instead, early layers **store**  patterns that are later **utilized in upper layers** .
2. **Memories do not act independently but are composed layer-wise** 
  - Individual key-value memories are not direct predictors.
  - Instead, the model **aggregates multiple memories**  at each layer to form a final output distribution.
3. **Residual connections mostly retain earlier predictions rather than overriding them**  
  - In most cases, the **residual connection output remains the same**  through layers, with minor refinements.
  - Occasionally, **feed-forward layers override residual outputs** , leading to large shifts in predictions.

### **Practical Implications**  
- **Better model interpretability:** 
  - Automating pattern detection in keys could improve transparency in transformers. 
- **Memory efficiency:**  
  - Understanding how transformers use feed-forward layers could enable **parameter reduction techniques** . 
- **Security concerns:**  
  - Key-value memories **store training data patterns** , posing **data leakage risks** .

### **Future Research Directions**  
1. **Embedding Space Transformations Across Layers** 
  - How does the representation space evolve through layers?
  - Why does token prediction improve only in upper layers?
2. **Extending to Other Transformer Architectures** 
  - Do BERT, T5, or vision transformers exhibit the same key-value behavior?
3. **Improving Transformer Efficiency**  
  - Can we **prune redundant keys**  while preserving performance?

### **Conclusion**

This paper **demystifies the function of feed-forward layers in transformers**  by revealing their **role as key-value memories**.

The insights gained provide a foundation for: 
- **Understanding how transformers make predictions.**
- **Optimizing transformer architectures for efficiency.**
- **Developing interpretability and security mechanisms for language models.**

**Final Thoughts**
This paper is particularly valuable for researchers **exploring transformer internals**.
Its key contribution is a **paradigm shift**  in understanding feed-forward layers—not just as nonlinear projections, but as **dynamic memory components that store patterns and predict next-token distributions** .

---

