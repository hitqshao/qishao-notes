---
title: how llm works
date: 2024-01-02 23:32:49
permalink: /pages/dc7036/
---
1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]
2. TurboTransformers: An Efficient GPU Serving System For Transformer Models [82]
3. Improving the Efficiency of Transformers for Resource-Constrained Devices [8]
4. Bag of Tricks for Optimizing Transformer Efficiency [5]
5. Making Transformer inference faster on GPUs[Blog]
6. Energy-efficient Inference Service of Transformer-based Deep Learning Models on GPUs [4]
7. Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs [TACO 2023 Ref 2]
8. hugging face https://huggingface.co/docs/transformers/performance
9. 

---
### 1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]
:thumbsup: :thumbsup: :thumbsup: :thumbsup: 

### 4. Making Transformer inference faster on GPUs[Blog]
https://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190
