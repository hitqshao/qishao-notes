---
title: Memory Usage in Training LLM
date: 2024-05-29 23:32:49
permalink: /pages/dc7038/
---

**1) Nvidia Paper on Traning LLM**
Reducing Activation Recomputation in Large Transformer Models

**2) Blog Understanding and Estimating GPU Memory**
Understanding and Estimating GPU Memory Demands for Training LLMs in practice

**3) Blog Memory-Efficient Training**
Memory-Efficient Training of Large Language Models: Overcoming Constraints on Consumer GPUs for Large Neural Networks

**4）Stanford Paper Low-Memory Neural Network Training:A Technical Report**
Low-Memory Neural Network Training:A Technical Report

**5) Blog Gradient / Activation checkpointing**
https://iq.opengenus.org/gradient-checkpointing/

**6) Tianqi Chen Gradient Checkpointing Paper**
Training Deep Nets with Sublinear Memory Cost

**7）UCSD Efficient Finetuning of LLMs**
https://cseweb.ucsd.edu/classes/wi24/cse234-a/slides/CSE234-GuestLecture-SumanthHegde.pdf
