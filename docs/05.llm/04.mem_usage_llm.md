---
title: Memory Usage in Training LLM
date: 2024-05-29 23:32:49
permalink: /pages/dc7038/
---

**1) Nvidia Paper on Traning LLM** <br>
Reducing Activation Recomputation in Large Transformer Models

**2) Blog Understanding and Estimating GPU Memory** <br>
Understanding and Estimating GPU Memory Demands for Training LLMs in practice

**3) Blog Memory-Efficient Training** <br>
Memory-Efficient Training of Large Language Models: Overcoming Constraints on Consumer GPUs for Large Neural Networks

**4）Stanford Paper Low-Memory Neural Network Training:A Technical Report** <br>
Low-Memory Neural Network Training:A Technical Report

**5) Blog Gradient / Activation checkpointing** <br>
https://iq.opengenus.org/gradient-checkpointing/

**6) Tianqi Chen Gradient Checkpointing Paper** <br>
Training Deep Nets with Sublinear Memory Cost

**7）UCSD Efficient Finetuning of LLMs** <br>
https://cseweb.ucsd.edu/classes/wi24/cse234-a/slides/CSE234-GuestLecture-SumanthHegde.pdf
