---
title: Summery of Inner Workings of LLM
date: 2025-01-26 23:32:49
permalink: /pages/dc7046/
---

# Discovered Inner Behaviors of Transformer-based Language Models

In this blog, we will explore the **discovered inner behaviors** of Transformer-based language models (LMs) as outlined in the paper :+1: :+1: *"A Primer on the Inner Workings of Transformer-based Language Models"*. The paper provides a comprehensive overview of the internal mechanisms that enable these models to perform complex language tasks. We will break down the findings into five key sections:

1. **Attention Block**
2. **Feedforward Network Block**
3. **Residual Stream**
4. **Emergent Multi-component Behavior**
5. **Factuality and Hallucinations in Model Predictions**

Let’s dive into each of these sections to understand how these components contribute to the overall functioning of Transformer-based LMs.

---

## 1. Attention Block

The **attention mechanism** is a cornerstone of Transformer models, allowing them to contextualize token representations at each layer. The attention block consists of multiple **attention heads**, each responsible for attending to different parts of the input sequence. The paper categorizes the behaviors of attention heads into two main groups:

### 1.1 Attention Heads with Interpretable Attention Weights Patterns
- **Positional Heads**: These heads attend to specific positions relative to the current token, such as the previous or next token. For example, **previous token heads** are crucial for copying information from the previous token to the current position, which is essential for tasks like name concatenation.
- **Subword Joiner Heads**: These heads focus on subwords that belong to the same word, helping the model understand word-level structures.
- **Syntactic Heads**: These heads attend to tokens with specific syntactic roles, such as subjects or objects, and are crucial for understanding grammatical relationships.
- **Duplicate Token Heads**: These heads attend to previous occurrences of the same token, which is useful for tasks like identifying repeated names in a context.

### 1.2 Attention Heads with Interpretable QK and OV Circuits
- **Copying Heads**: These heads have **OV (output-value) circuits** that exhibit copying behavior, meaning they can replicate information from one part of the sequence to another.
- **Induction Heads**: These heads are responsible for completing patterns. For example, given a sequence like "A B ... A", the model predicts "B". This mechanism involves two heads: a **previous token head** that writes information into the residual stream, and an **induction head** that reads this information to complete the pattern.
- **Copy Suppression Heads**: These heads reduce the logit score of a token if it appears in the context and is being confidently predicted. This mechanism improves model calibration by preventing naive copying.
- **Successor Heads**: These heads predict the next element in an ordinal sequence (e.g., "Monday" → "Tuesday"). They rely on the output of the first **feedforward network (FFN)** block, which encodes a numerical structure.

### 1.3 Other Noteworthy Attention Properties
- **Domain Specialization**: Some heads are specialized for specific domains, such as non-English languages or coding sequences.
- **Attention Sinks**: Certain heads attend to special tokens (e.g., BOS or punctuation) when their specialized function is not applicable. This behavior is crucial for streaming generation and model performance.

---

## 2. Feedforward Network Block

The **feedforward network (FFN)** block is another critical component of Transformer models. It consists of two learnable weight matrices and an element-wise non-linear activation function. The FFN block has been studied extensively, with a focus on the behavior of individual neurons.

### 2.1 Neuron's Input Behavior
- **Position Ranges**: Some neurons fire exclusively on specific position ranges within the input sequence.
- **Skill Neurons**: These neurons activate based on the task of the input prompt, such as detecting whether the input is Python code or French text.
- **Concept-Specific Neurons**: These neurons respond to specific concepts, such as grammatical features or semantic roles.

### 2.2 Neuron's Output Behavior
- **Knowledge Neurons**: These neurons are responsible for predicting factual information, such as the capital of a country.
- **Linguistically Acceptable Predictions**: Some neurons ensure that the model's predictions are grammatically correct, such as predicting the correct verb number based on the subject.
- **Token Frequency Neurons**: These neurons adjust the logits of tokens based on their frequency in the training data, shifting the output distribution towards or away from the unigram distribution.

### 2.3 Polysemantic Neurons
- **N-gram Detectors**: Many neurons in early layers specialize in detecting n-grams, but they often fire on a large number of unrelated n-grams, indicating **polysemanticity**.
- **Dead Neurons**: Some neurons in models like OPT remain inactive (zero activation) due to the ReLU activation function.

### 2.4 Universality of Neurons
- **Universal Neurons**: Across different models, a small subset of neurons (1-5%) activate on the same inputs. These include **alphabet neurons**, **previous token neurons**, and **entropy neurons**, which modulate the model's uncertainty over the next token prediction.

---

## 3. Residual Stream

The **residual stream** is the main communication channel in a Transformer model. It carries information from one layer to the next, with each layer adding its updates to the stream.

### 3.1 Information Flow in the Residual Stream
- **Direct Path**: The direct path from the input embedding to the unembedding matrix mainly models bigram statistics.
- **Memory Management**: Some components, such as attention heads and FFN neurons, remove information from the residual stream to manage memory. For example, certain neurons write vectors in the opposite direction of what they read, effectively canceling out information.

### 3.2 Outlier Dimensions
- **Rogue Dimensions**: These are dimensions in the residual stream with unusually large magnitudes. They are associated with the generation of **anisotropic representations**, where the residual stream states of random tokens tend to point in the same direction. Ablating these dimensions significantly decreases model performance, suggesting they encode task-specific knowledge.

### 3.3 Features in the Residual Stream
- **Sparse Autoencoders (SAEs)**: SAEs have been used to identify interpretable features in the residual stream. These features include **local context features**, **partition features** (which promote or suppress specific sets of tokens), and **suppression features** (which reduce the likelihood of certain tokens).

---

## 4. Emergent Multi-component Behavior

Transformer models achieve their remarkable performance through the **interaction of multiple components**. The paper highlights several examples of emergent behaviors that arise from these interactions.

### 4.1 Evidence of Multi-component Behavior
- **Induction Mechanism**: This mechanism involves two attention heads working together to complete patterns. A **previous token head** writes information into the residual stream, and an **induction head** reads this information to predict the next token.
- **Function Vectors**: Multiple attention heads create **function vectors** that describe a task when given in-context examples. Intervening in the residual stream with these vectors can produce outputs that align with the encoded task.

### 4.2 Circuits Analysis
- **IOI Circuit**: In the **Indirect Object Identification (IOI)** task, the model identifies the correct name (e.g., "Mary") in a sentence like "When Mary and John went to the store, John gave a drink to _____". The circuit involves **duplicate token heads**, **name mover heads**, and **negative mover heads** (which suppress incorrect predictions).
- **Greater-than Circuit**: In the **greater-than task**, the model predicts a year greater than a given year (e.g., "1814" → "1815"). The circuit involves attention heads that attend to the initial date and FFNs that compute the correct year.

### 4.3 Generality of Circuits
- **Fine-tuning**: The functionality of circuit components remains consistent after fine-tuning, suggesting that fine-tuning improves the encoding of task-relevant information rather than rearranging the circuit.
- **Grokking**: The sudden emergence of generalization capabilities in models (known as **grokking**) is linked to the formation of sparse circuits that replace dense, memorizing sub-networks.

---

## 5. Factuality and Hallucinations in Model Predictions

One of the challenges with large language models is their tendency to generate **factually incorrect or nonsensical outputs** (hallucinations). The paper explores the internal mechanisms behind these behaviors.

### 5.1 Intrinsic Views on Hallucinatory Behavior
- **Probing for Truthfulness**: Probes trained on middle and last layers' representations can predict the truthfulness of model outputs.
- **Truthfulness Directions**: Linear interventions in the direction of "truthfulness" can enhance the factual accuracy of model predictions.

### 5.2 Recall of Factual Associations
- **Factual Recall Circuit**: The model recalls factual information through a multi-step process. Early-middle FFNs add information about the subject to the residual stream, while later attention heads extract the correct attribute (e.g., the capital of a country).
- **Additive Mechanism**: Attention heads' OV circuits decode attributes by combining information from the subject and relation. Some heads are **subject heads** (independent of the relation), **relation heads** (independent of the subject), and **mixed heads** (dependent on both).

### 5.3 Factuality Issues and Model Editing
- **Model Editing**: Techniques like **causal interventions** and **knowledge neuron localization** have been used to edit factual associations in models. However, these approaches face challenges like **catastrophic forgetting** and **downstream performance loss**.

---

## Conclusion

The inner workings of Transformer-based language models are complex and multifaceted. By understanding the behaviors of individual components like attention heads, FFN neurons, and the residual stream, we can gain insights into how these models process and generate language. Moreover, the emergent behaviors that arise from the interaction of these components highlight the importance of studying models as a whole, rather than focusing on individual parts.

As research in this area continues, we can expect to uncover even more fascinating details about how these models work, paving the way for more interpretable, reliable, and safe AI systems.

---

*This blog is based on the paper "A Primer on the Inner Workings of Transformer-based Language Models" by Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà.*

---

## [7 2024] Interpreting Attention Layer Outputs with Sparse Autoencoders

The paper provides a layer-wise understanding of how attention outputs evolve in terms of feature utilization:

- Early Layers (0-3):
  - Features:
    - Focus on simple syntactic patterns (e.g., token pairs, local relationships).
    - Begin building short-range context features.
  - Utilization:
    - Early heads primarily focus on short-range interactions and token-to-token dependencies.
- Middle Layers (4-9):
  - Features:
    - Capture more abstract, semantic patterns (e.g., grammatical and reasoning constructs, topic tracking).
    - Generate long-range context features and induction features.
  - Utilization:
    - Middle heads integrate global information and start building context-aware features (e.g., reasoning or induction patterns).
- Late Layers (10-11):
  - Features:
    - Refine and finalize features for specific tasks like grammatical adjustments, long-range predictions, and sequence completions.
  - Utilization:
    - Late heads mostly adjust or finalize token choices using refined long-range context features.

![image](https://github.com/user-attachments/assets/28f89d23-0afb-4b65-87ec-a878e3965de9)

The study also provides insights into how these features are distributed and utilized across layers and attention heads:

**1. Long-Range Context Features**  
- **Function** : Capture information that spans long distances in the text (e.g., maintaining the topic or theme of a paragraph).
 
- **Source** : Typically generated in **middle to late layers** .
  - Middle layers begin integrating broader semantic and contextual information.

  - Late layers refine the contextual understanding for tasks like logical reasoning or high-level decision-making.
 
- **Head Specialization** : Some heads specialize in aggregating context from far-away tokens, while others focus on more localized interactions.

**2. Short-Range Context Features**  
- **Function** : Focus on localized relationships, such as syntactic dependencies (e.g., a word's immediate neighbors).
 
- **Source** : Dominantly found in **early and middle layers** .
  - Early layers handle low-level syntactic features like token pairs and adjacent word dependencies.

  - Some middle-layer heads expand on short-range features by integrating grammatical constructs.
 
- **Head Specialization** : Specific attention heads in early layers are responsible for capturing these short-range patterns.

**3. Induction Features**  
- **Function** : Capture patterns for **in-context learning** , such as recognizing repeated prefixes in sequences (e.g., completing "...ABC...AB" with "C").
 
- **Source** : Strongly tied to **specific induction heads**  in attention layers.
  - These heads are often polysemantic but include roles specialized for induction tasks.
 
  - Example: In GPT-2 Small, the study identifies two **layer 5 heads (5.1 and 5.5)**  that specialize in **long-prefix**  and **short-prefix induction** , respectively.
 
- **Head Specialization** : 
  - Induction features are directly associated with **attention heads**  that attend to patterns of repeated tokens.


---

[331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting

- Key Findings:
  - Unfaithful Explanations: CoT explanations can be plausible but systematically unfaithful, failing to reflect the true reasoning process.
  - Biasing Features: Models are heavily influenced by biasing features (e.g., reordering multiple-choice options), which are not mentioned in explanations.
  - Accuracy Drop: Biasing models toward incorrect answers leads to a 36% drop in accuracy on BIG-Bench Hard tasks.
  - Social Bias: Models justify stereotype-aligned answers without acknowledging the influence of social biases.
  - Counterfactual Simulatability: Models rarely acknowledge biasing features, making explanations systematically unfaithful.

They instruct llm with bias：
![image](https://github.com/user-attachments/assets/9c02c120-64d7-488c-b1af-bebeb28e8582)

![image](https://github.com/user-attachments/assets/c11dfc4e-35af-4ce2-b88f-fd51f8980805)
