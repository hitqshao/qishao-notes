---
title: Summery of Inner Workings of LLM
date: 2025-01-26 23:32:49
permalink: /pages/dc7046/
---

# Discovered Inner Behaviors of Transformer-based Language Models

In this blog, we will explore the **discovered inner behaviors** of Transformer-based language models (LMs) as outlined in the paper :+1: :+1: *"A Primer on the Inner Workings of Transformer-based Language Models"*. The paper provides a comprehensive overview of the internal mechanisms that enable these models to perform complex language tasks. We will break down the findings into five key sections:

1. **Attention Block**
2. **Feedforward Network Block**
3. **Residual Stream**
4. **Emergent Multi-component Behavior**
5. **Factuality and Hallucinations in Model Predictions**

Let’s dive into each of these sections to understand how these components contribute to the overall functioning of Transformer-based LMs.

---

## 1. Attention Block

The **attention mechanism** is a cornerstone of Transformer models, allowing them to contextualize token representations at each layer. The attention block consists of multiple **attention heads**, each responsible for attending to different parts of the input sequence. The paper categorizes the behaviors of attention heads into two main groups:

### 1.1 Attention Heads with Interpretable Attention Weights Patterns
- **Positional Heads**: These heads attend to specific positions relative to the current token, such as the previous or next token. For example, **previous token heads** are crucial for copying information from the previous token to the current position, which is essential for tasks like name concatenation.
- **Subword Joiner Heads**: These heads focus on subwords that belong to the same word, helping the model understand word-level structures.
- **Syntactic Heads**: These heads attend to tokens with specific syntactic roles, such as subjects or objects, and are crucial for understanding grammatical relationships.
- **Duplicate Token Heads**: These heads attend to previous occurrences of the same token, which is useful for tasks like identifying repeated names in a context.

### 1.2 Attention Heads with Interpretable QK and OV Circuits
- **Copying Heads**: These heads have **OV (output-value) circuits** that exhibit copying behavior, meaning they can replicate information from one part of the sequence to another.
- **Induction Heads**: These heads are responsible for completing patterns. For example, given a sequence like "A B ... A", the model predicts "B". This mechanism involves two heads: a **previous token head** that writes information into the residual stream, and an **induction head** that reads this information to complete the pattern.
- **Copy Suppression Heads**: These heads reduce the logit score of a token if it appears in the context and is being confidently predicted. This mechanism improves model calibration by preventing naive copying.
- **Successor Heads**: These heads predict the next element in an ordinal sequence (e.g., "Monday" → "Tuesday"). They rely on the output of the first **feedforward network (FFN)** block, which encodes a numerical structure.

### 1.3 Other Noteworthy Attention Properties
- **Domain Specialization**: Some heads are specialized for specific domains, such as non-English languages or coding sequences.
- **Attention Sinks**: Certain heads attend to special tokens (e.g., BOS or punctuation) when their specialized function is not applicable. This behavior is crucial for streaming generation and model performance.

---

## 2. Feedforward Network Block

The **feedforward network (FFN)** block is another critical component of Transformer models. It consists of two learnable weight matrices and an element-wise non-linear activation function. The FFN block has been studied extensively, with a focus on the behavior of individual neurons.

### 2.1 Neuron's Input Behavior
- **Position Ranges**: Some neurons fire exclusively on specific position ranges within the input sequence.
- **Skill Neurons**: These neurons activate based on the task of the input prompt, such as detecting whether the input is Python code or French text.
- **Concept-Specific Neurons**: These neurons respond to specific concepts, such as grammatical features or semantic roles.

### 2.2 Neuron's Output Behavior
- **Knowledge Neurons**: These neurons are responsible for predicting factual information, such as the capital of a country.
- **Linguistically Acceptable Predictions**: Some neurons ensure that the model's predictions are grammatically correct, such as predicting the correct verb number based on the subject.
- **Token Frequency Neurons**: These neurons adjust the logits of tokens based on their frequency in the training data, shifting the output distribution towards or away from the unigram distribution.

### 2.3 Polysemantic Neurons
- **N-gram Detectors**: Many neurons in early layers specialize in detecting n-grams, but they often fire on a large number of unrelated n-grams, indicating **polysemanticity**.
- **Dead Neurons**: Some neurons in models like OPT remain inactive (zero activation) due to the ReLU activation function.

### 2.4 Universality of Neurons
- **Universal Neurons**: Across different models, a small subset of neurons (1-5%) activate on the same inputs. These include **alphabet neurons**, **previous token neurons**, and **entropy neurons**, which modulate the model's uncertainty over the next token prediction.

---

## 3. Residual Stream

The **residual stream** is the main communication channel in a Transformer model. It carries information from one layer to the next, with each layer adding its updates to the stream.

### 3.1 Information Flow in the Residual Stream
- **Direct Path**: The direct path from the input embedding to the unembedding matrix mainly models bigram statistics.
- **Memory Management**: Some components, such as attention heads and FFN neurons, remove information from the residual stream to manage memory. For example, certain neurons write vectors in the opposite direction of what they read, effectively canceling out information.

### 3.2 Outlier Dimensions
- **Rogue Dimensions**: These are dimensions in the residual stream with unusually large magnitudes. They are associated with the generation of **anisotropic representations**, where the residual stream states of random tokens tend to point in the same direction. Ablating these dimensions significantly decreases model performance, suggesting they encode task-specific knowledge.

### 3.3 Features in the Residual Stream
- **Sparse Autoencoders (SAEs)**: SAEs have been used to identify interpretable features in the residual stream. These features include **local context features**, **partition features** (which promote or suppress specific sets of tokens), and **suppression features** (which reduce the likelihood of certain tokens).

---

## 4. Emergent Multi-component Behavior

Transformer models achieve their remarkable performance through the **interaction of multiple components**. The paper highlights several examples of emergent behaviors that arise from these interactions.

### 4.1 Evidence of Multi-component Behavior
- **Induction Mechanism**: This mechanism involves two attention heads working together to complete patterns. A **previous token head** writes information into the residual stream, and an **induction head** reads this information to predict the next token.
- **Function Vectors**: Multiple attention heads create **function vectors** that describe a task when given in-context examples. Intervening in the residual stream with these vectors can produce outputs that align with the encoded task.

### 4.2 Circuits Analysis
- **IOI Circuit**: In the **Indirect Object Identification (IOI)** task, the model identifies the correct name (e.g., "Mary") in a sentence like "When Mary and John went to the store, John gave a drink to _____". The circuit involves **duplicate token heads**, **name mover heads**, and **negative mover heads** (which suppress incorrect predictions).
- **Greater-than Circuit**: In the **greater-than task**, the model predicts a year greater than a given year (e.g., "1814" → "1815"). The circuit involves attention heads that attend to the initial date and FFNs that compute the correct year.

### 4.3 Generality of Circuits
- **Fine-tuning**: The functionality of circuit components remains consistent after fine-tuning, suggesting that fine-tuning improves the encoding of task-relevant information rather than rearranging the circuit.
- **Grokking**: The sudden emergence of generalization capabilities in models (known as **grokking**) is linked to the formation of sparse circuits that replace dense, memorizing sub-networks.

---

## 5. Factuality and Hallucinations in Model Predictions

One of the challenges with large language models is their tendency to generate **factually incorrect or nonsensical outputs** (hallucinations). The paper explores the internal mechanisms behind these behaviors.

### 5.1 Intrinsic Views on Hallucinatory Behavior
- **Probing for Truthfulness**: Probes trained on middle and last layers' representations can predict the truthfulness of model outputs.
- **Truthfulness Directions**: Linear interventions in the direction of "truthfulness" can enhance the factual accuracy of model predictions.

### 5.2 Recall of Factual Associations
- **Factual Recall Circuit**: The model recalls factual information through a multi-step process. Early-middle FFNs add information about the subject to the residual stream, while later attention heads extract the correct attribute (e.g., the capital of a country).
- **Additive Mechanism**: Attention heads' OV circuits decode attributes by combining information from the subject and relation. Some heads are **subject heads** (independent of the relation), **relation heads** (independent of the subject), and **mixed heads** (dependent on both).

### 5.3 Factuality Issues and Model Editing
- **Model Editing**: Techniques like **causal interventions** and **knowledge neuron localization** have been used to edit factual associations in models. However, these approaches face challenges like **catastrophic forgetting** and **downstream performance loss**.

---

## Conclusion

The inner workings of Transformer-based language models are complex and multifaceted. By understanding the behaviors of individual components like attention heads, FFN neurons, and the residual stream, we can gain insights into how these models process and generate language. Moreover, the emergent behaviors that arise from the interaction of these components highlight the importance of studying models as a whole, rather than focusing on individual parts.

As research in this area continues, we can expect to uncover even more fascinating details about how these models work, paving the way for more interpretable, reliable, and safe AI systems.

---

*This blog is based on the paper "A Primer on the Inner Workings of Transformer-based Language Models" by Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà.*

---

## [7 2024] Interpreting Attention Layer Outputs with Sparse Autoencoders

The paper provides a layer-wise understanding of how attention outputs evolve in terms of feature utilization:

- Early Layers (0-3):
  - Features:
    - Focus on simple syntactic patterns (e.g., token pairs, local relationships).
    - Begin building short-range context features.
  - Utilization:
    - Early heads primarily focus on short-range interactions and token-to-token dependencies.
- Middle Layers (4-9):
  - Features:
    - Capture more abstract, semantic patterns (e.g., grammatical and reasoning constructs, topic tracking).
    - Generate long-range context features and induction features.
  - Utilization:
    - Middle heads integrate global information and start building context-aware features (e.g., reasoning or induction patterns).
- Late Layers (10-11):
  - Features:
    - Refine and finalize features for specific tasks like grammatical adjustments, long-range predictions, and sequence completions.
  - Utilization:
    - Late heads mostly adjust or finalize token choices using refined long-range context features.

![image](https://github.com/user-attachments/assets/28f89d23-0afb-4b65-87ec-a878e3965de9)

The study also provides insights into how these features are distributed and utilized across layers and attention heads:

**1. Long-Range Context Features**  
- **Function** : Capture information that spans long distances in the text (e.g., maintaining the topic or theme of a paragraph).
 
- **Source** : Typically generated in **middle to late layers** .
  - Middle layers begin integrating broader semantic and contextual information.

  - Late layers refine the contextual understanding for tasks like logical reasoning or high-level decision-making.
 
- **Head Specialization** : Some heads specialize in aggregating context from far-away tokens, while others focus on more localized interactions.

**2. Short-Range Context Features**  
- **Function** : Focus on localized relationships, such as syntactic dependencies (e.g., a word's immediate neighbors).
 
- **Source** : Dominantly found in **early and middle layers** .
  - Early layers handle low-level syntactic features like token pairs and adjacent word dependencies.

  - Some middle-layer heads expand on short-range features by integrating grammatical constructs.
 
- **Head Specialization** : Specific attention heads in early layers are responsible for capturing these short-range patterns.

**3. Induction Features**  
- **Function** : Capture patterns for **in-context learning** , such as recognizing repeated prefixes in sequences (e.g., completing "...ABC...AB" with "C").
 
- **Source** : Strongly tied to **specific induction heads**  in attention layers.
  - These heads are often polysemantic but include roles specialized for induction tasks.
 
  - Example: In GPT-2 Small, the study identifies two **layer 5 heads (5.1 and 5.5)**  that specialize in **long-prefix**  and **short-prefix induction** , respectively.
 
- **Head Specialization** : 
  - Induction features are directly associated with **attention heads**  that attend to patterns of repeated tokens.


---

## [331] Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting

- Key Findings:
  - Unfaithful Explanations: CoT explanations can be plausible but systematically unfaithful, failing to reflect the true reasoning process.
  - Biasing Features: Models are heavily influenced by biasing features (e.g., reordering multiple-choice options), which are not mentioned in explanations.
  - Accuracy Drop: Biasing models toward incorrect answers leads to a 36% drop in accuracy on BIG-Bench Hard tasks.
  - Social Bias: Models justify stereotype-aligned answers without acknowledging the influence of social biases.
  - Counterfactual Simulatability: Models rarely acknowledge biasing features, making explanations systematically unfaithful.

They instruct llm with bias：

![image](https://github.com/user-attachments/assets/9c02c120-64d7-488c-b1af-bebeb28e8582)

![image](https://github.com/user-attachments/assets/c11dfc4e-35af-4ce2-b88f-fd51f8980805)

---

## [9831] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

Key findings:
- Chain-of-thought prompting significantly improves the performance of large language models on a variety of reasoning tasks.
- Chain-of-thought prompting is an emergent property of model scale, meaning that it only provides significant performance gains when used with very large language models (around 100 billion parameters).
- The improvements from chain-of-thought prompting are robust across different language models, datasets, and annotators

Usage cases:
- Arithmetic reasoning: CoT prompting can help language models solve math word problems that require multiple steps, such as the GSM8K benchmark.    
- Commonsense reasoning: CoT prompting can also improve the performance of language models on tasks that require commonsense reasoning, such as the StrategyQA dataset, which requires models to infer a multi-hop strategy to answer questions.    
- Symbolic reasoning: CoT prompting has also been shown to be effective for symbolic reasoning tasks, such as last letter concatenation, which requires the model to concatenate the last letters of words in a name.

---

## [641] Towards Reasoning in Large Language Models: A Survey

![image](https://github.com/user-attachments/assets/a54b39f5-4293-4d16-ad88-ce289ed787a9)

Large language models (LLMs) have made impressive strides in natural language processing, but their ability to reason remains a hot topic. This blog post delves into the fascinating world of reasoning in LLMs, exploring the techniques, evaluations, and key findings that are shaping this field.

### What is Reasoning?
Reasoning is the process of using evidence, logic, and past experiences to form conclusions or make decisions. It's a fundamental aspect of human intelligence, allowing us to solve problems, think critically, and understand the world around us.   There are different types of reasoning, including:   

Deductive reasoning: Drawing a conclusion based on the truth of the premises (e.g., if all mammals have kidneys and all whales are mammals, then all whales have kidneys).    
Inductive reasoning: Drawing a conclusion based on observations or evidence (e.g., if every winged creature we've seen is a bird, then a new winged creature is likely a bird).    
Abductive reasoning: Drawing a conclusion based on the best explanation for a set of observations (e.g., if the car won't start and there's a puddle under it, then the car probably has a leak).    

### Techniques for Enhancing Reasoning in LLMs
Researchers are constantly developing new techniques to improve or elicit reasoning in LLMs. Some of the most promising methods include:

- Fully supervised fine-tuning: This involves fine-tuning a pre-trained LLM on a dataset containing explicit reasoning examples. For instance, a model could be trained to generate rationales explaining its predictions.    
- Prompting and in-context learning: This approach involves prompting LLMs with a question and a few examples of how to solve similar questions. Chain-of-thought prompting is a popular technique where the examples include intermediate reasoning steps, guiding the LLM to generate its own reasoning process.
  - Prompting & In-Context Learning: in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ tripples
    - manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation
  -  Rationale Engieering： creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs.
    -  Rationale refinement
      -  complexity-based prompting to create rationales with more reasoning steps. Their experiments show that the performance of LLMs improves with the increased rationale complexity
      -  algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations
    - Rationale exploration: decoding strategy, sampling a divese set of rationale, instead of the greedy one
    - Rationale verification

![image](https://github.com/user-attachments/assets/3cda04b5-8ce4-4149-910e-920c0113efa0)
    
- Hybrid methods: These methods combine techniques like pre-training or fine-tuning LLMs on datasets that include reasoning, along with prompting techniques to elicit reasoning.
  - LLMs trained on datasets containing scientific and mathematical data can achieve better performance on reasoning tasks like quantitative reasoning problems when using CoT prompting.
  - bootstrapping & self improving: using LLMs to self-improve their reasoning abilities through a process known  as bootstrapping.
      - Specifically, with CoT prompting, the model first generates initial rationales. And then, the model is finetuned on rationales that lead to correct answers. This process can be repeated, with each iteration resulting in an improved model that can generate better training data.

### Evaluating Reasoning in LLMs
Evaluating the reasoning abilities of LLMs is crucial. Researchers use various methods and benchmarks to assess their performance, including:

- End task performance: This involves measuring the accuracy of LLMs on tasks requiring reasoning, such as arithmetic, commonsense, and symbolic reasoning benchmarks.    
- Analysis of reasoning: This approach focuses on directly assessing the reasoning steps taken by LLMs, rather than just the final answer. This can involve analyzing the quality of the generated rationales or using formal metrics to evaluate the reasoning process.

### Key Findings and Implications
Research in reasoning in LLMs has yielded some interesting findings:

- Emergent ability: Reasoning seems to be an emergent ability of LLMs, becoming more pronounced as the models get larger (around 100 billion parameters or more).    
- Chain-of-thought prompting: This technique has been shown to significantly improve the performance of LLMs on various reasoning tasks.    
- Complex reasoning challenges: Despite progress, LLMs still struggle with complex reasoning tasks, suggesting that current benchmarks might be too simple.

### Open Questions and Future Directions
The field of reasoning in LLMs is still evolving, with many open questions and exciting avenues for future research:

- True reasoning or mimicry?: Are LLMs truly capable of reasoning, or are they simply learning to mimic human reasoning through pattern recognition?    
- Improving reasoning capabilities: How can we further enhance the reasoning capabilities of LLMs? This could involve developing new training methods, model architectures, or prompting techniques.

By addressing these questions and continuing to explore the intricacies of reasoning in LLMs, we can unlock their full potential and pave the way for more intelligent and reliable language-based AI systems.
