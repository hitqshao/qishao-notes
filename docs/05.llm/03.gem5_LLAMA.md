---
title: How to run llama.cpp with gem5
date: 2024-01-21 23:32:49
permalink: /pages/dc7037/
---

**1) LLama.cpp**

llama support compilation of x86, arm and gpu.

a) github download llama.cpp
https://github.com/ggerganov/llama.cpp.git
b) gem5 support ARM architecture better, thus we compile llama.cpp with arm.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/4bff59c8-4554-404b-a73f-73780860d6d5)

Then we start to compile:
make UNAME_M=aarch64

the compile tool chain is based on aarch64-linux-gnu-gc-10. It will generate "main" binary if compress successfully.

use file main to check the file:

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/563b9fe1-681c-46cf-9eae-c332bc5f3aa1)

c) download a model to llama.cpp/models directory.

Here I downloaded llama-2-7b-chat.Q2_K.gguf. It utilize 2bit quantization and only needs 3GB memory.

GGML_TYPE_Q2_K - "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)


![image](https://github.com/hitqshao/qishao-notes/assets/23403286/d7ac5770-c955-479d-82b0-e87c09a3b347)

d) then we can run the main binary and model, my prompt is "How are you".

./main  -m ./models/llama-2-7b-chat.Q2_K.gguf -p "How are you" -n 16

The last line in the following figure is the output.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/bb225f87-1c44-46b5-940c-7634c9bdcc24)

**2) gem5**

after we have build gem5 successfully, we can run the model with gem5.

Here I plan to run with 8 core.

>build/ARM/gem5.fast 
>--outdir=./m5out/llm_9 
>./configs/example/se.py -c
>$LLAMA_path/llama.cpp/main-arm 
>'--options=-m $LLAMA_path/llama-2-7b-chat.Q2_K.gguf -p Hi -n 16' 
>--cpu-type=ArmAtomicSimpleCPU --mem-size=8GB -n 8

>The output is like the following:

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/bba1eb42-4df2-4199-814f-2fe556959a6f)

The left several columns are output of LLAMA model. The followings are cpu ID with instruction executed.

The output of the model is "Hiï¼ŒI'm a 30-year-old male, and..."

However, only 4 core has been used, since LLAMA.cpp default threads configuration is 4.

Then we can configure the model with 8 thread.

>build/ARM/gem5.fast
>--outdir=./m5out/llm_9 
>./configs/example/se.py -c
>$LLAMA_path/llama.cpp/main-arm 
>'--options=-m $LLAMA_path/llama-2-7b-chat.Q2_K.gguf -p Hi -n 16 **-t 8**' 
>--cpu-type=ArmAtomicSimpleCPU --mem-size=8GB -n 8

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/26bedfc9-2b30-4f76-8eba-535159bc0749)

Now, you can see that by default, only CPU0 execute 2.9 billion instruction with the output of "Hi" in 8 core simulation. 
However, with default 4 core, it has to run 5.4 billion instruction to the same result. This complies to the number of cores runing parallely.

