---
title: Estimation of LLM
date: 2025-01-21 23:32:49
permalink: /pages/dc7045/
---

## LLM Training time

Llama3 405B

C = 6 * N * D
- C compute
- N parameter number
- D token number

C = 6 * 405 * 10^9 * 15*10^12 = 3.6 * 10^25

C/(16KGPU*TFlops)

C/(16 * 1000 * 400 * 10^12) = 97 days

[Source](https://www.factorialfunds.com/blog/thoughts-on-llama-3)

same as:

![image](https://github.com/user-attachments/assets/723e951d-9eab-40b9-b42c-916c5c3084cc)

[Page125](大语言模型)

Also in 

![image](https://github.com/user-attachments/assets/4635d893-655c-403d-82f5-fc16e972a670)

[Source](https://arxiv.org/pdf/2305.10403#page=36&zoom=100,63,210)
From paper *PaLM 2 Technical Report*

This is far different from:

![image](https://github.com/user-attachments/assets/e18267cb-ad5f-465a-b19f-1a13eef6d49f)

*Source:Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM*

Still needs time to check.

## why it is 6 fold?

[reference](https://medium.com/@kailaspsudheer/the-transformers-arithmetic-527111099527)

![image](https://github.com/user-attachments/assets/79df3f9d-e6d0-4b97-8461-b7b1ddf9659c)

**In short, for backward pass, the back propagation loss needs to be multiplied by weight to pass get the input-loss for privous layer.**
**It also needs to multiplied by the input of this layer to get the derivative of the weights for updating the weight.**

Those two above is multiplication operation, it also needs another two add operation to sum up input-loss for privous layer and update the weigth, one for each.

Paper waiting to be read:
- Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
- ZeRO: Memory Optimizations Toward Training Trillion Parameter Models
- How Does Critical Batch Size Scale in Pre-training?
- Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management :+1:
  ![image](https://github.com/user-attachments/assets/77b9b55d-54c7-46e4-b66a-70d11305704e)
- Comparative Study of Large Language Model Architectures on Frontier
- Optimizing Distributed Training on Frontier for Large Language Models
- Deep Optimizer States: Towards Scalable Training of Transformer Models Using Interleaved Offloading
- The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
- Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies
- Towards Scalable Automated Alignment of LLMs: A Survey
- MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length
- Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models
- [C1731] Training Compute-Optimal Large Language Models :+1: 
- [C138] Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers
- [C1082] Scaling Language Models: Methods, Analysis & Insights from Training Gopher
