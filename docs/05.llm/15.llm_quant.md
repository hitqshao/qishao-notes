---
title: LLM Quantization & Outlier
date: 2025-01-27 23:32:49
permalink: /pages/dc7050/
---

1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization
2. [43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models

---
## 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization

### Problems
The problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:

- High Dynamic Range of Activations
  - The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.    
  - This means that the values within these tensors vary significantly in magnitude.
  - Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.    
  - Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.
    
- Presence of Structured Outliers
  - The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).    
  - These outliers are not random; they appear to be correlated with specific input tokens and embedding dimensions.    
  - Further analysis revealed that these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP]).    
  - While this attention behavior might be beneficial for the model's performance, the outliers that cause it also create challenges for quantization.
    
- Sensitivity to Quantization Noise
  - Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.    
  - Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.    
  - This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.

### Solutions

solutions proposed in the paper:

- Mixed-precision PTQ
  - The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.    
  - To address this, they proposed using a higher bit-width (16-bit) for the more sensitive activation tensors, particularly the residual sum after the feed-forward network (FFN).    
  - This higher bit-width allows for more accurate representation of both the FFN's input and output, minimizing potential errors.    
  - Additionally, they explored using low-bit (2-4) quantization for weights and token embeddings, which can significantly reduce model size without much accuracy loss.    

- Per-embedding-group PTQ
  - The authors identified that outliers in the activation tensors primarily reside in a few specific embedding dimensions.  
  - To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.    
  - This method involves splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group.    
  - To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.    
  - This approach effectively handles outliers without significantly increasing computational overhead.

- Quantization-aware training (QAT)
  - The authors also explored QAT, where the model is trained with simulated quantization operations.    
  - This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.    
  - During QAT, they used learnable ranges for both weights and activations, further enhancing the model's adaptability to quantization.

---

## 2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models

### **Key Takeaways in Three Sentences**  
1. The study demonstrates that **low-bit floating-point formats, particularly FP8, provide superior quantization accuracy for LLMs compared to INT8** , with comparable hardware efficiency at 8-bit precision.
2. The **Mixture of Formats Quantization (MoFQ) approach optimally selects between INT and FP per layer** , improving accuracy without increasing computational overhead.
3. MoFQ achieves **state-of-the-art results in both W4-only and W8A8 quantization** , outperforming existing methods like GPTQ, AWQ, LLM.int8(), and SmoothQuant while maintaining **efficient inference speed** .

### **Abstract**

The study finds that optimal quantization formats vary across layers in LLMs, leading to the **Mixture of Formats Quantization (MoFQ)**  approach, which selects the best format per layer.\
MoFQ achieves superior or comparable performance to current quantization methods for weight-only (W-only) and weight-activation (WA) quantization without additional hardware overhead.

### **Introduction** 

Quantization minimizes LLMs' size and inference costs, with prior work focusing on low-bit integer formats.\
However, as LLMs grow, integer quantization becomes less effective, requiring optimizations or alternatives. Low-bit floating-point formats have emerged as viable alternatives, with FP8 already supported in NVIDIA’s H100 GPUs.

The study:
1. Compares INT and FP formats in terms of hardware efficiency and quantization error. 
2. Proposes **Mixture of Formats Quantization (MoFQ)** , selecting the best format per layer.
3. Implements an inference system for W-only quantization, maintaining performance parity with INT-based systems.

### **Background and Related Works**

**Integer vs. Floating-Point Formats**  
- **Integer (INT)** : Uniformly distributed values.
- **Floating-Point (FP)** : Non-uniform distribution, allowing higher precision for small values but reduced precision for large values.
- **Hardware efficiency** : FP operations typically cost more than INT, but at 8-bit, FP8 and INT8 MAC (Multiply-Accumulate) units have nearly identical area costs.
 
**Post-Training Quantization (PTQ) for LLMs**

Two main PTQ strategies:
1. **Weight-Only (W-only) Quantization:**  Applies to weights only, e.g., W4A16.
2. **Weight-Activation (WA) Quantization:**  Quantizes both weights and activations, e.g., W8A8.

State-of-the-art (SOTA) methods:
 
- **LLM.int8()** : Uses mixed precision (INT8+FP16). 
- **SmoothQuant** : Redistributes quantization difficulty from activations to weights.
- **GPTQ & AWQ** : Use second-order information and pre-scaling techniques to improve quantization.


### **Comparative Analysis of INT and FP Formats**

**A. Hardware Cost of INT vs. FP MAC Units**
- **At 8-bit precision, FP8 and INT8 MACs require nearly the same hardware area** , aligning with H100 GPU capabilities.

**B. Quantization Error Comparison**  
1. **4-bit Weight-Only (W4) Quantization**  (LLaMA-65B model): 
  - Some layers perform better with INT4, while others favor FP4, indicating **layer-dependent format preference** .

![image](https://github.com/user-attachments/assets/97be028e-fdb3-4883-a92e-b784d1ff1f87)

2. **8-bit Weight-Activation (W8A8) Quantization** : 
  - **Weights** : INT8 generally has lower quantization error. 
  - **Activations** : FP8 shows **better robustness**  for dynamic activation tensors.
  - Best choice: INT8 for weights, FP8 for activations—but hardware constraints necessitate using the same format per layer.

![image](https://github.com/user-attachments/assets/7af41adf-5821-4c0b-8599-1385c1982c87)

### **Exploiting INT and FP Complementarity**

**A. Improved Low-Bit FP4 Format** 
- IEEE floating-point format reserves exponent values for NaN and Inf. 
- **Reallocating NaN & Inf to normalized numbers improves FP4 precision**  by 35%.

**B. Mixture of Formats Quantization (MoFQ)**

- Selects the best quantization format (INT or FP) **per layer**  based on quantization error.
- Works for both **W-only and WA quantization** .
- **Algorithm** : Iterates through layers, computes quantization error for INT and FP, and selects the lower-error format.

**C. Low-Bit W-Only Inference System**  
- **INT4 and FP4 require conversion to FP16 before computation**  due to FP16 activations.
- **W8A8 quantization** : FP16 activations are converted to FP8 or INT8 based on next-layer format selection.
- **No additional hardware overhead for FP-based or MoFQ-based inference**  compared to INT-based quantization.

![image](https://github.com/user-attachments/assets/7d5e0e92-deda-4e40-8775-b428fff2ca0c)


### **Conclusion**  
- **Comparative study** : INT and FP formats have complementary strengths. 
- **Key finding** : **FP8 and INT8 MAC units have similar hardware costs at low-bit quantization** . 
- **MoFQ method** : 
  - Selects the best quantization format **per layer** . 
  - **Achieves state-of-the-art accuracy**  in W4-only and W8A8 quantization. 
  - **No additional inference latency or hardware overhead** .
