---
title: LLM Quantization & Outlier
date: 2025-01-27 23:32:49
permalink: /pages/dc7050/
---

1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization
2. [43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models
3. [26] FP8-LM: Training FP8 Large Language Models :+1: from Microsoft
4. [139] FP8 Formats for Deep Learning
5. [41] With Shared Microexponents, A Little Shifting Goes a Long Way :+1: from Meta, Microsoft :+1:
6. [34] Stable and low-precision training for large-scale vision-language models :+1:
   *mentioned in Deepseek paper mixed precision training section. Not read yet.*
---
## 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization

### Problems
The problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:

- High Dynamic Range of Activations
  - The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.    
  - This means that the values within these tensors vary significantly in magnitude.
  - Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.    
  - Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.
    
- Presence of Structured Outliers
  - The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).    
  - These outliers are not random; they appear to be correlated with specific input tokens and embedding dimensions.    
  - Further analysis revealed that these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP]).    
  - While this attention behavior might be beneficial for the model's performance, the outliers that cause it also create challenges for quantization.
    
- Sensitivity to Quantization Noise
  - Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.    
  - Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.    
  - This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.

### Solutions

solutions proposed in the paper:

- Mixed-precision PTQ
  - The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.    
  - To address this, they proposed using a higher bit-width (16-bit) for the more sensitive activation tensors, particularly the residual sum after the feed-forward network (FFN).    
  - This higher bit-width allows for more accurate representation of both the FFN's input and output, minimizing potential errors.    
  - Additionally, they explored using low-bit (2-4) quantization for weights and token embeddings, which can significantly reduce model size without much accuracy loss.    

- Per-embedding-group PTQ
  - The authors identified that outliers in the activation tensors primarily reside in a few specific embedding dimensions.  
  - To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.    
  - This method involves splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group.    
  - To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.    
  - This approach effectively handles outliers without significantly increasing computational overhead.

- Quantization-aware training (QAT)
  - The authors also explored QAT, where the model is trained with simulated quantization operations.    
  - This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.    
  - During QAT, they used learnable ranges for both weights and activations, further enhancing the model's adaptability to quantization.

---

## 2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models

### **Key Takeaways in Three Sentences**  
1. The study demonstrates that **low-bit floating-point formats, particularly FP8, provide superior quantization accuracy for LLMs compared to INT8** , with comparable hardware efficiency at 8-bit precision.
2. The **Mixture of Formats Quantization (MoFQ) approach optimally selects between INT and FP per layer** , improving accuracy without increasing computational overhead.
3. MoFQ achieves **state-of-the-art results in both W4-only and W8A8 quantization** , outperforming existing methods like GPTQ, AWQ, LLM.int8(), and SmoothQuant while maintaining **efficient inference speed** .

### **Abstract**

The study finds that optimal quantization formats vary across layers in LLMs, leading to the **Mixture of Formats Quantization (MoFQ)**  approach, which selects the best format per layer.\
MoFQ achieves superior or comparable performance to current quantization methods for weight-only (W-only) and weight-activation (WA) quantization without additional hardware overhead.

### **Introduction** 

Quantization minimizes LLMs' size and inference costs, with prior work focusing on low-bit integer formats.\
However, as LLMs grow, integer quantization becomes less effective, requiring optimizations or alternatives. Low-bit floating-point formats have emerged as viable alternatives, with FP8 already supported in NVIDIA’s H100 GPUs.

The study:
1. Compares INT and FP formats in terms of hardware efficiency and quantization error. 
2. Proposes **Mixture of Formats Quantization (MoFQ)** , selecting the best format per layer.
3. Implements an inference system for W-only quantization, maintaining performance parity with INT-based systems.

### **Background and Related Works**

**Integer vs. Floating-Point Formats**  
- **Integer (INT)** : Uniformly distributed values.
- **Floating-Point (FP)** : Non-uniform distribution, allowing higher precision for small values but reduced precision for large values.
- **Hardware efficiency** : FP operations typically cost more than INT, but at 8-bit, FP8 and INT8 MAC (Multiply-Accumulate) units have nearly identical area costs.
 
**Post-Training Quantization (PTQ) for LLMs**

Two main PTQ strategies:
1. **Weight-Only (W-only) Quantization:**  Applies to weights only, e.g., W4A16.
2. **Weight-Activation (WA) Quantization:**  Quantizes both weights and activations, e.g., W8A8.

State-of-the-art (SOTA) methods:
 
- **LLM.int8()** : Uses mixed precision (INT8+FP16). 
- **SmoothQuant** : Redistributes quantization difficulty from activations to weights.
- **GPTQ & AWQ** : Use second-order information and pre-scaling techniques to improve quantization.


### **Comparative Analysis of INT and FP Formats**

**A. Hardware Cost of INT vs. FP MAC Units**
- **At 8-bit precision, FP8 and INT8 MACs require nearly the same hardware area** , aligning with H100 GPU capabilities.

**B. Quantization Error Comparison**  
1. **4-bit Weight-Only (W4) Quantization**  (LLaMA-65B model): 
  - Some layers perform better with INT4, while others favor FP4, indicating **layer-dependent format preference** .

![image](https://github.com/user-attachments/assets/97be028e-fdb3-4883-a92e-b784d1ff1f87)

2. **8-bit Weight-Activation (W8A8) Quantization** : 
  - **Weights** : INT8 generally has lower quantization error. 
  - **Activations** : FP8 shows **better robustness**  for dynamic activation tensors.
  - Best choice: INT8 for weights, FP8 for activations—but hardware constraints necessitate using the same format per layer.

![image](https://github.com/user-attachments/assets/7af41adf-5821-4c0b-8599-1385c1982c87)

### **Exploiting INT and FP Complementarity**

**A. Improved Low-Bit FP4 Format** 
- IEEE floating-point format reserves exponent values for NaN and Inf. 
- **Reallocating NaN & Inf to normalized numbers improves FP4 precision**  by 35%.

**B. Mixture of Formats Quantization (MoFQ)**

- Selects the best quantization format (INT or FP) **per layer**  based on quantization error.
- Works for both **W-only and WA quantization** .
- **Algorithm** : Iterates through layers, computes quantization error for INT and FP, and selects the lower-error format.

**C. Low-Bit W-Only Inference System**  
- **INT4 and FP4 require conversion to FP16 before computation**  due to FP16 activations.
- **W8A8 quantization** : FP16 activations are converted to FP8 or INT8 based on next-layer format selection.
- **No additional hardware overhead for FP-based or MoFQ-based inference**  compared to INT-based quantization.

![image](https://github.com/user-attachments/assets/7d5e0e92-deda-4e40-8775-b428fff2ca0c)


### **Conclusion**  
- **Comparative study** : INT and FP formats have complementary strengths. 
- **Key finding** : **FP8 and INT8 MAC units have similar hardware costs at low-bit quantization** . 
- **MoFQ method** : 
  - Selects the best quantization format **per layer** . 
  - **Achieves state-of-the-art accuracy**  in W4-only and W8A8 quantization. 
  - **No additional inference latency or hardware overhead** .
 
---

## [26] FP8-LM: Training FP8 Large Language Models

### **Abstract**

The paper explores **FP8 low-bit data formats**  for training large language models (LLMs), significantly reducing **memory usage and computation costs**  while maintaining accuracy.

The authors introduce an **FP8 automatic mixed-precision training framework**  with three levels of FP8 utilization, improving mixed-precision and distributed parallel training. **Key results**  show that training the **GPT-175B model on an H100 GPU platform**  using FP8: 
- **Reduces memory usage by 39%**
- **Speeds up training by 75% compared to BF16 (Megatron-LM)**
- **Outperforms Nvidia Transformer Engine by 37%** 
The **FP8 training methodology is generalizable**  to fine-tuning, instruction tuning, and reinforcement learning with human feedback (RLHF). The framework is **open-sourced**  at [aka.ms/MS.AMP](https://github.com/Azure/MS-AMP) .

### **1. Introduction**

LLMs have demonstrated exceptional performance in various domains but are extremely expensive to train.\
The cost of training models like **GPT-3 (175B) or PaLM (540B)**  is enormous, requiring **thousands of GPUs or TPUs** .\
Low-precision training is a **promising solution**  as it: 
- **Increases speed**
- **Reduces memory usage**
- **Minimizes communication overhead**

Most existing frameworks, such as **Megatron-LM, MetaSeq, and Colossal-AI** , use **FP32, FP16, or BF16 mixed-precision training** , but **FP8 offers significant efficiency gains** : 

- **2× speed-up**
- **50%-75% memory and communication savings**

#### **Challenges of FP8 Training**  
1. **Data underflow/overflow issues**  due to FP8’s limited dynamic range.
2. **Numerical instabilities and divergence**  during training.
#### **Proposed FP8 Mixed-Precision Framework**  
- Introduces **three levels of FP8 utilization**  (gradients, optimizer states, and distributed learning).
- Uses **precision decoupling**  and **automatic scaling**  to mitigate numerical instability.
- Achieves **29%-39% memory savings**  and **63%-65% communication cost reductions** .

### **2. FP8 LLM Training**

#### **2.1 FP8 Gradient and All-Reduce Communication**  
- Traditional mixed-precision training uses **FP16/FP32 for gradients** , leading to high communication costs.
- Applying **FP8 directly to gradients**  results in **loss of accuracy**  due to underflow/overflow.
- The paper proposes an **automatic scaling technique**  to adapt scaling factors dynamically, preventing numerical instability.

#### **2.2 FP8 Optimizer**  
- The **Adam optimizer**  typically consumes **16 bytes per parameter**  due to high-precision storage of gradients and optimizer states.
- The proposed **FP8 optimizer**  stores: 
  - FP8 first-order moment
  - FP16 master weights (with tensor scaling)
  - FP16 second-order moment
- This reduces **memory consumption from 16 bytes to 6 bytes per parameter**  (2.6× savings).

> My main takeaway is that direction of gradient matters, instead of magnitude.

![image](https://github.com/user-attachments/assets/aa8bdd9a-c5ea-4910-bfdf-836eca32f9b5)

#### **2.3 FP8 Distributed Parallel Training**  
- **Tensor Parallelism** : Uses **FP8 for weight and activation tensors** , reducing compute and communication overhead.
- **Sequence Parallelism** : Converts activation tensors to **FP8 before communication** , reducing costs.
- **ZeRO (Zero Redundancy Optimizer) Support** : Distributes **full tensors**  across devices while preserving **FP8 scaling factors** .

### **3. Experimentation**
#### **3.1 Experimental Setup**  
- **Training Dataset** : Collected from **CommonCrawl, The Pile, C4, OpenWebText, Wikipedia, RedPajama** , and other curated sources.
- **Model Configuration** : Uses a **decoder-only Transformer**  architecture (like GPT-3), with **RoPE embeddings and Flash Attention** .

#### **3.2 Main Results**
##### **Model Performance**  
- **Loss curves of FP8 models match BF16 models** , confirming **accuracy preservation**
- **Zero-shot evaluations**  on **Lambada, HellaSwag, BoolQ, PIQA, COPA**  show **comparable performance between FP8 and BF16** .
- **Fine-tuning (SFT & RLHF)** : FP8 achieves: 
  - 27% faster fine-tuning
  - 32% reduction in model weight memory
  - 62% optimizer state memory savings

**System Performance**  
- **Memory reduction** : FP8 achieves **28%-39% lower memory usage**  than BF16.
- **Training speed improvement** : 
  - 75% faster training for GPT-175B
  - 37% faster than Nvidia Transformer Engine
- **Communication efficiency** : 
  - 63%-65% reduction in weight gradient communication
  - 34% lower activation-related communication costs

#### **3.3 Ablation Study**  
- **Gradient Scaling** : **Automatic scaling**  reduces **underflow/overflow errors** , improving training stability.
- **Optimizer Precision** : 
  - FP16 master weights outperform FP8 master weights in accuracy preservation.
  - FP8 first-order gradient moment is viable, but FP8 second-order moment leads to divergence.
- **Parallelism Optimization** : 
  - **FP8 sequence and tensor parallelism**  reduce communication costs by **34%** .
  - **FP8 ZeRO**  maintains a balanced GPU memory load while saving memory.

### **4. Related Work**  
- **Mixed-Precision Training** : Prior work focused on **FP16/BF16** , but **FP8 remains underexplored** .
- **Low-Precision LLM Training** : 
  - **OPT, Bloom, Gopher, Chinchilla**  used **BF16**  for better numerical stability.
  - FP8 support was limited before Nvidia Hopper GPUs.
  - This work provides the **first systematic FP8 training framework**  for **pre-training and fine-tuning LLMs** .


### **5. Conclusion**  
- Introduces a **new FP8 mixed-precision training framework**  with **automatic scaling**  and **precision decoupling** . 
- Achieves **significant reductions in memory, compute, and communication costs** . 
- **Maintains model accuracy**  across **GPT models from 125M to 175B parameters** .
- Demonstrates **versatility**  in pre-training, instruction tuning, and RLHF.
- **Future work**  includes scaling to even larger models, training multi-modal models, and deploying FP8 LLMs on edge devices.

### *Key Summary in 3 Sentences**

This paper introduces an **FP8 mixed-precision training framework**  that reduces memory consumption by **39%** , speeds up training by **75%** , and **outperforms Nvidia Transformer Engine by 37%**  while maintaining LLM accuracy.\
The framework uses **automatic scaling and precision decoupling**  to stabilize training, supports **FP8 optimizers and distributed training** , and generalizes to **fine-tuning and reinforcement learning with human feedback (RLHF)** .\
These findings establish **FP8 as the next-generation precision format for training LLMs** , significantly lowering costs while preserving model performance.

---

## [139] FP8 Formats for Deep Learning
### **1. Introduction**
Deep learning models require increasing computational resources, necessitating lower-precision formats for efficiency.

FP8 is a **natural evolution**  from FP16 and BF16, reducing **compute and memory costs**  while maintaining **accuracy comparable to FP16** .

### Key contributions:
- **Two FP8 formats:**  
  - **E4M3** : 4-bit exponent, 3-bit mantissa (for weights and activations).
  - **E5M2** : 5-bit exponent, 2-bit mantissa (for gradients).
 
- **Training and inference in FP8**  match FP16/BF16 accuracy across CNNs, RNNs, and Transformers.
- **Post-training quantization (PTQ)**  using FP8 **outperforms int8**  while preserving model accuracy.


### **2. Aspects of FP8 Usage in Deep Learning**  
- **FP8 computations**  will be performed in **higher precision (FP16/FP32)** , with final results cast back to FP8.
- **Scaling factors**  are applied to **optimize FP8 precision** , similar to **loss-scaling in FP16 mixed precision** .
- **Handling of special values (NaNs, Infs) is modified in E4M3**  to increase dynamic range.

### **3. FP8 Binary Interchange Format**

![image](https://github.com/user-attachments/assets/dc054ba5-6892-4c63-888e-ef24c9789d8a)

FP8 includes **two encodings** : 
- **E4M3** : 
  - **Used for weights and activations** .
  - **No representation for infinities**  (max value: **448** ).
  - **Single NaN representation to extend range** .
 
- **E5M2** : 
  - **Used for gradients** . 
  - **Standard IEEE-like format** , supporting **NaNs and infinities** . 
  - Larger range (up to **57,344** ).

![image](https://github.com/user-attachments/assets/ac2dbf07-fd59-4902-a55b-0bb4b0385408)
  
#### **3.1 Special Value Representations**  
- **E4M3 removes infinities**  and limits NaNs to a **single pattern** , extending its **dynamic range** . 
- **E5M2 follows IEEE-754** , allowing **straightforward conversion from FP16** .

#### **3.2 Exponent Bias**  
- **E4M3 bias = 7, E5M2 bias = 15**  (matching IEEE-style representation). 
- Some models require **per-tensor scaling**  rather than a fixed exponent bias (Figure 2).

### **4. Empirical Results**
#### **4.1 Training**  
- FP8 training achieves **accuracy comparable to FP16/BF16**  across CNNs, RNNs, and Transformers.
- **Image Classification** : 
  - FP8 accuracy is **within statistical variation**  of FP16 for most CNNs (ResNet, MobileNet, VGG, etc.).
- **Language Translation** : 
  - FP8 BLEU scores **match FP16**  for Transformer and GNMT models.
- **NLP Models (Table 4, Figure 1)** : 
  - GPT models (126M to 175B parameters) trained in FP8 **match FP16 in perplexity** .
#### **4.2 Inference**  
- **FP8 post-training quantization (PTQ) outperforms int8** , retaining **full precision accuracy**  for:
  - BERT (F1 score on SQuAD).
  - GPT-3 (perplexity on Wikitext103).
- **FP8-trained models require no additional quantization steps** , simplifying deployment.

#### **4.3 Per-Tensor Scaling**  
- **Fixed exponent bias fails**  when additional tensors (e.g., residuals) are stored in FP8.
- **Per-tensor scaling maintains accuracy** , making FP8 viable for **expanded use beyond GEMMs** .


### **5. Conclusions**  
- **FP8 formats (E4M3, E5M2) efficiently reduce training and inference costs while maintaining accuracy** .
- **FP8 training is on par with FP16/BF16** , without hyperparameter changes.
- **FP8 simplifies inference**  by eliminating the need for quantization-aware training (QAT) required for int8.
- **Future work** : Expanding FP8 usage to **more tensor types and operations**  beyond matrix multiplications.


### **Key Takeaways in Three Sentences**

FP8 formats (E4M3 for weights/activations, E5M2 for gradients) **significantly reduce computation and memory overhead**  while maintaining **accuracy equivalent to FP16/BF16**  across CNNs, RNNs, and Transformer models.

**Post-training quantization (PTQ) with FP8 outperforms int8** , allowing for **simpler and more effective deployment**  of trained models. The study **validates FP8 training up to 175B parameters** , proving its scalability for large-scale deep learning applications.




