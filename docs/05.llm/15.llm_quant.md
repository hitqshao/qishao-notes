---
title: LLM Quantization & Outlier
date: 2025-01-27 23:32:49
permalink: /pages/dc7050/
---

1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization

---
## 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization

### Problems
The problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:

- High Dynamic Range of Activations
  - The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.    
  - This means that the values within these tensors vary significantly in magnitude.
  - Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.    
  - Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.
    
- Presence of Structured Outliers
  - The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).    
  - These outliers are not random; they appear to be correlated with specific input tokens and embedding dimensions.    
  - Further analysis revealed that these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP]).    
  - While this attention behavior might be beneficial for the model's performance, the outliers that cause it also create challenges for quantization.
    
- Sensitivity to Quantization Noise
  - Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.    
  - Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.    
  - This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.

### Solutions

solutions proposed in the paper:

- Mixed-precision PTQ
  - The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.    
  - To address this, they proposed using a higher bit-width (16-bit) for the more sensitive activation tensors, particularly the residual sum after the feed-forward network (FFN).    
  - This higher bit-width allows for more accurate representation of both the FFN's input and output, minimizing potential errors.    
  - Additionally, they explored using low-bit (2-4) quantization for weights and token embeddings, which can significantly reduce model size without much accuracy loss.    

- Per-embedding-group PTQ
  - The authors identified that outliers in the activation tensors primarily reside in a few specific embedding dimensions.  
  - To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.    
  - This method involves splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group.    
  - To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.    
  - This approach effectively handles outliers without significantly increasing computational overhead.

- Quantization-aware training (QAT)
  - The authors also explored QAT, where the model is trained with simulated quantization operations.    
  - This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.    
  - During QAT, they used learnable ranges for both weights and activations, further enhancing the model's adaptability to quantization.
