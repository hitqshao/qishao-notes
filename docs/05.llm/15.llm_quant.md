---
title: LLM Mixed Precision & Quantization & Outlier
date: 2025-01-27 23:32:49
permalink: /pages/dc7050/
---

1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization
2. [43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models
3. [26] FP8-LM: Training FP8 Large Language Models :+1: from Microsoft
4. [139] FP8 Formats for Deep Learning
5. [41] With Shared Microexponents, A Little Shifting Goes a Long Way :+1: from Meta, Microsoft :+1:
6. [34] Stable and low-precision training for large-scale vision-language models :+1:
   *mentioned in Deepseek paper mixed precision training section. Not read yet.*
7. [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization
8. [47] Microscaling Data Formats for Deep Learning
9. [Unread 2 Y2024] To FP8 and Back Again: Quantifying the Effects of Reducing Precision on LLM Training Stability
10. [145] Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model
11. [Unread 27]Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference
12. [45] PB-LLM: Partially Binarized Large Language Models

---
## 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization

### Problems
The problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:

- High Dynamic Range of Activations
  - The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.    
  - This means that the values within these tensors vary significantly in magnitude.
  - Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.    
  - Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.
    
- Presence of Structured Outliers
  - The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).    
  - These outliers are not random; they appear to be correlated with specific input tokens and embedding dimensions.    
  - Further analysis revealed that these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP]).    
  - While this attention behavior might be beneficial for the model's performance, the outliers that cause it also create challenges for quantization.
    
- Sensitivity to Quantization Noise
  - Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.    
  - Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.    
  - This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.

### Solutions

solutions proposed in the paper:

- Mixed-precision PTQ
  - The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.    
  - To address this, they proposed using a higher bit-width (16-bit) for the more sensitive activation tensors, particularly the residual sum after the feed-forward network (FFN).    
  - This higher bit-width allows for more accurate representation of both the FFN's input and output, minimizing potential errors.    
  - Additionally, they explored using low-bit (2-4) quantization for weights and token embeddings, which can significantly reduce model size without much accuracy loss.    

- Per-embedding-group PTQ
  - The authors identified that outliers in the activation tensors primarily reside in a few specific embedding dimensions.  
  - To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.    
  - This method involves splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group.    
  - To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.    
  - This approach effectively handles outliers without significantly increasing computational overhead.

- Quantization-aware training (QAT)
  - The authors also explored QAT, where the model is trained with simulated quantization operations.    
  - This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.    
  - During QAT, they used learnable ranges for both weights and activations, further enhancing the model's adaptability to quantization.

---

## 2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models

### **Key Takeaways in Three Sentences**  
1. The study demonstrates that **low-bit floating-point formats, particularly FP8, provide superior quantization accuracy for LLMs compared to INT8** , with comparable hardware efficiency at 8-bit precision.
2. The **Mixture of Formats Quantization (MoFQ) approach optimally selects between INT and FP per layer** , improving accuracy without increasing computational overhead.
3. MoFQ achieves **state-of-the-art results in both W4-only and W8A8 quantization** , outperforming existing methods like GPTQ, AWQ, LLM.int8(), and SmoothQuant while maintaining **efficient inference speed** .

### **Abstract**

The study finds that optimal quantization formats vary across layers in LLMs, leading to the **Mixture of Formats Quantization (MoFQ)**  approach, which selects the best format per layer.\
MoFQ achieves superior or comparable performance to current quantization methods for weight-only (W-only) and weight-activation (WA) quantization without additional hardware overhead.

### **Introduction** 

Quantization minimizes LLMs' size and inference costs, with prior work focusing on low-bit integer formats.\
However, as LLMs grow, integer quantization becomes less effective, requiring optimizations or alternatives. Low-bit floating-point formats have emerged as viable alternatives, with FP8 already supported in NVIDIA’s H100 GPUs.

The study:
1. Compares INT and FP formats in terms of hardware efficiency and quantization error. 
2. Proposes **Mixture of Formats Quantization (MoFQ)** , selecting the best format per layer.
3. Implements an inference system for W-only quantization, maintaining performance parity with INT-based systems.

### **Background and Related Works**

**Integer vs. Floating-Point Formats**  
- **Integer (INT)** : Uniformly distributed values.
- **Floating-Point (FP)** : Non-uniform distribution, allowing higher precision for small values but reduced precision for large values.
- **Hardware efficiency** : FP operations typically cost more than INT, but at 8-bit, FP8 and INT8 MAC (Multiply-Accumulate) units have nearly identical area costs.
 
**Post-Training Quantization (PTQ) for LLMs**

Two main PTQ strategies:
1. **Weight-Only (W-only) Quantization:**  Applies to weights only, e.g., W4A16.
2. **Weight-Activation (WA) Quantization:**  Quantizes both weights and activations, e.g., W8A8.

State-of-the-art (SOTA) methods:
 
- **LLM.int8()** : Uses mixed precision (INT8+FP16). 
- **SmoothQuant** : Redistributes quantization difficulty from activations to weights.
- **GPTQ & AWQ** : Use second-order information and pre-scaling techniques to improve quantization.


### **Comparative Analysis of INT and FP Formats**

**A. Hardware Cost of INT vs. FP MAC Units**
- **At 8-bit precision, FP8 and INT8 MACs require nearly the same hardware area** , aligning with H100 GPU capabilities.

**B. Quantization Error Comparison**  
1. **4-bit Weight-Only (W4) Quantization**  (LLaMA-65B model): 
  - Some layers perform better with INT4, while others favor FP4, indicating **layer-dependent format preference** .

![image](https://github.com/user-attachments/assets/97be028e-fdb3-4883-a92e-b784d1ff1f87)

2. **8-bit Weight-Activation (W8A8) Quantization** : 
  - **Weights** : INT8 generally has lower quantization error. 
  - **Activations** : FP8 shows **better robustness**  for dynamic activation tensors.
  - Best choice: INT8 for weights, FP8 for activations—but hardware constraints necessitate using the same format per layer.

![image](https://github.com/user-attachments/assets/7af41adf-5821-4c0b-8599-1385c1982c87)

### **Exploiting INT and FP Complementarity**

**A. Improved Low-Bit FP4 Format** 
- IEEE floating-point format reserves exponent values for NaN and Inf. 
- **Reallocating NaN & Inf to normalized numbers improves FP4 precision**  by 35%.

**B. Mixture of Formats Quantization (MoFQ)**

- Selects the best quantization format (INT or FP) **per layer**  based on quantization error.
- Works for both **W-only and WA quantization** .
- **Algorithm** : Iterates through layers, computes quantization error for INT and FP, and selects the lower-error format.

**C. Low-Bit W-Only Inference System**  
- **INT4 and FP4 require conversion to FP16 before computation**  due to FP16 activations.
- **W8A8 quantization** : FP16 activations are converted to FP8 or INT8 based on next-layer format selection.
- **No additional hardware overhead for FP-based or MoFQ-based inference**  compared to INT-based quantization.

![image](https://github.com/user-attachments/assets/7d5e0e92-deda-4e40-8775-b428fff2ca0c)


### **Conclusion**  
- **Comparative study** : INT and FP formats have complementary strengths. 
- **Key finding** : **FP8 and INT8 MAC units have similar hardware costs at low-bit quantization** . 
- **MoFQ method** : 
  - Selects the best quantization format **per layer** . 
  - **Achieves state-of-the-art accuracy**  in W4-only and W8A8 quantization. 
  - **No additional inference latency or hardware overhead** .
 
---

## [26] FP8-LM: Training FP8 Large Language Models

### **Abstract**

The paper explores **FP8 low-bit data formats**  for training large language models (LLMs), significantly reducing **memory usage and computation costs**  while maintaining accuracy.

The authors introduce an **FP8 automatic mixed-precision training framework**  with three levels of FP8 utilization, improving mixed-precision and distributed parallel training. **Key results**  show that training the **GPT-175B model on an H100 GPU platform**  using FP8: 
- **Reduces memory usage by 39%**
- **Speeds up training by 75% compared to BF16 (Megatron-LM)**
- **Outperforms Nvidia Transformer Engine by 37%** 
The **FP8 training methodology is generalizable**  to fine-tuning, instruction tuning, and reinforcement learning with human feedback (RLHF). The framework is **open-sourced**  at [aka.ms/MS.AMP](https://github.com/Azure/MS-AMP) .

### **1. Introduction**

LLMs have demonstrated exceptional performance in various domains but are extremely expensive to train.\
The cost of training models like **GPT-3 (175B) or PaLM (540B)**  is enormous, requiring **thousands of GPUs or TPUs** .\
Low-precision training is a **promising solution**  as it: 
- **Increases speed**
- **Reduces memory usage**
- **Minimizes communication overhead**

Most existing frameworks, such as **Megatron-LM, MetaSeq, and Colossal-AI** , use **FP32, FP16, or BF16 mixed-precision training** , but **FP8 offers significant efficiency gains** : 

- **2× speed-up**
- **50%-75% memory and communication savings**

#### **Challenges of FP8 Training**  
1. **Data underflow/overflow issues**  due to FP8’s limited dynamic range.
2. **Numerical instabilities and divergence**  during training.
#### **Proposed FP8 Mixed-Precision Framework**  
- Introduces **three levels of FP8 utilization**  (gradients, optimizer states, and distributed learning).
- Uses **precision decoupling**  and **automatic scaling**  to mitigate numerical instability.
- Achieves **29%-39% memory savings**  and **63%-65% communication cost reductions** .

### **2. FP8 LLM Training**

#### **2.1 FP8 Gradient and All-Reduce Communication**  
- Traditional mixed-precision training uses **FP16/FP32 for gradients** , leading to high communication costs.
- Applying **FP8 directly to gradients**  results in **loss of accuracy**  due to underflow/overflow.
- The paper proposes an **automatic scaling technique**  to adapt scaling factors dynamically, preventing numerical instability.

#### **2.2 FP8 Optimizer**  
- The **Adam optimizer**  typically consumes **16 bytes per parameter**  due to high-precision storage of gradients and optimizer states.
- The proposed **FP8 optimizer**  stores: 
  - FP8 first-order moment
  - FP16 master weights (with tensor scaling)
  - FP16 second-order moment
- This reduces **memory consumption from 16 bytes to 6 bytes per parameter**  (2.6× savings).

> My main takeaway is that direction of gradient matters, instead of magnitude.

![image](https://github.com/user-attachments/assets/aa8bdd9a-c5ea-4910-bfdf-836eca32f9b5)

#### **2.3 FP8 Distributed Parallel Training**  
- **Tensor Parallelism** : Uses **FP8 for weight and activation tensors** , reducing compute and communication overhead.
- **Sequence Parallelism** : Converts activation tensors to **FP8 before communication** , reducing costs.
- **ZeRO (Zero Redundancy Optimizer) Support** : Distributes **full tensors**  across devices while preserving **FP8 scaling factors** .

### **3. Experimentation**
#### **3.1 Experimental Setup**  
- **Training Dataset** : Collected from **CommonCrawl, The Pile, C4, OpenWebText, Wikipedia, RedPajama** , and other curated sources.
- **Model Configuration** : Uses a **decoder-only Transformer**  architecture (like GPT-3), with **RoPE embeddings and Flash Attention** .

#### **3.2 Main Results**
##### **Model Performance**  
- **Loss curves of FP8 models match BF16 models** , confirming **accuracy preservation**
- **Zero-shot evaluations**  on **Lambada, HellaSwag, BoolQ, PIQA, COPA**  show **comparable performance between FP8 and BF16** .
- **Fine-tuning (SFT & RLHF)** : FP8 achieves: 
  - 27% faster fine-tuning
  - 32% reduction in model weight memory
  - 62% optimizer state memory savings

**System Performance**  
- **Memory reduction** : FP8 achieves **28%-39% lower memory usage**  than BF16.
- **Training speed improvement** : 
  - 75% faster training for GPT-175B
  - 37% faster than Nvidia Transformer Engine
- **Communication efficiency** : 
  - 63%-65% reduction in weight gradient communication
  - 34% lower activation-related communication costs

#### **3.3 Ablation Study**  
- **Gradient Scaling** : **Automatic scaling**  reduces **underflow/overflow errors** , improving training stability.
- **Optimizer Precision** : 
  - FP16 master weights outperform FP8 master weights in accuracy preservation.
  - FP8 first-order gradient moment is viable, but FP8 second-order moment leads to divergence.
- **Parallelism Optimization** : 
  - **FP8 sequence and tensor parallelism**  reduce communication costs by **34%** .
  - **FP8 ZeRO**  maintains a balanced GPU memory load while saving memory.

### **4. Related Work**  
- **Mixed-Precision Training** : Prior work focused on **FP16/BF16** , but **FP8 remains underexplored** .
- **Low-Precision LLM Training** : 
  - **OPT, Bloom, Gopher, Chinchilla**  used **BF16**  for better numerical stability.
  - FP8 support was limited before Nvidia Hopper GPUs.
  - This work provides the **first systematic FP8 training framework**  for **pre-training and fine-tuning LLMs** .


### **5. Conclusion**  
- Introduces a **new FP8 mixed-precision training framework**  with **automatic scaling**  and **precision decoupling** . 
- Achieves **significant reductions in memory, compute, and communication costs** . 
- **Maintains model accuracy**  across **GPT models from 125M to 175B parameters** .
- Demonstrates **versatility**  in pre-training, instruction tuning, and RLHF.
- **Future work**  includes scaling to even larger models, training multi-modal models, and deploying FP8 LLMs on edge devices.

### *Key Summary in 3 Sentences**

This paper introduces an **FP8 mixed-precision training framework**  that reduces memory consumption by **39%** , speeds up training by **75%** , and **outperforms Nvidia Transformer Engine by 37%**  while maintaining LLM accuracy.\
The framework uses **automatic scaling and precision decoupling**  to stabilize training, supports **FP8 optimizers and distributed training** , and generalizes to **fine-tuning and reinforcement learning with human feedback (RLHF)** .\
These findings establish **FP8 as the next-generation precision format for training LLMs** , significantly lowering costs while preserving model performance.

---

## [139] FP8 Formats for Deep Learning
### **1. Introduction**
Deep learning models require increasing computational resources, necessitating lower-precision formats for efficiency.

FP8 is a **natural evolution**  from FP16 and BF16, reducing **compute and memory costs**  while maintaining **accuracy comparable to FP16** .

### Key contributions:
- **Two FP8 formats:**  
  - **E4M3** : 4-bit exponent, 3-bit mantissa (for weights and activations).
  - **E5M2** : 5-bit exponent, 2-bit mantissa (for gradients).
 
- **Training and inference in FP8**  match FP16/BF16 accuracy across CNNs, RNNs, and Transformers.
- **Post-training quantization (PTQ)**  using FP8 **outperforms int8**  while preserving model accuracy.


### **2. Aspects of FP8 Usage in Deep Learning**  
- **FP8 computations**  will be performed in **higher precision (FP16/FP32)** , with final results cast back to FP8.
- **Scaling factors**  are applied to **optimize FP8 precision** , similar to **loss-scaling in FP16 mixed precision** .
- **Handling of special values (NaNs, Infs) is modified in E4M3**  to increase dynamic range.

### **3. FP8 Binary Interchange Format**

![image](https://github.com/user-attachments/assets/dc054ba5-6892-4c63-888e-ef24c9789d8a)

FP8 includes **two encodings** : 
- **E4M3** : 
  - **Used for weights and activations** .
  - **No representation for infinities**  (max value: **448** ).
  - **Single NaN representation to extend range** .
 
- **E5M2** : 
  - **Used for gradients** . 
  - **Standard IEEE-like format** , supporting **NaNs and infinities** . 
  - Larger range (up to **57,344** ).

![image](https://github.com/user-attachments/assets/ac2dbf07-fd59-4902-a55b-0bb4b0385408)
  
#### **3.1 Special Value Representations**  
- **E4M3 removes infinities**  and limits NaNs to a **single pattern** , extending its **dynamic range** . 
- **E5M2 follows IEEE-754** , allowing **straightforward conversion from FP16** .

#### **3.2 Exponent Bias**  
- **E4M3 bias = 7, E5M2 bias = 15**  (matching IEEE-style representation). 
- Some models require **per-tensor scaling**  rather than a fixed exponent bias (Figure 2).

### **4. Empirical Results**
#### **4.1 Training**  
- FP8 training achieves **accuracy comparable to FP16/BF16**  across CNNs, RNNs, and Transformers.
- **Image Classification** : 
  - FP8 accuracy is **within statistical variation**  of FP16 for most CNNs (ResNet, MobileNet, VGG, etc.).
- **Language Translation** : 
  - FP8 BLEU scores **match FP16**  for Transformer and GNMT models.
- **NLP Models (Table 4, Figure 1)** : 
  - GPT models (126M to 175B parameters) trained in FP8 **match FP16 in perplexity** .
#### **4.2 Inference**  
- **FP8 post-training quantization (PTQ) outperforms int8** , retaining **full precision accuracy**  for:
  - BERT (F1 score on SQuAD).
  - GPT-3 (perplexity on Wikitext103).
- **FP8-trained models require no additional quantization steps** , simplifying deployment.

#### **4.3 Per-Tensor Scaling**  
- **Fixed exponent bias fails**  when additional tensors (e.g., residuals) are stored in FP8.
- **Per-tensor scaling maintains accuracy** , making FP8 viable for **expanded use beyond GEMMs** .


### **5. Conclusions**  
- **FP8 formats (E4M3, E5M2) efficiently reduce training and inference costs while maintaining accuracy** .
- **FP8 training is on par with FP16/BF16** , without hyperparameter changes.
- **FP8 simplifies inference**  by eliminating the need for quantization-aware training (QAT) required for int8.
- **Future work** : Expanding FP8 usage to **more tensor types and operations**  beyond matrix multiplications.


### **Key Takeaways in Three Sentences**

FP8 formats (E4M3 for weights/activations, E5M2 for gradients) **significantly reduce computation and memory overhead**  while maintaining **accuracy equivalent to FP16/BF16**  across CNNs, RNNs, and Transformer models.

**Post-training quantization (PTQ) with FP8 outperforms int8** , allowing for **simpler and more effective deployment**  of trained models. The study **validates FP8 training up to 175B parameters** , proving its scalability for large-scale deep learning applications.

---

## [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization
### **Abstract**
The study introduces **ParetoQ** , a unified framework for evaluating extremely low-bit quantization (1-bit to 4-bit) in large language models (LLMs).\
It identifies a **learning transition**  between 2-bit and 3-bit models, with 3-bit and higher retaining pre-trained distributions, while 2-bit and below undergo drastic representation changes.\
By optimizing training strategies and quantization functions, **ParetoQ achieves state-of-the-art (SOTA) performance**  across multiple bit-widths.\
Notably, a **ternary (1.58-bit) 600M model surpasses a previous 3B ternary model** , using only one-fifth of the parameters.\
The study finds that **2-bit quantization offers a superior balance between accuracy, model size, and hardware efficiency**  compared to 4-bit and binary quantization.

### **1. Introduction**
As models scale, lower-precision computation is gaining traction due to **memory savings and computational efficiency** .\
Prior studies suggest conflicting optimal quantization levels (1.58-bit, 2-bit, 4-bit, etc.), but **no unified framework existed**  to systematically compare their effectiveness.

### **Key Questions:** 
- What is the optimal trade-off between bit-width and model size?
- How does quantization impact **scaling laws**  in low-bit settings?
- What training strategies and quantization functions yield **Pareto-optimal results** ?

### **ParetoQ Approach:**  
- Incorporates **five key dimensions** : model size (**N** ), token count (**D** ), quantization precision (**P** ), training strategy (**S** ), and quantization function (**F** ). 
- Identifies **bit-specific training schemes and quantization functions** . 
- Establishes that **binary quantization significantly degrades accuracy** , while **ternary (1.58-bit), 2-bit, and 3-bit quantization match or exceed 4-bit performance** .

### **2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs**
#### **2.1 Training Budget Allocation**  
- **Post-Training Quantization (PTQ)**  is easier to implement but **performs poorly**  below 4-bit.
- **Quantization-Aware Training (QAT)**  integrates quantization during training, **improving low-bit performance** .
- **Optimal budget split:**  **90% full-precision training, 10% QAT fine-tuning** .
- **Finding-1:**  **QAT fine-tuning outperforms both PTQ and QAT from scratch** , achieving the best trade-off between accuracy and efficiency.

#### **2.2 Fine-tuning Characteristics**  
- Fine-tuning **improves accuracy across all bit-widths** , including binary and ternary models.
- **Lower-bit models (≤2-bit) require more fine-tuning (30B tokens), while 3-bit and 4-bit saturate at 10B tokens** .
- **Finding-2:**  **Bit-width transition effect:**  
  - **3-bit & 4-bit recover near full precision with fine-tuning** . 
  - **1-bit to 2-bit undergo substantial weight transformations** , requiring more tokens.
  - QAT serves as "compensation" for 3-bit+ but "reconstruction" for ≤2-bit models.

### **3. A Hitchhiker’s Guide to Quantization Method Choices**
#### **3.1 Trade-offs in Low-bit Quantization**  
1. **Range Clipping** : Lower-bit quantization suffers from outlier effects, requiring range clipping or **learnable scales** .
2. **Quantization Grids** : 
  - Binary & Ternary require balanced levels.
  - 2-bit prefers symmetric distribution*.
  - 3-bit and 4-bit benefit from including "0" in quantization levels.
#### **3.2 Introducing ParetoQ**
- **Combines the best quantization functions per bit-width** : 
  - **1-bit** : Elastic Binarization.
  - **1.58-bit, 2-bit** : **Stretched Elastic Quant (SEQ)** .
  - **3-bit, 4-bit** : **Learned Step Size Quantization (LSQ)** . 
- **Finding-3:**  No single best function for all bit-widths.
  **Learnable range settings outperform fixed statistical methods** , especially for **sub-4-bit quantization** .


### **4. Pareto-Optimality of Extremely Low-Bit LLMs**
#### **4.1 Accuracy-Compression Trade-off**  
- Ternary (1.58-bit), 2-bit, and 3-bit outperform 4-bit in accuracy-size efficiency.
- 2-bit and ternary quantization sit on the Pareto frontier.
#### **4.2 Hardware Constraints**  
- Ternary (1.58-bit) appears efficient but is difficult to implement due to indexing overhead. 
- 2-bit is more hardware-friendly**  due to **simpler storage and arithmetic operations.
#### **4.3 Accuracy-Speed Trade-off**  
- 2-bit achieves higher speed at the same accuracy as 4-bit.
- 2-bit kernels are significantly faster than 4-bit kernels in large matrix multiplications.

### **6. Related Work**  
- **Early quantization research**  focused on **8-bit and 4-bit LLMs** .
- Recent **sub-4-bit research**  explored **ternary, 2-bit, and 1-bit models** , but lacked a **unified comparison framework** .
- **ParetoQ is the first study to systematically compare sub-4-bit quantization schemes** .

**7. Conclusions**  
- ParetoQ unifies training and quantization schemes across five bit-widths.
- Ternary (1.58-bit), 2-bit, and 3-bit quantization outperform 4-bit in the accuracy-size trade-off.
- 2-bit is the most practical choice due to its hardware efficiency. 
- First framework that ensures fair comparisons across different quantization methods.

**Key Takeaways (3 Sentences)**  
1. ParetoQ introduces a unified framework for extreme low-bit quantization, identifying 2-bit as the most efficient trade-off between accuracy, memory, and computational speed.
2. A clear transition exists between 2-bit and 3-bit models, where lower-bit quantization requires substantial fine-tuning to compensate for drastic weight transformations.
3. With its optimized quantization functions and training schemes, ParetoQ achieves state-of-the-art performance across all bit-widths, surpassing previous specialized methods.

---
## [47] Microscaling Data Formats for Deep Learning
![image](https://github.com/user-attachments/assets/e1533c6c-54ca-47f8-946a-2d6fe7d08aef)


### **Introduction**
The rapid advancement of deep learning models has led to increased computational and storage costs.\
One approach to mitigating these costs is reducing bit-width precision in data formats, moving beyond traditional **FP32**  to lower-bit formats such as **FP16, BFloat16, FP8, and INT8**.\
However, per-tensor scaling in low-bit-width formats struggles with dynamic range limitations.\
**Microscaling (MX) data formats**  introduce **per-block scaling factors**  to enhance efficiency, maintain accuracy, and ease deployment in AI hardware.

### **Microscaling (MX) Data Formats**
MX formats encode numerical values in **fixed-size blocks** , where each block consists of: 
- A **shared scaling factor (X)**
- Multiple **narrow bit-width elements (Pi)**

This approach **extends the dynamic range**  beyond what per-tensor scaling allows, making sub-8-bit computations feasible.\
MX formats are **hardware-efficient**  while minimizing accuracy loss and **ensuring seamless adoption in existing AI frameworks**.

### **Concrete MX Formats**
MX formats are categorized based on **block size, scale format, and element bit-width**.

| Format | Block Size | Scale Data Format | Scale Bits | Element Format | Element Bit-width | 
| --- | --- | --- | --- | --- | --- | 
| MXFP8 | 32 | E8M0 | 8 | FP8 (E4M3/E5M2) | 8 | 
| MXFP6 | 32 | E8M0 | 8 | FP6 (E2M3/E3M2) | 6 | 
| MXFP4 | 32 | E8M0 | 8 | FP4 (E2M1) | 4 | 
| MXINT8 | 32 | E8M0 | 8 | INT8 | 8 | 

### **Scalar Float to MX Format Conversion**
To convert floating-point data to an MX format, the **shared scaling factor (X)**  is computed based on the largest absolute value in a block.\
Each element is then **normalized using X and quantized**  to the desired format. The conversion follows a **quantization algorithm**  that: 
1. **Determines the scaling exponent**  from the maximum value in the block.
2. **Computes X as a power of two** .
3. **Quantizes elements (Pi) based on X** .

### **Compute Flow and Training Pipeline**
For deep learning workloads, **dot-product operations (e.g., matrix multiplication, convolutions) are performed in MX formats** , while **non-dot operations (e.g., activations, normalization, residual add) remain in higher-precision formats like FP32 or BFloat16** .\
Training involves keeping a **master FP32 copy of weights**  while performing compute-intensive operations in MX formats.\
**Quantization-aware fine-tuning**  is often required for best accuracy, particularly for lower-bit formats like MXFP6 and MXFP4.

**Experimental Results**
**Inference** MX data formats were tested across **language models, vision transformers, speech recognition, and recommendation models**.

**Direct-Cast Inference (No Fine-Tuning)**  
- **MXINT8 performs nearly identically to FP32**  across all tasks, making it a **drop-in replacement** . 
- **MXFP8 and MXFP6 maintain good accuracy** , but MXFP6 requires fine-tuning for best results. 
- **MXFP4 suffers from significant accuracy loss** , especially in complex models.

**Post-Training Quantization (PTQ) with Error Diffusion**  
- Error diffusion (similar to **GPFQ-based post-training quantization** ) helps recover accuracy. 
- **MXFP6 achieves results close to FP32**  after error diffusion. 
- **MXFP4 remains significantly worse than FP32, limiting its practical use in inference** .

**Finetuned Inference**  
- **MXFP6 achieves FP32-level accuracy after fine-tuning** .
- **MXFP4 improves slightly but still lags behind** .
- MXINT8 continues to serve as the most effective **low-friction alternative to FP32** .

**Generative Model Inference (GPT-3, LLaMA)**  
- **MXINT8 closely matches FP32 performance**  on GPT-3 and LLaMA.
- **MXFP6 and MXFP8 perform well in most tasks** , but some degradation is observed in complex benchmarks.
- **MXFP4 shows noticeable loss** , especially in **zero-shot settings** .

**Training with MX Formats**
For the **first time, MX formats enable sub-8-bit training of large-scale transformers**  with minimal accuracy loss.

**Training with MXFP6**  
- **MXFP6 (E3M2) trains models with no accuracy drop compared to FP32** .
- This represents the **first demonstration of 6-bit training for large transformer models**  without modifications to the training recipe.
- **Hyperparameters remain unchanged from FP32 training**, making MXFP6 a practical choice.

**Training with MXFP4 + MXFP6**  
- **MXFP4 weights combined with MXFP6 activations/gradients**  yield slightly worse performance but remain viable for training.
- **Loss curves show only a minor increase in training loss** , proving feasibility.

### **Conclusion**
Microscaling (MX) data formats introduce **per-block scaling**  to **reduce bit-width**  while maintaining **high accuracy, hardware efficiency, and seamless integration** .

### Key findings: 
1. MXINT8 is an effective drop-in replacement for FP32 inference.
2. MXFP6 enables sub-8-bit inference and training with minimal accuracy loss.
3. MXFP4 combined with MXFP6 remains viable for training but suffers in inference. 
4. First-ever demonstration of training large generative models using 6-bit weights, activations, and gradients** .
MX formats offer a compelling path toward **lower precision deep learning**  without sacrificing model quality.


### **Three-Sentence Summary**
Microscaling (MX) data formats introduce **per-block scaling factors** , improving the efficiency and accuracy of sub-8-bit deep learning computations. **MXINT8 serves as a near-lossless drop-in replacement for FP32 inference** , while **MXFP6 enables large-scale deep learning models to be trained with sub-8-bit precision without altering training recipes** . The results demonstrate that **MX formats significantly reduce computational and storage costs while maintaining model performance** , making them a strong alternative to traditional floating-point formats.
