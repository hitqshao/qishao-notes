---
title: llm optimizations
date: 2024-11-03 23:32:49
permalink: /pages/dc7039/
---

1. [1900] Mixed Precision Training
2. [1519] Training Compute-Optimal Large Language Models
3. [440] Measuring the Effects of Data Parallelism on Neural Network Training
4. [341] ZeRO-Offload: Democratizing Billion-Scale Model Training
5. [255] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU
6. [142] Performance, Design, and Autotuning of Batched GEMM for GPUs
7. [31] LLM Inference Unveiled Survey and Roofline Model Insights
8. [13] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving
9. [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization

---
### [1900] Mixed Precision Training

![image](https://github.com/user-attachments/assets/015ccb67-ef34-4c76-ad52-10618c578ad7)

![image](https://github.com/user-attachments/assets/1a564342-e231-42a4-99d5-e9fef4500a1a)


**Loss Scaling**

Note that much of the FP16 representable range was left unused, while many values were below the minimum representable range and became zeros.\
Scaling up the gradients will shift them to occupy more of the representable range and preserve values that are otherwise lost to zeros.\
This particular network diverges when gradients are not scaled, but scaling them by a factor of 8 (increasing the exponents by 3) is sufficient to match the accuracy achieved with FP32 training.

activation gradient values below 2^−27 in magnitude were irrelevant to the training of this model, but values in the [2^−27, 2^−24) range were important to preserve.

One efficient way to shift the gradient values into FP16-representable range is to scale the loss value computed in the forward pass, prior to starting back-propagation.

By chain rule back-propagation ensures that all the gradient values are scaled by the same amount.

The gradients need to be unscaled before the final weight update.


---
### [1519] Training Compute-Optimal Large Language Models

**Fix model sizes and vary number of training tokens**
![image](https://github.com/user-attachments/assets/7c6f0cec-cc66-4b28-a1eb-035aee0ef342)

On the left we show all of our different runs. We launched a range of model sizes going from 70M to 10B, each for four different cosine cycle lengths.\
From these curves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate the optimal model size (center) for a given compute budget and the optimal number of training tokens (right).\
In green, we show projections of optimal model size and training token count based on the number of FLOPs used to train Gopher (5.76 × 1023).

**IsoFLOP profiles**
![image](https://github.com/user-attachments/assets/bf7aa98a-ca33-474e-bdfe-706a95f20a94)

For various model sizes, we choose the number of training tokens such that the final FLOPs is a constant.\
The cosine cycle length is set to match the target FLOP count.\
We find a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train (left).\
Using the location of these valleys, we project optimal model size and number of tokens for larger models (center and right).\
In green, we show the estimated number of parameters and tokens for an optimal model trained with the compute budget of Gopher.

---
### [440] Measuring the Effects of Data Parallelism on Neural Network Training [Google]

#### Steps to Result Depends on Batch Size in a Similar Way Across Problems.

In all cases, as the batch size grows, there is an initial period of perfect scaling (indicated with a dashed line) where the steps needed to achieve the error goal
halves for each doubling of the batch size.\
Then there is a region of diminishing returns that eventually leads to a region of maximal data parallelism where additional parallelism provides no benefit whatsoever.

![image](https://github.com/user-attachments/assets/afb695c3-1503-45cc-a123-37cd8110880e)

![image](https://github.com/user-attachments/assets/a132a4fc-f469-4605-b522-558345f0b9c3)

If the curves in Figure 1 were very sensitive to the goal validation error, then measuring the steps needed to reach our particular choice of the goal would not be a meaningful proxy for training speed.\

For small changes in the goal validation error, we do not care about vertical shifts as long as the transition points between the three scaling regions remain relatively unchanged.\

Figure 2 shows that varying the error goal only vertically shifts the stepsto-result curve, at least for modest variations centered around a good absolute validation error.

Furthermore, although we ultimately care about out-of-sample error, if our plots looked very different when measuring the steps needed to reach a particular training error, then we would need to include both curves when presenting our results.


#### Validating Our Measurement Protocol

#### Some Models Can Exploit Much Larger Batch Sizes Than Others

![image](https://github.com/user-attachments/assets/d4e1c07a-5e03-42f4-94fc-e750175b1396)

**This might be the begining of the scale law.**

#### Momentum Extends Perfect Scaling to Larger Batch Sizes, but Matches Plain SGD at Small Batch Sizes

#### The Data Set Matters, at Least Somewaht

#### Regularization Can Be More Helpful at Some Batch Sizes than Others

#### The Best Learning Rate and Momentrum Vary with Batch Size

#### Solution Quality Depends on Compute Budget More Than Batch Size

Taken together, these observations suggest that in practice the relevant question is not which batch size leads to the best performance, but rather how compute budget varies as a function of batch size. 

![image](https://github.com/user-attachments/assets/a7164eb8-4ff2-4e9f-a68f-8538d4ce54b4)

---
### [142] Performance, Design, and Autotuning of Batched GEMM for GPUs

![image](https://github.com/user-attachments/assets/ad7a256c-91eb-4087-a8de-ccfeb05ea8a3)

![image](https://github.com/user-attachments/assets/f205bad1-19a3-4843-8fca-cc12cfa7298a)

---
### [341] ZeRO-Offload: Democratizing Billion-Scale Model Training

Key Sentence: Offload calucation of updating fp32 parameter in CPU to save memory.

Mixed precision training often keeps two copies of the parameters, one in float-16 (fp16) and the other in float-32 (fp32).\
The gradients are stored in fp16. \
In addition to the parameters and gradients, the Adam optimizer keeps track of the momentum and variance of the gradients. These optimizer states are stored in fp32.

training a model in mixed precision with the Adam optimizer requires at least:
- 2 bytes of memory for each fp16 parameter and gradient
- 4 byte of memory for each fp32 parameter
- moementum and variance of each gradient.

![image](https://github.com/user-attachments/assets/43a0baa2-56b3-4c54-bfcc-6890f8bf44ef)

- 2M parameter FP16
- 2M gradient FP16
- 12M for parameter, momentum, variance FP32


**In total, a model with M parameters requires 16×M bytes of memory.**

![image](https://github.com/user-attachments/assets/399fc07d-70d2-4db4-b6d6-988f7b82aaed)

#### Offload update of parameter to CPU

FWD-BWD Super node in GPU \
Update Super node in CPU

![image](https://github.com/user-attachments/assets/b99080e4-74a2-41ea-bfcc-a1548089f3b4)

#### CPU optimizer

1) SIMD vector instruction [15] for fully exploiting the hardware parallelism supported on CPU architectures.
2) Loop unrolling [31], an effective technique for increasing instruction level parallelism that is crucial for better memory bandwidth utilization.
3) OMP multithreading for effective utilization of multiple cores and threads on the CPU in parallel.

#### One-Step Delayed Parameter Update
Despite using a highly optimized CPU optimizer, the CPU computation overhead can become a bottleneck during training with very small batch sizes, when the GPU computation time is not much larger than CPU compute.\
For such limited cases, we develop one-step delayed parameter update (DPU) that overlaps CPU and GPU compute to hide the CPU computation overhead by delaying the parameter update by a single step.

![image](https://github.com/user-attachments/assets/6778a198-a4a5-48e9-b029-bb4161b46d42)

---
### [255] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU

- We formally define a search space of possible offloading strategies by considering computation schedule, tensor placement, and computation delegation.\
We prove that our search space captures a computation order with I/O complexity within 2× of optimality.\
We then develop a linear programming-based search algorithm to optimize the throughput within the search space.

- We show that it is possible to compress both the weights and KV cache for LLMs like OPT-175B to 4 bits without retraining or calibration, all with negligible
accuracy loss.\
This is achieved through fine-grained groupwise quantization (Shen et al., 2020), which is suitable for reducing I/O costs and memory usage during offloading.

![image](https://github.com/user-attachments/assets/f82b1417-22f4-42a3-9206-d09994df5307)

![image](https://github.com/user-attachments/assets/d7d16c50-4988-4daf-8e16-fa97a5788407)

**All existing systems (Aminabadi et al., 2022; HuggingFace, 2022) traverse the graph row-by-row, as shown in Fig. 3(a).** \
This is reasonable because it is the fastest way to finish the generation for one batch and the KV cache can be freed immediately after a row.\
However, because every two contiguous squares do not share weights, this schedule has to repeatedly load the weights and incurs huge I/O costs.

---
### [13] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving

cost-performance trade-offs for inference strategies:
- greedy search
- majority voting
- best-of-n
- weighted voting 
- two different tree search algorithms, using different model sizes and compute budgets.

Smaller models (e.g., Llemma-7B) can outperform larger models given the same computation budgets \
Smaller models paired with advanced inference algorithms yield Pareto-optimal cost-performance tradeoffs. For instance, the Llemma-7B model, equipped with our novel tree search algorithm, consistently outperforms Llemma-34B with standard majority voting on the MATH benchmark across all FLOPs budgets.

![image](https://github.com/user-attachments/assets/2e085a00-2755-48b9-8d23-bf1f8dba1488)

the accuracy of the language model will ultimately saturate to a fixed limit which is determined by the output probabilities assigned by the model, exhibiting exponential convergence speed through sampling and voting.\
this highlights the necessity for more sophisticated inference algorithms.

the commonly-used MCTS method does not perform well with weighted voting, as it often yields many unfinished solutions, hence having less effective votes.

To address this issue, we propose a novel tree search algorithm, **REward BAlanced SEarch (REBASE)**, which pairs well with weighted voting and achieves a Pareto-optimal trade-off between accuracy and inference
compute.\
The key idea of REBASE is to use a node-quality reward to control node expansion, which eliminates the need for explicit rollouts while ensuring enough candidate solutions for voting.

![image](https://github.com/user-attachments/assets/d3214e44-c71a-4f18-9da4-4e4e2f543fd3)

#### Inference Strategy
Greedy search. This strategy generates tokens one at a time by selecting the highest probability token at each step. It is computationally efficient but often suboptimal in terms of diversity.
- Best-of-n. This strategy, also known as rejection sampling, generates a set of candidates and chooses the one with the highest score given by the reward model.
- Majority voting. In this strategy, a set of candidates are generated, and the final answer to the problem is determined by the most frequently occurring answer in all the outputs.
- Weighted majority voting. This strategy is a variant of majority voting in which the candidates are weighted based on the scores given by the reward model.

Sampling-based if it uses a standard autoregressive sampling algorithm (e.g., temperature sampling) to generate the candidate set (greedy search is separate, in that it only has a single deterministic candidate).

A tree-search variant uses a tree search to generate the candidate set.

**Informally, as long as the reward model is “better than random”, i.e., assigning higher rewards to correct solutions on average, the accuracy limit of weighted majority voting is higher than that of majority voting.**

- Monte Carlo Tree Search(MCTS)

MCTS consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods.

a significant portion of the paths in the search tree are used to estimate and select nodes, and **these paths do not necessarily become a part of the final candidate solution**, although MCTS ensures that the sampled solutions comprise high-quality intermediate steps.

sampling methods generate multiple solutions in parallel and independently, and **all the generated sequences are included in the candidate solutions**.\
However, the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no mechanism for pruning poor steps or exploiting promising ones.

- Reward Balanced Search

The REBASE tree search method, illustrated in Fig. 3, inherits the exploitation and pruning properties of tree search, while using a reward model alone to estimate quality of intermediate nodes.\
This saves on computation compared to methods such as MCTS, since it does not involve estimate node quality with explicit rollouts.\
In short, the underlying idea is to use a process reward model to **determine how much each node should be expanded at each depth**.

Namely, REBASE expands nodes at a given depth according to their softmax-normalized reward scores, subject to a total expansion budget. We describe this procedure in more detail below.

![image](https://github.com/user-attachments/assets/ba68a712-fdd5-4df0-a4a1-91a0b51e02a6)

![image](https://github.com/user-attachments/assets/7bb5aca0-ad12-4f72-967e-ab362fc15830)

#### Compute Model Size

**Scaling law of compute-optimal inference for model size.**


Initially, sampling many times from smaller models is compute-optimal.\
At larger compute budgets the larger models are preferable, since the performance of small models has saturated.\
As highlighted in the right panel of Fig. 1, the optimal model size varies based on the inference budget.

**Llemma-7B achieves competitive accuracy to Llemma-34B with less compute.**

Llemma-7B requires around 2× less total FLOPs than Llemma-34B to achieve comparable accuracy.\
This held across inference strategies (sampling strategies, MCTS, REBASE) and tasks (MATH, GSM8K).\
This result suggests that, with the same training dataset and model family, generating more tokens with a suitable inference strategy using a smaller model can have more favorable cost-performance tradeoffs than using a larger model.

#### Compute-Optimal Inference Strategy

**Weaker models gain more from tree search.**

weaker models, as indicated by their lower greedy search accuracy, benefit more from tree search methods like REBASE

**REBASE saturates later than sampling with higher accuracy.**

drawing samples from REBASE corresponds to sampling from a policy that assigns high probability to true answers compared to sampling from the underlying language model.

---
### [0] ProTrain Efficient LLM Training via Adaptive Memory Management [AMD]

#### Prior Art

1) They only support coarse-grained control, such as the fixed parameter replication mode (ZeRO-2 or ZeRO-3), and binary options for offloading and gradient checkpointing.
For instance, FSDP requires *all model states* to be either entirely offloaded to the CPU or kept on the GPU, and all transformer blocks either use
gradient checkpointing or not at all.
2) They require significant manual effort to specify various configurations.
In DeepSpeed, users must select the ZeRO optimization stage, configure offloading options (CPU or NVMe) for both parameters and optimizer states, and set multiple thresholds for parameter fetching and collective communications.

#### ProTrain
1) To reduce memory consumption, ProTrain adaptively decides whether to use offloading or gradient checkpointing, determines the amount of model states and activations to offload and the number of transformer blocks to apply gradient checkpointing, all without user inputs.
2) For computation, ProTrain keeps forward/backward computation on the GPU for efficiency, while dynamically determining the portion of parameter updates to be performed on the CPU and GPU.

ProTrain proposes a Chunk-Based Model State Management system that organizes model states into uniformly sized chunks, and further introduces persistent chunks and chunk buffers to minimize unnecessary data copying and reduce dynamic memory allocations.

ProTrain also proposes Block-Wise Activation Management to handle activations at the transformer block level, performing swapping or gradient checkpointing as needed for each block.

#### Background

For the training of large models, it is a common practice to adopt mixed-precision training , which uses reduced precision data types for FWD （Forward Propagation）and BWD (Backward Propagation), while maintaining higher precision for OPTIM (parameter updating) to ensure accuracy.

Memory consumption during training primarily comes from two sources:
- model states
- residual states

**Model states** include
- parameters
- gradients
- optimizer states
**residual states** consist of
- activations
- temporary buffers

The computational complexity of the FWD and BWD stages scales with model size and batch size, necessitating their execution on GPUs due to the intensive
computational demands.

In contrast, the OPTIM stage involves simpler operations and can be efficiently offloaded to the CPU (40), which brings significant GPU memory savings by allocating memory-intensive optimizer states on the CPU.

#### Chunk Based Model State Management

![image](https://github.com/user-attachments/assets/76bba3a2-73bb-49f9-b735-6e2ec9f89044)

#### Block-Wise Activation Mangement Layout and Memory Usage Trend

The balance between:
- Activation Swapping
Swapping indicates that the block will be swapped out the block.

- Gradient Checkpointing
checkpointing means that the entire block will be recomputed by saving the input tensor of

![image](https://github.com/user-attachments/assets/99ead105-fa46-4668-8ba0-dd9cc65eabe1)

**The concept of checkpointing comes from TianQi Chen's Paper.**

Training Deep Nets with Sublinear Memory Cost

https://github.com/cybertronai/gradient-checkpointing

![image](https://github.com/user-attachments/assets/de7cd168-c6b2-405e-88a4-89b7363ac75f)


---
### [31] LLM Inference Unveiled Survey and Roofline Model Insights

![image](https://github.com/user-attachments/assets/ff1e6971-5ae9-4793-9785-23828db358c4)

The inference process of Large Language Models (LLMs) is divided into two stages:
- the Prefill Stage
- the Decode Stage

The Prefill Stage serves as the initial step in LLM inference. \
In this stage, the model takes a prompt sequence as input and engages in the generation of a key-value cache (KV cache) for each Transformer layer within the LLM.\
The KV cache plays a crucial role in storing and organizing information that the model deems relevant for subsequent token generation. \
Each Transformer layer is equipped with its own unique KV cache, and this prefilling process establishes the foundation for the subsequent decoding stage.

The Decode Stage represents the core of the LLM inference process.\
In the Decode Stage, the model uses the KV caches prepared earlier and might add new information to them.\
The goal here is to generate tokens, which are essentially words or parts of words.\
This happens step by step.\
The creation of each new token is influenced by the tokens that were generated before it, like building a sentence word by word.


#### Memory Bound & Compute Bound

If the layer is memory-bound, consider optimization techniques such as quantization, kernel fusion and increasing batch size to alleviate the memory footprint.

![image](https://github.com/user-attachments/assets/63f22384-aec3-4d31-9c37-1778fdc04c4a)

It implies that the layer is constrained by computation (compute-bound), with some memory units potentially remaining idle.\
In this case, we should investigate strategies such as enabling low-bit computation to enhance computational efficiency.

#### Memory Access

Quantizing tensors in LLM can significantly reduce memory access, resulting in fewer data bytes to be moved for the same amount of computation.


This increase in arithmetic intensity contributes to the Roofline model, leading to three scenarios:


1) After quantization, the arithmetic intensity remains within the memory-bound range.\
With the improvement in arithmetic intensity, the average data access per computation is reduced, alleviating the pressure on data memory access.\
Consequently, the the-oretical performance is enhanced. This can greatly boost the performance during the memory-bound decode stage.\
2) The arithmetic intensity transitions from being memory-bound to compute-bound.\
This shift also reduces the pressure on data memory access, resulting in improved theoretical performance.
3) Both before and after quantization, the arithmetic intensity remains within the compute-bound range.\
In this case, there is no performance improvement.
For example, this scenario can occur during the compute-bound prefill stage or when the batch size is large in the decode stage.


Large Batch Size: It can be seen that on the right of Figure8, there is no difference from W1-W8.
Compute-bound prefill stage: It can be seen that on the right of Figure9, there is no difference from W1-W8.

![image](https://github.com/user-attachments/assets/b826c5ab-8fa6-4ee5-b832-291e92068df4)

The trend is from **Memory Bound** to **Compute Bound**.

#### Algorithms for Fast Decoding

- Early Exiting
- Contextual Sparsity
The paper reveals that contextual sparsity can go up as high as 80%, meaning that the majority of the weights can be left out while still preserving the original model performance.\
However, the chosen weights are dynamic and different for different input tokens.
 
To save compute, the paper proposes to train a small MLP network as the Sparse Predictor in front of the Multi-Head Attention (MHA) and the Feed-Forward Networks (FFN) of the LLM

- Mixture of Expert

The mixture of expert (MoE) technique is a well-studied topic (Yuksel et al. [2012]) that effectively decouples the parameter count of the model and the computation FLOPs required by the model training and inference.

An expert network is inserted into the transformer architecture to replace the FFN layers. Also, a gating function is introduced between the Multi-Head Attention and the expert network which aims to select the best-fit expert or experts for the given input token.

![image](https://github.com/user-attachments/assets/a5be8384-39e3-4a21-8428-695bbc082c45)

Although both rely on the input token to determine sparse structure, We deliberately separate MoE and the contextual sparsity techniques because the latter operates on pre-trained dense language models and exploits the sparsity from the dense neural networks, while the prior trains a sparse model from the beginning.

#### Speculative Decoding
- LLM Distribution Preserving
- Building a Three of Draft Tokens
- Knoweledge Distillation and Self-Speculative Decoding

![image](https://github.com/user-attachments/assets/d1f005c8-c8b9-4cb8-a81e-efbdcef1b231)

#### Parallel Decoding

- Simultaneously Predicting Multiple Future Tokens
- Retrieval of Frequent N-grams
- Hierarchical Structure In Language
- Jacobi and Gaussian-Seidel Iterative Algorithms

![image](https://github.com/user-attachments/assets/dc57ae33-9549-4e10-bdcd-c4dce4f91ee4)


#### Memory Mangement and Workload Offloading

The length of the user’s input prompt may vary, affecting the length of the sequence in the prefill phase.\
Additionally, the sequence length increases incrementally during the decode phase as tokens are generated.

**Page Attention** efficiently handles the KV cache by dividing it into blocks.

This mapping is similar to how virtual memory works in a CPU’s memory management system.

DeepSpeed-inference introduces **ZeRO-Inference**, which offloads the weights of large models to CPU memory.

This mechanism performs well with large batch sizes because the increased batch size increase the computation requirement and make the computation latency overlap the latency of fetching model weights,


**FlexGen** provides a way to explore different ways of offloading computations considering constraints imposed by available hardware resources from the GPU, CPU, and disk.

---
### [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization

FlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in LLMs, with the remaining layers using 8 bits.

The 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by large matrix calculation.\
Our work achieves up to 2× speedup and 2.3× memory reduction for LLMs with negligible loss in accuracy.

![image](https://github.com/user-attachments/assets/dee83087-aa07-4441-b6f6-31a8eafedaed)

**Framework**
![image](https://github.com/user-attachments/assets/d386534e-97ad-4ac6-a34e-83545e1e68b2)

![image](https://github.com/user-attachments/assets/1924a296-1948-46c1-b835-a99bfa229b03)

