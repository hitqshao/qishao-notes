---
title: Memory Optimizations in LLM
date: 2025-01-26 23:32:49
permalink: /pages/dc7048/
---

## **Memory Optimizations**
- **Activation Checkpointing**  
  Recomputation during backward pass.
- **Quantization-Aware Training (QAT)**  
  Train with INT8/FP8 precision.
- **Dynamic Memory Allocation**  
  Buffer reuse to avoid fragmentation.
- **Low-Rank Gradient Projection (GaLore)**  
  **NEW** Compress gradients via low-rank approximations during training.

## [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources

- Use SGD instead of Adam for fine-tuning weights.
- Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.
- SGD also avoid state memory of ADAM.

![image](https://github.com/user-attachments/assets/c1e7d018-84e2-49f7-bd74-cdf4edaa0343)

![image](https://github.com/user-attachments/assets/a98a808f-1cb1-4432-b0d0-7c51e9f4ee48)
