---
title: Memory Optimizations in LLM
date: 2025-01-26 23:32:49
permalink: /pages/dc7048/
---

## **Memory Optimizations**
- **Activation Checkpointing**  
  Recomputation during backward pass.
- **Quantization-Aware Training (QAT)**  
  Train with INT8/FP8 precision.
- **Dynamic Memory Allocation**  
  Buffer reuse to avoid fragmentation.
- **Low-Rank Gradient Projection (GaLore)**  
  **NEW** Compress gradients via low-rank approximations during training.

---

### [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources

- Use SGD instead of Adam for fine-tuning weights.
- Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.
- SGD also avoid state memory of ADAM.

![image](https://github.com/user-attachments/assets/c1e7d018-84e2-49f7-bd74-cdf4edaa0343)

![image](https://github.com/user-attachments/assets/a98a808f-1cb1-4432-b0d0-7c51e9f4ee48)

---

### [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors

This paper discovers that LORA can be approximated by a random projection.

LORA restricts overall weights update matrices to be low-rank.

FLORA use *random projection matrix*, which allows high-rank update gradients.

> Our intuition arises from investigating LoRA and observing that a LoRA update is dominated by a random projection, which compresses the gradient into a
lower-dimensional space.
> Our FLORA resamples the random projection and is able to mitigate the low-rank limitation of LoRA. Further, our approach only stores the compressed gradient
accumulation and momentum, thus saving the memory usage of optimization states to the sublinear level.

Gradident Accumulation:
- Gradient accumulation stores the sum of gradients over multiple batches to simulate a larger effective batch size (helpful when memory limits prevent using large batches).
- Normally, this requires a memory buffer equal to the model size to store the full gradient matrix.

Momentum
- Momentum smooths gradient updates by keeping an exponentially weighted moving average (EMA) of past gradients.
- Maintaining momentum for large models requires significant memory since an additional buffer equal to the model size is needed.

FLORA Compression:
- compress gradients accumulation: Applying a random projection matrix A to reduce the dimensionality of the gradients.
- compress momentum: Using random projection to compress the momentum term M.
