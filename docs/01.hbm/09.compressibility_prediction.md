---
title: compressibility prediction
date: 2024-04-04 11:42:24
permalink: /pages/f07699/
---

1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO 2018]
2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA 2019]
3. MBZip: Multiblock Data Compression [TACO 2017]
4. Compresso: Pragmatic Main Memory Compression [MICRO]

---
### 1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO]
Year: 2018

**Attach´e does not use the free space made available by compression.**

**Compression Predictor (COPR), predicts if the memory block is compressed.**
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/5fdac0b6-1921-4b9f-bbf5-2c86549e74e5)

**Global Indication(GI)**: GI is composed of eight two-bit saturating counters, each of which keeps track of the compressibility of 18th the memory space. 
GI can be used as an accurate indicator for predicting the compressibility within a memory space if there is abundant similarity in compressibility.

**Page-Level Predictor (PaPR)**: By exploiting the similarity in the compressibility of cachelines within an OS page [12], [18], [37], PaPR provides compression predictions at the page granularity.

**Line-Level Predictor (LiPR)**:  LiPR is a set-associative cache structure indexed by the page number. LiPR uses the two-bit values of PaPR to determine if the neighboring cachelines have the same compressibility.

### 2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA]
Year: 2019

**Transparent Memory-Compression (TMC) can provide bandwidth benefits of memory compression in an OS-transparent manner by trying to exploit only the increased bandwidth and not the extra capacity.**

**Line Location Predictor (LLP)** that can determine the location of the line with 98% accuracy and a dynamic solution that disables compression if the benefits of compression are smaller than the overheads.

**If we use HBM, we dont need to care too much about the bandwidth and metadata.**

We propose a history-based Line Location Predictor (LLP), that can identify the correct location of the line with a high accuracy (98%). The LLP is based on the observation that lines within a page tend to have similar compressibility.

LLP contains the Last Compressibility Table (LCT), that tracks the last compression status seen for a given index. The LCT is indexed with the hash of the page address. 
So, for a given access, the index corresponding to the page address is used to predict the compressibility, then line location.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/4c1c1b42-c4e0-4250-8be7-af0cb8edaa03)

Even though the LLP is quite small, it provides an accuracy of 98%, much higher than the hit-rate of the metadata cache.

LCP stores metadata inline with block.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/cdccf9e7-f9b5-48b0-9c10-977047f060b3)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/54517fe5-abf0-44cb-ac6b-4759942be1df)


### 3. MBZip: Multiblock Data Compression [TACO]

A write to a location in memory may not change the existing data in that location and is thus a redundant write. Such write requests to cache have been termed as silent stores [25]/writes [21].

**We find that, on average, across 21 benchmarks, 9.6% of the writes are silent. More than 15% of the writes are silent in benchmarks such as bwaves, GemsFDTD, lbm, leslie3d, mcf, mesa, sjeng, soplex, vortex2, and zeusmp.**

In such a scenario, we essentially issue one read request (to read the existing data) and no write request. However, if the write request is not silent, we add the overhead of a read request to the existing write request.

We observe that there is a strong correlation to a write being silent or nonsilent both across writes made to the same address during the course of the program execution and across writes to consecutive addresses. To exploit this correlation, we propose using a 2b bimodal predictor (indexed using the page addresses) to predict whether a write request is silent. The accuracy of our predictor (4kB structure) is around 94.4%, on average.

### 4. Compresso: Pragmatic Main Memory Compression

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/50a52444-7ec2-447f-b830-9ef060884785)
 
We associate a 2-bit saturating counter with each entry in the metadata cache (Fig. 5b). The counter is incremented when any writeback to the associated page results in a cache line overflow and is decremented upon cache line underflows (i.e., new data being more compressible).

Another 3-bit global predictor changes state based on page overflows in the system. We speculatively increase a page’s size to the maximum (4KB)
when the local as well as global predictors have the higher bit set.

Hence, a page is stored uncompressed if it receives multiple streaming cache line overflows during a phase when the overall system is experiencing page overflows

### 5. Enabling Technologies for Memory Compression:Metadata, Mapping, and Prediction

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/ac9fa284-dbe5-4ce1-af70-bb4e23cc39fd)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/d26a752b-c871-44a2-9c2c-cc51cdcf88ad)

**Metadata is coexist with data. Thus they have to predict, even read.**

Reads are problematic because the size of the block is encoded in the block itself. Therefore, the read has to either be performed in two phases (read the metadata from the 0th chip, then read data from the appropriate subset of chips) or the read has to conservatively read data from all 9 chips in parallel.

The first is PCbased, where the PC of the load instruction serves as the index into a predictor table. This assumes that a load tends to access the same type of data record, with relatively uniform compressibility.

The second is page-based, where the physical page number serves as the index into a predictor table. This assumes that the data records in a single page are of a similar type and have uniform compressibility.

On a look-up, the highest-valued saturating counter indicates the predicted size of the block. In case of a tie, we conservatively predict the larger block size.

They keep a counter for each block compression length, and try to keep counter for each block length. Then predict by voting, following majority wins low.




