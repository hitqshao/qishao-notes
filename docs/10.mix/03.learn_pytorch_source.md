---
title: Understanding Pytorch Source Code
date: 2024-10-15 15:32:49
permalink: /pages/f00002/
---

# aten

**Purpose**
ATen (short for "A Tensor Library") is the core tensor library in PyTorch. It provides the following:

- Tensor Abstraction: Defines the Tensor class and provides fundamental tensor operations.
- Backend Management: Handles the execution of operations on various backends (e.g., CPU, CUDA).
- Dispatcher Integration: Works with the dispatcher to route tensor operations to the correct implementation.

**Key Roles**

* Tensor Definition:
  - Provides the Tensor class, which is the foundational data structure in PyTorch.
  - Source: aten/src/ATen/Tensor.h.

* Tensor Operations:
  - Implements a wide variety of tensor operations, such as matrix multiplication, element-wise addition, and reductions.
  - Operations are defined in:
    - aten/src/ATen/native/ (backend-specific implementations like cpu, cuda, or quantized).
    - Example: aten/src/ATen/native/LinearAlgebra.cpp (matrix-related ops).

* Dispatcher Entry Points:
  - Provides entry points for the PyTorch dispatcher system, which routes tensor operations to their specific implementations.
  - Function signatures for tensor operations are declared in aten/src/ATen/Functions.h and implemented in backend-specific files.

* Backend Management:
  - Supports multiple device backends, such as CPU, CUDA, ROCm, XLA, and others.
  - Backend-specific code is implemented in native directories (e.g., aten/src/ATen/native/cuda/ for CUDA).

* Default Fallbacks:
  - Provides fallback implementations for some operations when a specific backend implementation is not available.


## ./aten/src/ATen/core/dispatch

Dispatcher

<details>
  <summary>Code</summary>

```cpp
  /**
   * Register a new operator schema.
   *
   * If a schema with the same operator name and overload name already exists,
   * this function will check that both schemas are exactly identical.
   */
  RegistrationHandleRAII registerDef(FunctionSchema schema, std::string debug, std::vector<at::Tag> tags = {});

  /**
   * Register a kernel to the dispatch table for an operator.
   * If dispatch_key is nullopt, then this registers a fallback kernel.
   *
   * @return A RAII object that manages the lifetime of the registration.
   *         Once that object is destructed, the kernel will be deregistered.
   */
  // NB: steals the inferred function schema, as we may need to hold on to
  // it for a bit until the real schema turns up
  RegistrationHandleRAII registerImpl(OperatorName op_name, c10::optional<DispatchKey> dispatch_key, KernelFunction kernel, c10::optional<impl::CppSignature> cpp_signature, std::unique_ptr<FunctionSchema> inferred_function_schema, std::string debug);

  /**
   * Register a new operator by name.
   */
  RegistrationHandleRAII registerName(OperatorName op_name);

  /**
   * Register a fallback kernel for a backend.
   * If an operator is called but there is no concrete kernel for the dispatch
   * key of the given operator arguments, it will check if there is such a
   * fallback kernel for the given dispatch key and, if yes, call that one.
   */
  RegistrationHandleRAII registerFallback(DispatchKey dispatch_key, KernelFunction kernel, std::string debug);

  /**
   * Use to register whenever we had a TORCH_LIBRARY declaration in the frontend
   * API.  These invocations are only permitted once per program, so we raise
   * an error if this is called again for the same namespace.
   */
  RegistrationHandleRAII registerLibrary(std::string ns, std::string debug);
```
</details>

# C10


**Purpose**
C10 (short for "Caffe 2.0") is a lower-level, foundational library within PyTorch. It provides the following:

* Device Abstraction: Defines device and tensor metadata.
* Dispatcher System: Implements the core dispatch mechanism for routing tensor operations.
* Type Management: Handles scalar types, tensor data types, and other utilities.
* Utilities: Provides logging, threading, and memory management.

**Key Roles**
* Dispatcher:
  - C10's dispatcher is the routing mechanism for PyTorch tensor operations.
  - Routes function calls (like torch.add) to backend-specific implementations.
  - Files:
    - c10/Dispatcher.h: Core dispatcher implementation.
    - c10/core/KernelFunction.h: Manages kernel function registration.

* Device Abstraction:
  - Provides c10::Device and c10::DeviceType to manage devices like CPU, CUDA, XLA, etc.
  - Files:
    - c10/core/Device.h: Device abstraction.
    - c10/core/DeviceType.h: Enumerates device types.

* Type Management:
  - Handles scalar and tensor data types.
  - Provides abstractions for c10::ScalarType (e.g., float, int, bool).
  - Files:
    - c10/core/ScalarType.h: Scalar type definitions.
    - c10/core/TensorTypeId.h: Tensor type identifiers.

* Memory Management:
  - Abstracts memory allocation across devices.
  - Supports custom allocators for tensors (e.g., caching allocators for CUDA).
  - Files:
    - c10/core/Allocator.h: Base allocator interface.
    - **c10/cuda/CUDACachingAllocator.cpp: CUDA memory management**.

* Threading:
  - Provides utilities for multi-threaded execution.
  - Files:
    - c10/util/ThreadPool.h: Thread pool implementation.
    - c10/util/ThreadLocal.h: Thread-local storage utilities.

* Utilities:
  - Provides logging, error handling, and common utilities.
  - Files:
    - c10/util/Logging.h: Logging macros.
    - c10/util/Exception.h: Exception handling utilities.

## core

**GeneratorImpl.h**

Random Number Generator

> A Pseudo Random Number Generator (PRNG) is an engine that uses an algorithm to generate a seemingly random sequence of numbers, that may be later be used in creating a random distribution. \
> Such an engine almost always maintains a state and requires a seed to start off the creation of random numbers.\
> Often times, users have found it beneficial to be able to explicitly create, retain, and destroy PRNG states and also be able to have control over the seed value.

# torch
## csrc
### autograd

**variable.h**

Variable

A `Variable` augments a `Tensor` with the ability to interact in our autograd machinery.\
Conceptually, `Variable`s travel along `Edge`s between `Node`s in the autograd graph.\
A `Variable` can either be a leaf, like a weight in a neural network, or an interior variable, when it is the result of an operation between variables. Every `Variable` also stores another `Variable` called its `grad` (gradient).\
If the variable is a leaf, its
gradient will be accumulated into this variable.

Every Tensor is a Variable, but sometimes we colloquially refer to Variables that don't require gradients as Tensors (since none of the autograd machinery for Variables applies).\
Historically, Variables and Tensors were separate concepts, but now they are exactly the same (i.e. we have using Variable = at::Tensor`).


Gradient Edges

Furthermore, `Variable`s have the notion of a `gradient_edge`, which is the edge in the autograd graph that connects the variable to a particular input
of the gradient function that will be invoked with the variable during the backward pass.

More precisely, this gradient function can be one of two things:
- 1. A `grad_fn`, if the variable is in the interior of the graph. This is the gradient of the function that produced the variable.
- 2. A `grad_accumulator`, if the variable is a leaf, which accumulates a scalar gradient value into its `grad` variable.

Versioning

Another major feature of `Variable`s are *versions*. \
Versions are incremented when an in-place mutation of a variable occurs.\
Versions are useful when constructing `SavedVariable`s, which take a snapshot of a `Variable` at a certain version.\
You can retrieve a `Variable`'s version through its `current_version()` method.

Views

It is possible for a  `Variable` to be a *view* of another `Variable`, in which case it tracks that `Variable`'s data and autograd history. Beyond
construction, the interface of a view is identical to that of a regular `Variable`. \
You can determine whether `Variable` is in fact a view by probing its `is_view()` method. \
Note that the *view* semantics are only meaningful for `Variable` relations that are relevant to autograd.

 
