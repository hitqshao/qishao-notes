---
title: Understanding Pytorch Source Code AOT
date: 2024-12-15 15:32:49
permalink: /pages/f00003/
---

## aot dispatch create joint graph

./torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py.

aot_dispatch_autograd

The aot_dispatch_autograd function is responsible for tracing, partitioning, and compiling a given function for automatic differentiation using Ahead-Of-Time (AOT) Autograd.

It handles the creation of **forward and backward graphs**, manages metadata, and ensures that the compiled function can be executed efficiently with support for gradient computation.

Inputs
- flat_fn: The original function to be traced and compiled.
- flat_args: A list of arguments to be passed to the function.
- aot_config: Configuration for AOT Autograd, which includes settings for partitioning, logging, and compilation.
- fw_metadata: Metadata about the function's inputs and outputs, including information about views and mutations.

Outputs
- compiled_function: A compiled version of the original function that includes both the forward and backward passes, optimized for execution with support for gradient computation.


Major Functions in aot_dispatch_autograd

**aot_dispatch_autograd_graph**
- Purpose: Traces the original function and creates a joint forward-backward FX graph.
- Steps: Calls aot_dispatch_autograd_graph to trace the function and generate the FX graph.
  - Returns the FX graph, joint inputs, and subclass metadata (if any).

**partition_fn:**
* Purpose: Partitions the joint FX graph into separate forward and backward graphs.
* Steps: Uses the partition function specified in aot_config to split the FX graph into forward and backward modules.
   - Returns the forward and backward modules.
   - **min_cut_rematerialization_partition** :+1: :+1:

**fw_compiler and bw_compiler:**
* Purpose: Compiles the forward and backward FX graphs into executable functions.
* Steps: Uses the forward and backward compilers specified in aot_config to compile the FX modules.
  - Returns the compiled forward and backward functions.

**CompiledFunction:**
* Purpose: A custom autograd function that wraps the compiled forward and backward functions.
* Steps: Defines the forward and backward static methods to handle the execution of the compiled functions.
  - Manages the saving and restoring of tensors and symbolic integers for gradient computation.

**create_runtime_wrapper**
* Purpose: Creates a runtime wrapper for the compiled function to handle input mutations and other runtime considerations.
* Steps: Wraps the CompiledFunction.apply method with additional logic for handling input mutations and AMP (Automatic Mixed Precision) settings.
  - Returns the wrapped function.

<details>
  <summary>Code</summary>

```python
def aot_dispatch_autograd(...)
    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(  # type: ignore[misc]
        flat_fn, flat_args, aot_config, fw_metadata=fw_metadata
    )
    ...
    fw_module, bw_module = aot_config.partition_fn(
        fx_g, joint_inputs, num_fwd_outputs=num_inner_fwd_outputs
    )
```
</details>

### aot_dispatch_autograd_graph

The aot_dispatch_autograd_graph function is responsible for preparing and tracing a given function (flat_fn) with its arguments (flat_args) for automatic differentiation using AOT (Ahead-Of-Time) Autograd.\
It processes the function to handle input mutations, creates a joint forward-backward function, and generates an **FX graph** for the function.\
The function ensures that the graph is functional (i.e., free of in-place operations) and can handle tensor subclasses if necessary.

* pytree.tree_map: This function processes the traced_tangents to ensure they are detached and contiguous if they are tensors, preparing them for tracing.
* fn_prepped_for_autograd: Prepares the original function for autograd by incorporating metadata about views and mutations, ensuring correct handling of these aspects during tracing.
* ***create_joint***: Creates a joint forward-backward function that traces both the forward and backward passes together, enabling efficient autograd processing.
* create_functionalized_fn: Converts the joint function into a functional form, handling input mutations and tracing the joint structure, ensuring compatibility with autograd.
* aot_dispatch_subclass: Handles tracing for tensor subclasses, ensuring that the autograd process can correctly handle these specialized tensor types.
* ***_create_graph***: Creates an FX graph from the joint function and its inputs, providing a lower-level representation of the function for optimization and execution.
* ***fx_g.graph.eliminate_dead_code***: Eliminates any dead code from the FX graph to optimize it, improving performance and reducing unnecessary computations.
* ***fx_g.recompile***: Recompiles the FX graph after eliminating dead code, ensuring that the graph is up-to-date and optimized for execution.

<details>
  <summary>Code</summary>

```python
    ### dispatch_and_compile_graph.py
    fn_prepared_for_autograd = fn_prepped_for_autograd(
        flat_fn,
        fw_metadata,
    )
    joint_fn_to_trace = create_joint(fn_prepared_for_autograd, aot_config=aot_config)

    joint_fn_to_trace, updated_joint_inputs = create_functionalized_fn(
        joint_fn_to_trace,
        joint_inputs,
        meta=fw_metadata,
        aot_config=aot_config,
        trace_joint=True,
    )

    subclass_tracing_info = aot_dispatch_subclass(
        joint_fn_to_trace,
        updated_joint_inputs,
        is_joint_structure=True,
        meta=fw_metadata,
        fw_only=flat_fn,
    )
    ...
    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)
    ...
    fx_g.graph.eliminate_dead_code()
    fx_g.recompile()
```
</details>


#### create_joint
The create_joint function is designed to create a joint forward-backward function for automatic differentiation.\
It ensures that the function can be traced and differentiated correctly, handling the computation of gradients and preserving the necessary metadata.

* Inputs 
- fn: A callable function that returns a tuple of (outputs, mask). The mask indicates which outputs require tangents.
- aot_config: Configuration for AOT (Ahead-Of-Time) Autograd, which includes settings like whether tangents are needed.
* Outputs
- return a tuple of (outs, mask), where `mask` tells us which outputs are meant to have tangents.
- compute tangents for every output that requires grad.


***inner_fn***

This is the core function that computes the forward pass, identifies the outputs that require gradients, and performs the backward pass to compute the gradients.

- Calls the original function fn with the primal inputs to get the outputs and a mask indicating which outputs require tangents.
- Identifies the inputs and outputs that need gradients.
- Sets up stack trace preservation hooks for the gradient functions. **setup_stacktrace_preservation_hooks**
- Calls torch.autograd.grad to compute the gradients of the needed outputs with respect to the inputs that require gradients.
- Returns the original outputs and the computed gradients.

#### _create_graph

_create_graph wraps make_fx.

The make_fx function is a utility in PyTorch that traces a given function f and its inputs to produce an FX graph.\
This graph represents the operations performed by the function in a way that can be further analyzed, transformed, and optimized.\
The function supports different tracing modes (real, fake, symbolic) and can handle decomposition of complex operations into simpler ones.

- tracing_mode Handling: Determines the mode of tracing (real, fake, symbolic) and sets up the appropriate context for each mode.
- ShapeEnv: Manages symbolic shapes during tracing, especially in symbolic mode.
- FakeTensorMode: Creates fake tensors to simulate tensor operations without actual computation, used in fake and symbolic modes.
- ProxyTorchDispatchMode: Sets up a proxy mode to intercept and record tensor operations during tracing.
- wrap_fake: Wraps input tensors as fake tensors or symbolic integers based on the tracing mode.
- dispatch_trace: Performs the actual tracing of the function, recording the operations into an FX graph.

  
## TorchInductor

TorchInductor is the backend of pytorch, converting compute graph into target-specific code.\
It also utilize optimization techiniques, memory optimization, parallelism and low-level codegen.

In previous part, after aot_dispatch_autograd() obtain forward/backward FX graph, it will compile the graph with forward/backward compiler.

inductor calls compile_fx_inner() by default. THe kernel function is fx_codegen_and_compile(),\
which optimizes FX graph optimization and generate code.

The fx_codegen_and_compile function is responsible for generating and compiling a Torch FX (Functional Transformations) graph module.\
It performs several steps to optimize and prepare the graph for execution, including handling tensor shapes, converting operations, and compiling the graph into an executable function.\
It supports various modes such as AOT (Ahead-Of-Time) compilation and inference.

File location: ./torch/_inductor/compile_fx.py

- _recursive_post_grad_passes: Applies post-gradient passes to the graph, optimizing it for inference or training.
- split_const_gm: Splits the graph module into constant and non-constant parts if runtime constant folding is enabled.
- GraphLowering: Lowers the FX graph to a lower-level representation, preparing it for code generation and execution.
- graph.run:Executes the lowered graph with the provided example inputs.
- graph.compile_to_fn: Compiles the lowered graph into an executable function.
- CompiledFxGraph: Creates a compiled FX graph object that includes the compiled function, graph metadata, and metrics.

### _recursicv_post_grad_passes()
Graph optimization.

#### group_batch_fusion_passes()
operator fusion.

#### remove_noops_ops()
remove aten.clone/alias operations.

#### fuse_ddp_communication

#### dempose_auto_functionalized

split high-level oprations

### GraphLowering
Lower FX graph into Inductor IR(lower-level IR)

### GraphLowering.compile_to_fn()

source code: compile_fx.py

generate target-specific codes

source code: graph.py

compile_to_fn -> compile_to_module() -> codegen_with_cpp_wrapper

**codegen_with_cpp_wrapper**
 - CPU one pass
 - GPU two pass.
   - JIT-compile the model with python wrapper code and run it to generate autotuned kernel binaries in the first pass;
   - and then generate cpp wrapper code and compile it to a dynamic library in the second pass.

**GPU**

1. First pass: compiled = self.compile_to_module().call; compiled(real_inputs)
2. Second pass: codegen()

<details>
  <summary>Code</summary>

```python
    def codegen(self):
        from .scheduler import Scheduler

        self.init_wrapper_code()

        self.scheduler = Scheduler(self.buffers)
        V.debug.draw_orig_fx_graph(self.orig_gm, self.scheduler.nodes)

        self.scheduler.codegen()
        return self.wrapper_code.generate(self.is_inference)
```
</details>

From above code, we can see that it instantiate scheduler and use codegen function is scheduler.

scheduler.py

<details>
  <summary>Code</summary>

```python
class Scheduler:
    @dynamo_timed
    def __init__(self, nodes):
        self.compute_dependencies()
        self.topological_sort_schedule()
        self.dead_node_elimination()
        if config.reorder_for_compute_comm_overlap:
            comms.decide_global_ordering_of_comms(self.nodes)
        self.compute_ancestors()
        metrics.ir_nodes_pre_fusion += len(self.nodes)
        V.debug.ir_pre_fusion(self.nodes)
        self.num_orig_nodes = len(self.nodes)
        self.name_to_fused_node = {n.get_name(): n for n in self.nodes}
        self.create_foreach_nodes()
        self.topological_sort_schedule()
        self.logged_slow_fusion = set()
        self.fuse_nodes()
        if config.reorder_for_compute_comm_overlap:
            # Refresh node_users and inverse_users to reflect fused nodes
            self.compute_node_users()
            self.nodes = comms.reorder_compute_and_comm_for_overlap(self.nodes)
        self.compute_last_usage()
        V.debug.ir_post_fusion(self.nodes)
        V.debug.graph_diagram(self.nodes)
        self.debug_draw_graph()

        # used during codegen:
        self.current_device: torch.device = None  # type: ignore[assignment]
        self.buffer_names_to_free = set()
```
</details>

#### scheduler.__init__()
kernel fusion optimization

##### compute_dependencies()
Create dependency edges between nodes, handling aliasing and mutation properly.

##### fuse_nodes()
Mutates self.nodes to combine nodes into FusedSchedulerNodes.

This relies on two key functions to control the logic:
- self.can_fuse(): checks if a fusion is legal
- self.score_fusion(): assigns priority to a given fusion

**fuse_nodes_once**
- get_possibble_fusions()
  - can_fuse and not will_fusion_create_cycle
  - speedup_by_fusion
  - fused_nodes.remove(node1/node2)
  - fused_nodes.add(node3)
- topological_sort_schedule()
- self.prune_redundant_deps()

#### Scheduler.codegen()
**get_backend(device).codegen_node(node)**

This will call specific backend, such as gpu to generate code for nodes.

There is a *cuda_cpp_scheduling.py* defines class CUDACPPScheduling with codegen_template method.


