---
title: Understanding Pytorch Source Code AOT
date: 2024-12-15 15:32:49
permalink: /pages/f00003/
---

# AOT
## aot dispatch

./torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py.

aot_dispatch_autograd

The aot_dispatch_autograd function is responsible for tracing, partitioning, and compiling a given function for automatic differentiation using Ahead-Of-Time (AOT) Autograd.

It handles the creation of **forward and backward graphs**, manages metadata, and ensures that the compiled function can be executed efficiently with support for gradient computation.

Inputs
- flat_fn: The original function to be traced and compiled.
- flat_args: A list of arguments to be passed to the function.
- aot_config: Configuration for AOT Autograd, which includes settings for partitioning, logging, and compilation.
- fw_metadata: Metadata about the function's inputs and outputs, including information about views and mutations.

Outputs
- compiled_function: A compiled version of the original function that includes both the forward and backward passes, optimized for execution with support for gradient computation.


Major Functions in aot_dispatch_autograd

**aot_dispatch_autograd_graph**
- Purpose: Traces the original function and creates a joint forward-backward FX graph.
- Steps: Calls aot_dispatch_autograd_graph to trace the function and generate the FX graph.
  - Returns the FX graph, joint inputs, and subclass metadata (if any).

**partition_fn:**
* Purpose: Partitions the joint FX graph into separate forward and backward graphs.
* Steps: Uses the partition function specified in aot_config to split the FX graph into forward and backward modules.
   - Returns the forward and backward modules.

**fw_compiler and bw_compiler:**
* Purpose: Compiles the forward and backward FX graphs into executable functions.
* Steps: Uses the forward and backward compilers specified in aot_config to compile the FX modules.
  - Returns the compiled forward and backward functions.

**CompiledFunction:**
* Purpose: A custom autograd function that wraps the compiled forward and backward functions.
* Steps: Defines the forward and backward static methods to handle the execution of the compiled functions.
  - Manages the saving and restoring of tensors and symbolic integers for gradient computation.

**create_runtime_wrapper**
* Purpose: Creates a runtime wrapper for the compiled function to handle input mutations and other runtime considerations.
* Steps: Wraps the CompiledFunction.apply method with additional logic for handling input mutations and AMP (Automatic Mixed Precision) settings.
  - Returns the wrapped function.

<details>
  <summary>Code</summary>

```python
def aot_dispatch_autograd(...)
    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(  # type: ignore[misc]
        flat_fn, flat_args, aot_config, fw_metadata=fw_metadata
    )
    ...
    fw_module, bw_module = aot_config.partition_fn(
        fx_g, joint_inputs, num_fwd_outputs=num_inner_fwd_outputs
    )
```
</details>

### aot_dispatch_autograd_graph

The aot_dispatch_autograd_graph function is responsible for preparing and tracing a given function (flat_fn) with its arguments (flat_args) for automatic differentiation using AOT (Ahead-Of-Time) Autograd.\
It processes the function to handle input mutations, creates a joint forward-backward function, and generates an **FX graph** for the function.\
The function ensures that the graph is functional (i.e., free of in-place operations) and can handle tensor subclasses if necessary.

* pytree.tree_map: This function processes the traced_tangents to ensure they are detached and contiguous if they are tensors, preparing them for tracing.
* fn_prepped_for_autograd: Prepares the original function for autograd by incorporating metadata about views and mutations, ensuring correct handling of these aspects during tracing.
* ***create_joint***: Creates a joint forward-backward function that traces both the forward and backward passes together, enabling efficient autograd processing.
* create_functionalized_fn: Converts the joint function into a functional form, handling input mutations and tracing the joint structure, ensuring compatibility with autograd.
* aot_dispatch_subclass: Handles tracing for tensor subclasses, ensuring that the autograd process can correctly handle these specialized tensor types.
* ***_create_graph***: Creates an FX graph from the joint function and its inputs, providing a lower-level representation of the function for optimization and execution.
* ***fx_g.graph.eliminate_dead_code***: Eliminates any dead code from the FX graph to optimize it, improving performance and reducing unnecessary computations.
* ***fx_g.recompile***: Recompiles the FX graph after eliminating dead code, ensuring that the graph is up-to-date and optimized for execution.

<details>
  <summary>Code</summary>

```python
    ### dispatch_and_compile_graph.py
    fn_prepared_for_autograd = fn_prepped_for_autograd(
        flat_fn,
        fw_metadata,
    )
    joint_fn_to_trace = create_joint(fn_prepared_for_autograd, aot_config=aot_config)

    joint_fn_to_trace, updated_joint_inputs = create_functionalized_fn(
        joint_fn_to_trace,
        joint_inputs,
        meta=fw_metadata,
        aot_config=aot_config,
        trace_joint=True,
    )

    subclass_tracing_info = aot_dispatch_subclass(
        joint_fn_to_trace,
        updated_joint_inputs,
        is_joint_structure=True,
        meta=fw_metadata,
        fw_only=flat_fn,
    )
    ...
    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)
    ...
    fx_g.graph.eliminate_dead_code()
    fx_g.recompile()
```
</details>


#### create_joint
The create_joint function is designed to create a joint forward-backward function for automatic differentiation.\
It ensures that the function can be traced and differentiated correctly, handling the computation of gradients and preserving the necessary metadata.

* Inputs 
- fn: A callable function that returns a tuple of (outputs, mask). The mask indicates which outputs require tangents.
- aot_config: Configuration for AOT (Ahead-Of-Time) Autograd, which includes settings like whether tangents are needed.
* Outputs
- return a tuple of (outs, mask), where `mask` tells us which outputs are meant to have tangents.
- compute tangents for every output that requires grad.


***inner_fn***

This is the core function that computes the forward pass, identifies the outputs that require gradients, and performs the backward pass to compute the gradients.

- Calls the original function fn with the primal inputs to get the outputs and a mask indicating which outputs require tangents.
- Identifies the inputs and outputs that need gradients.
- Sets up stack trace preservation hooks for the gradient functions. **setup_stacktrace_preservation_hooks**
- Calls torch.autograd.grad to compute the gradients of the needed outputs with respect to the inputs that require gradients.
- Returns the original outputs and the computed gradients.

#### _create_graph

_create_graph wraps make_fx.

The make_fx function is a utility in PyTorch that traces a given function f and its inputs to produce an FX graph.\
This graph represents the operations performed by the function in a way that can be further analyzed, transformed, and optimized.\
The function supports different tracing modes (real, fake, symbolic) and can handle decomposition of complex operations into simpler ones.

- tracing_mode Handling: Determines the mode of tracing (real, fake, symbolic) and sets up the appropriate context for each mode.
- ShapeEnv: Manages symbolic shapes during tracing, especially in symbolic mode.
- FakeTensorMode: Creates fake tensors to simulate tensor operations without actual computation, used in fake and symbolic modes.
- ProxyTorchDispatchMode: Sets up a proxy mode to intercept and record tensor operations during tracing.
- wrap_fake: Wraps input tensors as fake tensors or symbolic integers based on the tracing mode.
- dispatch_trace: Performs the actual tracing of the function, recording the operations into an FX graph.
