---
title: Blogs to be read
date: 2024-08-06 15:32:49
permalink: /pages/f00001/
---
# Blogs watcher
## Pytorch Code Analysis

:+1: :+1: :+1: :+1:
1. https://hurray0.com/menu/151/
2. https://hurray0.com/menu/152/

1. https://www.cnblogs.com/int-me-X/category/2371391.html
2. https://cloud.tencent.com/developer/article/2346580
3. https://github.com/search?q=repo%3Akeithyin%2Fread-pytorch-source-code%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0&type=code
4. https://mlgdg.github.io/2019/12/05/Pytorch%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/
   
:+1: :+1: :+1: :+1:
1. https://www.52coding.com.cn/2019/05/05/PyTorch0/

## CUDA Optimization

### CUDA Warp-level Primitives

https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/

https://developer.nvidia.com/blog/cooperative-groups/

https://blog.csdn.net/kunhe0512/article/details/125492263

### CUDA Kernel Optimization
#### GEMM
:+1: :+1: :+1: :+1: [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM)

https://github.com/hitqshao/NVIDIA_SGEMM_PRACTICE

https://netfiles.pw/cuda-matrix-multiplication-performance-optimization-guide/

https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/

https://leimao.github.io/blog/Row-Major-VS-Column-Major/

[NVIDIA-developer-blog code-samples](https://github.com/NVIDIA-developer-blog/code-samples/blob/master/posts/tensor-cores/simpleTensorCoreGEMM.cu)

https://0mean1sigma.com/

- Step2 Global Memory Calescing
- Step3 GPU Shared Memory
- Step4 1D Thread Coarsening using GPU Registers
- Step5 2D Thread Coarsening using GPU Registers
- Step6 Vectorized Memory Accesses

#### Kernel Optimization
[Part V - 1D Convolution in CUDA (Optimized)](https://www.vrushankdes.ai/diffusion-policy-inference-optimization/part-v---1d-convolution-in-cuda-optimized)

[Part II - CUDA Kernel Optimization Tips](https://www.vrushankdes.ai/diffusion-policy-inference-optimization/part-ii---cuda-kernel-optimization-tips)

### Optimization Papers

This paper mention a bit about float4 memory accessing performance.
[CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization](https://lightsighter.org/pdfs/cudadma-sc11.pdf)

### Softmax

Demo of diy softmax and import in pytorch
https://github.com/fattorib/CudaSoftmax

:+1: :+1: :+1: :+1: ICS Paper on using tensor core to accelerate Reduction and Scan.
https://arxiv.org/pdf/1811.09736

Volta Tensor Coreï¼š
https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf

## Papers to be read
- Optimization Principles and Application Performance Evaluation of a Multithreaded GPU Using CUDA
- Performance Study of GPU applications using SYCL and CUDA on Tesla V100 GPU
- A performance prediction model for the CUDA GPGPU platform
- 3.5-D Blocking Optimization for Stencil Computations on Modern CPUs and GPUs
- Benchmarking Optimization Algorithms for Auto-Tuning GPU Kernels
- [316] Auto-tuning a high-level language targeted to GPU codes
- [91] Kernel Tuner: A search-optimizing GPU code auto-tuner
- Meta-programming and auto-tuning in the search for high performance GPU code
- [79] Autotuning in High-Performance Computing Applications
- [105] Optimizing CUDA code by kernel fusion: application on BLAS
- [24] A review of CUDA optimization techniques and tools for structured grid computing
