---
title: Blogs to be read
date: 2024-12-26 15:32:49
permalink: /pages/f00001/
---
# Blogs watcher
## Pytorch Code Analysis

:+1: :+1: :+1: :+1:
1. https://hurray0.com/menu/151/
2. https://hurray0.com/menu/152/

1. https://www.cnblogs.com/int-me-X/category/2371391.html
2. https://cloud.tencent.com/developer/article/2346580
3. https://github.com/search?q=repo%3Akeithyin%2Fread-pytorch-source-code%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0&type=code
4. https://mlgdg.github.io/2019/12/05/Pytorch%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/


:+1: :+1: :+1: :+1:
https://www.52coding.com.cn/2019/05/05/PyTorch0/

:+1: :+1: :+1: :+1:
Paper: SURVEY AND EVALUATION OF CONVERGING ARCHITECTURE IN LLMS BASED ON FOOTSTEPS OF OPERATIONS

:+1: :+1: :+1: :+1:
[PyTorch – Internal Architecture Tour](https://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour/)

:+1: :+1: :+1: :+1:
[PyData Montreal slides for the talk: PyTorch under the hood](https://blog.christianperone.com/2019/02/pydata-montreal-slides-for-the-talk-pytorch-under-the-hood/)

:+1: :+1: :+1: :+1:
[PyTorch 2 Internals – Talk](https://blog.christianperone.com/2023/12/pytorch-2-internals-talk/)

[llama.cpp source code analysis](https://www.cnblogs.com/learnhow/p/18447779)

[llama.cpp source code analysis](https://forsworns.github.io/zh/blogs/20240623/)

## CUDA Optimization

### CUDA Warp-level Primitives

https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/

https://developer.nvidia.com/blog/cooperative-groups/

https://blog.csdn.net/kunhe0512/article/details/125492263

### CUDA Kernel Optimization
#### GEMM
:+1: :+1: :+1: :+1: [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM)

[Programming tensor cores using nvcuda-wmma](https://xinyinicole.com/blogs/programming-tensor-cores-using-nvcuda-wmma/)

https://github.com/hitqshao/NVIDIA_SGEMM_PRACTICE

https://netfiles.pw/cuda-matrix-multiplication-performance-optimization-guide/

https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/

https://leimao.github.io/blog/Row-Major-VS-Column-Major/

[NVIDIA-developer-blog code-samples](https://github.com/NVIDIA-developer-blog/code-samples/blob/master/posts/tensor-cores/simpleTensorCoreGEMM.cu)

[KAUST] https://vccvisualization.org/teaching/CS380/CS380_fall2021_lecture_26.pdf

https://0mean1sigma.com/

- Step2 Global Memory Calescing
- Step3 GPU Shared Memory
- Step4 1D Thread Coarsening using GPU Registers
- Step5 2D Thread Coarsening using GPU Registers
- Step6 Vectorized Memory Accesses

#### Kernel Optimization
[Part V - 1D Convolution in CUDA (Optimized)](https://www.vrushankdes.ai/diffusion-policy-inference-optimization/part-v---1d-convolution-in-cuda-optimized)

[Part II - CUDA Kernel Optimization Tips](https://www.vrushankdes.ai/diffusion-policy-inference-optimization/part-ii---cuda-kernel-optimization-tips)

### Optimization Papers

This paper mention a bit about float4 memory accessing performance.
[CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization](https://lightsighter.org/pdfs/cudadma-sc11.pdf)

### Softmax

Demo of diy softmax and import in pytorch
https://github.com/fattorib/CudaSoftmax

:+1: :+1: :+1: :+1: ICS Paper on using tensor core to accelerate Reduction and Scan.
https://arxiv.org/pdf/1811.09736

Volta Tensor Core：
https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf

## Papers to be read
- Optimization Principles and Application Performance Evaluation of a Multithreaded GPU Using CUDA
- Performance Study of GPU applications using SYCL and CUDA on Tesla V100 GPU
- A performance prediction model for the CUDA GPGPU platform
- 3.5-D Blocking Optimization for Stencil Computations on Modern CPUs and GPUs
- Benchmarking Optimization Algorithms for Auto-Tuning GPU Kernels
- [316] Auto-tuning a high-level language targeted to GPU codes
- [91] Kernel Tuner: A search-optimizing GPU code auto-tuner
- Meta-programming and auto-tuning in the search for high performance GPU code
- [79] Autotuning in High-Performance Computing Applications
- [105] Optimizing CUDA code by kernel fusion: application on BLAS
- [24] A review of CUDA optimization techniques and tools for structured grid computing

## LLM Related
- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
- [LLMs相关知识及面试题](https://wdndev.github.io/llm_interview_note/#/)
- Gating Dropout: Communication-efficient Regularization for Sparsely Activated Transformers
  This paper discuss about token routing to expert in another node. All-to-all communication.
  ![image](https://github.com/user-attachments/assets/b958d7ab-261c-4a35-9550-237aa2e4d01e)

- PipeDream: inter-batch pipeline parallelism
  ![image](https://github.com/user-attachments/assets/31828b88-2a1e-483c-bada-a838c23f5cc2)
- BPIPE: Memory-Balanced Pipeline Parallelism for Training Large Language Models
  ![image](https://github.com/user-attachments/assets/95f482d2-2c0e-454c-b877-35a9efe1932a)
- [SCALING FP8 TRAINING TO TRILLION-TOKEN LLMS](https://arxiv.org/pdf/2409.12517)
- [FP8-LM: Training FP8 Large Language Models](https://arxiv.org/pdf/2310.18313)


