---
title: CUDA Softmax
date: 2024-12-27 15:32:49
permalink: /pages/f00006/
---

# CUDA Softmax

## Softmax calculation:

### Naive softmax

![image](https://github.com/user-attachments/assets/1a949b2e-b1f3-434f-b005-3e10729d203e)

### Safe softmax
Safe softmax avoid the overflow or under flow due to the exponent.
![image](https://github.com/user-attachments/assets/8625f0c4-fa77-4305-8c6a-412ca5420aff)

Three passes
- get max
- calc divident
- calc element-wise normalization

### Online normalizer calculation

Two passes

- get max and calc divident
- calc element-wise normalization

Online normalizer calculation for softmax
https://arxiv.org/pdf/1805.02867

![image](https://github.com/user-attachments/assets/5ba08ef2-c3c8-48e2-9cfb-ff21096a0638)


## Optimization of Softmax has the following ideas:

## warp-level sync

each warp can process one row.\
first step, each warp process the first 32 elements in one row.\
next step, each warp processed the next 32 elements in the row, until the end of the row.\
This achieves **memory coalescing**.

In the last step, each thread collects correspoding subset max and divident:
- thread [0] : max of (A[0],A[32],A[96]...)
- thread [1] : max of (A[1],A[33],A[97]...)
- thread [31]: max of (A[31],A[63],A[127]...)

Now the result of 32 threads in the warp to be reduced to sum for divident and max.

One way is to choose one of the thread, maybe thread 0 to iterate through all 32 threads and gets the max.
- use __shfl_xor_sync 
