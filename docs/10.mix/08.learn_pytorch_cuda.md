---
title: Understanding Pytorch CUDA
date: 2024-12-30 15:32:49
permalink: /pages/f00008/
---
# Pytorch CUDA

before dive into deeper source code, this is the instinct of pytorch source code:

gradually lower code from python to cuda code

* **torch**
  - pytorch interface, python oriented
  -./csrc.**autograd mechanism**
    - engine
    - function
    - variable
* **c10** core functions
  -**cuda memory management**
  -**dispatcher mechanism**
* **aten**
  - native_functions.yaml
    - register all implementations with backend specification
```
    - func: _slow_conv2d_forward.output(Tensor self, Tensor weight, SymInt[2] kernel_size,
      Tensor? bias, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) output) -> Tensor(a!)
    python_module: nn
    dispatch:
    CPU: slow_conv2d_forward_out_cpu
    CUDA: slow_conv2d_forward_out_cuda
```
  - src/ATen/Native/core/boxing/
    - WrapFunctionIntoFunctor.h support dispatcher, unbox function and call into real implementation
  - ./src/ATen/cuda
    - general functions
      
       ![image](https://github.com/user-attachments/assets/0e78c6b5-ac3e-488c-9296-670f1e8558f6)
  - ./src/ATen/native/cuda
    - implementation of different layers
      
       ![image](https://github.com/user-attachments/assets/41d8b9a7-60cf-4c4b-8f6f-45570df2c49e)

## Pytorch CUDA Memory Allocation

### Lower level

./c10/cuda/CUDACachingAllocator.cpp

```
Block* malloc(
      c10::DeviceIndex device,
      size_t orig_size,
      cudaStream_t stream) {
    ...
    // First, try to get a block from the existing pool.
    bool block_found =
        // Search pool
        get_free_block(params)
        // Trigger callbacks and retry search
        || (trigger_free_memory_callbacks(params) && get_free_block(params));
    ...
    if (!block_found) {
      // Do garbage collection if the flag is set.
      if (C10_UNLIKELY(
              set_fraction &&
              CUDAAllocatorConfig::garbage_collection_threshold() > 0.0)) {
        garbage_collect_cached_blocks(context);
      }
    ...
      // Attempt allocate
      // WARNING: alloc_block may release the allocator lock when calling
      // cudaMalloc. So far this function has not modified allocator state, but
      // keep in mind that any observed allocator state may change across calls
      // to alloc_block since it may release the lock.
      block_found = alloc_block(params, false, context, lock)
          // Free enough available cached blocks to satisfy alloc and retry
          // alloc.
          || (release_available_cached_blocks(params, context) &&
              alloc_block(params, false, context, lock))
          // Free all non-split cached blocks and retry alloc.
          || (C10_LIKELY(captures_underway.size() == 0) &&
              release_cached_blocks(context) &&
              alloc_block(params, true, context, lock));
...
}
```

./aten/src/ATen/cuda

- CachingHostAllocator.h
```
inline TORCH_CUDA_CPP_API at::DataPtr HostAlloc(size_t size) {
  return getCachingHostAllocator()->allocate(size);
}
```
- CachingHostAllocator.cpp
```
struct CUDACachingHostAllocatorImpl
    : public CachingHostAllocatorImpl<CUDAStream, EventPool::Event> {
 private:
  void allocate_host_memory(size_t size, void** ptr) override {
    // Pinned memory pointers allocated by any device can be directly used by
    // any other device, regardless of the current device at the time of
    // allocation, since we assume unified addressing. So we grab any existing
    // primary context, if available. See pytorch/pytorch#21081.
    ...
    // Use cudaHostAlloc for allocating pinned memory (global lock in driver)
    C10_CUDA_CHECK(cudaHostAlloc(ptr, size, cudaHostAllocDefault));
    ...
    }
  }

```
### intermediate level

code like cudnn use get_workspace_size to allocate space

./aten/src/ATen/native/cuda/MixedDtypesLinear.cu
```
  // Allocate workspace for CUTLASS mixed datatypes GEMM kernel.
  const auto workspace_size = Gemm::get_workspace_size(arguments);
```

code like:
./aten/src/ATen/native/cuda/ForeachReduceOp.cu

allocate memory by using at::zeros or at::empty 

```
  auto output_per_tensor = at::zeros(
      {static_cast<int64_t>(ntensors) * max_chunks_per_tensor}, options);
```
at::zeros is based on:\
./aten/src/ATen/native/cuda/EmptyTensor.cpp
```
TensorBase empty_cuda(
    IntArrayRef size,
    ScalarType dtype,
    std::optional<Device> device_opt,
    std::optional<c10::MemoryFormat> memory_format_opt) {
  at::globalContext().lazyInitDevice(c10::DeviceType::CUDA);
  const auto device = device_or_default(device_opt);
  TORCH_INTERNAL_ASSERT(device.is_cuda());
  const DeviceGuard device_guard(device);
  auto* allocator = at::cuda::getCUDADeviceAllocator();
  constexpr c10::DispatchKeySet cuda_dks(c10::DispatchKey::CUDA);
  return at::detail::empty_generic(
      size, allocator, cuda_dks, dtype, memory_format_opt);
}
```

./aten/src/ATen/EmptyTensor.cpp

We have specify allocator and size, so we will call cuda caching allocator to allocate memory.

```
TensorBase empty_generic(
    IntArrayRef size,
    c10::Allocator* allocator,
    c10::DispatchKeySet ks,
    ScalarType scalar_type,
    std::optional<c10::MemoryFormat> memory_format_opt) {
  return _empty_generic(size, allocator, ks, scalar_type, memory_format_opt);
}

template <typename T>
TensorBase _empty_generic(
    ArrayRef<T> size,
    c10::Allocator* allocator,
    c10::DispatchKeySet ks,
    ScalarType scalar_type,
    std::optional<c10::MemoryFormat> memory_format_opt) {
  at::detail::check_size_nonnegative(size);
  at::detail::raise_warning_for_complex_half(scalar_type);
  caffe2::TypeMeta dtype = scalarTypeToTypeMeta(scalar_type);
  auto size_bytes = computeStorageNbytesContiguous(size, dtype.itemsize());
  auto storage_impl = c10::make_intrusive<StorageImpl>(
      c10::StorageImpl::use_byte_size_t(),
      size_bytes,
      allocator,
      /*resizeable=*/true);

  auto tensor = detail::make_tensor_base<TensorImpl>(
      std::move(storage_impl), ks, dtype);
  // Default TensorImpl has size [0]
  // NB: test for meta dispatch key to avoid guarding on zero-ness
  if (ks.has(c10::DispatchKey::Meta) || size.size() != 1 || size[0] != 0) {
    tensor.unsafeGetTensorImpl()->generic_set_sizes_contiguous(size);
  }

  if (memory_format_opt.has_value()) {
    // Restriding a just-created empty contiguous tensor does nothing.
    if (*memory_format_opt != MemoryFormat::Contiguous) {
      tensor.unsafeGetTensorImpl()->empty_tensor_restride(*memory_format_opt);
    }
  }

  return tensor;
}
```

### python level

__init__.pyi.in

```
class _cuda_CUDAAllocator: ...

def _cuda_customAllocator(alloc_fn: _int, free_fn: _int) -> _cuda_CUDAAllocator: ...
def _cuda_changeCurrentAllocator(allocator: _cuda_CUDAAllocator) -> None: ...
def _cuda_getAllocator() -> _cuda_CUDAAllocator: ...
```

./torch/cuda/memory.py

```
__all__ = [
    "caching_allocator_alloc",
    "caching_allocator_delete",
    "caching_allocator_enable",
    ...
    "memory_allocated",
    ...
]

def caching_allocator_alloc(size, device: Union[Device, int] = None, stream=None):
    r"""Perform a memory allocation using the CUDA memory allocator.

    Memory is allocated for a given device and a stream, this
    function is intended to be used for interoperability with other
    frameworks. Allocated memory is released through
    :func:`~torch.cuda.caching_allocator_delete`.

    Args:
        size (int): number of bytes to be allocated.
        device (torch.device or int, optional): selected device. If it is
            ``None`` the default CUDA device is used.
        stream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then
            the default stream for the selected device is used.

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    if device is None:
        device = torch.cuda.current_device()
    device = _get_device_index(device)
    if stream is None:
        stream = torch.cuda.current_stream(device)
    if isinstance(stream, torch.cuda.streams.Stream):
        stream = stream.cuda_stream
    ...
    with torch.cuda.device(device):
        return torch._C._cuda_cudaCachingAllocator_raw_alloc(size, stream)
```

./torch/csrc/cuda/Module.cpp

```
    {"_cuda_cudaCachingAllocator_raw_alloc",
     THCPModule_cudaCachingAllocator_raw_alloc,
     METH_VARARGS,
     nullptr},

PyObject* THCPModule_cudaCachingAllocator_raw_alloc(
    PyObject* _unused,
    PyObject* args) {
  HANDLE_TH_ERRORS
  PyObject* size_o = nullptr;
  PyObject* stream_o = nullptr;
  if (!PyArg_ParseTuple(args, "OO", &size_o, &stream_o)) {
    THPUtils_invalidArguments(
        args,
        nullptr,
        "caching_allocator_alloc",
        1,
        "(ssize_t size, intptr_t stream);");
    return nullptr;
  }
  auto size = PyLong_AsSsize_t(size_o);
  cudaStream_t stream = static_cast<cudaStream_t>(PyLong_AsVoidPtr(stream_o));
  void* mem = nullptr;
  {
    pybind11::gil_scoped_release no_gil;
    mem = c10::cuda::CUDACachingAllocator::raw_alloc_with_stream(size, stream);
  }
  return PyLong_FromVoidPtr(mem);
  END_HANDLE_TH_ERRORS
}
```


### code gen for memory allocation

./torch/_inductor/codegen/cuda/cuda_kernel.py
```
    def call_kernel(
        self,
        name: str,
        node: "CUDATemplateBuffer",  # type: ignore[name-defined]
    ) -> None:
        if node.get_workspace_size() > 0:
            ws = WorkspaceArg(
                count=node.get_workspace_size(),
                device=V.graph.get_current_device_or_throw(),
                zero_mode=WorkspaceZeroMode.UNINITIALIZED,
                outer_name=WorkspaceArg.unique_name(),
            )
            wrapper.generate_workspace_allocation(ws)
            workspace = str(ws.outer_name)
            call_args.append(
                workspace
                if V.graph.cpp_wrapper
                else f"c_void_p({workspace}.data_ptr())"
            )
      .....
```

./torch/_inductor/codegen/wrapper.py
```
    def generate_workspace_allocation(self, ws: WorkspaceArg):
        name = ws.get_name()
        line = AllocateLine(self, ws)
        if ws.zero_mode == WorkspaceZeroMode.UNINITIALIZED:
            self.writeline(line)
        elif ws.zero_mode == WorkspaceZeroMode.ZERO_ON_CALL:
            self.writeline(line)
            self.writeline(self.make_zero_buffer(name))
        elif ws.zero_mode == WorkspaceZeroMode.ZERO_PER_GRAPH:
            prior = self.allocated_workspaces.get(name)
            if prior:
                assert isinstance(prior, AllocateLine)
                # expand existing allocation
                prior.node = WorkspaceArg.maximum(prior.node, ws)
            else:
                self.writeline(line)
                self.writeline(self.make_zero_buffer(name))
                self.allocated_workspaces[name] = line
        else:
            raise AssertionError(ws.zero_mode)

        if config.triton.autotune_at_compile_time:
            self.kernel_autotune_calls.writeline(
                PythonWrapperCodegen.make_allocation(
                    self,
                    name,
                    ws.device,
                    ws.dtype,
                    shape=(V.graph.sizevars.size_hint(ws.count),),
                    stride=(1,),
                )
            )
            if ws.zero_mode != WorkspaceZeroMode.UNINITIALIZED:
                self.kernel_autotune_calls.writeline(
                    PythonWrapperCodegen.make_zero_buffer(self, name)
                )
```

## Pytorch Adam CUDA Kernel

### native_functions.yaml

./aten/src/ATen/native/native_functions.yaml

```
- func: _fused_adam_(Tensor(a!)[] self,
Tensor(b!)[] grads,
Tensor(c!)[] exp_avgs,
Tensor(d!)[] exp_avg_sqs,
Tensor(e!)[] max_exp_avg_sqs,
Tensor[] state_steps, *,
float lr, float beta1, float beta2,
float weight_decay, float eps, bool amsgrad,
bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()
  # Unlike "foreach" functions, lists of tensors should be guaranteed to be on the same device (for now).
  variants: function
  dispatch:
    CPU: _fused_adam_kernel_cpu_
    CUDA: _fused_adam_kernel_cuda_
    MPS: _fused_adam_kernel_mps_
  autogen: _fused_adam, _fused_adam.out
```

### FusedAdamKernel.cu

./aten/arc/ATen/native/cuda

```
// The following overload simply has a Tensor lr
void _fused_adam_kernel_cuda_(
    at::TensorList params,
    at::TensorList grads,
    at::TensorList exp_avgs,
    at::TensorList exp_avg_sqs,
    at::TensorList max_exp_avg_sqs,
    at::TensorList state_steps,
    const at::Tensor& lr,
    const double beta1,
    const double beta2,
    const double weight_decay,
    const double eps,
    const bool amsgrad,
    const bool maximize,
    const std::optional<at::Tensor>& grad_scale,
    const std::optional<at::Tensor>& found_inf) {
  if (lr.is_cpu()) {
    _fused_adam_kernel_cuda_(
        params,
        grads,
        exp_avgs,
        exp_avg_sqs,
        max_exp_avg_sqs,
        state_steps,
        lr.item<double>(),
        beta1,
        beta2,
        weight_decay,
        eps,
        amsgrad,
        maximize,
        grad_scale,
        found_inf);
    return;
  }

  // Manually check devices since we specify no device check in
  // native_functions.yaml
  ...

  if (amsgrad) {
    ...
  } else {
    TORCH_CHECK(
        at::native::check_fast_path_restrictions(
            {params, grads, exp_avgs, exp_avg_sqs}),
        "params, grads, exp_avgs, and exp_avg_sqs must have same dtype, device, and layout");
    _fused_adam_cuda_impl_(
        params,
        grads,
        exp_avgs,
        exp_avg_sqs,
        state_steps,
        lr,
        beta1,
        beta2,
        weight_decay,
        eps,
        maximize,
        grad_scale,
        found_inf);
  }
}
```

### fused_adam_impl.cu

./aten/arc/ATen/native/cuda

```
void _fused_adam_cuda_impl_(
    at::TensorList params,
    at::TensorList grads,
    at::TensorList exp_avgs,
    at::TensorList exp_avg_sqs,
    at::TensorList state_steps,
    const double lr,
    const double beta1,
    const double beta2,
    const double weight_decay,
    const double eps,
    const bool maximize,
    const std::optional<at::Tensor>& grad_scale,
    const std::optional<at::Tensor>& found_inf) {
  std::vector<std::vector<at::Tensor>> tensor_lists{
      params.vec(), grads.vec(), exp_avgs.vec(), exp_avg_sqs.vec()};

  const float* grad_scale_ptr =
      grad_scale.has_value() ? grad_scale->data_ptr<float>() : nullptr;
  const float* found_inf_ptr =
      found_inf.has_value() ? found_inf->data_ptr<float>() : nullptr;
  const float* lr_ptr = nullptr;

  AT_DISPATCH_FLOATING_TYPES_AND2(
      kHalf,
      kBFloat16,
      params[0].scalar_type(),
      "fused_adam_kernel_cuda",
      [&]() {
        multi_tensor_apply_for_fused_optimizer<4>(
            tensor_lists,
            state_steps,
            FusedAdamMathFunctor<scalar_t, 4, ADAM_MODE::ORIGINAL, false>(),
            lr_ptr, // unused
            lr,
            beta1,
            beta2,
            weight_decay,
            eps,
            maximize,
            grad_scale_ptr,
            found_inf_ptr);
      });
}
```

The callable function is **FusedAdamMathFunctor<scalar_t, 4, ADAM_MODE::ORIGINAL, false>()**.

### MultiTensorApply.cuh

```

template <typename T, typename U, typename... ArgTypes>
C10_LAUNCH_BOUNDS_1(kBlockSize)
__global__ void multi_tensor_apply_kernel(
    T tensorListMeta,
    U callable,
    ArgTypes... args) {
  // Hand the chunk information to the user-supplied functor to process however
  // it likes.
  callable(kChunkSize, tensorListMeta, args...);
}

template <int depth, typename T, typename... ArgTypes>
void multi_tensor_apply_for_fused_optimizer(
    std::vector<std::vector<at::Tensor>>& tensor_lists,
    at::TensorList state_steps,
    T callable,
    ArgTypes... args) {
    ...
    for (const auto& chunk : c10::irange(chunks)) {
            multi_tensor_apply_kernel<<<
            loc_block_info,
            kBlockSize,
            0,
            at::cuda::getCurrentCUDAStream()>>>(
            tensorListMeta, callable, args...);
        C10_CUDA_KERNEL_LAUNCH_CHECK();
    }
}
```

---

## Pytorch linear lowering

###  python_nn_functions.cpp
Depth 31/29

./torch/csrc/autograd/generated/python_nn_functions.cpp

operator() (__closure=0x7ffc7dcf9e88, input=..., weight=..., bias=...)

torch::autograd::THPVariable_linear

### linear.h
Depth 29

./build/aten/src/ATen/ops/linear.h

at::linear (input=..., weight=..., bias=...)

###  Operators_0.cpp
at namespace
Depth 28

at::_ops::linear::call (input=..., weight=..., bias=...)
build/aten/src/ATen/Operators_0.cpp:3601


### dispatch

c10 namespace

./aten/src/ATen/Core/dispatch/Dispatcher.h dispatch

![image](https://github.com/user-attachments/assets/cf488804-1b5d-4ade-ba21-fa0ec46733f7)

### boring part, traversing from dispatcher to actual kernel call

#### boxing and unbox
Depth 23

c10 namespace

aten/src/ATen/core/boxing/KernelFunction_impl.h 105

c10::callUnboxedKernelFunction at 

aten/src/ATen/core/boxing/KernelFunction_impl.h:53

c10::impl::wrap_kernel_functor_unboxed_ at

aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:468

#### From c10 to at namespace

Please notice the dispatchkeyset, that is related to dispatch mechanism in pytorch.

```
c10::impl::detail::WrapFunctionIntoFunctor_ in aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13\
to
at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear in
build/aten/src/ATen/RegisterCompositeImplicitAutograd.cpp:2620


Depth 20

at::native::linear (input=..., weight=..., bias_opt=...) at aten/src/ATen/native/Linear.cpp:95

Tensor linear(const Tensor& input, const Tensor& weight, const c10::optional<Tensor>& bias_opt) {
  ...
  if (input_dim == 2 && bias->defined()) {
    // Fused op is marginally faster.
    return at::addmm(*bias, input, weight.t());
  }
  ...
}


Depth 19

at::addmm (self=..., mat1=..., mat2=..., beta=..., alpha=...) at build/aten/src/ATen/ops/addmm.h:36

Depth 18

at::_ops::addmm::call (self=..., mat1=..., mat2=..., beta=..., alpha=...) at aten/src/ATen/Operators_0.cpp:7151

Depth 12

c10::impl::detail::WrapFunctionIntoFunctor_<
c10::CompileTimeFunctionPointer<
at::Tensor(c10::DispatchKeySet, const at::Tensor&, const at::Tensor&, const at::Tensor&, const c10::Scalar&, const c10::Scalar&),
torch::autograd::VariableType::(anonymous namespace)::addmm>,
at::Tensor,
c10::guts::typelist::typelist<c10::DispatchKeySet, const at::Tensor&, const at::Tensor&, const at::Tensor&, const c10::Scalar&, const c10::Scalar&>
>::operator() (args#5=..., args#4=..., args#3=..., args#2=..., args#1=..., args#0=..., this=0x30c7a930)
aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13

```

to torch::autograd::VariableType::(anonymous namespace)::addmm (ks=..., self=..., mat1=..., mat2=..., beta=..., alpha=...) at aten/src/autograd/generated/VariableType_0.cpp:6898

### RedispatchFunction

This is generated function, but I guess most of redispatch function will be generated here.

:+1:

at::redispatch::addmm (dispatchKeySet=..., self=..., mat1=..., mat2=..., beta=..., alpha=...) at build/aten/src/ATen/RedispatchFunctions.h:8517

And then:
at::(anonymous namespace)::wrapper_CUDA_addmm build/aten/src/ATen/RegisterCUDA.cpp

**linear**
```
    // aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
    inline at::Tensor linear(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias={}) {
        return at::_ops::linear::redispatch(dispatchKeySet, input, weight, bias);
    }
```

**relu**
```
    inline at::Tensor relu(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
        return at::_ops::relu::redispatch(dispatchKeySet, self);
    }
```

**softmax**
```
    inline at::Tensor softmax(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype=c10::nullopt) {
        return at::_ops::softmax_int::redispatch(dispatchKeySet, self, dim, dtype);
    }
```
### Blas.cpp

Blas.cpp calls to at::cuda::blas::gemm function.

./aten/src/ATen/native/cuda/Blas.cpp

Notice at::cuda::blas::gemm function. it calls into gemm function.
```
    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(
        at::ScalarType::Half,
        at::ScalarType::BFloat16,
        scalar_type,
        "addmm_cuda",
        [&] {
          using opmath_t = at::opmath_type<scalar_t>;
          opmath_t alpha_val = alpha.to<opmath_t>();
          opmath_t beta_val = beta.to<opmath_t>();
          const scalar_t* mat1_ptr = args.mata->const_data_ptr<scalar_t>();
          const scalar_t* mat2_ptr = args.matb->const_data_ptr<scalar_t>();
          scalar_t* result_ptr = args.result->mutable_data_ptr<scalar_t>();
          at::cuda::blas::gemm<scalar_t>(
              args.transa,
              args.transb,
              args.m,
              args.n,
              args.k,
              alpha_val,
              mat1_ptr,
              args.lda,
              mat2_ptr,
              args.ldb,
              beta_val,
              result_ptr,
              args.result_ld);
        });
    switch (activation) {
      case Activation::RELU:
        at::relu_(const_cast<Tensor&>(*args.result));
        break;
      case Activation::GELU:
        at::gelu_(const_cast<Tensor&>(*args.result), "tanh");
        break;
      default: break;
    }
```
