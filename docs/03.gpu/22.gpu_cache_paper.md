---
title: GPU Cache's Papers
date: 2024-09-06
permalink: /pages/45879/
---

1. [22] Adaptive Memory-Side Last-Level GPU Chacing
2. [42] Understaning the Tradeoffs between Software-Managed vs. Hardware-Managed Caches in GPUs
3. [88] Locality-Driven Dynamic GPU Cache Bypassing


---
## 1. Adaptive Memory-Side Last-Level GPU Chacing
### Introduction

GPUs typically feature a two-level on-chip cache hierarchy in which the first-level caches are private to each SM while the last-level cache (LLC) is a shared memory-side cache that is partitioned into
equally-sized slices and accessed via the NoC.

In fact, we find that GPU workloads have large read-only shared data footprints. For such sharing-intensive workloads, multiple SMs experience a bandwidth bottleneck when
they serialize on accesses to the same shared cache line.

While this is a practical solution for a limited number of cores in a CPU, it does not scale to a large number of SMs due to limitations in scaling GPU die size.

Shared LLCs incur a performance bottleneck for workloads that frequently access data shared by multiple SMs.

A shared memory-side LLC consists of multiple slices each caching a specific memory partition, i.e., a specific address range of the entire memory space is served by a particular
memory controller.

As a result, **a shared cache line appears in a single LLC slice, which leads to a severe performance bottleneck if multiple SMs concurrently access the same shared data.**

We find that GPU applications with **high degrees of read-only data sharing significantly benefit from a private LLC organization.**

To that end, this paper proposes adaptive memory-side caching to dynamically choose between a shared or private memory-side LLC.

These observations suggest an opportunity to improve performance by dynamically adapting a memory-side LLC to the needs of an application’s sharing behavior.

**adaptive memory-side caching to dynamically choose between a shared or private memory-side LLC.** \
Selecting a shared versus private LLC is done using a lightweight performance model.

By default, the GPU assumes a shared LLC.

Profiling information is periodically collected to predict LLC miss rate and bandwidth under a private LLC organization while executing under a shared LLC. If deemed beneficial, the LLC is adapted to
a private cache.

The LLC reverts back to a shared organization periodically and when a new kernel gets launched.

#### Main Idea

Private LLC could have replicate data, but waste memory space.

Shared LLC could access shared data a lot, introducing bottleneck.

Switch between those two modes.

![image](https://github.com/user-attachments/assets/8138dd9c-12e8-430d-8c65-43cae6d84a53)

---
## 2. Understanding the Tradeoffs between Software-Managed vs. Hardware-Managed Caches in GPUs

On one hand, the kernels utilizing the L1 caches may support higher degrees of thread-level parallelism, offer more opportunities for data to be 
allocated in registers, and sometimes result in lower dynamic instruction counts.

On the other hand, the applications utilizing shared memory enable more coalesced accesses and tend to achieve higher degrees of memory-level parallelism.

### Main Idea

Even if a matrix totally fits into cache, the *L1 D-cache version is surprisingly much slower (43.8%) than the shared-memory version.*

Our results show that for most applications, the GPU kernels utilizing shared memory deliver significantly higher performance than those leveraging L1 D-caches.\
The fundamental reasons are MLP and coalescing.

For a few benchmarks for which the L1 D-cache versions have higher performance, *the performance impact is mainly due to improved thread-level parallelism (TLP) and allocating more data to registers*.

Overall, rather than cache hit rates, the subtle factors including MLP, coalescing, and TLP often have more profound performance impacts.

![image](https://github.com/user-attachments/assets/0e261ecc-f3b0-49b6-b395-279278728977)

### Study

**the register usage is the limiting factor on how many TBs can run concurrently on an SM.**

![image](https://github.com/user-attachments/assets/0f68ab5a-0e84-49da-862f-9754f545b857)

(a) Why is the D-cache version slightly slower than the shared-memory version even with a perfect cache?
Since the array access ‘A[a+WA*ty+k]’ of the D-cache version cannot be coalesced into a single cache access, it suffers from additional pipeline stalls even though all accesses hit in cache.

(b) With a realistic cache, why is the D-cache version much slower?
We found that performance is not determined by the total number of cache misses.\
Instead it depends more on how these cache-misses overlap with each other, i.e., the degrees of memory-level parallelism (MLP).

In cache-version, the cache-misses overlap is bad, as seen in the figure.

> In other words, in cache version, each iteration it has a cache miss. In shared-memory version, in the first two load-to-sharememory, there is large MLP.
> Cache-version is like a man eat food every day. Sharedmemory-version is like a man only eat food in first ten years of his life.


---

## 3. Locality-Driven Dynamic GPU Cache Bypassing 


### Contribution
For area and energy efficiency, we propose to decouple the tag and data stores of the existing L1 D-cache and integrate the locality filtering capability into the tag store through simple and cost-effective hardware extensions.

### Introduction

More importantly, large number of the incoming memory requests with no or low reuse may evict cache lines with high reuse,resulting in cache pollution. In this case, even advanced cache replacement policies (e.g. RRIP [14] and SHiP [29]) are ineffective to address such contention problems on GPUs [24].

### Two paths: cache and bypass
![image](https://github.com/user-attachments/assets/e84718d5-b024-4fd0-85fb-adeabc79af45)

1. For cacheable accesses, the first path, which sends the memory requests into L1 D-cache and is labeled as ‘L1 D-path’, is used.\
   Considering the small number of cache lines and MSHR entries, these resources can be quickly occupied if all memory requests are diverted into L1 D-Path.
2. The second path is for un-cacheable accesses, such as global memory accesses in NVIDIA’s Kepler architecture (not cached in Kepler’s L1).\
   It diverts the memory requests to bypass the L1 D-cache (labeled as ‘Bypass Path’ in Figure 1) and directly sends requests through an interconnect into the next level memory hierarchy.

### Categorize application according to friendly to cache

![image](https://github.com/user-attachments/assets/b51c548f-4790-457e-a1af-d7218a7c9c98)

New tag store entry now contains a Reference Count (RC) field and a Position field.\
The RC field (6-bit) holds the reference frequency (reuse) accumulated for the address.\
The Position field (2-bit) connects a tag store entry with a data line in the data store using a pointer to record the data line’s position.

As shown in below figure, if RC is over thresholds, install the block into cache. Or else, just bypass.
![image](https://github.com/user-attachments/assets/fea29db6-abe5-4f47-89ed-c496db351ec6)





































