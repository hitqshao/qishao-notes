---
title: Unified Memory Paper List
date: 2023-11-11 
permalink: /pages/44771e/
---

1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory
2. In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing
3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions
4. Performance Evaluation of Advanced Features in CUDA Unified Memory
5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory
6. Unified Memory: GPGPU-Sim/UVM Smart Integration
7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads
8. An Intelligent Framework for Oversubscription Management in CPU-GPU Unified Memory
9. Architectural Support for Address Translation on GPUs Designing Memory Management Units for CPU/GPUs with Unified Address Spaces
10. Machine Learning Guided Optimal Use of GPU Unified Memory
11. Towards High Performance Paged Memory for GPUs

---

**Unified Memory History**
copied from ***Evolution of Nvidia GPU from microarchitectures Pascal to Ampere***

CUDA 4 introduced UVA (Unified Virtual Addressing) to provide a single virtual memory address space for both CPU and GPU memory and enable pointers to be accessed from GPU code no matter where in the system they reside. UVA enables Zero-Copy memory, a pinned CPU memory accessible by GPU code directly, over PCIe, without the need for memory copy. This provides some of the convince of Unified Memory, but at the cost of worse performance, because GPU always accesses it with PCIe’s low bandwidth and high latency.[1]

Later, CUDA 6 introduced Unified Memory, which creates a pool of managed memory that programs running on the CPU and GPU can access without explicit data movement. However, only when CPU and GPU processes are not running together because of the limitation of the Kepler and Maxwell GPU microarchitecture. Also, the Unified Memory address space was limited to the size of the GPU memory.[1, 3] 

CUDA 8 and Pascal microarchitectures improve Unified Memory functionality by **adding 49-bit virtual addressing and page faulting capability**. The larger 49-bit virtual addresses are sufficient to enable GPUs to access the entire system memory plus the memory of all GPUs in the system. Because of the memory page faulting functionality, the CUDA system software does not need to synchronize all managed memory allocations to the GPU before each kernel lunch. Instead, when a thread running on GPU faults on non-resident memory access(**demanding page**), it stalls until the page can be migrated and the page table updated. Alternatively, the page may be mapped for remote access over PCIe or NVLink interconnects.[1, 3, 6] 

These new features of Unified Memory enable oversubscription of memory, which means that application running on a GPU can use data sets larger than ten their device memory.[1] While the Unified Memory model makes GPU programming more convenient, it comes at a cost; handling page faults and page migrations can be expensive. CUDA 8 addresses this issue with features like prefetch and memory advice.

---
### 1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory
Same author with **In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing**

---
### 3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions

UVM supports memory oversubscription, giving GPU programs the ability to use a larger amount of memory than the physical memory, without worrying about the problem of memory shortage. 

Advanced optimization techniques, mainly prefetching and memory usage hints [1], can be used to fine-tune the performance of UVM applications, mitigating the overheads caused by UVM.


![image](https://github.com/hitqshao/qishao-notes/assets/23403286/fd76fb3e-7747-424b-8235-cdefe81cbf23)


2）Prefetching and Hints
Prefetching and UVM hints are the major approaches provided by CUDA, with the hope that page faults and memory thrashing could be prevented by fine-tuning the behavior of UVM at runtime.

By calling cudaMemPrefetchAsync (PF), a memory block could be prefetched to GPU. UVM hints provide informed decisions on page handling by indicating the access patterns of data. 

Changing UVM hints is done by invoking cudaMemAdvise with one of the following policies：

• cudaMemAdviseSetAccessedBy (AB) implies that the device keeps a direct mapping in its page table. When the data is migrated, the mapping is re-established.
• cudaMemAdviseSetPreferredLocation (PL) pins the data and prevents the page to be migrated, which is useful when the page is mainly accessed on one side.
• cudaMemAdviseSetReadMostly (RM) indicates the data region is read-intensive. It creates a read-only copy of the page on the faulting side, allowing  on current access on both sides.

Only one policy (AB, PL, or RM) could be specified for each memory block, but each policy can be used along with prefetching.

Suggestions: To ensure performance under all oversubscription conditions, programmer needs to choose the UVM hints dynamically based on the application’s memory usage and available GPU memory. As a prerequisite, the size of the FALL pages needs to be estimated or measured by experiment. Before kernel launch, the program should first check the size of available GPU memory (e.g. via the cudaMemGetInfo API). If no oversubscription will happen, or the available memory is larger than the size of FALL pages, the programmer could set hints based on the conclusions provided by related researches [24]. Otherwise, based on our findings, applying the hint AB is a preferable choice.

---
### 4. Performance Evaluation of Advanced Features in CUDA Unified Memory

CUDA has introduced new features for optimizing the data migration on UM, i.e., memory advises and prefetch. Instead of solely relying on page faults, the memory advises feature allows the programmer to provide data access pattern for each memory object so that the runtime can optimize migration decisions. The prefetch proactively triggers asynchronous data migration to GPU before the data is accessed, which reduces page faults and, consequently, the overhead in handling page faults.

-Using memory advises improves application performance in oversubscription execution on the Intel platform and in-memory executions on the IBM platform.

-UM prefetch provides a significant performance improvement on the Intel-Volta/Pascal-PCI-E based systems while it does not show a performance improvement on the Power9-Volta-NVLink based system

UM was first introduced in CUDA 6.0 [21]. Only until the recent Nvidia Pascal microarchitecture that has hardware support for page faults.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/888e3ff6-18dc-4174-9ba3-998f5a30b651)


• cudaMemAdviseSetAccessedBy establishes a direct mapping of data to a specified device. Figure 2c illustrates an example of a physical page on GPU being remotely access from the host. When cudaMemAdviseSetPreferredLocation is applied, CUDA runtime tries to build a direct mapping to the page to avoid data migration so that the destination can access data remotely. Differently from cudaMemAdviseSetPreferredLocation, this cudaMemAdviseSetAccessedBy does not try to
pin pages on a specific device; instead, its main effect is to establish mapping on the remote device. This advice takes effect on the creation of the memory pages. The mapping will be re-established after the pages are migrated.

• cudaMemAdviseSetPreferredLocation sets the preferred physical location of pages. This advice pins a page and prevents it from migrating to other memories. Figure 2b illustrates a page preferred on the host side, and GPU uses remote mapping to access the page. This advice established a direct (remote) mapping to the memory page.  When accessing the page remotely, data is fetched through the remote memory instead of generating a page fault. If the underlying hardware does not support the remote mapping, the page will be migrated as in the standard UM.  cudaMemAdviseSetPreferredLocation is useful for applications with little data sharing between CPU and GPU, i.e., part of the application is executed completely on the GPU, and the rest of the application executes on the host. Data that is being used mostly by the GPU can be pinned to the GPU with the advice, avoiding memory thrashing. 

• cudaMemAdviseSetReadMostly implies a read-intensive data region. In the basic UM, accessing a page on a remote side triggers page migration. However, with cudaMemAdviseSetReadMostly, a read-only duplicate of the page will be created on the faulting side, which prevents page faults and data migration in the future. Figure 2a  illustrates an example, where the second access (step 5) has no page fault and is local access. This mechanism, however, results in a high  **overhead if there is any update to this memory region because all copies of the corresponding page will be invalidated to preserve consistency between different copies**. Thus, this advice is often used in read-only data structures, such as lookup tables and application parameters. 

**In general, we found both memory advises and prefetch to be simple and effective.**

---
### 5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory

Cons in traditional GPU:
Complicated asynchronous user-directed constructs to overlap data migration and kernel execution are used to address this issue. The second challenge is memory over-subscription. When the working set of the GPU kernel cannot fit in the device memory, the programmers have to painstakingly redefine the data structures and tile the data to transfer back and  forth in chunks.

This flow is inspired by -> 11. ***Towards High Performance Paged Memory for GPU***.

1 Scheduled threads generate global memory accesses. 

2 Each SM has its own load/store unit. Every load/store unit has its own TLB. Load/store unit performs a TLB look up to find whether the translation for the issued memory access is cached in TLB or not. A TLB miss is relayed to the GMMU. 

3 The GMMU walks through the page table looking for a PTE corresponding to the requested page with valid flag set. A far-fault occurs if there is no PTE for the requested page or the valid flag is not set. Then the far-fault is registered in the Far-fault Miss Status Handling Registers (MSHRs). 

4 The page is scheduled for transfer over CPU-GPU PCI-e interconnect. 

5 A 4KB page is allocated on demand and data is migrated from host to device memory. 

6 The MSHRs are consulted to notify the corresponding load/store unit and the memory access is replayed. A new PTE entry is added to the page table with valid

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/140f624a-afd0-4f09-bdf9-49551b0b6fe5)

This paper introduces ***random, sequential and tree-based Neighborhood prefetcher in detail***.

And come up with pre-eviction for tree-based Neighborhood, different from LRU eviction used in Nvidia.

<p align="center">
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/c59d9159-49ab-4d3f-aa1d-c75bafb322d5)
</p>
---
### 10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019

To enable better performance of UM, CUDA allows developers to give the UM driver additional advice on managing a given GPU memory range via an API function named cudaMemAdvise(const void *, size_t, enum cudaMemoryAdvise, int). The first two parameters of this function accept a pointer to a memory range with a specified size. The memory range should be allocated via cudaMallocManaged or declared via __managed__variables. The third parameter sets the advice for the memory range. The last parameter indicates the associated device’s id, which can indicate either a CPU or GPU device. The details and differences of these four kinds of advice are presented as follows:

• Default: This represents the default on-demand page migration to accessing processor, using the first-touch policy.

• cudaMemAdviseSetReadMostly: This advice is used for the data which is mostly going to be read from and only occasionally written to. The UM driver may create read-only copies of the data in a processor’s memory when that processor accesses it. If this region encounters any write requests, then only the write occurred page will be valid and other copies will be invalid.

• cudaMemAdviseSetPreferredLocation: Once a target device is specified, this device memory can be set as the preferred location for the allocated data. The host memory can also be specified as the preferred location. Setting the preferred location does not cause data to migrate to that location immediately. The policy only guides what will happen when a fault occurs on the specified memory region: if data is already in the preferred location, the faulting processor will try to directly  establish a mapping to the region without causing page migration. Otherwise, the data will be migrated to the processor accessing it if the data is not in the preferred location or if a direct mapping cannot be established.

• cudaMemAdviseSetAccessedBy: This advice implies that the data will be accessed by a specified CPU or GPU device. It has no impact on the data location and will not cause data migration. It only causes the data to be always mapped in the specified processor’s page tables, when applicable. The mapping will be accordingly updated if the data is migrated somehow. This advice is useful to indicate that avoiding faults is important for some data, especially when the data is accessed by a GPU within  a system containing multiple GPUs with peer-to-peer access enabled.

