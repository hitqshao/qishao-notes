---
title: Unified Memory Paper List
date: 2023-11-11 
permalink: /pages/44771e/
---

1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory
2. In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing
3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions
4. Performance Evaluation of Advanced Features in CUDA Unified Memory
5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory
6. Unified Memory: GPGPU-Sim/UVM Smart Integration
7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads
8. An Intelligent Framework for Oversubscription Management in CPU-GPU Unified Memory
9. Architectural Support for Address Translation on GPUs Designing Memory Management Units for CPU/GPUs with Unified Address Spaces
10. 

---

**Unified Memory History**
copied from ***Evolution of Nvidia GPU from microarchitectures Pascal to Ampere***

CUDA 4 introduced UVA (Unified Virtual Addressing) to provide a single virtual memory address space for both CPU and GPU memory and enable pointers to be accessed from GPU code no matter where in the system they reside. UVA enables Zero-Copy memory, a pinned CPU memory accessible by GPU code directly, over PCIe, without the need for memory copy. This provides some of the convince of Unified Memory, but at the cost of worse performance, because GPU always accesses it with PCIeâ€™s low bandwidth and high latency.[1]

Later, CUDA 6 introduced Unified Memory, which creates a pool of managed memory that programs running on the CPU and GPU can access without explicit data movement. However, only when CPU and GPU processes are not running together because of the limitation of the Kepler and Maxwell GPU microarchitecture. Also, the Unified Memory address space was limited to the size of the GPU memory.[1, 3] 

CUDA 8 and Pascal microarchitectures improve Unified Memory functionality by adding 49-bit virtual addressing and page faulting capability. The larger 49-bit virtual addresses are sufficient to enable GPUs to access the entire system memory plus the memory of all GPUs in the system. Because of the memory page faulting functionality, the CUDA system software does not need to synchronize all managed memory allocations to the GPU before each kernel lunch. Instead, when a thread running on GPU faults on non-resident memory access, it stalls until the page can be migrated and the page table updated. Alternatively, the page may be mapped for remote access over PCIe or NVLink interconnects.[1, 3, 6] 

These new features of Unified Memory enable oversubscription of memory, which means that application running on a GPU can use data sets larger than ten their device memory.[1] While the Unified Memory model makes GPU programming more convenient, it comes at a cost; handling page faults and page migrations can be expensive. CUDA 8 addresses this issue with features like prefetch and memory advice.

---
### 1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory
Same author with **In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing**
