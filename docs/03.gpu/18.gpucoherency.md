---
title: GPU Cache Coherency
date: 2024-08-21
permalink: /pages/45875/
---

1. [213] Cache Coherence for GPU Architectures

---

### 1. [213] Cache Coherence for GPU Architectures

#### Another Design in CPU LLC
Library Cache Coherence (LCC), that implements sequential consistency on CMPs by stalling writes to cache blocks until they have been self-invalidated by all sharers.

Library Cache Coherence (LCC) [34, 54] is a time-based hardware coherence proposal that stores timestamps in a directory structure and delays stores to unexpired blocks to enforce sequential consistency on CMPs.\
The TC-Strong implementation of the TC framework is similar to LCC as both enforce write atomicity by stalling writes at the shared last level cache.

![image](https://github.com/user-attachments/assets/3e7e13cc-7d64-4234-897b-03754489dd1f)

#### Background
![image](https://github.com/user-attachments/assets/2e62c5ed-574b-4cc2-9f85-8b269dc80358)

We propose TC-Weak and a novel time-based memory fence mechanism to eliminate all write-stalling, improve performance, and reduce interconnect traffic compared to TC-Strong.
-  We find that the stalling of writes in TC-Strong causes poor performance on a GPU.
-  We also show that unlike for CPU applications [34, 54], the fixed timestamp prediction proposed by LCC is not suited for GPU applications.

#### GPU Memory System
Both thread-private and global memory are stored in off-chip GDDR DRAM and cached in the multi-level cache hierarchy, however only **global memory requires coherence**.

Memory accesses to the same cache block from different threads within a wavefront are merged into a single wide access by the Coalescing Unit.\
A memory instruction generates one memory access for every unique cache line accessed by the wavefront.\
All requests are handled in FIFO order by the in-order memory stage of a GPU core.\
:bow: **Writes to the same word by multiple scalar threads in a single wavefront do not have a defined behaviour [46]; only one write will succeed**.

#### GPU Cache Hierarchy
The GPU cache hierarchy consists of per-core private L1 data caches and a shared L2 cache.\
The L1 caches are not coherent.\
They follow a write-evict [46] (write-purge [24]), write no-allocate caching policy. \
The L2 caches are writeback with write-allocate.

Memory accesses generated by the coalescing unit in each GPU core are passed, one per cycle, to the per-core MSHR table. \
The MSHR table combines read accesses to the same cache line from different wavefronts to ensure **only a single read access per-cache line per-GPU core** is outstanding.

**Writes are not combined and, since they write-through, any number of write requests to the same cache line from a GPU core may be outstanding.**

**Point-to-point ordering in the interconnection network, L2 cache controllers and off-chip DRAM channels ensures that multiple outstanding writes from the same wavefront to the same
address complete in program order.**

This is another situation different from :bow:. It is different code in a program that write to the same address.

#### Atomic Operation
Atomic Operation. Read-modify-write atomic operations are performed at each memory partition by an Atomic Operation Unit.

#### Consistency and Coherence

A cache coherence protocol performs the following three duties [3].
-  It propagates newly written values to all privately cached copies.
-  It informs the writing thread or processor when a write has been completed and is visible to all threads and processors.
-  Lastly, a coherence protocol may ensure write atomicity [3], i.e., a value from a write is logically seen by all threads at once.

Write atomicity is commonly enforced in write-invalidate coherence protocols by requiring that all other copies of a cache block are invalidated before a write is completed.

**Memory consistency models may [4, 19, 57, 59] or may not [2, 19, 53] require write atomicity.**

#### Directory Protocols

##### MESI
** four-state coherence protocol with writeback L1 and L2 caches**
The write-allocate policy at L1 requires that write data be buffered until proper coherence permission has been obtained.\
This requires the addition of area and complexity to buffer stores in each GPU core.

##### GPU-VI
**two-state coherence protocol**
GPU-VI implements write-through, no write-allocate L1 caches.\
It requires that any write completing at the L2 invalidate all L1 copies.\
A write to a shared cache line cannot complete until the L2 controller has sent invalidation requests and received acknowledgments from all sharers.




