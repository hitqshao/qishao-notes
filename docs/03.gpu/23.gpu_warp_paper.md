---
title: GPU WARP Mangement Papers
date: 2024-09-06
permalink: /pages/45880/
---

1. [530 MICRO] Improving GPU Perfromance via Large Warps and Two-Level Warp Scheduling
2. [219] Improving GPGPU Resource Utilization Through Alternative Thread Blocking Scheduling
3. [94] Exploiting Inter-Warp Heterogeneity to Improve GPGPU Performance

---

# 1. Improving GPU Perfromance via Large Warps and Two-Level Warp Scheduling

## Main Idea in Short
1. Solve issue of branch divergence by managing large wrap and create diverged sub-warp from large warp
2. Two-level warp scheduling. If all warps are scheduled together, they might get stuck by memory access request at the same time.\
   Thus they group 32 warps into 4 fetch groups. Group0 is prioritized first and then following warps.

## Framework
![image](https://github.com/user-attachments/assets/f926ad58-a0fe-4c56-b529-4292b5ccc0ce)
![image](https://github.com/user-attachments/assets/b0ccc14d-60e6-4cf8-9d55-a5f05693d351)

## MISC

### Branch Divergence
Maintainance of branch divergence is well illustrated in the paper.

![image](https://github.com/user-attachments/assets/6cf92e7b-a3db-464f-a151-864a35541b51)

- Since a warp can only have a single active PC at any given time, when branch divergence occurs, one path must be chosen first and the other is pushed on a divergence
stack associated with the warp so that it can be executed later.
- The divergence stack is also used to bring the warp back together once the divergent paths have been executed and all threads have reached a control flow merge (CFM)
point.
- A divergence stack entry consists of three fields: a re-convergence PC, an active mask, and an execute PC.

## Core Pipeline
![image](https://github.com/user-attachments/assets/065f1360-ea1f-46b8-b5e2-bdb9e24f8e09)

---

# 2. Improving GPGPU Resource Utilization Through Alternative Thread Blocking Scheduling
## Main Idea in Short
Interaction between thread block scheduler and wrap scheduler for different characteristics of workloads

- resource contention
- inter-CTA locality


## Introduction
Two level of schedulers within a GPGPU:
- a warp (or a wavefront) scheduler to determine which warp is executed
- a thread block or CTA scheduler to assign CTAs to cores

By default, the current CTA scheduler in hardware assigns the maximum number of CTAs to each core.\
The maximum number of CTAs depends on the resources used by each thread and the upper limit is determined the architecture (e.g., 8 CTAs in the Tesla architecture that we evaluate).

Assigning the maximum number of CTAs does not necessarily result in maximum performance as additional CTAs degrade performance by likely creating resource contention.

>  *Other's work*
>  Cache Conscious Wavefront Scheduling (CCWS) [29] proposes a warp scheduler that tracks L1 cache accesses to throttle the number of warps scheduled.
>  Dynamic CTA scheduling (DYNCTA) [16] attempts to allocate the optimal number of CTAs to each core based on the application characteristics.

## Two Different scheduling CTA policy for different workloads
For workloads where the maximum number of CTAs does not maximize performance, we leverage a greedy warp scheduler [29] to propose a lazy CTA scheduling (LCS) where the maximum number of CTAs allocated to each core is reduced to avoid resource contention and performance degradation.

In addition, to exploit inter-CTA locality, we propose block CTA scheduling (in conjunction with an appropriate block-aware warp scheduling) to improve performance and efficiency.

## Observation
- Type I : Increased Performance
- Type II : Increased Performance and Saturate

Contention of L1 and increasing L2 miss rate can leads to degrade performance
- Type III : Decreased Performance
- Type IV : Increase then Decrease

**Core Activity**
- *IDLE*\
  There are no available warps that can be issued. This can occur when there are not sufficient warps (and CTAs) assigned to the core.
- *MEM_STALL* \
  Most of the warps in the core are stalled waiting for data reply from memory while other warps have no valid instruction to issue.
- *CORE_STALL* \
  The core pipeline is stalled and no warp can be issued.\
  While some of the warps in the core might be stalled waiting for data from memory, other warps are stalled because of core/pipeline resource contention (e.g., lack of MSHR entries).

![image](https://github.com/user-attachments/assets/a39e1839-f055-49df-baf9-f0bbf1ae2a2c)

## Method
We analyzed the behavior of the CTA scheduler through instrumentation.\
In the source code of the workloads, we used the PTX register %smid to determine which SM each CTA was assigned to.
![image](https://github.com/user-attachments/assets/516c6ff8-0618-44c3-aa01-928af6a746ec)

## Framework
###
Lazy CTA scheduling (LCS) that reduces the maximum number of CTAs that can be assigned to each core to improve performance and energy efficiency.\
Block CTA scheduling (BCS) where sequential CTA blocks are assigned to the same core to improve inter-CTA cache locality and an appropriate warp scheduler that exploits such locality.
![image](https://github.com/user-attachments/assets/25f19366-20a8-4f5f-8f31-b78323cdf3a9)

### LCS
In comparison, LCS only requires a single measurement during the execution of the first thread block and based on the data collected, the number of thread blocks allocated to the core is adjusted.

![image](https://github.com/user-attachments/assets/ddc71e56-eb23-40d9-9357-06d24f48852a)

It is simple. \
During the monitor phase, the number of instructions issued (inst) for each thread block x is measured. The monitor phase continues until the first thread block finishes execution.

![image](https://github.com/user-attachments/assets/bf3786f0-f606-4cb6-aafc-b4c8a5677370)

In Figure7 (b), Tnew = floor(10/4) = 3

### BCS
we focus on a block of size 2 CTAs. BCS is not applicable to workloads with one-dimensional CTAs as there is little inter-CTA L1 locality.
![image](https://github.com/user-attachments/assets/7c813299-4f24-4cdb-a6d5-2162bfb0366a)

### Increasing Efficiency of GPGPUs: mixed Concurrent Kernel Execution (mCKE)
Similar to prior work [2], the unused resource (e.g., register file, shared memory) can be powergated to improve energy-efficiency with the reduced number of thread blocks allocated to each core with LCS.\
In addition, the underutilized resources within a core provide opportunity for concurrent execution of different kernels on the same core, which we refer to as mixed concurrent kernel execution (mCKE).

**The main goal of CKE is to efficiently utilize the GPU by overlapping kernel execution.**

In the baseline CKE, each core can be stalled at different point in time while waiting for the response from the memory and result in the core being idle for significant amount of time.\
However, by interleaving the kernels on the same core with mCKE, the memory latency can be hidden (or overlapped) with other kernel execution
and effectively improve overall performance.\
This is similar to the benefits of two-level warp scheduling [20] where the memory accesses from the warps within a thread block are not necessarily schedule together but partitioned into different fetch groups.

> [20] V. Narasiman et al. Improving GPU Performance via Large Warps and Two-Level Warp Scheduling. In International Symposium on Microarchitecture (MICRO), pages 308â€“317, Porto Alegre, Brazil, 2011.

---
# 3. Exploiting Inter-Warp Heterogeneity to Improve GPGPU Performance

## Main Idea in Short
For almost-all-miss warp, bypass their request to reduce memory & cache contention.

## Observation
three new observations:
- GPGPU warps exhibit heterogeneous memory divergence behavior at the shared cache: some warps have most of their requests hit in the cache (high cache utility), while other warps see most of their request miss (low cache utility).
- a warp retains the same divergence behavior for long periods of execution
- due to high memory level parallelism, requests going to the shared cache can incur queuing delays as large as hundreds of cycles,  exacerbating the effects of memory divergence.

## Components
- a cache bypassing mechanism that exploits the latency tolerance of low cache utility warps to both alleviate queuing delay and
increase the hit rate for high cache utility warps
- a cache insertion policy that prevents data from high cache utility warps from being prematurely evicted
- a memory controller that prioritizes the few requests received from high cache utility warps to minimize stall time.

![image](https://github.com/user-attachments/assets/8ee29c66-5a15-40eb-90a8-15552730bd4d)

![image](https://github.com/user-attachments/assets/dbcc3d98-01c9-4ac4-a1ab-89ca9095bba7)











