---
title: GPU WARP Mangement Papers
date: 2024-09-06
permalink: /pages/45880/
---

1. [530 MICRO] Improving GPU Perfromance via Large Warps and Two-Level Warp Scheduling
2. [219] Improving GPGPU Resource Utilization Through Alternative Thread Blocking Scheduling

---

# 1. Improving GPU Perfromance via Large Warps and Two-Level Warp Scheduling

## Main Idea in Short
1. Solve issue of branch divergence by managing large wrap and create diverged sub-warp from large warp
2. Two-level warp scheduling. If all warps are scheduled together, they might get stuck by memory access request at the same time.\
   Thus they group 32 warps into 4 fetch groups. Group0 is prioritized first and then following warps.

## Framework
![image](https://github.com/user-attachments/assets/f926ad58-a0fe-4c56-b529-4292b5ccc0ce)
![image](https://github.com/user-attachments/assets/b0ccc14d-60e6-4cf8-9d55-a5f05693d351)

## MISC

### Branch Divergence
Maintainance of branch divergence is well illustrated in the paper.

![image](https://github.com/user-attachments/assets/6cf92e7b-a3db-464f-a151-864a35541b51)

- Since a warp can only have a single active PC at any given time, when branch divergence occurs, one path must be chosen first and the other is pushed on a divergence
stack associated with the warp so that it can be executed later.
- The divergence stack is also used to bring the warp back together once the divergent paths have been executed and all threads have reached a control flow merge (CFM)
point.
- A divergence stack entry consists of three fields: a re-convergence PC, an active mask, and an execute PC.

## Core Pipeline
![image](https://github.com/user-attachments/assets/065f1360-ea1f-46b8-b5e2-bdb9e24f8e09)

---

# 2. Improving GPGPU Resource Utilization Through Alternative Thread Blocking Scheduling
## Main Idea in Short
Interaction between thread block scheduler and wrap scheduler for different characteristics of workloads

- resource contention
- inter-CTA locality


## Introduction
Two level of schedulers within a GPGPU:
- a warp (or a wavefront) scheduler to determine which warp is executed
- a thread block or CTA scheduler to assign CTAs to cores

By default, the current CTA scheduler in hardware assigns the maximum number of CTAs to each core.\
The maximum number of CTAs depends on the resources used by each thread and the upper limit is determined the architecture (e.g., 8 CTAs in the Tesla architecture that we evaluate).

Assigning the maximum number of CTAs does not necessarily result in maximum performance as additional CTAs degrade performance by likely creating resource contention.

>  *Other's work*
>  Cache Conscious Wavefront Scheduling (CCWS) [29] proposes a warp scheduler that tracks L1 cache accesses to throttle the number of warps scheduled.
>  Dynamic CTA scheduling (DYNCTA) [16] attempts to allocate the optimal number of CTAs to each core based on the application characteristics.

## Two Different scheduling CTA policy for different workloads
For workloads where the maximum number of CTAs does not maximize performance, we leverage a greedy warp scheduler [29] to propose a lazy CTA scheduling (LCS) where the maximum number of CTAs allocated to each core is reduced to avoid resource contention and performance degradation.

In addition, to exploit inter-CTA locality, we propose block CTA scheduling (in conjunction with an appropriate block-aware warp scheduling) to improve performance and efficiency.

## Method
We analyzed the behavior of the CTA scheduler through instrumentation.\
In the source code of the workloads, we used the PTX register %smid to determine which SM each CTA was assigned to.
![image](https://github.com/user-attachments/assets/516c6ff8-0618-44c3-aa01-928af6a746ec)



