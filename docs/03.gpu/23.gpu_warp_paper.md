---
title: GPU WARP Mangement Papers
date: 2024-09-06
permalink: /pages/45880/
---

1. [530 MICRO] Improving GPU Perfromance via Large Warps and Two-Level Warp Scheduling

---

# Improving GPU Perfromance via Large Warps and Two-Level Warp Scheduling

## Main Idea in Short
1. Solve issue of branch divergence by managing large wrap and create diverged sub-warp from large warp
2. Two-level warp scheduling. If all warps are scheduled together, they might get stuck by memory access request at the same time.\
   Thus they group 32 warps into 4 fetch groups. Group0 is prioritized first and then following warps.

## Framework
![image](https://github.com/user-attachments/assets/f926ad58-a0fe-4c56-b529-4292b5ccc0ce)
![image](https://github.com/user-attachments/assets/b0ccc14d-60e6-4cf8-9d55-a5f05693d351)

## MISC

### Branch Divergence
Maintainance of branch divergence is well illustrated in the paper.

![image](https://github.com/user-attachments/assets/6cf92e7b-a3db-464f-a151-864a35541b51)

- Since a warp can only have a single active PC at any given time, when branch divergence occurs, one path must be chosen first and the other is pushed on a divergence
stack associated with the warp so that it can be executed later.
- The divergence stack is also used to bring the warp back together once the divergent paths have been executed and all threads have reached a control flow merge (CFM)
point.
- A divergence stack entry consists of three fields: a re-convergence PC, an active mask, and an execute PC.

## Core Pipeline
![image](https://github.com/user-attachments/assets/065f1360-ea1f-46b8-b5e2-bdb9e24f8e09)


