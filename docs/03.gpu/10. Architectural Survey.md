---
title: Architectural Survey
date: 2024-03-30
permalink: /pages/458722/
---

1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity

---
# 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity

Four major improvement
* mitigating the impact of control flow divergence
* alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main
memory
* increasing the available parallelism and concurrency
* improving pipeline execution and exploiting scalarization opportunities.


![image](https://github.com/hitqshao/qishao-notes/assets/23403286/d9c6b47b-d469-4154-9dc5-b0b0f4168d70)


## **Control flow divergence**

1. First, GPUs employ PDOM stack-based mechanism that serializes the execution of divergent paths. This serialization of divergent paths reduces the available thread level parallelism
(i.e., the number of active warps at a time) which limits the ability of GPUs to hide long memory instruction latency.
2. Control divergence limits the number of active threads in the running warps. As a result, SIMD execution units are not efficiently utilized when a diverged warp is executed.
3. Control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. Memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.
4. Irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per CTAs) to some GPU cores are larger than others.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/f9f7621e-ba28-4261-84bf-73b7ba00d9c4)


### 1. **Regrouping Divergent warps**<br>
Instead, DWF dynamically re-forms divergent warps into new non-divergent warps on the fly.<br>
Moreover, DWF does not reconverge diverged warp at IPDOM in order to amortize coalesced memory address of converged warps.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/c19066f6-bbe5-4589-9ae5-58be982e465b)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/bae42a54-ea88-477b-8103-61fd313f03c1)

### 2.  **Large Warp/CTA compaction**
  - Thread Block Compaction (TBC)<br>
   Allows a group of warps, that belong to the same thread block, to share the same PDOM stack.<br>
   **However, TBC stalls all warps within a CTA on any potentially divergent branch until all warps reach the branch point.**
    
    ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/0f41fa75-67e0-40e3-9f3b-fe1e55ca9d3e)

  The major difference between 1) and TBC is that 1) can only merge threads in a warp when they are ready in a queue. Thus it miss some potentials.
  
  TBC replace per-warp convergence stack with in-threadblock stack.

  - CAPRI<br>
    CAPRI dynamically identifies the compaction effectiveness of a branch and only stalls threads that are predicted to benefit from compaction.<br>
    
  - SLP
    proposed SIMD lane permutation (SLP) as an optimization to expand the applicability of compaction in case of conventional compaction technique is ineffective.
    
    ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/5c3a5f0d-40c5-4b17-9d73-e73d064fa170)

### 3. **Multi-path execution**<br>
   - DPS<br>
     Dual-path Stack<br>
   - Multi-path Execution<br>
   
### 4. **MIMD-like architecture**<br>
Rogers et al. [194] observed that regular applications perform better with a wider warp size, whereas divergent applications achieve better performance with a  smaller warp size. <br>
VWS groups sets of these smaller warps together by ganging their execution in the warp scheduler and thus amortizing the energy consumed by fetch, decode, and warp scheduling across more threads.

### 5. **Dynamic kernels/threads** <br>

Related Paper:
 **Characterization and Analysis of Dynamic Parallelism in Unstructured GPU Applications.** [108] <br>
 **Dynamic Thread Block Launch: A Lightweight Execution Mechanism to Support Irregular Applications on GPUs**. [85] <br>
By wang jing NVIDIA

:+1: :+1: :+1:
**These two paper has very thorough explanation of how kernels are launched, kernel parameters are gained and how thread create subkernel.**

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/6e30803e-5aaf-4bad-b626-f301c3487c1f)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/eba31c47-1954-4251-bd72-ab05c1f9ce64)

**CUDA enables dynamic parallsim, creating subkernels from each thread.**

----
**Copied from "Characterization"**

When a child kernel is launched, the parameter buffer pointer of the kernel is retrieved through the device runtime API cudaGetParameterBuffer.
Then the argument values are stored in the parameter buffer and the kernel is launched by calling cudaLaunchDevice. 
After that, the device runtime manager appends the child kernels to an execution queue and dispatches the kernel to SMXs according to a certain scheduling policy.
The CDP kernel launching overhead comprises of kernel parameter parsing, calling cudaGetParameterBuffer and cudaLaunchDevice, as well as the process that device runtime manager setups,enqueues, manages and dispatches the child kernels.

---

however, the huge kernel launching overhead could negate the performance benefit of DFP. The overhead is due to the large number of launched kernels, the associated memory footprint and the low number of running warps per core.The CPU launches GPU kernels by dispatching kernel launching commands. Kernel parameters are passed from CPU to 
GPU at the kernel launching time and stored in the GPU global

Wang et al. [236] proposed new mechanism, called Dynamic Thread Block Launch (DTBL), that employs light-weight thread block rather than heavy-weight device kernel for DFP.

---

## **Efficient utilization of memory bandwidth**

### **Alleviating cache thrashing, and resource contention**

#### 1. **Two-level warp scheduling**
   - TLRR<br>
     ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/586e0677-70fd-4623-b4ee-beaf001ad3dc)

   They proposed two-level round-robin warp scheduling (TL-RR), in which the warps are split into fetch groups.<br>
   TL-RR executes only one fetch group at a time and it schedules warps from the same fetch group in a round-robin fashion.<br>
   When the running warps reach a long latency operation, then the next fetch group is prioritized.<br>
   They try to alleviate the issue of threads in all warps **arrive the same memory latency instruction at the same time**.<br>

  ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/6851655b-1a31-45ec-bc79-90cf4b97435d)

  ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/2ff9dbbb-d756-42e1-b821-bc698417a9a0)

   - OWL
  OWL augments the TL-RR with CTA-awareness, such that warps are split into groups of CTAs basis rather than warps basis, resulting in increased intra-CTA locality. <br>
  OWL gives a group of CTAs higher priority when their data exist at the L1 cache such that they get the opportunity to reuse it, therefore improving L1 hit rates and alleviating cache contention.<br>

#### 2. **Coarse-grained CTA throttling**
  **DYNCTA** <br>
  - Always executing the maximum possible number of CTAs on a GPU core (i.e., increasing TLP to the maximum) does not always lead to better performance.
  - To alleviate resource contention, they proposed dynamic CTA scheduling mechanism (DYNCTA), which aims to allocate the optimal number of CTAs per GPU core that alleviate memory contention according to an application characteristics.
  - DYNCTA dynamically adjusts over sampling periods the number of active CTAs per GPU core that reduces the memory latency without sacrificing the available TLP.

  **LCS** <br>
    In contrast to DYNCTA that monitors the workload behavior for the entire kernel execution, LCS leverages GTO scheduler to find the optimal number of thread blocks at the early beginning of kernel execution.

#### 3. **Fine-grained warp throttling**  
   due to the massive multithreading and the limited capacity of L1 cache, divergent GPGPU applications cause severe cache contention.<br>
   - **CCWS** <br>
   uses a victim tag array, called lost locality detector, to detect warps that have lost locality due to thrashing. These warps are prioritized till they exploit their locality while other warps are descheduled.<br>
   - **DAWS** <br>
   DAWS is a divergence-based cache footprint predictor to calculate the amount of locality in loops required by each warp. <br>
   DAWS uses these predictions to prioritize a group of warps such that the cache footprint of these warps do not exceed the capacity of the L1 cache. <br>
   
#### 4. **Throttling and cache bypassing**  
  previous CTA or warp throttling techniques leave memory bandwidth and other chip resources (L2 cache, interconnection and execution units) significantly underutilized.
  - **PCAL** <br>
  At the beginning of kernel execution, PCAL executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources.
  Thus, PCAL reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.
  - **CCA** <br>
  CCA improves DAWS by allowing extra inactive warps and some streaming memory instructions from the active warps to bypass the L1 cache and utilize on-chip resources.

#### 5. **Critical warp awareness**
some warps may be assigned more workload and exhibit longer latency compared to other warps within the same Thread Block.
Hence, fast warps are idle at a synchronization barrier or at the end of kernel execution until the critical (i.e., the slowest) warp finishes execution.
Thus, the overall execution time is dominated by the performance of these critical warps.

**CAWA** dynamically identifies critical warps and coordinates warp scheduling and cache prioritization to accelerate the critical warp execution.

  - **Workload Imbalance** In a GPGPU kernel function, tasks are not always uniformly distributed to each thread/warp, and thereby some threads/warps have heavier workloads than others. Intuitively, the threads/warps with heavier workloads require longer time to process their tasks. Consequently, warps with heavier workloads often become the slowestrunning/critical warps.
  - **Diverging Branch Behavior**   At runtime, warps can undergo different control paths leading to different number of dynamic instructions across different warps. This problem could be worsened if threads in a warp also take diverging control paths, i.e., the branch divergence problem, leading to a larger instruction execution gap between warps.
  - **Contention in the Memory Subsystem**  Jog et al. observed that the memory subsystem has a significant impact on GPGPU applications [15, 16]. . Jia et al. also pointed out that interference in the Ll data cache as well as in the interconnection between the Ll data caches and the L2 cache are the major factors that limit GPU performance.More than 60% of the cache blocks that could be reused by the slower-running, critical warps are evicted before the re-references by the critical warps.
  - **Latency Introduced by the Warp Scheduler** Because of the particular warp execution order determined by the scheduler, when a warp becomes ready for execution, it can experience up to N cycles of scheduling delay, where N represents the number of warps. 

#### 6. **Cache management and bypassing**

**GCache**
To detect thrashing, they equip L2 cache tag array with extra bits (victim bits) to provide L1 cache with some information about the hot lines that have been evicted before. An adaptive cache replacement policy is used by L1 cache to protect these hot lines.

#### 7. **Ordering buffers**
The idea of MRPB is two-fold.<br>
First, a FIFO requests buffer is used to reorder memory references so that requests from the same warp are grouped and sent to the cache together in a more cache-friendly order. This results in drastically reducing cache contention and improving use of the limited per-thread cache capacity.<br>
Second, MRPB allows memory request that encounters associativity stall to bypass L1 cache.<br>

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/0db0f277-f82f-4dcf-b319-8646080ad6c5)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/8321f4fa-56cd-4046-b2b3-d61b96bdba7c)


#### 8. **Resource tuning**

Equalizer, a dynamic runtime system that tunes number of thread blocks, core and memory frequency to match the requirements of the running kernel, leading to efficient execution and energy saving.


### 2. **High-bandwidth many-thread-aware memory hierarchy**

#### 1. **Mitigating off-chip bandwidth bottleneck**

**LAMAR**
Emerging irregular workloads benefit from fine-grain (FG) memory access by avoiding unnecessary data transfers, that may be happened under CG policy,

they proposed a locality-aware memory hierarchy (LAMAR) that adaptively tunes the memory access granularity for the running kernel.

LAMAR employs CG accesses for kernels with high temporal and spatial locality, while applying FG accesses for irregular divergent workloads in attempt to reduce
memory over-fetching.

**CABA**
Vijaykumar et al. [231] proposed, Core-Assisted Bottleneck Acceleration (CABA) framework, that exploits the underutilized computational resources to perform useful work and alleviate different bottlenecks in GPU execution.

For instance, to alleviate memory bandwidth bottleneck, CABA dynamically creates assist warps that execute with the original warps side by side on the same GPU
core.

Assist warps opportunistically use idle computational units to perform data decompression for the incoming compressed cache blocks and compression for the outgoing cache blocks, leading to less transferring data from memory and mitigating memory bandwidth problem.

**Approximation**
An approximation technique in which the GPU drops some portion of load requests which miss in the cache after approximating their values.


#### 2. **Memory divergence normalization**

**Orchestrated Scheduling and Prefetching for GPGPUs**

they proposed prefetch-aware warp scheduling, that coordinates simple data prefetcher and warp scheduling in an intelligent manner such that the scheduling of
two consecutive warps are separated in time, and thus prefetching becomes more effective.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/933a4834-0b89-4496-bf94-6f76f1a003ba)

#### 3. **Interconnection network**
#### 4. **Main memory scheduling**

interconnection network which is between cores and memory controllers can destroy memory access row-buffer locality.

To reserve row locality and reduce complexity circuit design of FR-FCFS DRAM controller, they employ an interconnection arbitration scheme to prioritize memory requests accessing the same row first.

#### 5. **Heterogeneous memory management**

Agarwal et al. [5] showed that applying traditional Linux page placements policies, which have been used for CPUonly NUMA systems and aim to minimize the memory request latency, may not be effective in CPU–GPU NUMA systems. This is due to the fact that GPU performance is more sensitive to memory bandwidth.

**Bandwidth-aware placement** that maximizes GPU performance by balancing page placement across the memories based on the aggregate memory bandwidth available in a system.

#### 6. **CPU–GPU memory transfer overhead**

fine-grained CPU–GPU synchronization enabled by a hardware-managed full-empty bits to track when regions of data have been transferred.

Thus, the GPU is able to start execution once the required block of data is available.

Software level APIs are proposed to allow programmer to launch kernel earlier and overlap data transfer with execution.

---

### **Increasing parallelism and improving execution pipelining** 

Some applications have a low number of active thread blocks due to the small input size or the unavailability of some required resources in SM (e.g. registers or shared memory), thus they fail to efficiently utilize the execution units. This results in inefficient utilization of execution unit and hinders the GPU ability to hide long memory latency.<br>

Previous works proposed new techniques in order to reduce resource fragmentation and run the maximum number of warps per core.<br>

Further, other approaches proposed running multiple applications on the same GPU to exploit these underutilized resources and increase overall throughput. <br>

Another way to improve execution efficiency and increase parallelism is to exploit scalar opportunities and value similarity between the running warps such that scalar instructions can be executed concurrently along with other SIMT instructions.<br>


#### 1. **Reducing resource fragmentation and increasing parallelism**
:+1: :+1: :+1:

**Unifying**they proposed a unified local memory which integrates the register file, L1 cache, and scratchpad memory into one large on-chip storage. Then, the hardware can dynamically partition the on-chip storage according to each application’s needs. 

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/4d79262c-410b-4601-8d2a-55f983a2e924)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/f372f9b7-ed97-4d20-a439-b7e067e2c08a)


#### 2. **GPU multitasking**

Better Utilization and Virtualization<br>
- multiple applications execute simultaneously on different cores within the same GPU substrate.<br>
- mixed concurrent kernels execution, in which two applications execute concurrently on the same core. especially, mixture of memory-intensive and compute-intensive workloads<br>

Default strategy may cause high-priority application suffering from a long latency to execute. a task preemption strategy is required to improve GPU ultitasking
- context switching and draining
- To further reduce preemption latency, Park et al. [178] introduced core flushing which drops an execution of a thread block
without context saving and re-executes the dropped thread block from the beginning when it is relaunched.


#### 3. **Exploiting scalar and value similarity opportunities**

many GPGPU workloads have scalar instructions in which computation is identical across multiple threads within the same warp instruction (i.e., operands
are identical for all the threads in a warp). 

modern GPU microarchitecture, like AMD’s GCN [10], leverages these scalar opportunities by statically detecting scalar instructions and executing
them on a separate scalar unit attached with each GPU core.

A vector is defined as an affine, when the vector contains a consecutive strided values, i.e., the vector values can be represented as V(i) = b + i ∗ s, where
b is the base, s is the stride and i is the thread index.

:+1: :+1: :+1: **Microarchitectural mechanisms to exploit value structure in SIMT architectures**

This Paper has detailed explanation of microarchitecture in gpu execution core, how ALU and register files operates.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/d36be8c7-598f-4f6f-a76c-9141e18a11a8)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/76fe7226-5aa1-480a-bad2-1f9e25550134)


#### 4. **Improving execution pipelining**
many GPGPU applications do not have enough active threads that are ready to issue instructions and hide short read-after-write (RAW) dependencies caused by deep execution pipeline stages.

-  a low-power forwarded network that can considerably improve the performance of many compute-intensive GPGPU applications.
-  improve GPU performance by splitting the existing 32-bit datapath into two 16-bit datapath slices. As a result, the GPU instruction throughput can be increased by issuing dual 16-bit instructions from two different warps in parallel using the sliced 32-bit datapath.
-  a pre-execution approach for improving GPU latency hiding and performance by employing run-ahead outof-order execution [158].
when a warp stalls for a long-latency operation such as off-chip memory accesses, it continues to fetch and pre-execute successive instructions that are not on the long latency dependence chain resulting in hiding processing delay of operations and performance improvement.



### **Enhancing GPGPU programmability** 

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/46f249d3-a92d-48c6-ad4b-4e5e2cedc787)

#### 1. **Coherence and consistency model**
Current GPUs lack hardware cache coherence and require disabling of private L1 caches or employing software-based bulk coherence decisions (i.e., flush/invalidate all private L1 caches at synchronization points) if an application needs coherent memory view.

#### 2. **Transactional memory**

KILO TM does not rely on cache coherence nor global atomic operations.
 
Instead, it detects conflicts via a fine-grain value-based approach that supports thousands of concurrent transactions and requires negligible storage overhead.

#### 3. **Deterministic GPU**
#### 4. **Memory management**

Kim et al. [109] proposed GPUdmm, a high-performance dynamic memory management for GPU architecture. 
GPUdmm enables dynamic memory management for discrete GPU environments by using GPU memory as a cache of CPU memory with ondemand CPU–GPU data transfers.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/aced8bea-9237-43f8-8fbb-2238f1d401b7)

Pichai et al. [183] :+1: :+1: :+1: augmenting CCWS and TBC with TLB-awareness and a few simple adjustments can recover most of this lost performance and move address  translation overheads into a range considered acceptable in the CPU world.

### **CPU–GPU heterogeneous architecture** 

#### 1. **Impacts of CPU–GPU integration**
remaining CPU code tends to have lower instruction-level parallelism (ILP), more complex load/store operations to prefetch and more difficult branch rediction. <br>

Further, the serial code will not benefit significantly from SIMD instructions or increasing the number of CPU cores, owing to the limited availability of threadlevel parallelism (TLP) and data-level parallelism (DLP) that will be already captured and exploited by the GPU instead.

#### 2. **CPU–GPU programmability**

Heterogeneous System Coherence for Integrated CPU-GPU Systems

they replace the fine-grained 64B-block-level directory with a coarse-grained 1KB-region-level directory.


#### 3. **Exploiting heterogeneity**

COMPASS uses idle GPU core resources to act as data prefetchers for CPU execution and successfully improve the memory performance of single-thread  applications. <br>
Woo and Lee [247] proposed to collaboratively utilize CPU resources to act as programmable data prefetchers for GPGPU applications.

#### 4. **Shared resources management**



Two kinds of approaches have been explored to mitigate interference: 
- application-aware resource management
- throttlingbased management.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/dcb11cfd-9c69-439f-975d-e7c54fd954de)

SMS decouples memory controller into three stages.<br>
  - The first stage of SMS groups requests based on rowbuffer locality.
  - At the second stage, SMS ensures fairness between CPU and GPU memory requests by applying CPU-biased shortest job first scheduling policy or GPU-biased round robin scheduling policy. A dynamically configurable parameter is used to select between the two policies based on the system’s needs.
  - The last stage consists of simple per-bank FIFO queue to issue low-level memory commands.


**TAP: A TLP-Aware Cache Management Policy for a CPU-GPU Heterogeneous**
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/908d6507-51ce-4f35-8b88-095ae5eb78e5)

  - A core-sampling technique, which applies a different cache management policy to each GPU core and regularly collects statistics on the performance  
of these cores to see how these polices affect GPU applications.<br>
  - GPU cores typically access caches much more frequently than CPU cores.<br>
  - enforces a similar cache lifetime to both CPU and GPGPU applications and prevent GPGPU application to monopolize the shared cache.


One (CM-CPU) for boosting CPU performance in the presence of GPU interference.<br>
The other (CM-BAL) for improving both CPU and GPU performance in a balanced manner and thus overall system performance.<br>

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/ebaea4d9-549d-40bf-813d-31f0d3d8d343)

 propose GPU concurrency management that dynamically throttles/boosts TLP (i.e., number of active warps) of GPU cores in order to minimize shared resources interference between CPU and GPU.





















