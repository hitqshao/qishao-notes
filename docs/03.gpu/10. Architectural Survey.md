---
title: Architectural Survey
date: 2024-03-30
permalink: /pages/458722/
---

1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity

---
### 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity

Four major improvement
* mitigating the impact of control flow divergence
* alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main
memory
* increasing the available parallelism and concurrency
* improving pipeline execution and exploiting scalarization opportunities.


![image](https://github.com/hitqshao/qishao-notes/assets/23403286/d9c6b47b-d469-4154-9dc5-b0b0f4168d70)



**Control flow divergence**

1. First, GPUs employ PDOM stack-based mechanism that serializes the execution of divergent paths. This serialization of divergent paths reduces the available thread level parallelism
(i.e., the number of active warps at a time) which limits the ability of GPUs to hide long memory instruction latency.
2. Control divergence limits the number of active threads in the running warps. As a result, SIMD execution units are not efficiently utilized when a diverged warp is executed.
3. Control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. Memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.
4. Irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per CTAs) to some GPU cores are larger than others.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/f9f7621e-ba28-4261-84bf-73b7ba00d9c4)


1. **Regrouping Divergent warps**<br>
Instead, DWF dynamically re-forms divergent warps into new non-divergent warps on the fly.<br>
Moreover, DWF does not reconverge diverged warp at IPDOM in order to amortize coalesced memory address of converged warps.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/c19066f6-bbe5-4589-9ae5-58be982e465b)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/bae42a54-ea88-477b-8103-61fd313f03c1)

2.  **Large Warp/CTA compaction**
  - Thread Block Compaction (TBC)<br>
   Allows a group of warps, that belong to the same thread block, to share the same PDOM stack.<br>
   **However, TBC stalls all warps within a CTA on any potentially divergent branch until all warps reach the branch point.**
    
    ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/0f41fa75-67e0-40e3-9f3b-fe1e55ca9d3e)

  The major difference between 1) and TBC is that 1) can only merge threads in a warp when they are ready in a queue. Thus it miss some potentials.
  
  TBC replace per-warp convergence stack with in-threadblock stack.

  - CAPRI<br>
    CAPRI dynamically identifies the compaction effectiveness of a branch and only stalls threads that are predicted to benefit from compaction.<br>
    
  - SLP
    proposed SIMD lane permutation (SLP) as an optimization to expand the applicability of compaction in case of conventional compaction technique is ineffective.
    
    ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/5c3a5f0d-40c5-4b17-9d73-e73d064fa170)

3. **Multi-path execution**<br>
   - DPS<br>
     Dual-path Stack<br>
   - Multi-path Execution<br>
   
4. **MIMD-like architecture**<br>
Rogers et al. [194] observed that regular applications perform better with a wider warp size, whereas divergent applications achieve better performance with a  smaller warp size. <br>
VWS groups sets of these smaller warps together by ganging their execution in the warp scheduler and thus amortizing the energy consumed by fetch, decode, and warp scheduling across more threads.

5. **Dynamic kernels/threads** <br>

Related Paper:
 **Characterization and Analysis of Dynamic Parallelism in Unstructured GPU Applications.** [108] <br>
 **Dynamic Thread Block Launch: A Lightweight Execution Mechanism to Support Irregular Applications on GPUs**. [85] <br>
By wang jing NVIDIA

:+1: :+1: :+1:
**These two paper has very thorough explanation of how kernels are launched, kernel parameters are gained and how thread create subkernel.**

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/6e30803e-5aaf-4bad-b626-f301c3487c1f)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/eba31c47-1954-4251-bd72-ab05c1f9ce64)

CUDA enables dynamic parallsim, creating subkernels from each thread.
----
**Copied from "Characterization"**

When a child kernel is launched, the parameter buffer pointer of the kernel is retrieved through the device runtime API cudaGetParameterBuffer.
Then the argument values are stored in the parameter buffer and the kernel is launched by calling cudaLaunchDevice. 
After that, the device runtime manager appends the child kernels to an execution queue and dispatches the kernel to SMXs according to a certain scheduling policy.
The CDP kernel launching overhead comprises of kernel parameter parsing, calling cudaGetParameterBuffer and cudaLaunchDevice, as well as the process that device runtime manager setups,enqueues, manages and dispatches the child kernels.

---

however, the huge kernel launching overhead could negate the performance benefit of DFP. The overhead is due to the large number of launched kernels, the associated memory footprint and the low number of running warps per core.The CPU launches GPU kernels by dispatching kernel launching commands. Kernel parameters are passed from CPU to 
GPU at the kernel launching time and stored in the GPU global

Wang et al. [236] proposed new mechanism, called Dynamic Thread Block Launch (DTBL), that employs light-weight thread block rather than heavy-weight device kernel for DFP.

**Efficient utilization of memory bandwidth**

1. **Two-level warp scheduling**
   - RR<br>
     ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/586e0677-70fd-4623-b4ee-beaf001ad3dc)

   - OWL
  
2. **Coarse-grained CTA throttling**
3. **Fine-grained warp throttling**




