---
title: GPU Simulator
date: 2024-03-30
permalink: /pages/458722/
---

1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity

---
### 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity

Four major improvement
* mitigating the impact of control flow divergence
* alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main
memory
* increasing the available parallelism and concurrency
* improving pipeline execution and exploiting scalarization opportunities.


![image](https://github.com/hitqshao/qishao-notes/assets/23403286/d9c6b47b-d469-4154-9dc5-b0b0f4168d70)



**Control flow divergence**

1. First, GPUs employ PDOM stack-based mechanism that serializes the execution of divergent paths. This serialization of divergent paths reduces the available thread level parallelism
(i.e., the number of active warps at a time) which limits the ability of GPUs to hide long memory instruction latency.
2. Control divergence limits the number of active threads in the running warps. As a result, SIMD execution units are not efficiently utilized when a diverged warp is executed.
3. Control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. Memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.
4. Irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per CTAs) to some GPU cores are larger than others.

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/f9f7621e-ba28-4261-84bf-73b7ba00d9c4)


1. **Regrouping Divergent warps**<br>
Instead, DWF dynamically re-forms divergent warps into new non-divergent warps on the fly.<br>
Moreover, DWF does not reconverge diverged warp at IPDOM in order to amortize coalesced memory address of converged warps.
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/bae42a54-ea88-477b-8103-61fd313f03c1)

2.  **Large Warp/CTA compaction**
  - Thread Block Compaction (TBC)
   Allows a group of warps, that belong to the same thread block, to share the same PDOM stack.<br>
   However, TBC stalls all warps within a CTA on any potentially divergent branch until all warps reach the branch point.
    ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/0f41fa75-67e0-40e3-9f3b-fe1e55ca9d3e)

  - C
