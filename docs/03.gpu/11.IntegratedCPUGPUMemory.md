---
title: Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper
date: 2024-08-10
permalink: /pages/458724/
---

## Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper
:+1: :+1: :+1:


### Introduction 

#### limitation of state-of-art
1. UVM large overheads in handling page faults in GPU and suffers from read/write amplification due to page-level swapping.
2. Data object offloading requires offline profiling and application refactoring, limiting solution generality.
3. The performance of both solutions is constrained by data transfer bottlenecks between the CPU and GPU.

#### Grace Hopper Superchip
* NVLink-C2C (chip-to-chip)  cache-coherent interconnect
* a single virtual memory space is shared between the CPU and GPU (i.e., system memory)
* address translation is accelerated by hardware.
   1.  direct remote accesses at cacheline granularity
   2.  heuristic-guided page migrations.

By leveraging cacheline level access and Address Translation Service (ATS), which enables full access to all CPU and GPU memory allocations, the system memory eliminates the page-fault handling
overhead needed in managed memory in UVM, and minimizes the need for memory migrations.

While managed memory splits the virtual memory space into both the system page table and GPU page table, system memory relies on a single system-wide page table, shared between the CPU and the GPU.

### Grace Hooper Unified Memory System
* system-allocated memory
* CUDA managed memory
#### Memory Subsystem
