---
title: Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper
date: 2024-08-10
permalink: /pages/458724/
---

## Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper
:+1: :+1: :+1:


### Introduction 

#### limitation of state-of-art
1. UVM large overheads in handling page faults in GPU and suffers from read/write amplification due to page-level swapping.
2. Data object offloading requires offline profiling and application refactoring, limiting solution generality.
3. The performance of both solutions is constrained by data transfer bottlenecks between the CPU and GPU.

#### Grace Hopper Superchip
* NVLink-C2C (chip-to-chip)  cache-coherent interconnect
* a single virtual memory space is shared between the CPU and GPU (i.e., system memory)
* address translation is accelerated by hardware.
   1.  direct remote accesses at cacheline granularity
   2.  heuristic-guided page migrations.

By leveraging cacheline level access and Address Translation Service (ATS), which enables full access to all CPU and GPU memory allocations, the system memory eliminates the page-fault handling
overhead needed in managed memory in UVM, and minimizes the need for memory migrations.

While managed memory splits the virtual memory space into both the system page table and GPU page table, system memory relies on a single system-wide page table, shared between the CPU and the GPU.

### Grace Hooper Unified Memory System
* system-allocated memory
* CUDA managed memory

#### Memory Subsystem
The CPU is connected to 480 GB of LPDDR5X memory, while the GPU is equipped with 96 GB of HBM3 memory.\
These two processors, GPU and CPU, are interconnected via the Nvidia NVLink-C2C interconnect.

Results show that the GPU’s HBM3 memory achieved a bandwidth of 3.4 TB/s, compared to its theoretical bandwidth of 4 TB/s.\
The CPU’s LPDDR5X memory reached a bandwidth of 486 GB/s, close to its theoretical bandwidth of 500 GB/s.

We achieved a bandwidth of 375 GB/s for host-to-device (H2D) transfers and 297 GB/s for device-to-host (D2H) transfers, compared to the interconnect’s theoretical bandwidth of 450 GB/s.

##### NVLink-C2C Interconnect
In the Grace Hopper system, a processor (CPU or GPU) can directly access the other processor’s physical memory over the NVLink-C2C interconnect.\
Cacheline granularity, with transfer sizes as small as 64 bytes on the CPU side and 128 bytes on the GPU side.

##### System-level Address Translation

Grace CPU features a unique hardware unit called the **System Memory Management Unit (SMMU)**.

The SMMU is responsible for translating virtual addresses to physical addresses by performing page table walks.\
Compared to a traditional MMU, the SMMU provides additional support for virtual-to-physical address translation requests from the GPU.


**This is the flow that GPU TLB cached mapping and GPU wish to access physical memory stored in CPU memory system.**

![image](https://github.com/user-attachments/assets/a26fb78a-ada8-4098-8300-e15a44d27418)


Access Flow:
- A GPU thread accesses a virtual address.
- The data is not cached in the GPU cache hierarchy. This generates a cache miss.
- The virtual address is looked up in the GPU TLBs (Translation Lookaside Buffers) for virtual-to-physical translation.\
As the translation is already cached, it is used to perform an access to physical memory.
- The GMMU initiates a direct memory access (DMA) over the NVLink-C2C interconnect, at the cacheline granularity.
- The requested access is performed from CPU memory, and send back to the GPU.
- The access is completed, and memory is cached in the regular GPU cache hierarchy.

Compared to pre-Grace Hopper systems, which rely on **GPU page fault handling to access CPU memory**, this new approach has two main implications.
- First, GPU accesses to CPU-located memory no longer systematically trigger GPU page faults.
- Second, page faults are now generated by the SMMU and can be directly handled by the operating system’s page fault handling mechanism, simplifying the overall process.

##### Memory Management in Grace Hopper

two distinct page tables
- system-wide page table
- GPU-exclusive page table

Memory allocation
* allocations in CPU physical memory only
* allocations in GPU physical memory only
* allocations that can reside in either CPU or GPU physical memory

a system-wide page table, **located in CPU memory.**\
The operating system directly accesses this page table, creates and manages page table entries (PTEs). \
The SMMU uses this page table to provide virtual-to-physical address translation for both the CPU (when required by user applications) and the GPU (when requested over the NVLink-C2C interconnect). \
Memory pages in the system-wide page table can be physically located in either CPU or GPU memory, and they use the system page size, which is defined at the operating system level and constrained by the CPU architecture capabilities.\
When using the Grace CPU, the page size is either 4 KB or 64 KB.

GPU-exclusive Page Table 
The Grace Hopper system retains the local GPU page table from previous generations of Nvidia GPUs.\
This page table, located in GPU memory and **only accessible by the GPU**, stores virtual-to-physical translations for cudaMalloc allocations and cudaMallocManaged allocations when the physical location of the managed memory is on the GPU.\
The page size used by this page table is 2 MB.

#### System-Allocated Memory

**In general, when malloc is called, the operating system creates page table entries in the system page table without assigning physical memory to those pages.**
**During the first access to a virtual address in the allocation, known as first-touch, a page fault is triggered since the accessed virtual page is not mapped to physical memory.**
Classic first-touch. On Grace Hopper, this process applies to both CPU and GPU first-touch accesses.

- When a GPU thread generates a first-touch access to a virtual address, a GPU TLB miss is triggered.
- As a result, the GPU’s ATSTBU (Translation Buffer Unit) generates an address translation request and sends it to the SMMU over NVLink-C2C.**
- **To answer the request, the SMMU performs a page table walk in the system page table.
- If no physical memory is allocated to the page, the SMMU issues a page fault.
- OS handles the fault by updating the page table entry to point to GPU physical memory, as the first-touch originated from a GPU thread.
- Once the physical address is stored in the GPU’s TLB, GPU threads can perform memory access using direct memory access to the physical memory address, potentially located in CPU memory, over NVLink-C2C.

##### Automatic Delayed Access-counter-based Migrations
For system-allocated memory, the Grace Hopper system can be configured to automatically migrate memory regions between GPU and CPU physical memory.\
The default migration strategy, detailed in Nvidia’s open-source GPU driver, relies on hardware counters to track GPU accesses to memory ranges.\
When a counter value exceeds a user-defined threshold (by default, 256), the GPU issues a notification in the form of a hardware interrupt, which is handled by the **GPU driver on
the CPU.**\
**The driver then determines** whether to migrate the pages belonging to the associated virtual memory region.

#### CUDA Managed Memory
CUDA managed memory is primarily a software abstraction, implemented as part of the **CUDA runtime libraries and the Nvidia GPU driver**.
Programmers create managed memory allocations using the cudaMallocManaged() function.\
Similar to malloc, for post-Pascal systems, the virtual memory is not immediately mapped to physical memory.\
Instead, the location of the first-touch triggers this mapping operation.


> Please Notice that CUDA Managed Memory doesn't guarantee memory is allocated in GPU. It just means that the memory is allocated by this API.

##### On-demand page migration
CUDA managed memory relies on on-demand page migration to enable both GPU and CPU to access the shared virtual memory range.\
When the GPU tries to access a page, a page fault is triggered if a GPU TLB miss occurs and the GMMU fails to find the virtual address in the GPU-exclusive page table.\
This page fault causes a page migration from CPU memory to GPU memory.\
when GPU memory is overwhelmed, pages can also be evicted to CPU memory.\

Coherent dynamic memory allocation was introduced on Power9 platforms in CUDA 9.2.
This feature is supported by the ATS, which enables hardware-level address translations by allowing direct communication between CPU and GPU MMUs and eliminates the need for software-level address translation.

#####  Speculative prefetching
before they are accessed, in order to reduce the page fault handling overhead of CUDA managed memory on the critical path.\
These strategies include explicit prefetching, triggered through the cudaMemPrefetchAsync API, and implicit prefetching performed by GPU hardware prefetchers.


### Methodology
A snippet of code transformation from a typical CUDA code with explicit memory copy to Unified Memory.
![image](https://github.com/user-attachments/assets/3d65319c-3500-4d24-94d6-c108c3b20746)

We derive two versions for each application,
* one using CUDA managed memory
* one using system-allocated memory.


For this purpose, we first identify candidate memory allocations to replace, by locating explicit host-to-device data movements in the code.\
We replace the destination and source buffers in those data transfers by a single buffer, allocated using one of the two unified memory allocators, either the **system-level allocator (malloc)** or **CUDA managed memory allocator (cudaMallocManaged)**.\
GPU-only buffers, which are never meant be accessed by the CPU, and are typically only used for storing intermediary results on the GPU, are still allocated with cudaMalloc.

Phases:\
GPU context initialization and argument parsing, allocation, CPU-side buffers initialization, computation, and de-allocation.

### Overview
categorized into two classes.
* In some applications, the system memory version outperforms the managed memory version.\
The managed memory will trigger page faults when the GPU accesses data that is not in GPU memory, and start on-demand page migration.\
As pointed in multiple existing works [2, 9], the page fault handling can cause higher overhead than the data migration itself. \
The new cache-coherent NVlink-C2C enables direct data access to CPU memory at cacheline level without involving the expensive page fault mechanism, attributing to the observed speedup.

* the system memory version even outperforms the original explicit version. The significant difference in the allocation and de-allocation time depending on the type of memory management in use.

Our in-depth analysis in Section 5 identifies the main factors coming from
* the data structures that are initialized on GPU
* the different sizes of the integrated system pages and GPU-exclusive pages.

We also identified a difference in behavior for the GPU context initialization.\
In the *traditional explicit version and managed memory version*, **memory allocations, and data transfer** are done through specific CUDA APIs before kernel launches, which **implicitly initialize GPU context**.\
However, in the system memory version, due to the absence of explicit CUDA memory allocation and data copy API calls, **GPU context initialization occurs within the first kernel launch, apparently prolonging the computation time**.

![image](https://github.com/user-attachments/assets/3941db18-f567-4b0e-9c45-fca884687429)

**CUDA Managed Mode:**
once in the computation phase, GPU access to data triggers page migration, and a steep decrease in system memory and a sharp increase in GPU memory usage is observed.
hotspot represents a typical class of existing GPU applications, where data structures used in GPU computation are initialized on CPU.

![image](https://github.com/user-attachments/assets/9fb7f114-8df1-4b2e-bcf5-eccbf4003abc)

In this application, the end-to-end execution is significantly prolonged in the system memory version, compared to the managed memory version. \
However, we also notice that the **main difference is only constrained in the initialization phase**, \
where the GPU memory usage slowly ramps up in the system memory version (orange) but quickly reaches the peak in the managed memory version (blue).\
In fact, the computation phase in both versions are similar.
![image](https://github.com/user-attachments/assets/22437593-84c2-492a-ae77-ef09f9616441)

### CPU-GPU Integrated System Page Table

The following two factor will affect integrated page table.\
System memory uses a first-touch placement policy and pages always reside in the **system page table**.\
Managed memory also uses a first-touch placement policy but pages may reside in **either the system page table or GPU page table, depending on its physical location**.

#### CPU-side Initialization
**Common HPC**
perform data initialization, often including pre-processing, on the CPU before offloading data onto the GPU for computation.

In such a pattern, the first-touch policy will cause pages to be placed on the CPU during initialization.\
When the computation phase starts, in managed memory, **data is migrated on demand to the GPU memory often with additional pages from speculative prefetching**, which will result in both traffic on the NVLink-C2C and increased GPU memory utilization.\
Instead, in the system memory, **data will not be migrated on access but deferred**, which will result in only traffic on the NVLink-C2C link and no immediately increased GPU memory utilization.
**using system memory (left) and CUDA managed memory (right).**
![image](https://github.com/user-attachments/assets/5a435960-f7f9-4aa4-a282-518dca8ac8c6)


#### GPU-side Initialization

> Please Notice that Malloc and CUDA can both allocate memory, then initialized by GPU, which is far more different.

With CUDA managed memory, the **initialization is much shorter** than that in the system memory version, and no page migration is performed during the computation phase, as the **first touch by GPU has directly mapped data to GPU memory**.

With system memory, the **GPU first-touch policy triggers a replayable page fault**, as the page being first-touched is neither present in the GMMU page table, nor through address
translation.\
**The CPU then handles the page fault and populates the system page table, therefore slowing down the initialization time on the GPU.**

![image](https://github.com/user-attachments/assets/b59c2847-9e4c-4518-95a0-16c87a2a099b)

system-allocated memory performs better in cases of CPU-side initialization as the page faults are both triggered and handled on the CPU side.\
In the GPU-side initialization, page table initialization on the CPU-side significantly slows down the execution. In the latter cases, we observed that CUDA managed memory performed better.

### System Page Size

All pages in a **system allocation** use the system page size, while only pages resident on CPU memory in the managed memory uses the system page size.\
The system page sizes mainly impacts the **page initialization overhead that often occurs in application initialization phase**, and **migration performance between CPU and GPU memory that often occurs in the computation phase**.

We run each application in the system memory version by configuring the system pages in 4 KB and 64 KB.

A noticeable difference between 4 KB and 64 KB pages lies in the de-allocation time, which is significantly higher in 4 KB system pages, for all applications.

Rodinia applications, with the exception of SRAD, exhibit lower compute time for 4 KB pages compared to 64 KB pages (1.1×-2.1×).

![image](https://github.com/user-attachments/assets/a34addf0-8cf3-4578-a7d6-6132c7ace074)

One possible reason for the lower performance in 64 KB pages pages is the granularity of migrated pages may cause amplification, resulting in **unused data being migrated**.\
This performance loss could also partially be attributed to the **automatic migrations that might incur temporary latency increase when the computation accesses on pages that are being migrated**, reducing performance.\
In Rodinia applications, this is particularly noticeable as applications have a short computation time, where migrated data may not be sufficiently reused.

![image](https://github.com/user-attachments/assets/d75f00c6-e0b0-4efb-b2ec-61d83a4f7592)

With an increasing problem size, the speedup in the managed memory version is decreasing while the speedup in the system memory version is increasing.

In CUDA managed memory, when using 64 KB pages, the execution time is 10% lower than with 4 KB pages. This **limited impact** of the system page size is expected, as Qiskit has GPU-side data initialization, and **CUDA managed memory uses the GPU page table for GPU-resident data, with a constant 2 MB page size, independent of system page size**.

While the computation time remains stable between page sizes, the **initialization time is drastically reduced with 64 KB pages**, with a 5× improvement.\
This difference highlights the cost of GPU-side page initialization, where **memory pages are first-touched on the GPU-side, and page table initialization is performed on the CPU-side, representing a notable bottleneck in the application**.

### Page Migration

we compare the new automatic access-counter-based strategy in system-allocated memory on Grace Hopper with the on-demand page migration strategy in CUDA managed memory.

- access-counter-based
An application needs to have access patterns that can clearly expose hot pages to exploit the access-counter-based strategy in system-allocated memory.\
We examined all the test applications and choose SRAD as this application uses an iterative algorithm in its computation phase.\
Therefore, with a sufficient number of iterations, the access-counter-based page migration should migrate pages repeatedly accessed during computation iterations into GPU memory. \
- on-demand migration
**the on-demand page migration in the managed system version should migrate all accessed pages on their first access.**

For the managed memory version, due to page migration in the first iteration, the execution time of this iteration is significantly higher than the other iterations.

In the system memory version, from a performance standpoint, the computational phase consists of three sub-phases, as separated by dashed line on Figure 10.
- The first phase corresponds to the first iteration, with high execution time, primarily caused by the overhead of GPU first-touch on system-allocated data, as memory pages must be initialized on the CPU-side.
- The second phase (iteration 2-4), exhibits a decreasing iteration time but still slower than that of the managed memory version.
- In the final phase (iteration 5 and above), the iteration time stabilizes and outperform the managed memory version.

![image](https://github.com/user-attachments/assets/bc54e733-2336-49f9-b459-21d3bdc1bee0)

In the managed memory version, all reads are performed from GPU memory, even for the first iteration, where pages are being migrated, and exhibit non-zero reads over NVLink-C2C.\
This is because in managed memory, pages are first migrated, and then read from local GPU memory.\
In the system memory version, we observe that memory reads over NVLink-C2C decreases as reads from GPU memory increases gradually in iteration 1-4.\
This observation confirms that the **ccess-pattern-based automatic migrations are being triggered in this stage, which hinders performance in this period.**\
After the entire working set has been migrated to GPU memory, that is, for iterations 5-12, memory reads over NVLink-C2C remain nearly zero while reads from GPU memory stabilize at
1.5 GB per iteration.\
Consequently, the performance in iterations 5-12 improves to outperform that of the managed memory version.

### Memory Oversubscription
 
- First, pages can be evicted from GPU memory, and the required pages can be migrated into GPU. This is the expected behavior for CUDA managed memory.
- In addition, as Grace Hopper supports direct memory access over NVLink-C2C, data in CPU memory can be remotely accessed without migration.

![image](https://github.com/user-attachments/assets/6057e07f-e39f-4bb4-a8fa-4e6190627bad)

The system memory version of Rodinia applications, BFS, hotspot, needle, pathfinder, are less affected by oversubscription than the managed versions, as indicated by the increased speedup at increased over-subscription.\
This trend is because that the system-memory version always places data on CPU memory, and **performs accesses over NVLink-C2C link.**\
However, in the managed memory version, data is being migrated to the GPU, and evicted when the GPU memory has been exhausted.\
**This eviction and migration process significantly impacts the performance.**

For the 34-qubits quantum volume simulation (about 130% GPU memory oversubscription), a significant slowdown with managed memory is observed compared to the explicit copy version. Further analysis reveals that no page is migrated and **all data is accessed over NVLink-C2C at a low bandwidth**.\
We optimize the managed memory version using CUDA managed memory prefetching to transform the majority of data access to be read locally from GPU memory.

In previous in-memory scenarios, CUDA managed memory in both 4 KB and 64 KB pages exhibits similar execution times.\
However, in oversubscription scenarios, the system page size shows a high impact on execution time.

![image](https://github.com/user-attachments/assets/c3d2404e-79a1-41c9-8461-e170c9adb20e)

In the 34-qubit quantum simulation, **switching from 4 KB to 64 KB system pages shortens initialization and accelerates page migration** by 58%.\
Interestingly, the 30-qubit simulation shows a different preference on the system page size, nearly 3× slower computation when using 64 KB system pages as shown in Figure 13.\ This is unexpected, as the page size for GPU-resident memory is 2 MB in managed memory, and is not modified by the system page size.
**We suggest that this difference is due to some pages being evicted to CPU memory where the system page size is used. In the case of 64 KB pages, when those pages are migrated back to the GPU, the amount of migrated memory at a time is higher than 4 KB, affecting performance.**


 




