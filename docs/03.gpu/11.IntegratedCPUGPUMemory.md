---
title: Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper
date: 2024-08-10
permalink: /pages/458724/
---

## Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper
:+1: :+1: :+1:


### Introduction 

#### limitation of state-of-art
1. UVM large overheads in handling page faults in GPU and suffers from read/write amplification due to page-level swapping.
2. Data object offloading requires offline profiling and application refactoring, limiting solution generality.
3. The performance of both solutions is constrained by data transfer bottlenecks between the CPU and GPU.

#### Grace Hopper Superchip
* NVLink-C2C (chip-to-chip)  cache-coherent interconnect
* a single virtual memory space is shared between the CPU and GPU (i.e., system memory)
* address translation is accelerated by hardware.
   1.  direct remote accesses at cacheline granularity
   2.  heuristic-guided page migrations.

By leveraging cacheline level access and Address Translation Service (ATS), which enables full access to all CPU and GPU memory allocations, the system memory eliminates the page-fault handling
overhead needed in managed memory in UVM, and minimizes the need for memory migrations.

While managed memory splits the virtual memory space into both the system page table and GPU page table, system memory relies on a single system-wide page table, shared between the CPU and the GPU.

### Grace Hooper Unified Memory System
* system-allocated memory
* CUDA managed memory

#### Memory Subsystem
The CPU is connected to 480 GB of LPDDR5X memory, while the GPU is equipped with 96 GB of HBM3 memory.\
These two processors, GPU and CPU, are interconnected via the Nvidia NVLink-C2C interconnect.

Results show that the GPU’s HBM3 memory achieved a bandwidth of 3.4 TB/s, compared to its theoretical bandwidth of 4 TB/s.\
The CPU’s LPDDR5X memory reached a bandwidth of 486 GB/s, close to its theoretical bandwidth of 500 GB/s.

We achieved a bandwidth of 375 GB/s for host-to-device (H2D) transfers and 297 GB/s for device-to-host (D2H) transfers, compared to the interconnect’s theoretical bandwidth of 450 GB/s.

##### NVLink-C2C Interconnect
In the Grace Hopper system, a processor (CPU or GPU) can directly access the other processor’s physical memory over the NVLink-C2C interconnect.\
Cacheline granularity, with transfer sizes as small as 64 bytes on the CPU side and 128 bytes on the GPU side.

##### System-level Address Translation

Grace CPU features a unique hardware unit called the **System Memory Management Unit (SMMU)**.

The SMMU is responsible for translating virtual addresses to physical addresses by performing page table walks.\
Compared to a traditional MMU, the SMMU provides additional support for virtual-to-physical address translation requests from the GPU.


**This is the flow that GPU TLB cached mapping and GPU wish to access physical memory stored in CPU memory system.**

![image](https://github.com/user-attachments/assets/a26fb78a-ada8-4098-8300-e15a44d27418)


Access Flow:
- A GPU thread accesses a virtual address.
- The data is not cached in the GPU cache hierarchy. This generates a cache miss.
- The virtual address is looked up in the GPU TLBs (Translation Lookaside Buffers) for virtual-to-physical translation.\
As the translation is already cached, it is used to perform an access to physical memory.
- The GMMU initiates a direct memory access (DMA) over the NVLink-C2C interconnect, at the cacheline granularity.
- The requested access is performed from CPU memory, and send back to the GPU.
- The access is completed, and memory is cached in the regular GPU cache hierarchy.

Compared to pre-Grace Hopper systems, which rely on **GPU page fault handling to access CPU memory**, this new approach has two main implications.
- First, GPU accesses to CPU-located memory no longer systematically trigger GPU page faults.
- Second, page faults are now generated by the SMMU and can be directly handled by the operating system’s page fault handling mechanism, simplifying the overall process.

##### Memory Management in Grace Hopper

two distinct page tables
- system-wide page table
- GPU-exclusive page table

Memory allocation
* allocations in CPU physical memory only
* allocations in GPU physical memory only
* allocations that can reside in either CPU or GPU physical memory

a system-wide page table, **located in CPU memory.**\
The operating system directly accesses this page table, creates and manages page table entries (PTEs). \
The SMMU uses this page table to provide virtual-to-physical address translation for both the CPU (when required by user applications) and the GPU (when requested over the NVLink-C2C interconnect). \
Memory pages in the system-wide page table can be physically located in either CPU or GPU memory, and they use the system page size, which is defined at the operating system level and constrained by the CPU architecture capabilities.\
When using the Grace CPU, the page size is either 4 KB or 64 KB.

GPU-exclusive Page Table 
The Grace Hopper system retains the local GPU page table from previous generations of Nvidia GPUs.\
This page table, located in GPU memory and **only accessible by the GPU**, stores virtual-to-physical translations for cudaMalloc allocations and cudaMallocManaged allocations when the physical location of the managed memory is on the GPU.\
The page size used by this page table is 2 MB.

#### System-Allocated Memory

**In general, when malloc is called, the operating system creates page table entries in the system page table without assigning physical memory to those pages.**
**During the first access to a virtual address in the allocation, known as first-touch, a page fault is triggered since the accessed virtual page is not mapped to physical memory.**
Classic first-touch. On Grace Hopper, this process applies to both CPU and GPU first-touch accesses.

- When a GPU thread generates a first-touch access to a virtual address, a GPU TLB miss is triggered.
- As a result, the GPU’s ATSTBU (Translation Buffer Unit) generates an address translation request and sends it to the SMMU over NVLink-C2C.
- To answer the request, the SMMU performs a page table walk in the system page table.
- If no physical memory is allocated to the page, the SMMU issues a page fault.
- OS handles the fault by updating the page table entry to point to GPU physical memory, as the first-touch originated from a GPU thread.
- Once the physical address is stored in the GPU’s TLB, GPU threads can perform memory access using direct memory access to the physical memory address, potentially located in CPU memory, over NVLink-C2C.

##### Automatic Delayed Access-counter-based Migrations
For system-allocated memory, the Grace Hopper system can be configured to automatically migrate memory regions between GPU and CPU physical memory.\
The default migration strategy, detailed in Nvidia’s open-source GPU driver, relies on hardware counters to track GPU accesses to memory ranges.\
When a counter value exceeds a user-defined threshold (by default, 256), the GPU issues a notification in the form of a hardware interrupt, which is handled by the **GPU driver on
the CPU.**\
**The driver then determines** whether to migrate the pages belonging to the associated virtual memory region.

#### CUDA Managed Memory
CUDA managed memory is primarily a software abstraction, implemented as part of the **CUDA runtime libraries and the Nvidia GPU driver**.
Programmers create managed memory allocations using the cudaMallocManaged() function.\
Similar to malloc, for post-Pascal systems, the virtual memory is not immediately mapped to physical memory.\
Instead, the location of the first-touch triggers this mapping operation.

##### On-demand page migration
CUDA managed memory relies on on-demand page migration to enable both GPU and CPU to access the shared virtual memory range.\
When the GPU tries to access a page, a page fault is triggered if a GPU TLB miss occurs and the GMMU fails to find the virtual address in the GPU-exclusive page table.\
This page fault causes a page migration from CPU memory to GPU memory.\
when GPU memory is overwhelmed, pages can also be evicted to CPU memory.\

Coherent dynamic memory allocation was introduced on Power9 platforms in CUDA 9.2.
This feature is supported by the ATS, which enables hardware-level address translations by allowing direct communication between CPU and GPU MMUs and eliminates the need for software-level address translation.

#####  Speculative prefetching
before they are accessed, in order to reduce the page fault handling overhead of CUDA managed memory on the critical path.\
These strategies include explicit prefetching, triggered through the cudaMemPrefetchAsync API, and implicit prefetching performed by GPU hardware prefetchers.

### System-Allocated Memory
![image](https://github.com/user-attachments/assets/3d65319c-3500-4d24-94d6-c108c3b20746)


We derive two versions for each application,
* one using CUDA managed memory
* one using system-allocated memory.


For this purpose, we first identify candidate memory allocations to replace, by locating explicit host-to-device data movements in the code.\
We replace the destination and source buffers in those data transfers by a single buffer, allocated using one of the two unified memory allocators, either the **system-level allocator (malloc)** or **CUDA managed memory allocator (cudaMallocManaged)**.\
GPU-only buffers, which are never meant be accessed by the CPU, and are typically only used for storing intermediary results on the GPU, are still allocated with cudaMalloc.

Phases:\
GPU context initialization and argument parsing, allocation, CPU-side buffers initialization, computation, and de-allocation.

### Overview
categorized into two classes.
* In some applications, the system memory version outperforms the managed memory version.\
The managed memory will trigger page faults when the GPU accesses data that is not in GPU memory, and start on-demand page migration.\
As pointed in multiple existing works [2, 9], the page fault handling can cause higher overhead than the data migration itself. \
The new cache-coherent NVlink-C2C enables direct data access to CPU memory at cacheline level without involving the expensive page fault mechanism, attributing to the observed speedup.

* the system memory version even outperforms the original explicit version. The significant difference in the allocation and de-allocation time depending on the type of memory management in use.

Our in-depth analysis in Section 5 identifies the main factors coming from
* the data structures that are initialized on GPU
* the different sizes of the integrated system pages and GPU-exclusive pages.

We also identified a difference in behavior for the GPU context initialization.\
In the *traditional explicit version and managed memory version*, **memory allocations, and data transfer** are done through specific CUDA APIs before kernel launches, which **implicitly initialize GPU context**.\
However, in the system memory version, due to the absence of explicit CUDA memory allocation and data copy API calls, **GPU context initialization occurs within the first kernel launch, apparently prolonging the computation time**.

![image](https://github.com/user-attachments/assets/3941db18-f567-4b0e-9c45-fca884687429)

**CUDA Managed Mode:**
once in the computation phase, GPU access to data triggers page migration, and a steep decrease in system memory and a sharp increase in GPU memory usage is observed.
hotspot represents a typical class of existing GPU applications, where data structures used in GPU computation are initialized on CPU.

![image](https://github.com/user-attachments/assets/9fb7f114-8df1-4b2e-bcf5-eccbf4003abc)

In this application, the end-to-end execution is significantly prolonged in the system memory version, compared to the managed memory version. \
However, we also notice that the **main difference is only constrained in the initialization phase**, \
where the GPU memory usage slowly ramps up in the system memory version (orange) but quickly reaches the peak in the managed memory version (blue).\
In fact, the computation phase in both versions are similar.
![image](https://github.com/user-attachments/assets/22437593-84c2-492a-ae77-ef09f9616441)

 ### Page Migration
 
 ### Memory Oversubscription
 





 




