---
title: GPU Training Notes
date: 2024-09-08
permalink: /pages/45884/
---

Notes from [Youtube Link](https://www.youtube.com/playlist?list=PL6RdenZrxrw-zNX7uuGppWETdxt_JxdMj)
---

## 5. Atomics Reductions

Classic Sweep Reduction
![image](https://github.com/user-attachments/assets/77e87165-58a8-43ff-a049-495c6c8ec8f4)

![image](https://github.com/user-attachments/assets/ad2ec91a-307a-4589-bf38-fec7fd489af2)

---

## 6. Managed Memory
Managed Memory does not promise performance.\
It only paves ways for software programmer. For example, deepcopy.

We could restore the performance by using cudaMemPrefetchAsync

![image](https://github.com/user-attachments/assets/1466b29c-f4d9-4b8f-9535-b9f70de68b11)

**UM cannot do better than expertly written manual data movement, in most cases**

- Unified Memory: Page-faulting
- ATS: Nvidia with Power9. ATS service allows GPU to access CPU (Malloc) Memory\
  Only works in Power9, not for X86.
- HMM: Nvidia is working on HMM to allow similar with ATS.

![image](https://github.com/user-attachments/assets/4ed00e8e-9afb-45d6-bf56-7faf95801a64)

**cudaDeviceSynchronize() function is necessary**

After cudaLaunch kernel, CPU can execute immedidately, which might read data that has not been processed by GPU yet.\
Thus, synchronize to wait GPU finishing the processing.

---

## 7. Concurrency

### Pin Memory
- Statically allocated in Physical Memory.
- Stay out of paging system.

### Streams

- Sequential in Stream
- No order among Streams

![image](https://github.com/user-attachments/assets/5f2b4794-c6ab-46b5-89c8-97e8d95a0a16)

host Code could also be put into streams.

**Migration(unified memory) call could be more expensive than memcopy.**
- memcopy is handled by hardware engine
- unfied memory operate at page level and needs update of page tables.

### CUDA Event
Most used in timing.

### Concurrent Kernels
*Less efficient than saturating the device with a single kernel.*

Scheduler might launch blocks from one kernel as much as much possible, try to exhaust the GPU.\
If any resource is totally token, other kenel cannot launch.

### CUDA Graph

---

## 8. GPU Performance Analysis

### TOP-Level
- Memory Bound
- Compute Bound
- Latency Bound
  * view the issue from idle time of scheduler
  * view the issue that the workloads shoule be in memory bound or compute bound

### Example
If SM Utilization is low, it indicates that there might be latency problem since GPU cannot schedule enough threads.

## Compile

*nvcc -o t5 t5.cu -arch=sm_70 -lineinfo*

*nsys profile --stats=true ./t5*

### NVIDIA Nsight System
Mainly for CPU and GPU. Initial timeline, find where to optimize.
nsys-ui -> open *.qdrep

### NVIDIA Compute
Targeting at Kernel
*ncu-cli --page details -f -o t5.profout ./t5*

we could find bottleneck from source code.

![image](https://github.com/user-attachments/assets/f93475fb-e6a5-4eac-a4fb-ce467c15f1d8)

---

## 9. Cooperative Groups

### Block Level Sync
![image](https://github.com/user-attachments/assets/fa0baf0e-e17d-4257-9331-d6d4bf8e9c4f)

### Coalesced Group & Grid Wide Sync

![image](https://github.com/user-attachments/assets/379f94a2-1e79-4136-8813-c09318a377bc)

![image](https://github.com/user-attachments/assets/7d532030-7fbd-4342-a92a-64cf8e500975)

Sync is not just execution barrier but also visualability barrier.

- This reduce utilize shuffle.\
  Do not need shared memory in this case. Only thread-inter communication is necessary.
![image](https://github.com/user-attachments/assets/4688af49-b16f-4542-8596-fa3105aa094e)

- This reduce utilize shared memory and sync.
![image](https://github.com/user-attachments/assets/316c510c-a285-4e84-b064-91303a3a4751)

### Cooperative Launch Kernel
*Deadlock might be introduced.*\
If too many threads in kernel, SM is full of threads waiting in *grid.sync()*. They are waiting in the queue and idle thread cannot be scheduled.\
Thus, deadlock happens.





