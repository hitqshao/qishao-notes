---
title: Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (5)
date: 2024-08-15
permalink: /pages/45873/
---

### GPGPU-sim Memory Interface

#### Response Phase from Interconnect to SM

simt_core_cluster is at GPU level, it can fetch response into m_response_fifo.

**icnt_pop**, if return non null ptr, it is a memory fetch repsonse, push it into m_response_fifo.

```
// @@@@@@ shader.cc

void simt_core_cluster::icnt_cycle()
{
    // pop from upward queue (GMMU to CU) of cluster and push it to the one in core (SM/CU)
    if ( !m_gmmu_cu_queue.empty() ) {
      mem_fetch *mf = m_gmmu_cu_queue.front();
      ...
      m_core[cid]->accept_access_response(mf);
    } 
    
    // pop it from the downward queue (CU to GMMU) of the core (SM/CU) and push it to the one in cluster (TPC)
    for (unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++) {
       if (!m_core[i]->empty_cu_gmmu_queue()){
          mem_fetch *mf = m_core[i]->front_cu_gmmu_queue();
          ...
          m_cu_gmmu_queue.push_front(mf);
       }
    }

    // Forward response from GPU response fifo into shader core (SM) response fifo
    if( !m_response_fifo.empty() ) {
        mem_fetch *mf = m_response_fifo.front();
        unsigned cid = m_config->sid_to_cid(mf->get_sid());
        ...
            // data response
            if( !m_core[cid]->ldst_unit_response_buffer_full() ) {
                m_response_fifo.pop_front();
                // GPU ---> SM
                m_core[cid]->accept_ldst_unit_response(mf);
            }
        }
    }

    // Accept Response from Interconnect Network
    if( m_response_fifo.size() < m_config->n_simt_ejection_buffer_size ) {
        mem_fetch *mf = (mem_fetch*) ::icnt_pop(m_cluster_id);
        if (!mf) 
            return;

        // The packet size varies depending on the type of request: 
        // - For read request and atomic request, the packet contains the data 
        // - For write-ack, the packet only has control metadata
        ...
        m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size); 
        mf->set_status(IN_CLUSTER_TO_SHADER_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
        ...
        // Interconnect to GPU
        m_response_fifo.push_back(mf);
        m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);
    } 
}
```

The flow below is from SM to ldst unit and then to L1 cache.

The L1 cache is included in ldst unit.

```

void shader_core_ctx::accept_ldst_unit_response(mem_fetch * mf) 
{
   m_ldst_unit->fill(mf);
}

void ldst_unit::fill( mem_fetch *mf )
{
    mf->set_status(IN_SHADER_LDST_RESPONSE_FIFO,gpu_sim_cycle+gpu_tot_sim_cycle);
    m_response_fifo.push_back(mf);
}

void ldst_unit::cycle()
{
   writeback();
   ...
   if( !m_response_fifo.empty() ) {
       mem_fetch *mf = m_response_fifo.front();
       ...
    	   if( mf->get_type() == WRITE_ACK || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {
               m_core->store_ack(mf);
               m_response_fifo.pop_front();

               if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) {
                    m_gpu->getGmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());
               }
               ...
           } else {
              ...
              if (m_L1D->fill_port_free()) {
                   m_L1D->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
                   m_response_fifo.pop_front();
              }
           }
       }
   }
```

At this time the response is fed into L1 Cache, which is shared in SM.

But how is it responsed to core?

#### Response Phase From L1 Cache to Wrap execution

```
/// @@@@@@ gpu-cache.cc
/// Interface for response from lower memory level (model bandwidth restictions in caller)
void baseline_cache::fill(mem_fetch *mf, unsigned time){
    ...
    if ( m_config.m_alloc_policy == ON_MISS )
        m_tag_array->fill(e->second.m_cache_index,time);
    else if ( m_config.m_alloc_policy == ON_FILL )
        m_tag_array->fill(e->second.m_block_addr,time);

    m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);
    ...
}

/// Accept a new cache fill response: mark entry ready for processing
void mshr_table::mark_ready( new_addr_type block_addr, bool &has_atomic ){
    ...
    m_current_response.push_back( block_addr );
    ...
}

/// Returns next ready access
mem_fetch *mshr_table::next_access(){
    ...
    new_addr_type block_addr = m_current_response.front();
    ...
    mem_fetch *result = m_data[block_addr].m_list.front();
    return result;
}
```
