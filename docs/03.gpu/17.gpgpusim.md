---
title: Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (5)
date: 2024-08-15
permalink: /pages/45874/
---

### GPGPU-sim Memory Interface

#### Response Phase from Interconnect to SM

simt_core_cluster is at GPU level, it can fetch response into m_response_fifo.

**icnt_pop**, if return non null ptr, it is a memory fetch repsonse, push it into m_response_fifo.

```
// @@@@@@ shader.cc

void simt_core_cluster::icnt_cycle()
{
    // pop from upward queue (GMMU to CU) of cluster and push it to the one in core (SM/CU)
    if ( !m_gmmu_cu_queue.empty() ) {
      mem_fetch *mf = m_gmmu_cu_queue.front();
      ...
      m_core[cid]->accept_access_response(mf);
    } 
    
    // pop it from the downward queue (CU to GMMU) of the core (SM/CU) and push it to the one in cluster (TPC)
    for (unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++) {
       if (!m_core[i]->empty_cu_gmmu_queue()){
          mem_fetch *mf = m_core[i]->front_cu_gmmu_queue();
          ...
          m_cu_gmmu_queue.push_front(mf);
       }
    }

    // Forward response from GPU response fifo into shader core (SM) response fifo
    if( !m_response_fifo.empty() ) {
        mem_fetch *mf = m_response_fifo.front();
        unsigned cid = m_config->sid_to_cid(mf->get_sid());
        ...
            // data response
            if( !m_core[cid]->ldst_unit_response_buffer_full() ) {
                m_response_fifo.pop_front();
                // GPU ---> SM
                m_core[cid]->accept_ldst_unit_response(mf);
            }
        }
    }

    // Accept Response from Interconnect Network
    if( m_response_fifo.size() < m_config->n_simt_ejection_buffer_size ) {
        mem_fetch *mf = (mem_fetch*) ::icnt_pop(m_cluster_id);
        if (!mf) 
            return;

        // The packet size varies depending on the type of request: 
        // - For read request and atomic request, the packet contains the data 
        // - For write-ack, the packet only has control metadata
        ...
        m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size); 
        mf->set_status(IN_CLUSTER_TO_SHADER_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
        ...
        // Interconnect to GPU
        m_response_fifo.push_back(mf);
        m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);
    } 
}
```

The flow below is from SM to ldst unit and then to L1 cache.

The L1 cache is included in ldst unit.

```

void shader_core_ctx::accept_ldst_unit_response(mem_fetch * mf) 
{
   m_ldst_unit->fill(mf);
}

void ldst_unit::fill( mem_fetch *mf )
{
    mf->set_status(IN_SHADER_LDST_RESPONSE_FIFO,gpu_sim_cycle+gpu_tot_sim_cycle);
    m_response_fifo.push_back(mf);
}

void ldst_unit::cycle()
{
   writeback();
   ...
   if( !m_response_fifo.empty() ) {
       mem_fetch *mf = m_response_fifo.front();
       ...
    	   if( mf->get_type() == WRITE_ACK || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {
               m_core->store_ack(mf);
               m_response_fifo.pop_front();

               if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) {
                    m_gpu->getGmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());
               }
               ...
           } else {
              ...
              if (m_L1D->fill_port_free()) {
                   m_L1D->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
                   m_response_fifo.pop_front();
              }
           }
       }
   }
```

At this time the response is fed into L1 Cache, which is shared in SM.

But how is it responsed to core?

#### Response Phase From L1 Cache to Wrap execution

```
/// @@@@@@ gpu-cache.cc
/// Interface for response from lower memory level (model bandwidth restictions in caller)
void baseline_cache::fill(mem_fetch *mf, unsigned time){
    ...
    if ( m_config.m_alloc_policy == ON_MISS )
        m_tag_array->fill(e->second.m_cache_index,time);
    else if ( m_config.m_alloc_policy == ON_FILL )
        m_tag_array->fill(e->second.m_block_addr,time);

    m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);
    ...
}

/// Accept a new cache fill response: mark entry ready for processing
void mshr_table::mark_ready( new_addr_type block_addr, bool &has_atomic ){
    ...
    m_current_response.push_back( block_addr );
    ...
}

/// Returns next ready access
mem_fetch *mshr_table::next_access(){
    ...
    new_addr_type block_addr = m_current_response.front();
    ...
    mem_fetch *result = m_data[block_addr].m_list.front();
    return result;
}
```

cache wrapped the next_access, when cache->next_access() is called, it will call mshr next_access().


writeback() function()
1. m_next_wb store next_writeback_function, it could be hit in cache or just get response from interconnect\
   - scoreboard relase register
   - m_core->warp_inst_complete
2. update m_next_wb from MSHR in L1Cache
```
void ldst_unit::writeback()
{
    // process next instruction that is going to writeback
    if( !m_next_wb.empty() ) {
        if( m_operand_collector->writeback(m_next_wb) ) {
            bool insn_completed = false; 
            for( unsigned r=0; r < 4; r++ ) {
                if( m_next_wb.out[r] > 0 ) {
		    ...
		    else { // shared 
                        m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );
                        insn_completed = true; 
                    }
                }
            }
            if( insn_completed ) {
                m_core->warp_inst_complete(m_next_wb);
            }
            m_next_wb.clear();
            m_last_inst_gpu_sim_cycle = gpu_sim_cycle;
            m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;
        }
    }


    for( unsigned c = 0; m_next_wb.empty() && (c < m_num_writeback_clients); c++ ) {
        case 4: 
            if( m_L1D && m_L1D->access_ready() ) {
                mem_fetch *mf = m_L1D->next_access();
                m_next_wb = mf->get_inst();

                if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) { 
                    m_gpu->getGmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());
                }
		 
                delete mf;
                serviced_client = next_client; 
            }
    }
}
```

m_core->warp_inst_complete is simple:

```
void shader_core_ctx::warp_inst_complete(const warp_inst_t &inst)
{
  ...
  m_gpu->gpu_sim_insn += inst.active_count();
  inst.completed(gpu_tot_sim_cycle + gpu_sim_cycle);
  ...
}
```

#### How is memory request generated from warp inst?
1. 

2. ldst_unit::memory_cycle\
   In this memory_cycle, it will invoke each access request inside inst.\
   The access will be converted into memory request, sending out downstream to L1 cache
```
// shader.cc
bool ldst_unit::memory_cycle( warp_inst_t &inst, mem_stage_stall_type &stall_reason, mem_stage_access_type &access_type )
{
   if ( !inst.accessq_empty() ) {
       	const mem_access_t &access = inst.accessq_front();
        if( bypassL1D ) {
           // bypass L1 cache
           unsigned control_size = inst.is_store() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE;
           unsigned size = access.get_size() + control_size;
           if( m_icnt->full(size, inst.is_store() || inst.isatomic()) ) {
               stall_cond = ICNT_RC_FAIL;
           } else {
               mem_fetch *mf = m_mf_allocator->alloc(inst,access);
               m_icnt->push(mf);

	       inst.accessq_pop_front();
               //inst.clear_active( access.get_warp_mask() );
               if( inst.is_load() ) { 
                  for( unsigned r=0; r < 4; r++) 
                      if(inst.out[r] > 0) 
                          assert( m_pending_writes[inst.warp_id()][inst.out[r]] > 0 );
               } else if( inst.is_store() ) 
                  m_core->inc_store_req( inst.warp_id() );
           }
       } else {
           stall_cond = process_memory_access_queue(m_L1D,inst);
       }
   }
}

mem_stage_stall_type ldst_unit::process_memory_access_queue( cache_t *cache, warp_inst_t &inst )
{
    ...
    mem_fetch *mf = m_mf_allocator->alloc(inst,inst.accessq_front());
    enum cache_request_status status = cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);
    return process_cache_access( cache, mf->get_addr(), inst, events, mf, status );
}

mem_fetch *alloc( new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const 
{
    mem_access_t access( type, addr, size, wr );
    mem_fetch *mf = new mem_fetch( access, 
    		       NULL,
    		       wr?WRITE_PACKET_SIZE:READ_PACKET_SIZE, 
    		       -1, 
    		       m_core_id, 
    		       m_cluster_id,
    		       m_memory_config );
    	return mf;
}

```
