---
title: GPU Page Table Walk
date: 2024-08-29
permalink: /pages/45878/
---

1. [117 AMD] Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems :+1: :+1: :+1: :+1: :older_man:
2. [68 AMD] Sheduling Page Table Walks for Irregular GPU Applications
3. [7 HPCA] Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding

---
# 1. Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems

## Contributions
(1) servicing a TLB  miss from the GPU can be an order of magnitude slower than  that from the CPU and consequently it is imperative to enable 
many concurrent TLB misses to hide this larger latency;
(2) divergence in memory accesses impacts the GPU’s address translation more than the rest of the memory hierarchy, and research in designing address translation mechanisms tolerant to this effect is imperative;
(3) page faults from the GPU are considerably slower than that from the CPU and software-hardware  co-design is essential for efficient implementation of page faults from throughput-oriented accelerators like GPUs.

## Background
The IOMMU resides in the processor’s northbridge complex.\
The IOMMU can access the same x86-64 page table structures used by processes running on the CPU.\
This enables the accelerator to share the same set of page tables (and thus the same virtual address space) as the processes running on the CPU via the IOMMU.\
A software driver in the OS executing on the CPU manages the IOMMU.\
The runtime software, in coordination with the OS driver, sets up the IOMMU to enable accelerators access to the same virtual address spaces of the CPU.

## Address Translation
The GPU has its own TLB hierarchy that caches recently used address translations.\
On a GPU TLB miss, a translation request is sent as an ATS (Address Translation Service [17]) request packet over the PCIe®-based [27] internal interconnect to the IOMMU.\
**This interconnect carries PCIe® packets but latency and bandwidth are not necessarily constrained by PCIe®’s electrical specifications**.\
The IOMMU has its own TLB hierarchy which is checked first;\
on a miss there, a hardware page table walker in the IOMMU checks the page table.\
ATS requests are tagged with a process address space identifier (PASID) and the IOMMU maintains a table that matches PASIDs to page table base physical addresses.\
Once the address is successfully translated, the IOMMU sends an ATS response to the GPU.\
The protocol and packet formats for ATS requests and responses are part of the PCIe® standard specification and are the same across all accelerators.

The PCIe®’s ATS protocol enables devices (and accelerators) to prefetch translation requests for up to eight contiguous virtual address pages in a single ATS response from the IOMMU.\
By default, the GPU in our system allows the prefetch value to the maximum setting of eight.

**Comparison with CPUs: In the CPU, per-core Memory management Units (MMUs) are responsible for address translations.\
In contrast, the IOMMU services requests from all accelerators.\
Unlike the CPU’s MMU, the IOMMU is not tightly integrated with CPU’s data cache hierarchy.\
The data caches may contain the most up-to-date translations but the cached copies cannot be directly accessed by accelerators.**

## Page Fault
If the IOMMU’s page table walker fails to find the desired translation in the page table, it sends an ATS response to the GPU notifying it of this failure.\
This in turn corresponds to a page fault.\
In response, the GPU sends another request to the IOMMU called a Peripheral Page Request (PPR).\
The IOMMU places this request in a memory-mapped queue and raises an interrupt on the CPU.\
Multiple PPR requests can be queued before the CPU is interrupted.\
The OS must have a suitable IOMMU driver to process this interrupt and the queued PPR requests.\
In Linux, while in an interrupt context, the driver pulls PPR requests from the queue and places them in a work-queue for later processing. \
Presumably this design decision was made to minimize the time spent executing in an interrupt context, where lower priority interrupts would be disabled. \
At a later time, an OS worker-thread calls back into the driver to process page fault requests in the work-queue.\
Once the requests are serviced, the driver notifies the IOMMU.\
In turn, the IOMMU notifies the GPU.\
The GPU then sends another ATS request to retry the translation for the original faulting address.

## Important Analysis
We divide the time to handle a GPU page fault into three major parts:\
(1) “initialization”, the latency for the OS driver to read the fault requests from the PPR queue and pre-process it;
(2) “processing”, the latency to find a physical page and update the page table;
(3) “schedule”, the time between initialization and processing of a page fault request.
We observe that only **a small fraction of the time is spent in actually processing the work to service a page fault.** \
The OS’s scheduling delay introduced by the asynchronous handling of GPU page faults is the primary contributor to the latency.
**This suggests that page faults from the GPU can be handled more efficiently by modifying the OS driver to handle the faults synchronously whenever possible.**

## Concluding Remarks
Observations and opportunities:
1. Latency of servicing a TLB miss is significantly higher on a GPU than on a CPU (~25).
2. Increasing the number of concurrent page table walks supported by the hardware is key to supporting diverse heterogeneous applications.
3. Half of the programs we studied suffer performance degradation from GPU address translation overheads.
4. Larger pages are effective in reducing TLB misses. Heterogeneous software and hardware should enhance support for larger page sizes.
5. Divergence in memory accesses impacts address translation overhead more than cache and DRAM latency. Research into divergence-tolerant address translation mechanisms for throughput-oriented accelerators is important.
6. Prefetching address translations can degrade performance for programs with poor locality. Applicationdependent translation prefetching is desirable.

Observations and opportunities:
1. The latency to service a page fault from the GPU can be significantly higher than from the CPU.
2. Enhancements into system software to handle page faults synchronously can reduce this latency. 
3. Software-hardware co-design is needed service a large number of concurrent faults from the GPU/accelerators.
4. It is imperative to scale CPU performance and resources to scale the GPU page fault servicing. 
5. Future heterogeneous applications can reduce their physical memory footprints through the use of on-demand page faults from the GPU, although current applications may need to be re-written.

![image](https://github.com/user-attachments/assets/f3b7b7a9-6290-4c1d-abc6-dd1c901d5a13)

---
# 2. Sheduling Page Table Walks for Irregular GPU Applications [2018]
- better forward progress is achieved by prioritizing translation requests from the instructions that require less work to service their
address translation needs.
- batching walk requests originating from the same SIMD instruction could reduce unnecessary stalls.

## Background 
real hardware demonstrated that such divergent memory accesses can slow down an irregular GPU application by up to 3.7-4× due to address translation overheads alone [5].\
The study found that the negative impact of divergence could be greater on address translation than on the caches.

Due to the lack of sufficient spatial locality in such irregular applications, these requests often miss in TLBs, each generating a page table walk request.

we show that the order in which page table walk requests are serviced is also critical.

*First*, the number of page table walks generated due to the execution of a single SIMD memory instruction can vary widely based on how many distinct pages the instruction accesses and the TLB hits/misses it generates.
a completely divergent SIMD instruction can generate page table walk requests equal to the number of workitems in the wavefront (here, 64).\
*Second*, each page walk may itself need anywhere between one to four memory requests to complete.\
This happens due to hits/misses in page walk caches (PWCs) that store recently-used upperlevel entries of four-level page tables (d

## Introduction

![image](https://github.com/user-attachments/assets/506931a6-b757-4ff1-bbbe-025419796e71)

1. An address translation request is generated when executing a SIMD memory instruction (load/store).
2. A coalescer merges multiple requests to the same page (e.g., 4KB) generated by the same SIMD instruction.
3. The coalesced translation request looks up the GPU’s L1 TLB and then the GPU’s shared L2 (if L1 misses).
4. On a miss in the GPU’s L2 TLB, the request is sent to the IOMMU.
5. Upon arrival at the IOMMU, the request looks up the IOMMU’s TLBs. An IOMMU typically supports multiple independent page table walkers (e.g., 8-16) to concurrently service multiple page table walk requests (TLB misses).
6. On a miss, the request queues up as a page walk request in the IOMMU buffer.
7. When an IOMMU’s page table walker becomes free, it typically selects a pending request from the IOMMU buffer in FCFS order.
8. The page table walker first performs a PWC *(page table walk caches)* lookup and then completes the walk of the page table, generating one to four memory accesses.
9. On finishing a walk, the desired translation is returned to the TLBs and ultimately to the SIMD unit that requested it.

## Idea

![image](https://github.com/user-attachments/assets/580b28d0-71f8-428a-852a-514204e070ed)

![image](https://github.com/user-attachments/assets/56b3c46e-9db9-4b8e-bcf1-7066487b4d51)

---
# 3. Trans-FW: Short Circuiting Page Table Walk in Multi-GPU Systems via Remote Forwarding

![image](https://github.com/user-attachments/assets/aaa221fa-d677-4ca7-8ce0-7447d0565c21)

## Background
**Note that, each GPU has its own local memory and local page table.**

A GPU Memory Management Unit (GMMU) handles local page table walks.\
The GMMU comprises:\
- a page walk queue (PW-queue) to buffer the translation requests waiting for available page walk threads
- a page walk cache (PW-cache) that holds the recent translations to reduce the number of memory accesses of page table walks
- multi-threaded page table walk (PT-walk) that handles multiple translation requests concurrently.

**The UVM-driver on the CPU side is responsible for coordinating all GPU far faults.**\
The UVM driver manages a centralized page table in the host memory, which holds all valid and up-to-date address translations for all GPUs and in which GPU/CPU the physical addresses are located.

## Address Translation
- The memory requests generated by the same wavefront are first coalesced by the GPU memory coalescing unit.
- L1 data cache and the L1 TLB perform lookups in parallel in a virtually indexed physically tagged (VIPT) TLB-cache design.
- Upon L1 TLB misses, the L1 Miss Status Holding Register (MSHR) is first checked to filter out repetitive requests, and the outstanding requests are forwarded to the L2 TLB for lookup.
- Translations that miss in the L2 TLB and L2 MSHR are sent to the local PT-walk in the GMMU.

**Because there is limited number of PT-walk threads, L2 TLB misses may not be served immediately.**\
As a result, these translation requests will be stored in the PW-queue and wait for available PT-walk threads.\
- During the page table walking, the translation is first checked in the PW-cache
- if it misses the PW-cache, the GPU local page table is accessed, which can be expensive and involves multiple memory accesses.
- If the page walk fails, a far fault is propagated to the GMMU and kept in a structure called GPU Fault Buffer [6], [7].
- Each time a far fault arises, the GMMU sends an alert to the UMV-driver.
- Upon the receipt of a far fault, the UVM driver fetches the fault information and caches them on the host side.
- The cached page faults are processed in batch granularity (the batch size is 256 [53]).
- Per batch, the UVM-driver initiates threads to perform page table walks using the centralized page table, initiates data transfer, and updates the GPU local page tables [7].
- The translation request is replayed after the far fault is resolved.

## Handle Far Fault
### Hardware
there has been an increasing trend for multi-GPUs to leverage hardware *(e.g., host MMU/IOMMU2 [43], [44])* to accelerate the far fault handling.

- The process within each GPU is identical to the driver handled page faults.
- When a far fault is generated, it is sent to the host and then handled by host MMU.
- Specifically, upon receiving a translation request, the host MMU first performs a host MMU TLB lookup.
- If the translation misses in the TLB, the request waits in the host MMU PW-queue for PT-walk.
- The PT-walk process in the host MMU is similar to the GPU local PT-walk, including
  * host MMU PW-cache lookup
  * host MMU PT-walk for PW-cache misses
  * host MMU TLB update

![image](https://github.com/user-attachments/assets/05d9cb2e-7e1c-4c4b-b972-523181fdac9e)


### Software
UMV Driver

### Simulator Configuration

![image](https://github.com/user-attachments/assets/57d6e1f0-5183-497d-af22-41174caf732c)

## Simulation Result Analysis

*First*, handling local page faults causes significant overhead and accounts for 86.1% of the L2 TLB miss latency.\
The local page faults latency can be further breakdown into\
- waiting in the shared host MMU PW-queue
- missing the host MMU PW-cache
- migrating page to local memory CPU-GPU interconnection and request replayed when page fault is resolved.
*Second*, 25.0% of the latency is caused by requests waiting for the available page table walk thread in the PW-queue.\
The PW-queue queuing latency in shared host MMU is generally longer than that in the GMMU for most applications.\
Specifically, the average queuing latencies are 4.1% and 20.9% for the GMMU PW-queue and the host MMU PW-queue, respectively.\
This is because the host MMU handles page faults generated from all GPUs, encountering severer contention than local GPUs.

Room for improvement: We study the performance gains when we 
i) adopt infinite PW-cache in both GPU and host MMU
ii) employ infinite page table walking threads in both GPU and host MMU
iii) eliminate all GPU local page faults.











