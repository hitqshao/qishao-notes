---
title: gpu page table walk
date: 2024-08-29
permalink: /pages/45878/
---

1. [117] Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems :+1: :+1: :+1: :+1: :older_man:
2. [68] Sheduling Page Table Walks for Irregular GPU Applications


---
# 2. Sheduling Page Table Walks for Irregular GPU Applications [2018]
- better forward progress is achieved by prioritizing translation requests from the instructions that require less work to service their
address translation needs.
- batching walk requests originating from the same SIMD instruction could reduce unnecessary stalls.

## Background 
real hardware demonstrated that such divergent memory accesses can slow down an irregular GPU application by up to 3.7-4× due to address translation overheads alone [5].\
The study found that the negative impact of divergence could be greater on address translation than on the caches.

Due to the lack of sufficient spatial locality in such irregular applications, these requests often miss in TLBs, each generating a page table walk request.

we show that the order in which page table walk requests are serviced is also critical.

*First*, the number of page table walks generated due to the execution of a single SIMD memory instruction can vary widely based on how many distinct pages the instruction accesses and the TLB hits/misses it generates.
a completely divergent SIMD instruction can generate page table walk requests equal to the number of workitems in the wavefront (here, 64).\
*Second*, each page walk may itself need anywhere between one to four memory requests to complete.\
This happens due to hits/misses in page walk caches (PWCs) that store recently-used upperlevel entries of four-level page tables (d

## Introduction

![image](https://github.com/user-attachments/assets/506931a6-b757-4ff1-bbbe-025419796e71)

1. An address translation request is generated when executing a SIMD memory instruction (load/store).
2. A coalescer merges multiple requests to the same page (e.g., 4KB) generated by the same SIMD instruction.
3. The coalesced translation request looks up the GPU’s L1 TLB and then the GPU’s shared L2 (if L1 misses).
4. On a miss in the GPU’s L2 TLB, the request is sent to the IOMMU.
5. Upon arrival at the IOMMU, the request looks up the IOMMU’s TLBs. An IOMMU typically supports multiple independent page table walkers (e.g., 8-16) to concurrently service multiple page table walk requests (TLB misses).
6. On a miss, the request queues up as a page walk request in the IOMMU buffer.
7. When an IOMMU’s page table walker becomes free, it typically selects a pending request from the IOMMU buffer in FCFS order.
8. The page table walker first performs a PWC *(page table walk caches)* lookup and then completes the walk of the page table, generating one to four memory accesses.
9. On finishing a walk, the desired translation is returned to the TLBs and ultimately to the SIMD unit that requested it.

## Idea

![image](https://github.com/user-attachments/assets/580b28d0-71f8-428a-852a-514204e070ed)

![image](https://github.com/user-attachments/assets/56b3c46e-9db9-4b8e-bcf1-7066487b4d51)


