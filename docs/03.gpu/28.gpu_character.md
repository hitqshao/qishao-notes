---
title: GPU Workload Characteristics
date: 2024-10-30
permalink: /pages/45886/
---

1. [354] Benchmarking TPU, GPU, and CPU Platforms for Deep Learning
2. [59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI
3. [12] Effective Elastic Scaling of Deep Learning Workloads
4. [30 Year:2024] LLM Inference Unveiled: Survey and Roofline Model Insights
5. [1] Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference
6. [Blog] LLM Inference Series: 5. Dissecting model performance
7. [47 Year:2024] Understanding LLMs: A Comprehensive Overview from Training to Inference
8. [37 Year:2022] Reveal training performance mystery between TensorFlow and PyTorch in the single GPU environment :+1:  :+1:  :+1:  :+1:  :+1:
9. [46 Year:2019] Performance Characterization of DNN Training using TensorFlow and PyTorch on Modern Clusters
10. [163 Year:2020] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference
---

### 1. Benchmarking TPU, GPU, and CPU Platforms for Deep Learning

Paper from Harvard

![image](https://github.com/user-attachments/assets/110ddbc0-1ddf-40fa-b360-9e3f589494c6)

### 10. Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference

Sparsity across channels:

![image](https://github.com/user-attachments/assets/d0ab98ae-adcd-438a-97fe-db8c31becb3f)

Sparsity across layers:
![image](https://github.com/user-attachments/assets/68c5159a-f536-4714-aadf-0772b6c73dde)


***CSCC: Convolution Split Compression Calculation Algorithm for Deep Neural Network***
Sparsity across layers:
![image](https://github.com/user-attachments/assets/0496984d-6cad-4727-8ee7-f63023a1656c)

