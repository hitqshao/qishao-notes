---
title: GPU Workload Characteristics
date: 2024-10-30
permalink: /pages/45886/
---

1. [354] Benchmarking TPU, GPU, and CPU Platforms for Deep Learning
2. [59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI
3. [12] Effective Elastic Scaling of Deep Learning Workloads
4. [30 Year:2024] LLM Inference Unveiled: Survey and Roofline Model Insights
5. [1] Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference
6. [Blog] LLM Inference Series: 5. Dissecting model performance
7. [47 Year:2024] Understanding LLMs: A Comprehensive Overview from Training to Inference
8. [37 Year:2022] Reveal training performance mystery between TensorFlow and PyTorch in the single GPU environment :+1:  :+1:  :+1:  :+1:  :+1:
9. [46 Year:2019] Performance Characterization of DNN Training using TensorFlow and PyTorch on Modern Clusters
10. [163 Year:2020] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference
11. [204 Year:2016] Fathom: Reference Workloads for Modern Deep Learning Methods
---

### 1. Benchmarking TPU, GPU, and CPU Platforms for Deep Learning

Paper from Harvard

![image](https://github.com/user-attachments/assets/110ddbc0-1ddf-40fa-b360-9e3f589494c6)

---
### 2 [59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI
This paper compares early ml, like single node, single master multi worker, nvlinkd is just introduced to Alibaba at that time.

![image](https://github.com/user-attachments/assets/8e9eb798-a1bf-48da-aa17-d30c7e6973fc)

![image](https://github.com/user-attachments/assets/ec5cdae5-2150-48aa-9626-37869115bff7)

---
### 10. Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference

Sparsity across channels:

![image](https://github.com/user-attachments/assets/d0ab98ae-adcd-438a-97fe-db8c31becb3f)

Sparsity across layers:
![image](https://github.com/user-attachments/assets/68c5159a-f536-4714-aadf-0772b6c73dde)


***CSCC: Convolution Split Compression Calculation Algorithm for Deep Neural Network***
Sparsity across layers:
![image](https://github.com/user-attachments/assets/0496984d-6cad-4727-8ee7-f63023a1656c)

---
### 11.Fathom: Reference Workloads for Modern Deep Learning Methods

![image](https://github.com/user-attachments/assets/04833dea-d804-416e-914f-d59409ab0f5e)

![image](https://github.com/user-attachments/assets/4394bb14-fd24-46de-a604-f34b692fd139)

![image](https://github.com/user-attachments/assets/39728745-788b-4ac1-b777-937f25b9083d)
