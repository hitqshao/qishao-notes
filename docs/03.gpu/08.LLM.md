---
title: Large Language Model Paper List
date: 2023-12-19
permalink: /pages/458720/
---

1. Efficient Memory Management for Large Language Model Serving with PagedAttention [2023]
2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory [Apple 2023]

---
### 1. Efficient Memory Management for Large Language Model Serving with PagedAttention
Disscussed the GEMM in prompt and GEMV in auto regression.
In GEMV, LLM is memory bound. There is lot of fragment in KVCache.
It also quantize the memory necessity for parameter in KV Cache.
They came up the method similar to paging in OS to manage KV in KV cache, reducing the fragment.

### 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory

Upproject matrix and downprojection matrix:
https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/
Related paper:
Parameter-Efficient Transfer Learning for NLP

