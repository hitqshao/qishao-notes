---
title: Large Language Model Paper List
date: 2023-12-19
permalink: /pages/458720/
---

1. Efficient Memory Management for Large Language Model Serving with PagedAttention [2023]
2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory [Apple 2023]

---
### 1. Efficient Memory Management for Large Language Model Serving with PagedAttention
Disscussed the GEMM in prompt and GEMV in auto regression.
In GEMV, LLM is memory bound. There is lot of fragment in KVCache.
It also quantize the memory necessity for parameter in KV Cache.
They came up the method similar to paging in OS to manage KV in KV cache, reducing the fragment.

### 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory

Upproject matrix and downprojection matrix:
https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/
Related paper:
Parameter-Efficient Transfer Learning for NLP
This introduce low-rank.
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/14c7f4b6-7709-48b9-9871-77296acb8e19)

sliding window.
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/b3d007ea-743d-445b-8236-8e1aaaea816e)

1. high sparsity in FeedForward Layers, more than 90%
   Selectively only load parameters from memory either no-zero input or predicted have non-zero output 
2. Minimize data transfer and maximize flash memory throughout
   **Window sliding**: Load parameters for only the past few tokens, reusing activations from recently computed tokens. This sliding window approach reduces the number of IO requests to load weights.
   **Row-column bundling**: We store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. This increases throughput by reading larger chunks.

3. Predict FFN sparsity and avoid loading zeroed-out parameter
   to minimize the number of weights to be transferred from flash memory to DRAM.

4. Static memory preallocation

Also a model to predict the tradeoff between loading less data and reading larger chunks

Load only 2% of FFN layer from flash

1. Larger chunk
  Although throughput growth is not linear (larger chunks take longer to transfer), the latency for the initial byte becomes a smaller fraction of the total request time, resulting in more efficient data reading.
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/de3b1b16-fc7e-4ce9-bba6-aa6ad2bccf34)

2. Load From Flash
   
  2.1  inherent sparsity found in Feed-Forward Network (FFN) model
  
   **Selective Persistence Strategy**
      Retain the embeddings and matrices within the attention mechanism of the transformer constant.Attentions weights 1/3 of the model size.For the Feed-Forward Network (FFN) portions, only the non-sparse segments are dynamically loaded into DRAM as needed.
   
   **Anticipating ReLU Sparsity**   
      Relu activation can induce 90% sparsity. Optimize preceding layer, up project by low-rank predictor to identify the zeroed elements post-ReLU.   
      In contrast to their work, our predictor needs only the output of the current layer’s attention module and not the previous layer’s FFN module.
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/85f3c2dc-a352-4506-b4d4-08abb896e8c7)


   **Neuron Data Management via Sliding Window Technique**
     Our approach focuses on managing neuron data by employing a Sliding Window Technique. This methodology entails maintaining neuron data only for a recent subset of input tokens in the memory.
   
    The key aspect of this technique is the **selective loading of neuron data that differs between the current input token and its immediate predecessors**.
   
    Frees up memory resources previously allocated to neuron data from older tokens that are no longer within the sliding window
    ![image](https://github.com/hitqshao/qishao-notes/assets/23403286/10b9ef42-5aac-4826-9247-b90ad9a171e7)

   Let sagg(k) denote the cumulative use of neuron data across a sequence of k input tokens.
   This reduction in data loading is counterbalanced by the memory cost associated with storing sagg(k). In determining the size of the sliding window, the aim is to maximize it within the constraints imposed by the available memory capacity.

  2.2 Improve Transfer Throughput with Increased Chunk Sizes
  
   **Bundling Columns and Rows** for upward and downward projection
      
   **Bundling Based on Co-activation** fetch neuron with its cloest friend. But there is WARM-GUY problem.
      
  2.3 Optimized Data Management in DRAM
  
  When a substantial portion (approximately 25%) of the Feed-Forward Networks (FFNs) in DRAM needs to be rewritten. 
      
  When introducing data for new neurons, reallocating the matrix and appending new matrices can lead to significant overhead due to the need for rewriting existing neurons data in DRAM. This involves the preallocation of all necessary memory and the establishment of a corresponding data structure for efficient management.
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/39b7adfe-fad8-4bd3-9be3-f084c260fcec)

---
LLM Principles
### 1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions
![image](https://github.com/hitqshao/qishao-notes/assets/23403286/35d4a401-2b5f-43ea-8070-8d1043f9d8e2)

