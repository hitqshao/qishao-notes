---
title: Large Language Model Paper List
date: 2023-12-19
permalink: /pages/458720/
---

1. Efficient Memory Management for Large Language Model Serving with PagedAttention [2023]

---
### 1. Efficient Memory Management for Large Language Model Serving with PagedAttention
Disscussed the GEMM in prompt and GEMV in auto regression.
In GEMV, LLM is memory bound. There is lot of fragment in KVCache.
It also quantize the memory necessity for parameter in KV Cache.
They came up the method similar to paging in OS to manage KV in KV cache, reducing the fragment.

