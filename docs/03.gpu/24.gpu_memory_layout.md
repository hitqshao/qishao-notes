---
title: GPU Memory Layout Papers
date: 2024-09-06
permalink: /pages/45881/
---

1. [3] Static Cost Estimation for Data Layout Selection on GPUs


---
## 1. Static Cost Estimation for Data Layout Selection on GPUs

on the CPU, using Array-of-Struct (AoS) would be more efficient in general, because using AoS would encourage spatial locality. \
in old GPU models before the L1 and L2 cache were introduced, Struct-of-Array (SoA) would usually be preferred since it would achieve better coalescing of memory access.\
The GPU L1 and L2 cache brought back the opportunity of cache reuse, which means merging fields stored in discrete arrays in SoA into a single AoS might lead to better performance.

![image](https://github.com/user-attachments/assets/e66b1b8b-4b92-4825-8de2-0f1b2d693a73)


### Background
Accessing one field will also bring the adjacent fields into the cache.\
On the other hand, SoA may not utilize the cache well, as accessing x[i] will not result in loading y[i] into the cache since x[i] and y[i] are unlikely to be in the same cache line.

on the GPU, using SoA will be more likely to result in more coalesced memory accesses, all threads in the same warp are accessing the same field.\
for different logical data points simultaneously, **using SoA will lead to fewer memory transactions as the same fields are declared in adjacent memory locations.**

AoS might lead to worse coalesced accesses, as declaring different fields in the same struct will waste memory bandwidth when accessing only a single field. Thus, SoA was preferred
on the GPU before the GPU cache was introduced.

![image](https://github.com/user-attachments/assets/47be4998-06ec-4edf-81bc-555a4638e264)

![image](https://github.com/user-attachments/assets/2e9f4e11-0ebf-4922-bc78-b27bdce58880)


### Framework

1) *Block Size, Grid Size, Number of Blocks per SM, and Thread ID*:\
For a GPU kernel, the block size is the number of threads inside each CUDA thread block, and the grid size is the number of CUDA thread blocks in total.\
The number of blocks per SM is the maximum number of CUDA thread blocks each SM can have when executing the kernel, and it is bounded by
- the GPU architecture
- the amount of resources each block is requesting.
For each thread, the thread ID (tid) is its one-dimensional rank inside the global thread pool.

2) *Structure Size*
The structure size of a structure is the number of bytes of the structure including the padding bytes.

3) *Array Index*

4) *Stride*\
For a global access to data field x, the stride between a pair of adjacent threads in the same warp is the difference between the two memory addresses of the field x these two threads are accessing.\
The value of the stride depends on the difference of the array indices accessed between adjacent threads, as well as the structure size of the structure field x belongs to.\
Strides can be different for different pairs of adjacent threads.

5) *Instruction Distance*\
We define the L1 and L2 instruction distance between *two memory access instructions I1 and I2*, as the *number of unique memory locations accessed
by all threads sharing the L1 or L2 cache between I1 and I2*.\

> It is like memory footprint?

They estimate the best memory layout based on cost function.
![image](https://github.com/user-attachments/assets/39d8ac2f-109f-419e-9430-742fb3456603)

-  Estimating the Cost Coefficient
-  Estimating the Number of Blocks per SM
-  Estimating the Cache Hit Ratio
-  Estimating the Cost
-  Handling Dynamic Loop Lengths
-  

