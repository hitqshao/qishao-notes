---
title: GPU TLB
date: 2024-08-26
permalink: /pages/45877/
---

1. [90] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring
2. [6] SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs

---

### 1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring
On Volta and on all other architectures we examined:
- the L1 data cache is indexed by virtual addresses;
- the L2 data cache is indexed by physical addresses

### 2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs

#### Idea
SnakeByte allows multiple equal-sized pages coalescing into a page table entry (PTE).\
It records the validity of pages to be merged using a bit vector, and few bits are annexed to indicate the size of merged pages. 

#### TLB & PTW & GMMU
Departing from conventional paging schemes of CPUs that heavily rely on operating systems, hardware-based GPU memory management units (GMMUs) are essential to effectively separate device memory management from host
CPUs.\
Otherwise, GPUs require the frequent intervention of OS to handle page table walks (PTWs) and TLB misses, which significantly penalize the GPU performance.

Observations:
- GPU workloads demand a large number of TLB entries (e.g., 32K to 256K entries) to handle sizable working sets, but conventional TLBs cannot provide sufficient coverage.
- GPU workloads have variable ranges of page contiguity.

![image](https://github.com/user-attachments/assets/ca8c2089-866b-4c16-a853-3a0f2fc792bc)

##### Paper Idea

If contiguity exists, valid bits are accordingly set in the bit vector. When all pages in the page group are allocated with contiguity (i.e., all valid bits set), the first PTE of the page group called base PTE is promoted to be further coalesced into a larger page group.

##### Address Translation

An L1 TLB is private to a streaming multiprocessor (SM), and an L2 TLB is shared among SMs [41], [42].\
On a last-level TLB miss, a request is sent to a centralized GMMU [18], [41], [42] to walk through page tables, and the GMMU concurrently handles multiple PTW requests (e.g., 8-16 PTWs).\
To amortize the latency cost of PTWs, GPUs employ page walk caches that store recently used translations at different levels of page tables.\
Importantly, the GMMU execution has to be independent of host-side operations unlike the conventional paging schemes of CPUs that heavily rely on operating systems. \
Otherwise, GPUs involve frequent OS interventions, which significantly penalize the GPU performance [44], [54].

This observation is the primary motivation of SnakeByte that can flexibly manage variable-sized page groups and maximize TLB reach.

![image](https://github.com/user-attachments/assets/28e7240f-6b4a-4832-996b-70450bbef038)

![image](https://github.com/user-attachments/assets/0661a5c0-910f-4f72-b1df-368ecf94e376)

When eight 4KB pages are allocated with contiguity, the page group is promoted to be coalesced into the next level of page group.

**At the new page allocation, SnakeByte checks the contiguity of the new PTE with others in the page group.**

##### Simulation

- By recursively coalescing PTEs, SnakeByte inevitably loses fine-grained controls on the A/D bits for individual pages.\
  SnakeByte adds 8-bit access and dirty fields to a TLB entry to trace A/D states within a page group.
- GPUs have long shootdown delays (4.2us).
- The TLB hierarchy consists of a private L1 TLB per SM, a shared L2 TLB, and miss status holding registers (MSHRs).\
  An MSHR in an L1 TLB merges up to 16 misses.
- 16 page table walkers can concurrently access four-level page tables, and a page walk cache per page table level stores up to 16 recently used translations.
- When a new page is allocated, a sequential page prefetcher allocates 16 consecutive pages (total 64KB) at a time.
- To analyze the effect of page migration latency [9], [55], we add a 20us latency overhead for each 4KB page fault [55] with 8.48GB/s bandwidth for a 64KB prefetcher [18].
 
