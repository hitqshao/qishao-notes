---
title: GPU TLB
date: 2024-08-26
permalink: /pages/45877/
---

1. [90] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring
2. [6] SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs
3. [117] Observations and Opportunities in Architecting Shared Virtual Memory for Heterogeneous Systems :+1: :+1: :+1: :+1: :older_man:
4. [2023] TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs  for Challenging Isolation Guarantees of NVIDIA MIG :+1: :+1: :+1:
5. [31] Big data causing big (TLB) problems: taming random memory accesses on the GPU :+1: :+1: :+1:
---

### 1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring
On Volta and on all other architectures we examined:
- the L1 data cache is indexed by virtual addresses;
- the L2 data cache is indexed by physical addresses

### 2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs

#### Idea
SnakeByte allows multiple equal-sized pages coalescing into a page table entry (PTE).\
It records the validity of pages to be merged using a bit vector, and few bits are annexed to indicate the size of merged pages. 

#### TLB & PTW & GMMU
Departing from conventional paging schemes of CPUs that heavily rely on operating systems, hardware-based GPU memory management units (GMMUs) are essential to effectively separate device memory management from host
CPUs.\
Otherwise, GPUs require the frequent intervention of OS to handle page table walks (PTWs) and TLB misses, which significantly penalize the GPU performance.

Observations:
- GPU workloads demand a large number of TLB entries (e.g., 32K to 256K entries) to handle sizable working sets, but conventional TLBs cannot provide sufficient coverage.
- GPU workloads have variable ranges of page contiguity.

![image](https://github.com/user-attachments/assets/ca8c2089-866b-4c16-a853-3a0f2fc792bc)

##### Paper Idea

If contiguity exists, valid bits are accordingly set in the bit vector. When all pages in the page group are allocated with contiguity (i.e., all valid bits set), the first PTE of the page group called base PTE is promoted to be further coalesced into a larger page group.

##### Address Translation

An L1 TLB is private to a streaming multiprocessor (SM), and an L2 TLB is shared among SMs [41], [42].\
On a last-level TLB miss, a request is sent to a centralized GMMU [18], [41], [42] to walk through page tables, and the GMMU concurrently handles multiple PTW requests (e.g., 8-16 PTWs).\
To amortize the latency cost of PTWs, GPUs employ page walk caches that store recently used translations at different levels of page tables.\
Importantly, the GMMU execution has to be independent of host-side operations unlike the conventional paging schemes of CPUs that heavily rely on operating systems. \
Otherwise, GPUs involve frequent OS interventions, which significantly penalize the GPU performance [44], [54].

This observation is the primary motivation of SnakeByte that can flexibly manage variable-sized page groups and maximize TLB reach.

![image](https://github.com/user-attachments/assets/28e7240f-6b4a-4832-996b-70450bbef038)

![image](https://github.com/user-attachments/assets/0661a5c0-910f-4f72-b1df-368ecf94e376)

When eight 4KB pages are allocated with contiguity, the page group is promoted to be coalesced into the next level of page group.

**At the new page allocation, SnakeByte checks the contiguity of the new PTE with others in the page group.**

##### Simulation

- By recursively coalescing PTEs, SnakeByte inevitably loses fine-grained controls on the A/D bits for individual pages.\
  SnakeByte adds 8-bit access and dirty fields to a TLB entry to trace A/D states within a page group.
- GPUs have long shootdown delays (4.2us).
- The TLB hierarchy consists of a private L1 TLB per SM, a shared L2 TLB, and miss status holding registers (MSHRs).\
  An MSHR in an L1 TLB merges up to 16 misses.
- 16 page table walkers can concurrently access four-level page tables, and a page walk cache per page table level stores up to 16 recently used translations.
- When a new page is allocated, a sequential page prefetcher allocates 16 consecutive pages (total 64KB) at a time.
- To analyze the effect of page migration latency [9], [55], we add a 20us latency overhead for each 4KB page fault [55] with 8.48GB/s bandwidth for a 64KB prefetcher [18].



### 4. TunneLs for Bootlegging: Fully Reverse-Engineering GPU TLBs  for Challenging Isolation Guarantees of NVIDIA MIG
However, we surprisingly find that MIG does not partitation the last-level TLB, which is shared by all the compute units in a GPU.

#### UVM-Managed Pages
A module in the NVIDIA driver is in charge of allocating UVM-managed pages.\
The cudaMallocManaged() function registers a virtual address subspace for UVM use.\
The UVM module allocates pages when the GPU accesses addresses in the registered address space.\
Although three page sizes are supported (see Figure 1), we find that UVM actually only **allocates 64KB and 2MB pages**.

UVM starts with allocating 64KB pages, but it will merge the 64KB pages within a 2MB page into the 2MB page if the residency reaches certain conditions.\
For example, if the first 17 or more 64KB pages in a 2MB page are present on GPU, the page table entries for these 64KB pages will be purged and replaced with a 2MB entry;\
but if just the first 16 64KB pages are used, the merging operation will not be triggered.\
We find that some other residency patterns with less than 17 pages can also trigger the merging.\
For instance, if every other 64KB page is used (i.e., 16 ones as there are 32 64KB pages in a 2MB page), the merging will also happen.

![image](https://github.com/user-attachments/assets/45e7eaca-8a31-447a-a6f8-7520e28f2109)

#### TLB Sub-Entries
In [26], Nayak et al. claim that NVIDIA GPUs enforce TLB coalescing, which combines 16 address translations to occupy just one TLB entry if the corresponding virtual page numbers are consecutive and the mapped physical page frames are also contiguous.\
However, *the results of our experiments do not agree with this claim*.

We notice that address translations reside in one L2-uTLB or L3-uTLB entry as long as the virtual base addresses of the corresponding pages
are:
- within the same 1MB-aligned address range if the pages are 64KB
- within the same 32MB-aligned range if the pages are 2MB.

This observation disproves the existence of dynamically coalescing TLB entries and explains why we separate the base addresses by 0x100000 (i.e., 1MB) and 0x2000000 (i.e., 32MB) when using sequences of 64KB and 2MB pages respectively to perform the above-mentioned tasks.

**Instead of TLB coalescing, we conjecture that there are 16 sub-entries in one L2-uTLB or L3-uTLB entry**,\
and they have a one-to-one mapping relationship with the address translations for 16 pages of size 64KB or 2MB located in the same 1MB- or 32MB-aligned range.\
If any sub-entry encounters an eviction, the rest of them are also invalidated.\
**Interestingly, we find that the entries of L1-iTLB and L1-dTLB do not have such sub-entries.**


*Inclusivity and Exclusivity.*\
We find that the L2-uTLB is neither inclusive nor exclusive in all the inspected GPUs. The same is also true for the L3-uTLB.

*Reinsertion.*
We find that an L2-uTLB hit is reinserted into the L1 and an L3-uTLB hit is also reinserted into the L2 and L1.

### 5. Big data causing big (TLB) problems: taming random memory accesses on the GPU

If the data accesses are irregular, like hash table accesses or random sampling, the GPU performance can suffer.\
Especially when scaling such accesses beyond 2GB of data, a performance decrease of an order of magnitude is encountered.\
This is paper analyzes the source of the slowdown through extensive micro-benchmarking, attributing the root cause to the Translation Lookaside Buffer (TLB).

#### Introduction 
GPU data larger than 2GB, which, in some cases, may result in a ≈13.3x runtime decrease.\
we identified the Translation Lookaside Buffer (TLB) as the  source of this slowdown, where TLB misses cost hundreds of cycles per memory access.
- NVIDIA Kepler [15]
- NVIDIA Pascal [16]

the P100 shows a significantly better performance than the K80, as it has a newer hardware architecture.\
However, the slowdown for memory accesses >2GB is still significant with factors of 4.3x for random sampling and 3.3x for grouping.

#### Benchmark

##### Virtual Memory

The reasons why GPU use virtual address\
(1) Isolation: The indirection controls a program’s memory accesses and, thus, keeps it from disallowed memory accesses to internal
device data or to data of other applications using the same GPU.\
(2) Fragmentation: Memory fragmentation can be hidden with virtual pages, allowing a large consecutive region of virtual memory to be scattered across many positions in physical memory.\
This can also increase memory bandwidth if physical memory is scattered to multiple memory chips, which then can be accessed in parallel.


##### Benchmark

*pointer chasing with stride distance*

![image](https://github.com/user-attachments/assets/ffc46301-1aa9-43b0-814d-f93332aba085)

Every stride size smaller than the page size behaves like (1/2)*X: showing lower cycle counts but experiences the first TLB miss at the same position.

#### Observation

![image](https://github.com/user-attachments/assets/00839b1e-4e68-44a9-9202-94819117e199)

![image](https://github.com/user-attachments/assets/3dd1f116-73ef-4947-8689-4b600cfb2f70)

##### Summary
(1) We found three levels of TLBs for the K80 and two levels for the P100.\
(2) For both GPUs, the different TLB levels apparently use different page sizes, where the L1 TLB uses a small page size and the L2/L3 TLB use a 16x larger page size.\
(3) Compared to K80, the P100 always has 16x larger pages.\
(4) For data larger than 2GB, the K80 has a total delay of 241 cycles, while the P100 only has a 119 cycle delay.

#### Plausibility and Validation
- First, the sizes of the L1 TLB (16 entries) and L2 TLB (65 entries) for Kepler GPUs (K80).
- We can confirm this for the L1 TLB, while the K80 already uses 2MB pages for the L2 and L3 TLB (as shown by [10]).
- Third, every TPC has its own L1 TLB and every GPC has its own L2 TLB, while the L3 TLB is shared for all SMs.
- Fourth, we can see a significant performance drop in our investigated database operations when we access more data than ≈2GB.\
  Even with different page sizes for both GPUs, we can pinpoint the problem to the L3 TLB on the K80 and the L2 TLB on the P100.\
  We can even identify the L2 TLB boundary on the K80, where performance problems start at ≈130MB.
- Fifth, in [6], the performance of a grouping operator on Kepler GPUs was improved by reducing the number of threads to <1000
instead of multiple thousands for data accesses beyond 2GB.\
  With our results, we can explain that this is benefinicial because each thread can load one page translation in the L3 TLB (1032 entries).\
  The page translations stay in the TLB.

#### Argument for Unconventional Properties

two unconventional results:\
(1) TLB entry numbers not being the power of two\
(2) different page sizes for different TLB levels.

We evaluated the allocation size and found that the smaller page size is always used for allocations (128KB on K80, 2MB on P100).\
One possible explanation for the apparently larger page sizes in the L2/L3 TLB could be a **static pre-fetching algorithm**, which always loads 16 contiguous pages when a TLB miss occurs.\
This would result in one TLB miss and 15 TLB hits, when using the small page size as traversal stride.



























