<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Understanding Pytorch CUDA | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.50d68324.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.b768edde.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/94.386e8406.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.08aa09ef.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.fca6c3ed.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.40c1ee10.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.f184eb80.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.c3992f4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.ba5a7695.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.11910620.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.0696685a.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.b6f107e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.daaab02a.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.d41d755a.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.e5e299c3.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.70be0798.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.9a151267.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.b08a9f3b.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.b7629e59.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.5828e36e.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.223b71e2.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.0ecf0c5e.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.e3317ccb.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.43700aee.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.b7a33d18.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.5f059ad4.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.33fdfa7d.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.02dd6ae5.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.4cb260a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.42569639.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.cfcf64c5.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.b0b7f1ec.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.1a265d46.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.915f20f5.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.0f47cab8.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.d1c157c7.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.10b1855e.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.c380a080.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.74f957d1.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.6d7767ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.b4073052.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.08744815.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.8cfabb38.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.58fefa8e.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.45f488bc.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.8a1df451.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.ea675c00.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.bdb427c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.2248139c.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.5e1f18c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.6f8ff3db.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.239b40a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.f55d0c25.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.e191a32a.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.7dca925b.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.c1752c32.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.caeb35d2.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.4ad27b3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.3892b158.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.b54aecd8.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.783ba305.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.c75de8fc.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.440d8122.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.eb80288c.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.4bfd230c.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.a522534a.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.21513ff9.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.221987a7.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.fd1ba165.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.85c8b682.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.13410603.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.d232e96d.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.88d7bc59.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.f1d06327.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.d95bad55.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.cb231a96.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.ff42cd7d.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.d94ba878.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.08df02ed.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.b94bc7a4.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.201b3bea.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.8f1b8d22.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.ae143581.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.c5fb917b.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.38bd571c.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.d0a48cc3.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.3215d957.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.28dbdb44.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.ecd97632.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.7b957d82.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.a50df008.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.4b32cffd.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.d09ec149.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.b9b46527.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.68d742eb.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.eaa11eb6.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.31dc6bb9.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.22fa0879.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.e7d15bdd.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/f00000/" class="sidebar-link">leakage current moores law meets static power</a></li><li><a href="/qishao-notes/pages/f00001/" class="sidebar-link">Blogs to be read</a></li><li><a href="/qishao-notes/pages/f00002/" class="sidebar-link">Understanding Pytorch Source Code</a></li><li><a href="/qishao-notes/pages/f00009/" class="sidebar-link">Understanding Pytorch Source Torch</a></li><li><a href="/qishao-notes/pages/f00004/" class="sidebar-link">CUDA Merge</a></li><li><a href="/qishao-notes/pages/f00006/" class="sidebar-link">CUDA Softmax</a></li><li><a href="/qishao-notes/pages/f00007/" class="sidebar-link">CUDA LayerNorm</a></li><li><a href="/qishao-notes/pages/f00008/" aria-current="page" class="active sidebar-link">Understanding Pytorch CUDA</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/f00008/#pytorch-cuda-memory-allocation" class="sidebar-link">Pytorch CUDA Memory Allocation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#lower-level" class="sidebar-link">Lower level</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#intermediate-level" class="sidebar-link">intermediate level</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#python-level" class="sidebar-link">python level</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#code-gen-for-memory-allocation" class="sidebar-link">code gen for memory allocation</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/f00008/#pytorch-adam-cuda-kernel" class="sidebar-link">Pytorch Adam CUDA Kernel</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#native-functions-yaml" class="sidebar-link">native_functions.yaml</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#fusedadamkernel-cu" class="sidebar-link">FusedAdamKernel.cu</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#fused-adam-impl-cu" class="sidebar-link">fusedadamimpl.cu</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#multitensorapply-cuh" class="sidebar-link">MultiTensorApply.cuh</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/f00008/#pytorch-linear-lowering" class="sidebar-link">Pytorch linear lowering</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#python-nn-functions-cpp" class="sidebar-link">pythonnnfunctions.cpp</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#linear-h" class="sidebar-link">linear.h</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#operators-0-cpp" class="sidebar-link">Operators_0.cpp</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#dispatch" class="sidebar-link">dispatch</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#boxing" class="sidebar-link">boxing</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/f00008/#blas-cpp" class="sidebar-link">Blas.cpp</a></li></ul></li></ul></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/mix/#mix" data-v-06225672>mix</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-12-30</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">Understanding Pytorch CUDA<!----></h1> <!----> <div class="theme-vdoing-content content__default"><h1 id="pytorch-cuda"><a href="#pytorch-cuda" class="header-anchor">#</a> Pytorch CUDA</h1> <h2 id="pytorch-cuda-memory-allocation"><a href="#pytorch-cuda-memory-allocation" class="header-anchor">#</a> Pytorch CUDA Memory Allocation</h2> <h3 id="lower-level"><a href="#lower-level" class="header-anchor">#</a> Lower level</h3> <p>./c10/cuda/CUDACachingAllocator.cpp</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Block* malloc(
      c10::DeviceIndex device,
      size_t orig_size,
      cudaStream_t stream) {
    ...
    // First, try to get a block from the existing pool.
    bool block_found =
        // Search pool
        get_free_block(params)
        // Trigger callbacks and retry search
        || (trigger_free_memory_callbacks(params) &amp;&amp; get_free_block(params));
    ...
    if (!block_found) {
      // Do garbage collection if the flag is set.
      if (C10_UNLIKELY(
              set_fraction &amp;&amp;
              CUDAAllocatorConfig::garbage_collection_threshold() &gt; 0.0)) {
        garbage_collect_cached_blocks(context);
      }
    ...
      // Attempt allocate
      // WARNING: alloc_block may release the allocator lock when calling
      // cudaMalloc. So far this function has not modified allocator state, but
      // keep in mind that any observed allocator state may change across calls
      // to alloc_block since it may release the lock.
      block_found = alloc_block(params, false, context, lock)
          // Free enough available cached blocks to satisfy alloc and retry
          // alloc.
          || (release_available_cached_blocks(params, context) &amp;&amp;
              alloc_block(params, false, context, lock))
          // Free all non-split cached blocks and retry alloc.
          || (C10_LIKELY(captures_underway.size() == 0) &amp;&amp;
              release_cached_blocks(context) &amp;&amp;
              alloc_block(params, true, context, lock));
...
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div><p>./aten/src/ATen/cuda</p> <ul><li>CachingHostAllocator.h</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>inline TORCH_CUDA_CPP_API at::DataPtr HostAlloc(size_t size) {
  return getCachingHostAllocator()-&gt;allocate(size);
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>CachingHostAllocator.cpp</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>struct CUDACachingHostAllocatorImpl
    : public CachingHostAllocatorImpl&lt;CUDAStream, EventPool::Event&gt; {
 private:
  void allocate_host_memory(size_t size, void** ptr) override {
    // Pinned memory pointers allocated by any device can be directly used by
    // any other device, regardless of the current device at the time of
    // allocation, since we assume unified addressing. So we grab any existing
    // primary context, if available. See pytorch/pytorch#21081.
    ...
    // Use cudaHostAlloc for allocating pinned memory (global lock in driver)
    C10_CUDA_CHECK(cudaHostAlloc(ptr, size, cudaHostAllocDefault));
    ...
    }
  }

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><h3 id="intermediate-level"><a href="#intermediate-level" class="header-anchor">#</a> intermediate level</h3> <p>code like cudnn use get_workspace_size to allocate space</p> <p>./aten/src/ATen/native/cuda/MixedDtypesLinear.cu</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>  // Allocate workspace for CUTLASS mixed datatypes GEMM kernel.
  const auto workspace_size = Gemm::get_workspace_size(arguments);
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>code like:
./aten/src/ATen/native/cuda/ForeachReduceOp.cu</p> <p>allocate memory by using at::zeros or at::empty</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>  auto output_per_tensor = at::zeros(
      {static_cast&lt;int64_t&gt;(ntensors) * max_chunks_per_tensor}, options);
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>at::zeros is based on:<br>
./aten/src/ATen/native/cuda/EmptyTensor.cpp</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>TensorBase empty_cuda(
    IntArrayRef size,
    ScalarType dtype,
    std::optional&lt;Device&gt; device_opt,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  at::globalContext().lazyInitDevice(c10::DeviceType::CUDA);
  const auto device = device_or_default(device_opt);
  TORCH_INTERNAL_ASSERT(device.is_cuda());
  const DeviceGuard device_guard(device);
  auto* allocator = at::cuda::getCUDADeviceAllocator();
  constexpr c10::DispatchKeySet cuda_dks(c10::DispatchKey::CUDA);
  return at::detail::empty_generic(
      size, allocator, cuda_dks, dtype, memory_format_opt);
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>./aten/src/ATen/EmptyTensor.cpp</p> <p>We have specify allocator and size, so we will call cuda caching allocator to allocate memory.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>TensorBase empty_generic(
    IntArrayRef size,
    c10::Allocator* allocator,
    c10::DispatchKeySet ks,
    ScalarType scalar_type,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  return _empty_generic(size, allocator, ks, scalar_type, memory_format_opt);
}

template &lt;typename T&gt;
TensorBase _empty_generic(
    ArrayRef&lt;T&gt; size,
    c10::Allocator* allocator,
    c10::DispatchKeySet ks,
    ScalarType scalar_type,
    std::optional&lt;c10::MemoryFormat&gt; memory_format_opt) {
  at::detail::check_size_nonnegative(size);
  at::detail::raise_warning_for_complex_half(scalar_type);
  caffe2::TypeMeta dtype = scalarTypeToTypeMeta(scalar_type);
  auto size_bytes = computeStorageNbytesContiguous(size, dtype.itemsize());
  auto storage_impl = c10::make_intrusive&lt;StorageImpl&gt;(
      c10::StorageImpl::use_byte_size_t(),
      size_bytes,
      allocator,
      /*resizeable=*/true);

  auto tensor = detail::make_tensor_base&lt;TensorImpl&gt;(
      std::move(storage_impl), ks, dtype);
  // Default TensorImpl has size [0]
  // NB: test for meta dispatch key to avoid guarding on zero-ness
  if (ks.has(c10::DispatchKey::Meta) || size.size() != 1 || size[0] != 0) {
    tensor.unsafeGetTensorImpl()-&gt;generic_set_sizes_contiguous(size);
  }

  if (memory_format_opt.has_value()) {
    // Restriding a just-created empty contiguous tensor does nothing.
    if (*memory_format_opt != MemoryFormat::Contiguous) {
      tensor.unsafeGetTensorImpl()-&gt;empty_tensor_restride(*memory_format_opt);
    }
  }

  return tensor;
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br></div></div><h3 id="python-level"><a href="#python-level" class="header-anchor">#</a> python level</h3> <p><strong>init</strong>.pyi.in</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>class _cuda_CUDAAllocator: ...

def _cuda_customAllocator(alloc_fn: _int, free_fn: _int) -&gt; _cuda_CUDAAllocator: ...
def _cuda_changeCurrentAllocator(allocator: _cuda_CUDAAllocator) -&gt; None: ...
def _cuda_getAllocator() -&gt; _cuda_CUDAAllocator: ...
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>./torch/cuda/memory.py</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>__all__ = [
    &quot;caching_allocator_alloc&quot;,
    &quot;caching_allocator_delete&quot;,
    &quot;caching_allocator_enable&quot;,
    ...
    &quot;memory_allocated&quot;,
    ...
]

def caching_allocator_alloc(size, device: Union[Device, int] = None, stream=None):
    r&quot;&quot;&quot;Perform a memory allocation using the CUDA memory allocator.

    Memory is allocated for a given device and a stream, this
    function is intended to be used for interoperability with other
    frameworks. Allocated memory is released through
    :func:`~torch.cuda.caching_allocator_delete`.

    Args:
        size (int): number of bytes to be allocated.
        device (torch.device or int, optional): selected device. If it is
            ``None`` the default CUDA device is used.
        stream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then
            the default stream for the selected device is used.

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    &quot;&quot;&quot;
    if device is None:
        device = torch.cuda.current_device()
    device = _get_device_index(device)
    if stream is None:
        stream = torch.cuda.current_stream(device)
    if isinstance(stream, torch.cuda.streams.Stream):
        stream = stream.cuda_stream
    ...
    with torch.cuda.device(device):
        return torch._C._cuda_cudaCachingAllocator_raw_alloc(size, stream)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br></div></div><p>./torch/csrc/cuda/Module.cpp</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    {&quot;_cuda_cudaCachingAllocator_raw_alloc&quot;,
     THCPModule_cudaCachingAllocator_raw_alloc,
     METH_VARARGS,
     nullptr},

PyObject* THCPModule_cudaCachingAllocator_raw_alloc(
    PyObject* _unused,
    PyObject* args) {
  HANDLE_TH_ERRORS
  PyObject* size_o = nullptr;
  PyObject* stream_o = nullptr;
  if (!PyArg_ParseTuple(args, &quot;OO&quot;, &amp;size_o, &amp;stream_o)) {
    THPUtils_invalidArguments(
        args,
        nullptr,
        &quot;caching_allocator_alloc&quot;,
        1,
        &quot;(ssize_t size, intptr_t stream);&quot;);
    return nullptr;
  }
  auto size = PyLong_AsSsize_t(size_o);
  cudaStream_t stream = static_cast&lt;cudaStream_t&gt;(PyLong_AsVoidPtr(stream_o));
  void* mem = nullptr;
  {
    pybind11::gil_scoped_release no_gil;
    mem = c10::cuda::CUDACachingAllocator::raw_alloc_with_stream(size, stream);
  }
  return PyLong_FromVoidPtr(mem);
  END_HANDLE_TH_ERRORS
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div><h3 id="code-gen-for-memory-allocation"><a href="#code-gen-for-memory-allocation" class="header-anchor">#</a> code gen for memory allocation</h3> <p>./torch/_inductor/codegen/cuda/cuda_kernel.py</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    def call_kernel(
        self,
        name: str,
        node: &quot;CUDATemplateBuffer&quot;,  # type: ignore[name-defined]
    ) -&gt; None:
        if node.get_workspace_size() &gt; 0:
            ws = WorkspaceArg(
                count=node.get_workspace_size(),
                device=V.graph.get_current_device_or_throw(),
                zero_mode=WorkspaceZeroMode.UNINITIALIZED,
                outer_name=WorkspaceArg.unique_name(),
            )
            wrapper.generate_workspace_allocation(ws)
            workspace = str(ws.outer_name)
            call_args.append(
                workspace
                if V.graph.cpp_wrapper
                else f&quot;c_void_p({workspace}.data_ptr())&quot;
            )
      .....
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>./torch/_inductor/codegen/wrapper.py</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    def generate_workspace_allocation(self, ws: WorkspaceArg):
        name = ws.get_name()
        line = AllocateLine(self, ws)
        if ws.zero_mode == WorkspaceZeroMode.UNINITIALIZED:
            self.writeline(line)
        elif ws.zero_mode == WorkspaceZeroMode.ZERO_ON_CALL:
            self.writeline(line)
            self.writeline(self.make_zero_buffer(name))
        elif ws.zero_mode == WorkspaceZeroMode.ZERO_PER_GRAPH:
            prior = self.allocated_workspaces.get(name)
            if prior:
                assert isinstance(prior, AllocateLine)
                # expand existing allocation
                prior.node = WorkspaceArg.maximum(prior.node, ws)
            else:
                self.writeline(line)
                self.writeline(self.make_zero_buffer(name))
                self.allocated_workspaces[name] = line
        else:
            raise AssertionError(ws.zero_mode)

        if config.triton.autotune_at_compile_time:
            self.kernel_autotune_calls.writeline(
                PythonWrapperCodegen.make_allocation(
                    self,
                    name,
                    ws.device,
                    ws.dtype,
                    shape=(V.graph.sizevars.size_hint(ws.count),),
                    stride=(1,),
                )
            )
            if ws.zero_mode != WorkspaceZeroMode.UNINITIALIZED:
                self.kernel_autotune_calls.writeline(
                    PythonWrapperCodegen.make_zero_buffer(self, name)
                )
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div><h2 id="pytorch-adam-cuda-kernel"><a href="#pytorch-adam-cuda-kernel" class="header-anchor">#</a> Pytorch Adam CUDA Kernel</h2> <h3 id="native-functions-yaml"><a href="#native-functions-yaml" class="header-anchor">#</a> native_functions.yaml</h3> <div class="language- line-numbers-mode"><pre class="language-text"><code>- func: _fused_adam_(Tensor(a!)[] self,
Tensor(b!)[] grads,
Tensor(c!)[] exp_avgs,
Tensor(d!)[] exp_avg_sqs,
Tensor(e!)[] max_exp_avg_sqs,
Tensor[] state_steps, *,
float lr, float beta1, float beta2,
float weight_decay, float eps, bool amsgrad,
bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -&gt; ()
  # Unlike &quot;foreach&quot; functions, lists of tensors should be guaranteed to be on the same device (for now).
  variants: function
  dispatch:
    CPU: _fused_adam_kernel_cpu_
    CUDA: _fused_adam_kernel_cuda_
    MPS: _fused_adam_kernel_mps_
  autogen: _fused_adam, _fused_adam.out
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><h3 id="fusedadamkernel-cu"><a href="#fusedadamkernel-cu" class="header-anchor">#</a> FusedAdamKernel.cu</h3> <p>./aten/arc/ATen/native/cuda</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>// The following overload simply has a Tensor lr
void _fused_adam_kernel_cuda_(
    at::TensorList params,
    at::TensorList grads,
    at::TensorList exp_avgs,
    at::TensorList exp_avg_sqs,
    at::TensorList max_exp_avg_sqs,
    at::TensorList state_steps,
    const at::Tensor&amp; lr,
    const double beta1,
    const double beta2,
    const double weight_decay,
    const double eps,
    const bool amsgrad,
    const bool maximize,
    const std::optional&lt;at::Tensor&gt;&amp; grad_scale,
    const std::optional&lt;at::Tensor&gt;&amp; found_inf) {
  if (lr.is_cpu()) {
    _fused_adam_kernel_cuda_(
        params,
        grads,
        exp_avgs,
        exp_avg_sqs,
        max_exp_avg_sqs,
        state_steps,
        lr.item&lt;double&gt;(),
        beta1,
        beta2,
        weight_decay,
        eps,
        amsgrad,
        maximize,
        grad_scale,
        found_inf);
    return;
  }

  // Manually check devices since we specify no device check in
  // native_functions.yaml
  ...

  if (amsgrad) {
    ...
  } else {
    TORCH_CHECK(
        at::native::check_fast_path_restrictions(
            {params, grads, exp_avgs, exp_avg_sqs}),
        &quot;params, grads, exp_avgs, and exp_avg_sqs must have same dtype, device, and layout&quot;);
    _fused_adam_cuda_impl_(
        params,
        grads,
        exp_avgs,
        exp_avg_sqs,
        state_steps,
        lr,
        beta1,
        beta2,
        weight_decay,
        eps,
        maximize,
        grad_scale,
        found_inf);
  }
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br></div></div><h3 id="fused-adam-impl-cu"><a href="#fused-adam-impl-cu" class="header-anchor">#</a> fused_adam_impl.cu</h3> <p>./aten/arc/ATen/native/cuda</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>void _fused_adam_cuda_impl_(
    at::TensorList params,
    at::TensorList grads,
    at::TensorList exp_avgs,
    at::TensorList exp_avg_sqs,
    at::TensorList state_steps,
    const double lr,
    const double beta1,
    const double beta2,
    const double weight_decay,
    const double eps,
    const bool maximize,
    const std::optional&lt;at::Tensor&gt;&amp; grad_scale,
    const std::optional&lt;at::Tensor&gt;&amp; found_inf) {
  std::vector&lt;std::vector&lt;at::Tensor&gt;&gt; tensor_lists{
      params.vec(), grads.vec(), exp_avgs.vec(), exp_avg_sqs.vec()};

  const float* grad_scale_ptr =
      grad_scale.has_value() ? grad_scale-&gt;data_ptr&lt;float&gt;() : nullptr;
  const float* found_inf_ptr =
      found_inf.has_value() ? found_inf-&gt;data_ptr&lt;float&gt;() : nullptr;
  const float* lr_ptr = nullptr;

  AT_DISPATCH_FLOATING_TYPES_AND2(
      kHalf,
      kBFloat16,
      params[0].scalar_type(),
      &quot;fused_adam_kernel_cuda&quot;,
      [&amp;]() {
        multi_tensor_apply_for_fused_optimizer&lt;4&gt;(
            tensor_lists,
            state_steps,
            FusedAdamMathFunctor&lt;scalar_t, 4, ADAM_MODE::ORIGINAL, false&gt;(),
            lr_ptr, // unused
            lr,
            beta1,
            beta2,
            weight_decay,
            eps,
            maximize,
            grad_scale_ptr,
            found_inf_ptr);
      });
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br></div></div><p>The callable function is <strong>FusedAdamMathFunctor&lt;scalar_t, 4, ADAM_MODE::ORIGINAL, false&gt;()</strong>.</p> <h3 id="multitensorapply-cuh"><a href="#multitensorapply-cuh" class="header-anchor">#</a> MultiTensorApply.cuh</h3> <div class="language- line-numbers-mode"><pre class="language-text"><code>
template &lt;typename T, typename U, typename... ArgTypes&gt;
C10_LAUNCH_BOUNDS_1(kBlockSize)
__global__ void multi_tensor_apply_kernel(
    T tensorListMeta,
    U callable,
    ArgTypes... args) {
  // Hand the chunk information to the user-supplied functor to process however
  // it likes.
  callable(kChunkSize, tensorListMeta, args...);
}

template &lt;int depth, typename T, typename... ArgTypes&gt;
void multi_tensor_apply_for_fused_optimizer(
    std::vector&lt;std::vector&lt;at::Tensor&gt;&gt;&amp; tensor_lists,
    at::TensorList state_steps,
    T callable,
    ArgTypes... args) {
    ...
    for (const auto&amp; chunk : c10::irange(chunks)) {
            multi_tensor_apply_kernel&lt;&lt;&lt;
            loc_block_info,
            kBlockSize,
            0,
            at::cuda::getCurrentCUDAStream()&gt;&gt;&gt;(
            tensorListMeta, callable, args...);
        C10_CUDA_KERNEL_LAUNCH_CHECK();
    }
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br></div></div><hr> <h2 id="pytorch-linear-lowering"><a href="#pytorch-linear-lowering" class="header-anchor">#</a> Pytorch linear lowering</h2> <h3 id="python-nn-functions-cpp"><a href="#python-nn-functions-cpp" class="header-anchor">#</a> python_nn_functions.cpp</h3> <p>./torch/csrc/autograd/generated/python_nn_functions.cpp</p> <p><img src="https://github.com/user-attachments/assets/cdac5ad2-c360-4beb-ad8c-4f2ff5a0b5a5" alt="image"></p> <h3 id="linear-h"><a href="#linear-h" class="header-anchor">#</a> linear.h</h3> <p>./build/aten/src/ATen/ops/linear.h</p> <p>at::linear</p> <p><img src="https://github.com/user-attachments/assets/66f8896b-3071-4b2d-8149-3abe65e3bbd6" alt="image"></p> <h3 id="operators-0-cpp"><a href="#operators-0-cpp" class="header-anchor">#</a> Operators_0.cpp</h3> <p>./build/aten/src/ATen/Operators_0.cpp</p> <p><img src="https://github.com/user-attachments/assets/68ed34c7-96c4-48f7-bece-862203a55cda" alt="image"></p> <h3 id="dispatch"><a href="#dispatch" class="header-anchor">#</a> dispatch</h3> <p>./aten/src/ATen/Core/dispatch/Dispatcher.h dispatch</p> <p><img src="https://github.com/user-attachments/assets/cf488804-1b5d-4ade-ba21-fa0ec46733f7" alt="image"></p> <h3 id="boxing"><a href="#boxing" class="header-anchor">#</a> boxing</h3> <p>./aten/src/ATen/Core/boxing/kernelFunction_impl.h wraps functions</p> <h3 id="blas-cpp"><a href="#blas-cpp" class="header-anchor">#</a> Blas.cpp</h3> <p>Blas.cpp calls to at::cuda::blas::gemm function.</p> <p>./aten/src/ATen/native/cuda/Blas.cpp</p> <p>Notice at::cuda::blas::gemm function. it calls into gemm function.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(
        at::ScalarType::Half,
        at::ScalarType::BFloat16,
        scalar_type,
        &quot;addmm_cuda&quot;,
        [&amp;] {
          using opmath_t = at::opmath_type&lt;scalar_t&gt;;
          opmath_t alpha_val = alpha.to&lt;opmath_t&gt;();
          opmath_t beta_val = beta.to&lt;opmath_t&gt;();
          const scalar_t* mat1_ptr = args.mata-&gt;const_data_ptr&lt;scalar_t&gt;();
          const scalar_t* mat2_ptr = args.matb-&gt;const_data_ptr&lt;scalar_t&gt;();
          scalar_t* result_ptr = args.result-&gt;mutable_data_ptr&lt;scalar_t&gt;();
          at::cuda::blas::gemm&lt;scalar_t&gt;(
              args.transa,
              args.transb,
              args.m,
              args.n,
              args.k,
              alpha_val,
              mat1_ptr,
              args.lda,
              mat2_ptr,
              args.ldb,
              beta_val,
              result_ptr,
              args.result_ld);
        });
    switch (activation) {
      case Activation::RELU:
        at::relu_(const_cast&lt;Tensor&amp;&gt;(*args.result));
        break;
      case Activation::GELU:
        at::gelu_(const_cast&lt;Tensor&amp;&gt;(*args.result), &quot;tanh&quot;);
        break;
      default: break;
    }
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/10.mix/08.learn_pytorch_cuda.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/01/02, 07:17:30</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/f00007/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">CUDA LayerNorm</div></a> <!----></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/f00007/" class="prev">CUDA LayerNorm</a></span> <!----></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.50d68324.js" defer></script><script src="/qishao-notes/assets/js/2.b768edde.js" defer></script><script src="/qishao-notes/assets/js/94.386e8406.js" defer></script>
  </body>
</html>
