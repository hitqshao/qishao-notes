<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Memory Optimizations in LLM | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.e9e33a6e.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.7441cbb4.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/87.8f9fd9a3.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.c53d023c.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.9e86eb38.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.757f946a.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.e6ae12a1.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.6687cd08.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.82b78099.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.390f1b5e.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.cfb885bd.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.b28c8a79.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.cec09ef9.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.ad4c0b54.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.b6f7d42d.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.2175d877.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.6a1bb661.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.bde85f3a.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.8de01fd0.js"><link rel="prefetch" href="/qishao-notes/assets/js/114.9b6e6b26.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.dc4136f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.33791185.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.35cdaa5c.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.936b94c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.1c803b2c.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.b688542f.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.ba9d4baf.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.42f21f4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a7b3ac76.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.1adc4cd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.58ec3c1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.049ba004.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f485ff78.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.18fc9823.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.f10809d3.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.65953f3e.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.c0382aef.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.ff3643fe.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.d532d2c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.c28eefd0.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.219afea4.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.317fcf55.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.5feb66b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.84f10398.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.dae4dbf8.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.6cac6816.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.8607e1f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.1a265d46.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.eb741f11.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.1c8ad3d7.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.d139ff29.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.c5f2edca.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.1c1afaeb.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.b0d968c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.2bff55cb.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.df5a29a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.3ce5b7fb.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.81a4a21e.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.8e78455c.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.8e889fd4.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.8b03d117.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.120c9484.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.154399c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.2c439da7.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.75040c9a.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.4fddacf2.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.8cbd18e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.a2ff89d2.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.49992571.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.942dd3d6.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.b181d359.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.f0fbf90b.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.fe0350ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.3892b158.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.b54aecd8.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.02c5e9c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.c1fd8b9c.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.440d8122.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.6e3b44f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.4bfd230c.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.54a8be0f.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.93ce3c8c.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.1e07ff8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.b9fa42d0.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.26cb8349.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.13410603.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.50a41f16.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.90edf711.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.91aca1d2.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.25811434.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.0d058120.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.87ee8000.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.d94ba878.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.08df02ed.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.adde115b.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.c7815d38.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.974bd739.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.8503f2c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.9b3fe0ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.6a9c69fc.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.728c2e6d.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.e60bd10c.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.45ea777b.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.7d0e7d9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.f31553f7.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.71f6c077.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.26aaa7c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.af058ea0.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.3c6e339b.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.b0553f10.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.3534bb2b.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.9598664e.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.75b32e6e.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.bcf1afde.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Paper List</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" aria-current="page" class="active sidebar-link">Memory Optimizations in LLM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7048/#memory-optimizations" class="sidebar-link">Memory Optimizations</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7048/#c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources" class="sidebar-link">[C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7048/#c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors" class="sidebar-link">[C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7048/#c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection" class="sidebar-link">[C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7048/#c0-2024-compact-compressed-activations-for-memory-efficient-llm-training" class="sidebar-link">[C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" class="sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-01-26</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">Memory Optimizations in LLM<!----></h1> <!----> <div class="theme-vdoing-content content__default"><h2 id="memory-optimizations"><a href="#memory-optimizations" class="header-anchor">#</a> <strong>Memory Optimizations</strong></h2> <ul><li><strong>Activation Checkpointing</strong><br>
Recomputation during backward pass.</li> <li><strong>Quantization-Aware Training (QAT)</strong><br>
Train with INT8/FP8 precision.</li> <li><strong>Dynamic Memory Allocation</strong><br>
Buffer reuse to avoid fragmentation.</li> <li><strong>Low-Rank Gradient Projection (GaLore)</strong><br> <strong>NEW</strong> Compress gradients via low-rank approximations during training.</li></ul> <hr> <h3 id="c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources"><a href="#c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources" class="header-anchor">#</a> [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources</h3> <ul><li>Use SGD instead of Adam for fine-tuning weights.</li> <li>Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.</li> <li>SGD also avoid state memory of ADAM.</li></ul> <p><img src="https://github.com/user-attachments/assets/c1e7d018-84e2-49f7-bd74-cdf4edaa0343" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/a98a808f-1cb1-4432-b0d0-7c51e9f4ee48" alt="image"></p> <hr> <h3 id="c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors"><a href="#c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors" class="header-anchor">#</a> [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors</h3> <p>This paper discovers that LORA can be approximated by a random projection.</p> <p>LORA restricts overall weights update matrices to be low-rank.</p> <p>FLORA use <em>random projection matrix</em>, which allows high-rank update gradients.</p> <blockquote><p>Our intuition arises from investigating LoRA and observing that a LoRA update is dominated by a random projection, which compresses the gradient into a
lower-dimensional space.
Our FLORA resamples the random projection and is able to mitigate the low-rank limitation of LoRA. Further, our approach only stores the compressed gradient
accumulation and momentum, thus saving the memory usage of optimization states to the sublinear level.</p></blockquote> <p>Gradident Accumulation:</p> <ul><li>Gradient accumulation stores the sum of gradients over multiple batches to simulate a larger effective batch size (helpful when memory limits prevent using large batches).</li> <li>Normally, this requires a memory buffer equal to the model size to store the full gradient matrix.</li></ul> <p>Momentum</p> <ul><li>Momentum smooths gradient updates by keeping an exponentially weighted moving average (EMA) of past gradients.</li> <li>Maintaining momentum for large models requires significant memory since an additional buffer equal to the model size is needed.</li></ul> <p>FLORA Compression:</p> <ul><li>compress gradients accumulation: Applying a random projection matrix A to reduce the dimensionality of the gradients.</li> <li>compress momentum: Using random projection to compress the momentum term M.</li></ul> <hr> <h3 id="c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection"><a href="#c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection" class="header-anchor">#</a> [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</h3> <p><img src="https://github.com/user-attachments/assets/3ddb7188-8d90-4232-8be1-cb570a74bc56" alt="image"></p> <blockquote><p>Galore: gradient Low-Rank Projection (GaLore), a training strategy that allows fullparameter learning but is more memory-efficient than common low-rank adaptation  methods such as LoRA.
Key idea is to leverage the slowchanging low-rank structure of the gradient G(m×n) of the weight matrix W, rather than trying to approximate the weight matrix itself as low rank.
while the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network
architectures.</p></blockquote> <hr> <h3 id="c0-2024-compact-compressed-activations-for-memory-efficient-llm-training"><a href="#c0-2024-compact-compressed-activations-for-memory-efficient-llm-training" class="header-anchor">#</a> [C0 2024] CompAct: Compressed Activations for Memory-Efficient LLM Training</h3> <p><img src="https://github.com/user-attachments/assets/37a40cf7-5a3b-4c55-b847-1fb1e9c732a5" alt="image"></p> <blockquote><p>By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters.
CompAct saves low-rank compressed activations during the forward pass, instead of the full activation tensors.
The resulting gradients are low-rank as well, also reducing the size of optimizer states.
As CompAct decompresses the gradients back to full size only for the update step, it compresses a large part of the compute graph, which in turn translates to major memory savings.</p></blockquote> <p>CompAct is a logical next step from previous work, moving from <strong>low-rank parameters</strong>, through <strong>compressed low-rank gradients</strong> , to <strong>compressed activations</strong>.</p> <blockquote><p>compared to GaLore, our approach may be viewed as a simple change in the order of operations, applying the compression one step before GaLore does, to the <strong>activations</strong> rather than to the <strong>gradients</strong>.</p></blockquote> <p><img src="https://github.com/user-attachments/assets/c0e05d1b-b19b-4bb0-92df-4842010b6502" alt="image"></p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/13.llm_mem_opt.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/02/15, 18:49:43</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7047/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">List of LLM Optimization Techniques</div></a> <a href="/qishao-notes/pages/dc7049/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Reasoning in LLM</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7047/" class="prev">List of LLM Optimization Techniques</a></span> <span class="next"><a href="/qishao-notes/pages/dc7049/">Reasoning in LLM</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.e9e33a6e.js" defer></script><script src="/qishao-notes/assets/js/2.7441cbb4.js" defer></script><script src="/qishao-notes/assets/js/87.8f9fd9a3.js" defer></script>
  </body>
</html>
