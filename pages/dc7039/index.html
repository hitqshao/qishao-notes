<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>llm algorthms | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.3d52d2ea.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.d64792be.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/72.f8b80aa6.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.2df6e208.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.b3d468b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.cd5972d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.5d2c13e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.4398ccf7.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.7a4b874f.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.f5ba7842.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.d76d4c59.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.f2213733.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.e5199ea8.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.511a51da.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.32e682ff.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.f92358ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.9a4d2f8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.b0bbbf51.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.94570699.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.2218cfa9.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.fea2e10c.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.e6f9089a.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.270e03ec.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.9ea0c48c.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.40a95fcb.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.ccb25a19.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.11a29093.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.8fc13149.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.b7578f4a.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.7b32634d.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.72070da7.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.1c18de98.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.5aa26412.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.2bdc45a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.7df6fc1d.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.0a6a62f5.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.608b5568.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.347cf78c.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.42890ab3.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.ab1e56e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.577bd90c.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.aacb38b2.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.5fbdd7dc.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.a45c57e8.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.3f1934ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.1f319a95.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.c1f5a851.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.25fb7ffc.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.e0e57622.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.b294d3ec.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.b8c7460a.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.214a53de.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.21fe1a6c.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.31fd40e0.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.8591684f.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.8b2d00b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.5e92cb01.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.3f25286b.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.b000f071.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.c31143c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.872dd421.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.a583497c.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.37fe36b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.832e16e6.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.7fbcf9b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.2400d189.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.35178ff3.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.8115d37d.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.7797cb64.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.7e1a632c.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.c13258af.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.5e3bcf38.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.4168e143.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.ce6b7758.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.70d8bdaa.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.8a9c545c.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.c3409789.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.46f07fc3.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.efc56907.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.56bdeff0.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.c4c78431.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how llm works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" aria-current="page" class="active sidebar-link">llm algorthms</a><ul class="sidebar-sub-headers"></ul></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-11-03</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">llm algorthms<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[1900] Mixed Precision Training</li> <li>[1519] Training Compute-Optimal Large Language Models</li> <li>[440] Measuring the Effects of Data Parallelism on Neural Network Training</li> <li>[255] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU</li> <li>[142] Performance, Design, and Autotuning of Batched GEMM for GPUs</li> <li>[13] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving</li> <li>[1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization</li></ol> <hr> <h3 id="_1900-mixed-precision-training"><a href="#_1900-mixed-precision-training" class="header-anchor">#</a> [1900] Mixed Precision Training</h3> <p><img src="https://github.com/user-attachments/assets/015ccb67-ef34-4c76-ad52-10618c578ad7" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/1a564342-e231-42a4-99d5-e9fef4500a1a" alt="image"></p> <p><strong>Loss Scaling</strong></p> <p>Note that much of the FP16 representable range was left unused, while many values were below the minimum representable range and became zeros.<br>
Scaling up the gradients will shift them to occupy more of the representable range and preserve values that are otherwise lost to zeros.<br>
This particular network diverges when gradients are not scaled, but scaling them by a factor of 8 (increasing the exponents by 3) is sufficient to match the accuracy achieved with FP32 training.</p> <p>activation gradient values below 2^−27 in magnitude were irrelevant to the training of this model, but values in the [2^−27, 2^−24) range were important to preserve.</p> <p>One efficient way to shift the gradient values into FP16-representable range is to scale the loss value computed in the forward pass, prior to starting back-propagation.</p> <p>By chain rule back-propagation ensures that all the gradient values are scaled by the same amount.</p> <p>The gradients need to be unscaled before the final weight update.</p> <hr> <h3 id="_1519-training-compute-optimal-large-language-models"><a href="#_1519-training-compute-optimal-large-language-models" class="header-anchor">#</a> [1519] Training Compute-Optimal Large Language Models</h3> <p><strong>Fix model sizes and vary number of training tokens</strong> <img src="https://github.com/user-attachments/assets/7c6f0cec-cc66-4b28-a1eb-035aee0ef342" alt="image"></p> <p>On the left we show all of our different runs. We launched a range of model sizes going from 70M to 10B, each for four different cosine cycle lengths.<br>
From these curves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate the optimal model size (center) for a given compute budget and the optimal number of training tokens (right).<br>
In green, we show projections of optimal model size and training token count based on the number of FLOPs used to train Gopher (5.76 × 1023).</p> <p><strong>IsoFLOP profiles</strong> <img src="https://github.com/user-attachments/assets/bf7aa98a-ca33-474e-bdfe-706a95f20a94" alt="image"></p> <p>For various model sizes, we choose the number of training tokens such that the final FLOPs is a constant.<br>
The cosine cycle length is set to match the target FLOP count.<br>
We find a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train (left).<br>
Using the location of these valleys, we project optimal model size and number of tokens for larger models (center and right).<br>
In green, we show the estimated number of parameters and tokens for an optimal model trained with the compute budget of Gopher.</p> <hr> <h3 id="_440-measuring-the-effects-of-data-parallelism-on-neural-network-training-google"><a href="#_440-measuring-the-effects-of-data-parallelism-on-neural-network-training-google" class="header-anchor">#</a> [440] Measuring the Effects of Data Parallelism on Neural Network Training [Google]</h3> <h4 id="steps-to-result-depends-on-batch-size-in-a-similar-way-across-problems"><a href="#steps-to-result-depends-on-batch-size-in-a-similar-way-across-problems" class="header-anchor">#</a> Steps to Result Depends on Batch Size in a Similar Way Across Problems.</h4> <p>In all cases, as the batch size grows, there is an initial period of perfect scaling (indicated with a dashed line) where the steps needed to achieve the error goal
halves for each doubling of the batch size.<br>
Then there is a region of diminishing returns that eventually leads to a region of maximal data parallelism where additional parallelism provides no benefit whatsoever.</p> <p><img src="https://github.com/user-attachments/assets/afb695c3-1503-45cc-a123-37cd8110880e" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/a132a4fc-f469-4605-b522-558345f0b9c3" alt="image"></p> <p>If the curves in Figure 1 were very sensitive to the goal validation error, then measuring the steps needed to reach our particular choice of the goal would not be a meaningful proxy for training speed.\</p> <p>For small changes in the goal validation error, we do not care about vertical shifts as long as the transition points between the three scaling regions remain relatively unchanged.\</p> <p>Figure 2 shows that varying the error goal only vertically shifts the stepsto-result curve, at least for modest variations centered around a good absolute validation error.</p> <p>Furthermore, although we ultimately care about out-of-sample error, if our plots looked very different when measuring the steps needed to reach a particular training error, then we would need to include both curves when presenting our results.</p> <h4 id="validating-our-measurement-protocol"><a href="#validating-our-measurement-protocol" class="header-anchor">#</a> Validating Our Measurement Protocol</h4> <h4 id="some-models-can-exploit-much-larger-batch-sizes-than-others"><a href="#some-models-can-exploit-much-larger-batch-sizes-than-others" class="header-anchor">#</a> Some Models Can Exploit Much Larger Batch Sizes Than Others</h4> <p><img src="https://github.com/user-attachments/assets/d4e1c07a-5e03-42f4-94fc-e750175b1396" alt="image"></p> <p><strong>This might be the begining of the scale law.</strong></p> <h4 id="momentum-extends-perfect-scaling-to-larger-batch-sizes-but-matches-plain-sgd-at-small-batch-sizes"><a href="#momentum-extends-perfect-scaling-to-larger-batch-sizes-but-matches-plain-sgd-at-small-batch-sizes" class="header-anchor">#</a> Momentum Extends Perfect Scaling to Larger Batch Sizes, but Matches Plain SGD at Small Batch Sizes</h4> <h4 id="the-data-set-matters-at-least-somewaht"><a href="#the-data-set-matters-at-least-somewaht" class="header-anchor">#</a> The Data Set Matters, at Least Somewaht</h4> <h4 id="regularization-can-be-more-helpful-at-some-batch-sizes-than-others"><a href="#regularization-can-be-more-helpful-at-some-batch-sizes-than-others" class="header-anchor">#</a> Regularization Can Be More Helpful at Some Batch Sizes than Others</h4> <h4 id="the-best-learning-rate-and-momentrum-vary-with-batch-size"><a href="#the-best-learning-rate-and-momentrum-vary-with-batch-size" class="header-anchor">#</a> The Best Learning Rate and Momentrum Vary with Batch Size</h4> <h4 id="solution-quality-depends-on-compute-budget-more-than-batch-size"><a href="#solution-quality-depends-on-compute-budget-more-than-batch-size" class="header-anchor">#</a> Solution Quality Depends on Compute Budget More Than Batch Size</h4> <p>Taken together, these observations suggest that in practice the relevant question is not which batch size leads to the best performance, but rather how compute budget varies as a function of batch size.</p> <p><img src="https://github.com/user-attachments/assets/a7164eb8-4ff2-4e9f-a68f-8538d4ce54b4" alt="image"></p> <hr> <h3 id="_142-performance-design-and-autotuning-of-batched-gemm-for-gpus"><a href="#_142-performance-design-and-autotuning-of-batched-gemm-for-gpus" class="header-anchor">#</a> [142] Performance, Design, and Autotuning of Batched GEMM for GPUs</h3> <p><img src="https://github.com/user-attachments/assets/ad7a256c-91eb-4087-a8de-ccfeb05ea8a3" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/f205bad1-19a3-4843-8fca-cc12cfa7298a" alt="image"></p> <hr> <h3 id="_255-flexgen-high-throughput-generative-inference-of-large-language-models-with-a-single-gpu"><a href="#_255-flexgen-high-throughput-generative-inference-of-large-language-models-with-a-single-gpu" class="header-anchor">#</a> [255] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU</h3> <ul><li><p>We formally define a search space of possible offloading strategies by considering computation schedule, tensor placement, and computation delegation.<br>
We prove that our search space captures a computation order with I/O complexity within 2× of optimality.<br>
We then develop a linear programming-based search algorithm to optimize the throughput within the search space.</p></li> <li><p>We show that it is possible to compress both the weights and KV cache for LLMs like OPT-175B to 4 bits without retraining or calibration, all with negligible
accuracy loss.<br>
This is achieved through fine-grained groupwise quantization (Shen et al., 2020), which is suitable for reducing I/O costs and memory usage during offloading.</p></li></ul> <p><img src="https://github.com/user-attachments/assets/f82b1417-22f4-42a3-9206-d09994df5307" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/d7d16c50-4988-4daf-8e16-fa97a5788407" alt="image"></p> <p><strong>All existing systems (Aminabadi et al., 2022; HuggingFace, 2022) traverse the graph row-by-row, as shown in Fig. 3(a).</strong> <br>
This is reasonable because it is the fastest way to finish the generation for one batch and the KV cache can be freed immediately after a row.<br>
However, because every two contiguous squares do not share weights, this schedule has to repeatedly load the weights and incurs huge I/O costs.</p> <hr> <h3 id="_13-inference-scaling-laws-an-empirical-analysis-of-compute-optimal-inference-for-llm-problem-solving"><a href="#_13-inference-scaling-laws-an-empirical-analysis-of-compute-optimal-inference-for-llm-problem-solving" class="header-anchor">#</a> [13] Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving</h3> <p>cost-performance trade-offs for inference strategies:</p> <ul><li>greedy search</li> <li>majority voting</li> <li>best-of-n</li> <li>weighted voting</li> <li>two different tree search algorithms, using different model sizes and compute budgets.</li></ul> <p>Smaller models (e.g., Llemma-7B) can outperform larger models given the same computation budgets <br>
Smaller models paired with advanced inference algorithms yield Pareto-optimal cost-performance tradeoffs. For instance, the Llemma-7B model, equipped with our novel tree search algorithm, consistently outperforms Llemma-34B with standard majority voting on the MATH benchmark across all FLOPs budgets.</p> <p><img src="https://github.com/user-attachments/assets/2e085a00-2755-48b9-8d23-bf1f8dba1488" alt="image"></p> <p>the accuracy of the language model will ultimately saturate to a fixed limit which is determined by the output probabilities assigned by the model, exhibiting exponential convergence speed through sampling and voting.<br>
this highlights the necessity for more sophisticated inference algorithms.</p> <p>the commonly-used MCTS method does not perform well with weighted voting, as it often yields many unfinished solutions, hence having less effective votes.</p> <p>To address this issue, we propose a novel tree search algorithm, <strong>REward BAlanced SEarch (REBASE)</strong>, which pairs well with weighted voting and achieves a Pareto-optimal trade-off between accuracy and inference
compute.<br>
The key idea of REBASE is to use a node-quality reward to control node expansion, which eliminates the need for explicit rollouts while ensuring enough candidate solutions for voting.</p> <p><img src="https://github.com/user-attachments/assets/d3214e44-c71a-4f18-9da4-4e4e2f543fd3" alt="image"></p> <h4 id="inference-strategy"><a href="#inference-strategy" class="header-anchor">#</a> Inference Strategy</h4> <p>Greedy search. This strategy generates tokens one at a time by selecting the highest probability token at each step. It is computationally efficient but often suboptimal in terms of diversity.</p> <ul><li>Best-of-n. This strategy, also known as rejection sampling, generates a set of candidates and chooses the one with the highest score given by the reward model.</li> <li>Majority voting. In this strategy, a set of candidates are generated, and the final answer to the problem is determined by the most frequently occurring answer in all the outputs.</li> <li>Weighted majority voting. This strategy is a variant of majority voting in which the candidates are weighted based on the scores given by the reward model.</li></ul> <p>Sampling-based if it uses a standard autoregressive sampling algorithm (e.g., temperature sampling) to generate the candidate set (greedy search is separate, in that it only has a single deterministic candidate).</p> <p>A tree-search variant uses a tree search to generate the candidate set.</p> <p><strong>Informally, as long as the reward model is “better than random”, i.e., assigning higher rewards to correct solutions on average, the accuracy limit of weighted majority voting is higher than that of majority voting.</strong></p> <ul><li>Monte Carlo Tree Search(MCTS)</li></ul> <p>MCTS consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods.</p> <p>a significant portion of the paths in the search tree are used to estimate and select nodes, and <strong>these paths do not necessarily become a part of the final candidate solution</strong>, although MCTS ensures that the sampled solutions comprise high-quality intermediate steps.</p> <p>sampling methods generate multiple solutions in parallel and independently, and <strong>all the generated sequences are included in the candidate solutions</strong>.<br>
However, the intermediate steps in these sequences are not guaranteed to be of high quality, as there is no mechanism for pruning poor steps or exploiting promising ones.</p> <ul><li></li></ul> <hr> <h3 id="_1-flattenquant-breaking-through-the-inference-compute-bound-for-large-language-models-with-per-tensor-quantization"><a href="#_1-flattenquant-breaking-through-the-inference-compute-bound-for-large-language-models-with-per-tensor-quantization" class="header-anchor">#</a> [1] FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization</h3> <p>FlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in LLMs, with the remaining layers using 8 bits.</p> <p>The 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by large matrix calculation.<br>
Our work achieves up to 2× speedup and 2.3× memory reduction for LLMs with negligible loss in accuracy.</p> <p><img src="https://github.com/user-attachments/assets/dee83087-aa07-4441-b6f6-31a8eafedaed" alt="image"></p> <p><strong>Framework</strong> <img src="https://github.com/user-attachments/assets/d386534e-97ad-4ac6-a34e-83545e1e68b2" alt="image"></p> <p><img src="https://github.com/user-attachments/assets/1924a296-1948-46c1-b835-a99bfa229b03" alt="image"></p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/05.llm_algorithms.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2024/11/04, 03:34:45</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7038/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Memory Usage in Training LLM</div></a> <!----></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7038/" class="prev">Memory Usage in Training LLM</a></span> <!----></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2024
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.3d52d2ea.js" defer></script><script src="/qishao-notes/assets/js/2.d64792be.js" defer></script><script src="/qishao-notes/assets/js/72.f8b80aa6.js" defer></script>
  </body>
</html>
