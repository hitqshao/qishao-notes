<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LLM Mixed Precision &amp; Quantization &amp; Outlier | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.6c40634c.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.7441cbb4.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/89.1f746f3e.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.c53d023c.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.bdf0afb2.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.8843afa2.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.a6918a3d.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.d42ff2e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.ff8e8f1e.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.a14c0b58.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.68b02da3.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.f8c2c5fa.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.6daf8339.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.bf4aac26.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.b6f7d42d.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.ad77fb8a.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.880af1cb.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.85a5704d.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.875270e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.dc4136f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.f184eb80.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.bfce2e69.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.18b8bfd8.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.1c803b2c.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.a9574c67.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.f18c33d2.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.42f21f4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a7b3ac76.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.1adc4cd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.58ec3c1c.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.049ba004.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f485ff78.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.18fc9823.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.494aa23f.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.596c7d2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.dfda1a33.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.e1097275.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.b9f822cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.a37871e7.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.9c58429f.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.33fdfa7d.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.ac7b19be.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.84f10398.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.dae4dbf8.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.6cac6816.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.8607e1f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.f874a546.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.865ae9b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.1c8ad3d7.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.d1c157c7.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.02b760bc.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.3b4fb4a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.b0d968c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.2bff55cb.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.df5a29a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.3ce5b7fb.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.0d9482eb.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.f48930bf.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.9eff30fa.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.2b0ef473.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.024560b0.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.799b9cd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.1e6cb5dc.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.75040c9a.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.4fddacf2.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.8cbd18e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.e66e714b.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.0c7c06f7.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.667feb1a.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.b181d359.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.ddcf1d19.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.fe0350ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.3892b158.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.b54aecd8.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.54814309.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.c5f8aaa0.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.440d8122.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.58d85c05.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.7722f887.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.a6e7b8aa.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.6936f79c.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.1e07ff8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.b9fa42d0.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.26cb8349.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.13410603.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.50a41f16.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.90edf711.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.f1d06327.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.c3135bb9.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.5e89e75c.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.1873b8ba.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.ba851590.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.f90e1643.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.a85ea3da.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.e305f5b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.0d12552d.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.c26d87d0.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.35b5bc6e.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.6a9c69fc.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.728c2e6d.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.8f9fd9a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.e60bd10c.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.d1b8e683.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.04fe12c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.614f2a95.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.a6f703a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.af058ea0.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.c1580391.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.a195c338.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.d9f174d9.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.c679ff9e.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.8751a37c.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.7f7ae319.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" aria-current="page" class="active sidebar-link">LLM Mixed Precision &amp; Quantization &amp; Outlier</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization" class="sidebar-link">1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#problems" class="sidebar-link">Problems</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#solutions" class="sidebar-link">Solutions</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models" class="sidebar-link">2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways-in-three-sentences" class="sidebar-link">Key Takeaways in Three Sentences</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#abstract" class="sidebar-link">Abstract</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#introduction" class="sidebar-link">Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#background-and-related-works" class="sidebar-link">Background and Related Works</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#comparative-analysis-of-int-and-fp-formats" class="sidebar-link">Comparative Analysis of INT and FP Formats</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#exploiting-int-and-fp-complementarity" class="sidebar-link">Exploiting INT and FP Complementarity</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#conclusion" class="sidebar-link">Conclusion</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_26-fp8-lm-training-fp8-large-language-models" class="sidebar-link">[26] FP8-LM: Training FP8 Large Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#abstract-2" class="sidebar-link">Abstract</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction" class="sidebar-link">1. Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-fp8-llm-training" class="sidebar-link">2. FP8 LLM Training</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-experimentation" class="sidebar-link">3. Experimentation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-related-work" class="sidebar-link">4. Related Work</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-conclusion" class="sidebar-link">5. Conclusion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-summary-in-3-sentences" class="sidebar-link">Key Summary in 3 Sentences*</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_139-fp8-formats-for-deep-learning" class="sidebar-link">[139] FP8 Formats for Deep Learning</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-2" class="sidebar-link">1. Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-contributions" class="sidebar-link">Key contributions:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-aspects-of-fp8-usage-in-deep-learning" class="sidebar-link">2. Aspects of FP8 Usage in Deep Learning</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-fp8-binary-interchange-format" class="sidebar-link">3. FP8 Binary Interchange Format</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-empirical-results" class="sidebar-link">4. Empirical Results</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-conclusions" class="sidebar-link">5. Conclusions</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways-in-three-sentences-2" class="sidebar-link">Key Takeaways in Three Sentences</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#y2025-paretoq-scaling-laws-in-extremely-low-bit-llm-quantization" class="sidebar-link">[Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#abstract-3" class="sidebar-link">Abstract</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-3" class="sidebar-link">1. Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-questions" class="sidebar-link">Key Questions:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#paretoq-approach" class="sidebar-link">ParetoQ Approach:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-a-better-qat-scheduling-strategy-for-extreme-low-bit-llms" class="sidebar-link">2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-a-hitchhiker-s-guide-to-quantization-method-choices" class="sidebar-link">3. A Hitchhiker’s Guide to Quantization Method Choices</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-pareto-optimality-of-extremely-low-bit-llms" class="sidebar-link">4. Pareto-Optimality of Extremely Low-Bit LLMs</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_6-related-work" class="sidebar-link">6. Related Work</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_47-microscaling-data-formats-for-deep-learning" class="sidebar-link">[47] Microscaling Data Formats for Deep Learning</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#introduction-2" class="sidebar-link">Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#microscaling-mx-data-formats" class="sidebar-link">Microscaling (MX) Data Formats</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#concrete-mx-formats" class="sidebar-link">Concrete MX Formats</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#scalar-float-to-mx-format-conversion" class="sidebar-link">Scalar Float to MX Format Conversion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#compute-flow-and-training-pipeline" class="sidebar-link">Compute Flow and Training Pipeline</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#conclusion-2" class="sidebar-link">Conclusion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-findings" class="sidebar-link">Key findings:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#three-sentence-summary" class="sidebar-link">Three-Sentence Summary</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_15-35-fp8-versus-int8-for-efficient-deep-learning-inference" class="sidebar-link">15. [35] FP8 versus INT8 for efficient deep learning inference</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-and-motivation" class="sidebar-link">1. Introduction and Motivation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-hardware-considerations" class="sidebar-link">2. Hardware Considerations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-accuracy-comparison" class="sidebar-link">3. Accuracy Comparison</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-transformer-networks" class="sidebar-link">4. Transformer Networks</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-comparison-to-other-work" class="sidebar-link">5. Comparison to Other Work</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_6-fp8-to-int8-conversion" class="sidebar-link">6. FP8 to INT8 Conversion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_7-int-quantization-paradigm" class="sidebar-link">7. INT Quantization Paradigm</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_8-conclusion" class="sidebar-link">8. Conclusion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways" class="sidebar-link">Key Takeaways:</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_16-28-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models" class="sidebar-link">16. [28] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-and-motivation-2" class="sidebar-link">1. Introduction and Motivation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-comparative-analysis-of-int-and-fp-formats" class="sidebar-link">2. Comparative Analysis of INT and FP Formats</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-mixture-of-formats-quantization-mofq" class="sidebar-link">3. Mixture of Formats Quantization (MoFQ)</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-implementation-and-experiments" class="sidebar-link">4. Implementation and Experiments</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-key-contributions" class="sidebar-link">5. Key Contributions</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_6-conclusion" class="sidebar-link">6. Conclusion</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-01-27</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">LLM Mixed Precision &amp; Quantization &amp; Outlier<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization</li> <li>[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</li> <li>[26] FP8-LM: Training FP8 Large Language Models 👍 from Microsoft</li> <li>[139] FP8 Formats for Deep Learning</li> <li>[41] With Shared Microexponents, A Little Shifting Goes a Long Way 👍 from Meta, Microsoft 👍</li> <li>[34] Stable and low-precision training for large-scale vision-language models 👍
<em>mentioned in Deepseek paper mixed precision training section. Not read yet.</em></li> <li>[Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization</li> <li>[47] Microscaling Data Formats for Deep Learning</li> <li>[Unread 2 Y2024] To FP8 and Back Again: Quantifying the Effects of Reducing Precision on LLM Training Stability</li> <li>[145] Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model</li> <li>[Unread 27]Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</li> <li>[45] PB-LLM: Partially Binarized Large Language Models</li> <li>[55] QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models</li> <li>[33] SpinQuant: LLM quantization with learned rotations</li> <li>[35] FP8 versus INT8 for efficient deep learning inference</li> <li>[28] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</li></ol> <p><strong>Outlier</strong></p> <p>1.[106] Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling</p> <hr> <h2 id="_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization"><a href="#_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization" class="header-anchor">#</a> 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization</h2> <h3 id="problems"><a href="#problems" class="header-anchor">#</a> Problems</h3> <p>The problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:</p> <ul><li>High Dynamic Range of Activations
<ul><li>The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.</li> <li>This means that the values within these tensors vary significantly in magnitude.</li> <li>Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.</li> <li>Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.</li></ul></li> <li>Presence of Structured Outliers
<ul><li>The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).</li> <li>These outliers are not random; they appear to be correlated with specific input tokens and embedding dimensions.</li> <li>Further analysis revealed that these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP]).</li> <li>While this attention behavior might be beneficial for the model's performance, the outliers that cause it also create challenges for quantization.</li></ul></li> <li>Sensitivity to Quantization Noise
<ul><li>Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.</li> <li>Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.</li> <li>This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.</li></ul></li></ul> <h3 id="solutions"><a href="#solutions" class="header-anchor">#</a> Solutions</h3> <p>solutions proposed in the paper:</p> <ul><li><p>Mixed-precision PTQ</p> <ul><li>The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.</li> <li>To address this, they proposed using a higher bit-width (16-bit) for the more sensitive activation tensors, particularly the residual sum after the feed-forward network (FFN).</li> <li>This higher bit-width allows for more accurate representation of both the FFN's input and output, minimizing potential errors.</li> <li>Additionally, they explored using low-bit (2-4) quantization for weights and token embeddings, which can significantly reduce model size without much accuracy loss.</li></ul></li> <li><p>Per-embedding-group PTQ</p> <ul><li>The authors identified that outliers in the activation tensors primarily reside in a few specific embedding dimensions.</li> <li>To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.</li> <li>This method involves splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group.</li> <li>To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.</li> <li>This approach effectively handles outliers without significantly increasing computational overhead.</li></ul></li> <li><p>Quantization-aware training (QAT)</p> <ul><li>The authors also explored QAT, where the model is trained with simulated quantization operations.</li> <li>This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.</li> <li>During QAT, they used learnable ranges for both weights and activations, further enhancing the model's adaptability to quantization.</li></ul></li></ul> <hr> <h2 id="_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models"><a href="#_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models" class="header-anchor">#</a> 2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</h2> <h3 id="key-takeaways-in-three-sentences"><a href="#key-takeaways-in-three-sentences" class="header-anchor">#</a> <strong>Key Takeaways in Three Sentences</strong></h3> <ol><li>The study demonstrates that <strong>low-bit floating-point formats, particularly FP8, provide superior quantization accuracy for LLMs compared to INT8</strong> , with comparable hardware efficiency at 8-bit precision.</li> <li>The <strong>Mixture of Formats Quantization (MoFQ) approach optimally selects between INT and FP per layer</strong> , improving accuracy without increasing computational overhead.</li> <li>MoFQ achieves <strong>state-of-the-art results in both W4-only and W8A8 quantization</strong> , outperforming existing methods like GPTQ, AWQ, LLM.int8(), and SmoothQuant while maintaining <strong>efficient inference speed</strong> .</li></ol> <h3 id="abstract"><a href="#abstract" class="header-anchor">#</a> <strong>Abstract</strong></h3> <p>The study finds that optimal quantization formats vary across layers in LLMs, leading to the <strong>Mixture of Formats Quantization (MoFQ)</strong>  approach, which selects the best format per layer.<br>
MoFQ achieves superior or comparable performance to current quantization methods for weight-only (W-only) and weight-activation (WA) quantization without additional hardware overhead.</p> <h3 id="introduction"><a href="#introduction" class="header-anchor">#</a> <strong>Introduction</strong></h3> <p>Quantization minimizes LLMs' size and inference costs, with prior work focusing on low-bit integer formats.<br>
However, as LLMs grow, integer quantization becomes less effective, requiring optimizations or alternatives. Low-bit floating-point formats have emerged as viable alternatives, with FP8 already supported in NVIDIA’s H100 GPUs.</p> <p>The study:</p> <ol><li>Compares INT and FP formats in terms of hardware efficiency and quantization error.</li> <li>Proposes <strong>Mixture of Formats Quantization (MoFQ)</strong> , selecting the best format per layer.</li> <li>Implements an inference system for W-only quantization, maintaining performance parity with INT-based systems.</li></ol> <h3 id="background-and-related-works"><a href="#background-and-related-works" class="header-anchor">#</a> <strong>Background and Related Works</strong></h3> <p><strong>Integer vs. Floating-Point Formats</strong></p> <ul><li><strong>Integer (INT)</strong> : Uniformly distributed values.</li> <li><strong>Floating-Point (FP)</strong> : Non-uniform distribution, allowing higher precision for small values but reduced precision for large values.</li> <li><strong>Hardware efficiency</strong> : FP operations typically cost more than INT, but at 8-bit, FP8 and INT8 MAC (Multiply-Accumulate) units have nearly identical area costs.</li></ul> <p><strong>Post-Training Quantization (PTQ) for LLMs</strong></p> <p>Two main PTQ strategies:</p> <ol><li><strong>Weight-Only (W-only) Quantization:</strong>  Applies to weights only, e.g., W4A16.</li> <li><strong>Weight-Activation (WA) Quantization:</strong>  Quantizes both weights and activations, e.g., W8A8.</li></ol> <p>State-of-the-art (SOTA) methods:</p> <ul><li><strong>LLM.int8()</strong> : Uses mixed precision (INT8+FP16).</li> <li><strong>SmoothQuant</strong> : Redistributes quantization difficulty from activations to weights.</li> <li><strong>GPTQ &amp; AWQ</strong> : Use second-order information and pre-scaling techniques to improve quantization.</li></ul> <h3 id="comparative-analysis-of-int-and-fp-formats"><a href="#comparative-analysis-of-int-and-fp-formats" class="header-anchor">#</a> <strong>Comparative Analysis of INT and FP Formats</strong></h3> <p><strong>A. Hardware Cost of INT vs. FP MAC Units</strong></p> <ul><li><strong>At 8-bit precision, FP8 and INT8 MACs require nearly the same hardware area</strong> , aligning with H100 GPU capabilities.</li></ul> <p><strong>B. Quantization Error Comparison</strong></p> <ol><li><strong>4-bit Weight-Only (W4) Quantization</strong>  (LLaMA-65B model):</li></ol> <ul><li>Some layers perform better with INT4, while others favor FP4, indicating <strong>layer-dependent format preference</strong> .</li></ul> <p><img src="https://github.com/user-attachments/assets/97be028e-fdb3-4883-a92e-b784d1ff1f87" alt="image"></p> <ol start="2"><li><strong>8-bit Weight-Activation (W8A8) Quantization</strong> :</li></ol> <ul><li><strong>Weights</strong> : INT8 generally has lower quantization error.</li> <li><strong>Activations</strong> : FP8 shows <strong>better robustness</strong>  for dynamic activation tensors.</li> <li>Best choice: INT8 for weights, FP8 for activations—but hardware constraints necessitate using the same format per layer.</li></ul> <p><img src="https://github.com/user-attachments/assets/7af41adf-5821-4c0b-8599-1385c1982c87" alt="image"></p> <h3 id="exploiting-int-and-fp-complementarity"><a href="#exploiting-int-and-fp-complementarity" class="header-anchor">#</a> <strong>Exploiting INT and FP Complementarity</strong></h3> <p><strong>A. Improved Low-Bit FP4 Format</strong></p> <ul><li>IEEE floating-point format reserves exponent values for NaN and Inf.</li> <li><strong>Reallocating NaN &amp; Inf to normalized numbers improves FP4 precision</strong>  by 35%.</li></ul> <p><strong>B. Mixture of Formats Quantization (MoFQ)</strong></p> <ul><li>Selects the best quantization format (INT or FP) <strong>per layer</strong>  based on quantization error.</li> <li>Works for both <strong>W-only and WA quantization</strong> .</li> <li><strong>Algorithm</strong> : Iterates through layers, computes quantization error for INT and FP, and selects the lower-error format.</li></ul> <p><strong>C. Low-Bit W-Only Inference System</strong></p> <ul><li><strong>INT4 and FP4 require conversion to FP16 before computation</strong>  due to FP16 activations.</li> <li><strong>W8A8 quantization</strong> : FP16 activations are converted to FP8 or INT8 based on next-layer format selection.</li> <li><strong>No additional hardware overhead for FP-based or MoFQ-based inference</strong>  compared to INT-based quantization.</li></ul> <p><img src="https://github.com/user-attachments/assets/7d5e0e92-deda-4e40-8775-b428fff2ca0c" alt="image"></p> <h3 id="conclusion"><a href="#conclusion" class="header-anchor">#</a> <strong>Conclusion</strong></h3> <ul><li><strong>Comparative study</strong> : INT and FP formats have complementary strengths.</li> <li><strong>Key finding</strong> : <strong>FP8 and INT8 MAC units have similar hardware costs at low-bit quantization</strong> .</li> <li><strong>MoFQ method</strong> :
<ul><li>Selects the best quantization format <strong>per layer</strong> .</li> <li><strong>Achieves state-of-the-art accuracy</strong>  in W4-only and W8A8 quantization.</li> <li><strong>No additional inference latency or hardware overhead</strong> .</li></ul></li></ul> <hr> <h2 id="_26-fp8-lm-training-fp8-large-language-models"><a href="#_26-fp8-lm-training-fp8-large-language-models" class="header-anchor">#</a> [26] FP8-LM: Training FP8 Large Language Models</h2> <h3 id="abstract-2"><a href="#abstract-2" class="header-anchor">#</a> <strong>Abstract</strong></h3> <p>The paper explores <strong>FP8 low-bit data formats</strong>  for training large language models (LLMs), significantly reducing <strong>memory usage and computation costs</strong>  while maintaining accuracy.</p> <p>The authors introduce an <strong>FP8 automatic mixed-precision training framework</strong>  with three levels of FP8 utilization, improving mixed-precision and distributed parallel training. <strong>Key results</strong>  show that training the <strong>GPT-175B model on an H100 GPU platform</strong>  using FP8:</p> <ul><li><strong>Reduces memory usage by 39%</strong></li> <li><strong>Speeds up training by 75% compared to BF16 (Megatron-LM)</strong></li> <li><strong>Outperforms Nvidia Transformer Engine by 37%</strong>
The <strong>FP8 training methodology is generalizable</strong>  to fine-tuning, instruction tuning, and reinforcement learning with human feedback (RLHF). The framework is <strong>open-sourced</strong>  at <a href="https://github.com/Azure/MS-AMP" target="_blank" rel="noopener noreferrer">aka.ms/MS.AMP<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> .</li></ul> <h3 id="_1-introduction"><a href="#_1-introduction" class="header-anchor">#</a> <strong>1. Introduction</strong></h3> <p>LLMs have demonstrated exceptional performance in various domains but are extremely expensive to train.<br>
The cost of training models like <strong>GPT-3 (175B) or PaLM (540B)</strong>  is enormous, requiring <strong>thousands of GPUs or TPUs</strong> .<br>
Low-precision training is a <strong>promising solution</strong>  as it:</p> <ul><li><strong>Increases speed</strong></li> <li><strong>Reduces memory usage</strong></li> <li><strong>Minimizes communication overhead</strong></li></ul> <p>Most existing frameworks, such as <strong>Megatron-LM, MetaSeq, and Colossal-AI</strong> , use <strong>FP32, FP16, or BF16 mixed-precision training</strong> , but <strong>FP8 offers significant efficiency gains</strong> :</p> <ul><li><strong>2× speed-up</strong></li> <li><strong>50%-75% memory and communication savings</strong></li></ul> <h4 id="challenges-of-fp8-training"><a href="#challenges-of-fp8-training" class="header-anchor">#</a> <strong>Challenges of FP8 Training</strong></h4> <ol><li><strong>Data underflow/overflow issues</strong>  due to FP8’s limited dynamic range.</li> <li><strong>Numerical instabilities and divergence</strong>  during training.</li></ol> <h4 id="proposed-fp8-mixed-precision-framework"><a href="#proposed-fp8-mixed-precision-framework" class="header-anchor">#</a> <strong>Proposed FP8 Mixed-Precision Framework</strong></h4> <ul><li>Introduces <strong>three levels of FP8 utilization</strong>  (gradients, optimizer states, and distributed learning).</li> <li>Uses <strong>precision decoupling</strong>  and <strong>automatic scaling</strong>  to mitigate numerical instability.</li> <li>Achieves <strong>29%-39% memory savings</strong>  and <strong>63%-65% communication cost reductions</strong> .</li></ul> <h3 id="_2-fp8-llm-training"><a href="#_2-fp8-llm-training" class="header-anchor">#</a> <strong>2. FP8 LLM Training</strong></h3> <h4 id="_2-1-fp8-gradient-and-all-reduce-communication"><a href="#_2-1-fp8-gradient-and-all-reduce-communication" class="header-anchor">#</a> <strong>2.1 FP8 Gradient and All-Reduce Communication</strong></h4> <ul><li>Traditional mixed-precision training uses <strong>FP16/FP32 for gradients</strong> , leading to high communication costs.</li> <li>Applying <strong>FP8 directly to gradients</strong>  results in <strong>loss of accuracy</strong>  due to underflow/overflow.</li> <li>The paper proposes an <strong>automatic scaling technique</strong>  to adapt scaling factors dynamically, preventing numerical instability.</li></ul> <h4 id="_2-2-fp8-optimizer"><a href="#_2-2-fp8-optimizer" class="header-anchor">#</a> <strong>2.2 FP8 Optimizer</strong></h4> <ul><li>The <strong>Adam optimizer</strong>  typically consumes <strong>16 bytes per parameter</strong>  due to high-precision storage of gradients and optimizer states.</li> <li>The proposed <strong>FP8 optimizer</strong>  stores:
<ul><li>FP8 first-order moment</li> <li>FP16 master weights (with tensor scaling)</li> <li>FP16 second-order moment</li></ul></li> <li>This reduces <strong>memory consumption from 16 bytes to 6 bytes per parameter</strong>  (2.6× savings).</li></ul> <blockquote><p>My main takeaway is that direction of gradient matters, instead of magnitude.</p></blockquote> <p><img src="https://github.com/user-attachments/assets/aa8bdd9a-c5ea-4910-bfdf-836eca32f9b5" alt="image"></p> <h4 id="_2-3-fp8-distributed-parallel-training"><a href="#_2-3-fp8-distributed-parallel-training" class="header-anchor">#</a> <strong>2.3 FP8 Distributed Parallel Training</strong></h4> <ul><li><strong>Tensor Parallelism</strong> : Uses <strong>FP8 for weight and activation tensors</strong> , reducing compute and communication overhead.</li> <li><strong>Sequence Parallelism</strong> : Converts activation tensors to <strong>FP8 before communication</strong> , reducing costs.</li> <li><strong>ZeRO (Zero Redundancy Optimizer) Support</strong> : Distributes <strong>full tensors</strong>  across devices while preserving <strong>FP8 scaling factors</strong> .</li></ul> <h3 id="_3-experimentation"><a href="#_3-experimentation" class="header-anchor">#</a> <strong>3. Experimentation</strong></h3> <h4 id="_3-1-experimental-setup"><a href="#_3-1-experimental-setup" class="header-anchor">#</a> <strong>3.1 Experimental Setup</strong></h4> <ul><li><strong>Training Dataset</strong> : Collected from <strong>CommonCrawl, The Pile, C4, OpenWebText, Wikipedia, RedPajama</strong> , and other curated sources.</li> <li><strong>Model Configuration</strong> : Uses a <strong>decoder-only Transformer</strong>  architecture (like GPT-3), with <strong>RoPE embeddings and Flash Attention</strong> .</li></ul> <h4 id="_3-2-main-results"><a href="#_3-2-main-results" class="header-anchor">#</a> <strong>3.2 Main Results</strong></h4> <h5 id="model-performance"><a href="#model-performance" class="header-anchor">#</a> <strong>Model Performance</strong></h5> <ul><li><strong>Loss curves of FP8 models match BF16 models</strong> , confirming <strong>accuracy preservation</strong></li> <li><strong>Zero-shot evaluations</strong>  on <strong>Lambada, HellaSwag, BoolQ, PIQA, COPA</strong>  show <strong>comparable performance between FP8 and BF16</strong> .</li> <li><strong>Fine-tuning (SFT &amp; RLHF)</strong> : FP8 achieves:
<ul><li>27% faster fine-tuning</li> <li>32% reduction in model weight memory</li> <li>62% optimizer state memory savings</li></ul></li></ul> <p><strong>System Performance</strong></p> <ul><li><strong>Memory reduction</strong> : FP8 achieves <strong>28%-39% lower memory usage</strong>  than BF16.</li> <li><strong>Training speed improvement</strong> :
<ul><li>75% faster training for GPT-175B</li> <li>37% faster than Nvidia Transformer Engine</li></ul></li> <li><strong>Communication efficiency</strong> :
<ul><li>63%-65% reduction in weight gradient communication</li> <li>34% lower activation-related communication costs</li></ul></li></ul> <h4 id="_3-3-ablation-study"><a href="#_3-3-ablation-study" class="header-anchor">#</a> <strong>3.3 Ablation Study</strong></h4> <ul><li><strong>Gradient Scaling</strong> : <strong>Automatic scaling</strong>  reduces <strong>underflow/overflow errors</strong> , improving training stability.</li> <li><strong>Optimizer Precision</strong> :
<ul><li>FP16 master weights outperform FP8 master weights in accuracy preservation.</li> <li>FP8 first-order gradient moment is viable, but FP8 second-order moment leads to divergence.</li></ul></li> <li><strong>Parallelism Optimization</strong> :
<ul><li><strong>FP8 sequence and tensor parallelism</strong>  reduce communication costs by <strong>34%</strong> .</li> <li><strong>FP8 ZeRO</strong>  maintains a balanced GPU memory load while saving memory.</li></ul></li></ul> <h3 id="_4-related-work"><a href="#_4-related-work" class="header-anchor">#</a> <strong>4. Related Work</strong></h3> <ul><li><strong>Mixed-Precision Training</strong> : Prior work focused on <strong>FP16/BF16</strong> , but <strong>FP8 remains underexplored</strong> .</li> <li><strong>Low-Precision LLM Training</strong> :
<ul><li><strong>OPT, Bloom, Gopher, Chinchilla</strong>  used <strong>BF16</strong>  for better numerical stability.</li> <li>FP8 support was limited before Nvidia Hopper GPUs.</li> <li>This work provides the <strong>first systematic FP8 training framework</strong>  for <strong>pre-training and fine-tuning LLMs</strong> .</li></ul></li></ul> <h3 id="_5-conclusion"><a href="#_5-conclusion" class="header-anchor">#</a> <strong>5. Conclusion</strong></h3> <ul><li>Introduces a <strong>new FP8 mixed-precision training framework</strong>  with <strong>automatic scaling</strong>  and <strong>precision decoupling</strong> .</li> <li>Achieves <strong>significant reductions in memory, compute, and communication costs</strong> .</li> <li><strong>Maintains model accuracy</strong>  across <strong>GPT models from 125M to 175B parameters</strong> .</li> <li>Demonstrates <strong>versatility</strong>  in pre-training, instruction tuning, and RLHF.</li> <li><strong>Future work</strong>  includes scaling to even larger models, training multi-modal models, and deploying FP8 LLMs on edge devices.</li></ul> <h3 id="key-summary-in-3-sentences"><a href="#key-summary-in-3-sentences" class="header-anchor">#</a> <em>Key Summary in 3 Sentences</em>*</h3> <p>This paper introduces an <strong>FP8 mixed-precision training framework</strong>  that reduces memory consumption by <strong>39%</strong> , speeds up training by <strong>75%</strong> , and <strong>outperforms Nvidia Transformer Engine by 37%</strong>  while maintaining LLM accuracy.<br>
The framework uses <strong>automatic scaling and precision decoupling</strong>  to stabilize training, supports <strong>FP8 optimizers and distributed training</strong> , and generalizes to <strong>fine-tuning and reinforcement learning with human feedback (RLHF)</strong> .<br>
These findings establish <strong>FP8 as the next-generation precision format for training LLMs</strong> , significantly lowering costs while preserving model performance.</p> <hr> <h2 id="_139-fp8-formats-for-deep-learning"><a href="#_139-fp8-formats-for-deep-learning" class="header-anchor">#</a> [139] FP8 Formats for Deep Learning</h2> <h3 id="_1-introduction-2"><a href="#_1-introduction-2" class="header-anchor">#</a> <strong>1. Introduction</strong></h3> <p>Deep learning models require increasing computational resources, necessitating lower-precision formats for efficiency.</p> <p>FP8 is a <strong>natural evolution</strong>  from FP16 and BF16, reducing <strong>compute and memory costs</strong>  while maintaining <strong>accuracy comparable to FP16</strong> .</p> <h3 id="key-contributions"><a href="#key-contributions" class="header-anchor">#</a> Key contributions:</h3> <ul><li><p><strong>Two FP8 formats:</strong></p> <ul><li><strong>E4M3</strong> : 4-bit exponent, 3-bit mantissa (for weights and activations).</li> <li><strong>E5M2</strong> : 5-bit exponent, 2-bit mantissa (for gradients).</li></ul></li> <li><p><strong>Training and inference in FP8</strong>  match FP16/BF16 accuracy across CNNs, RNNs, and Transformers.</p></li> <li><p><strong>Post-training quantization (PTQ)</strong>  using FP8 <strong>outperforms int8</strong>  while preserving model accuracy.</p></li></ul> <h3 id="_2-aspects-of-fp8-usage-in-deep-learning"><a href="#_2-aspects-of-fp8-usage-in-deep-learning" class="header-anchor">#</a> <strong>2. Aspects of FP8 Usage in Deep Learning</strong></h3> <ul><li><strong>FP8 computations</strong>  will be performed in <strong>higher precision (FP16/FP32)</strong> , with final results cast back to FP8.</li> <li><strong>Scaling factors</strong>  are applied to <strong>optimize FP8 precision</strong> , similar to <strong>loss-scaling in FP16 mixed precision</strong> .</li> <li><strong>Handling of special values (NaNs, Infs) is modified in E4M3</strong>  to increase dynamic range.</li></ul> <h3 id="_3-fp8-binary-interchange-format"><a href="#_3-fp8-binary-interchange-format" class="header-anchor">#</a> <strong>3. FP8 Binary Interchange Format</strong></h3> <p><img src="https://github.com/user-attachments/assets/dc054ba5-6892-4c63-888e-ef24c9789d8a" alt="image"></p> <p>FP8 includes <strong>two encodings</strong> :</p> <ul><li><p><strong>E4M3</strong> :</p> <ul><li><strong>Used for weights and activations</strong> .</li> <li><strong>No representation for infinities</strong>  (max value: <strong>448</strong> ).</li> <li><strong>Single NaN representation to extend range</strong> .</li></ul></li> <li><p><strong>E5M2</strong> :</p> <ul><li><strong>Used for gradients</strong> .</li> <li><strong>Standard IEEE-like format</strong> , supporting <strong>NaNs and infinities</strong> .</li> <li>Larger range (up to <strong>57,344</strong> ).</li></ul></li></ul> <p><img src="https://github.com/user-attachments/assets/ac2dbf07-fd59-4902-a55b-0bb4b0385408" alt="image"></p> <h4 id="_3-1-special-value-representations"><a href="#_3-1-special-value-representations" class="header-anchor">#</a> <strong>3.1 Special Value Representations</strong></h4> <ul><li><strong>E4M3 removes infinities</strong>  and limits NaNs to a <strong>single pattern</strong> , extending its <strong>dynamic range</strong> .</li> <li><strong>E5M2 follows IEEE-754</strong> , allowing <strong>straightforward conversion from FP16</strong> .</li></ul> <h4 id="_3-2-exponent-bias"><a href="#_3-2-exponent-bias" class="header-anchor">#</a> <strong>3.2 Exponent Bias</strong></h4> <ul><li><strong>E4M3 bias = 7, E5M2 bias = 15</strong>  (matching IEEE-style representation).</li> <li>Some models require <strong>per-tensor scaling</strong>  rather than a fixed exponent bias (Figure 2).</li></ul> <h3 id="_4-empirical-results"><a href="#_4-empirical-results" class="header-anchor">#</a> <strong>4. Empirical Results</strong></h3> <h4 id="_4-1-training"><a href="#_4-1-training" class="header-anchor">#</a> <strong>4.1 Training</strong></h4> <ul><li>FP8 training achieves <strong>accuracy comparable to FP16/BF16</strong>  across CNNs, RNNs, and Transformers.</li> <li><strong>Image Classification</strong> :
<ul><li>FP8 accuracy is <strong>within statistical variation</strong>  of FP16 for most CNNs (ResNet, MobileNet, VGG, etc.).</li></ul></li> <li><strong>Language Translation</strong> :
<ul><li>FP8 BLEU scores <strong>match FP16</strong>  for Transformer and GNMT models.</li></ul></li> <li><strong>NLP Models (Table 4, Figure 1)</strong> :
<ul><li>GPT models (126M to 175B parameters) trained in FP8 <strong>match FP16 in perplexity</strong> .</li></ul></li></ul> <h4 id="_4-2-inference"><a href="#_4-2-inference" class="header-anchor">#</a> <strong>4.2 Inference</strong></h4> <ul><li><strong>FP8 post-training quantization (PTQ) outperforms int8</strong> , retaining <strong>full precision accuracy</strong>  for:
<ul><li>BERT (F1 score on SQuAD).</li> <li>GPT-3 (perplexity on Wikitext103).</li></ul></li> <li><strong>FP8-trained models require no additional quantization steps</strong> , simplifying deployment.</li></ul> <h4 id="_4-3-per-tensor-scaling"><a href="#_4-3-per-tensor-scaling" class="header-anchor">#</a> <strong>4.3 Per-Tensor Scaling</strong></h4> <ul><li><strong>Fixed exponent bias fails</strong>  when additional tensors (e.g., residuals) are stored in FP8.</li> <li><strong>Per-tensor scaling maintains accuracy</strong> , making FP8 viable for <strong>expanded use beyond GEMMs</strong> .</li></ul> <h3 id="_5-conclusions"><a href="#_5-conclusions" class="header-anchor">#</a> <strong>5. Conclusions</strong></h3> <ul><li><strong>FP8 formats (E4M3, E5M2) efficiently reduce training and inference costs while maintaining accuracy</strong> .</li> <li><strong>FP8 training is on par with FP16/BF16</strong> , without hyperparameter changes.</li> <li><strong>FP8 simplifies inference</strong>  by eliminating the need for quantization-aware training (QAT) required for int8.</li> <li><strong>Future work</strong> : Expanding FP8 usage to <strong>more tensor types and operations</strong>  beyond matrix multiplications.</li></ul> <h3 id="key-takeaways-in-three-sentences-2"><a href="#key-takeaways-in-three-sentences-2" class="header-anchor">#</a> <strong>Key Takeaways in Three Sentences</strong></h3> <p>FP8 formats (E4M3 for weights/activations, E5M2 for gradients) <strong>significantly reduce computation and memory overhead</strong>  while maintaining <strong>accuracy equivalent to FP16/BF16</strong>  across CNNs, RNNs, and Transformer models.</p> <p><strong>Post-training quantization (PTQ) with FP8 outperforms int8</strong> , allowing for <strong>simpler and more effective deployment</strong>  of trained models. The study <strong>validates FP8 training up to 175B parameters</strong> , proving its scalability for large-scale deep learning applications.</p> <hr> <h2 id="y2025-paretoq-scaling-laws-in-extremely-low-bit-llm-quantization"><a href="#y2025-paretoq-scaling-laws-in-extremely-low-bit-llm-quantization" class="header-anchor">#</a> [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization</h2> <h3 id="abstract-3"><a href="#abstract-3" class="header-anchor">#</a> <strong>Abstract</strong></h3> <p>The study introduces <strong>ParetoQ</strong> , a unified framework for evaluating extremely low-bit quantization (1-bit to 4-bit) in large language models (LLMs).<br>
It identifies a <strong>learning transition</strong>  between 2-bit and 3-bit models, with 3-bit and higher retaining pre-trained distributions, while 2-bit and below undergo drastic representation changes.<br>
By optimizing training strategies and quantization functions, <strong>ParetoQ achieves state-of-the-art (SOTA) performance</strong>  across multiple bit-widths.<br>
Notably, a <strong>ternary (1.58-bit) 600M model surpasses a previous 3B ternary model</strong> , using only one-fifth of the parameters.<br>
The study finds that <strong>2-bit quantization offers a superior balance between accuracy, model size, and hardware efficiency</strong>  compared to 4-bit and binary quantization.</p> <h3 id="_1-introduction-3"><a href="#_1-introduction-3" class="header-anchor">#</a> <strong>1. Introduction</strong></h3> <p>As models scale, lower-precision computation is gaining traction due to <strong>memory savings and computational efficiency</strong> .<br>
Prior studies suggest conflicting optimal quantization levels (1.58-bit, 2-bit, 4-bit, etc.), but <strong>no unified framework existed</strong>  to systematically compare their effectiveness.</p> <h3 id="key-questions"><a href="#key-questions" class="header-anchor">#</a> <strong>Key Questions:</strong></h3> <ul><li>What is the optimal trade-off between bit-width and model size?</li> <li>How does quantization impact <strong>scaling laws</strong>  in low-bit settings?</li> <li>What training strategies and quantization functions yield <strong>Pareto-optimal results</strong> ?</li></ul> <h3 id="paretoq-approach"><a href="#paretoq-approach" class="header-anchor">#</a> <strong>ParetoQ Approach:</strong></h3> <ul><li>Incorporates <strong>five key dimensions</strong> : model size (<strong>N</strong> ), token count (<strong>D</strong> ), quantization precision (<strong>P</strong> ), training strategy (<strong>S</strong> ), and quantization function (<strong>F</strong> ).</li> <li>Identifies <strong>bit-specific training schemes and quantization functions</strong> .</li> <li>Establishes that <strong>binary quantization significantly degrades accuracy</strong> , while <strong>ternary (1.58-bit), 2-bit, and 3-bit quantization match or exceed 4-bit performance</strong> .</li></ul> <h3 id="_2-a-better-qat-scheduling-strategy-for-extreme-low-bit-llms"><a href="#_2-a-better-qat-scheduling-strategy-for-extreme-low-bit-llms" class="header-anchor">#</a> <strong>2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs</strong></h3> <h4 id="_2-1-training-budget-allocation"><a href="#_2-1-training-budget-allocation" class="header-anchor">#</a> <strong>2.1 Training Budget Allocation</strong></h4> <ul><li><strong>Post-Training Quantization (PTQ)</strong>  is easier to implement but <strong>performs poorly</strong>  below 4-bit.</li> <li><strong>Quantization-Aware Training (QAT)</strong>  integrates quantization during training, <strong>improving low-bit performance</strong> .</li> <li><strong>Optimal budget split:</strong> <strong>90% full-precision training, 10% QAT fine-tuning</strong> .</li> <li><strong>Finding-1:</strong> <strong>QAT fine-tuning outperforms both PTQ and QAT from scratch</strong> , achieving the best trade-off between accuracy and efficiency.</li></ul> <h4 id="_2-2-fine-tuning-characteristics"><a href="#_2-2-fine-tuning-characteristics" class="header-anchor">#</a> <strong>2.2 Fine-tuning Characteristics</strong></h4> <ul><li>Fine-tuning <strong>improves accuracy across all bit-widths</strong> , including binary and ternary models.</li> <li><strong>Lower-bit models (≤2-bit) require more fine-tuning (30B tokens), while 3-bit and 4-bit saturate at 10B tokens</strong> .</li> <li><strong>Finding-2:</strong> <strong>Bit-width transition effect:</strong> <ul><li><strong>3-bit &amp; 4-bit recover near full precision with fine-tuning</strong> .</li> <li><strong>1-bit to 2-bit undergo substantial weight transformations</strong> , requiring more tokens.</li> <li>QAT serves as &quot;compensation&quot; for 3-bit+ but &quot;reconstruction&quot; for ≤2-bit models.</li></ul></li></ul> <h3 id="_3-a-hitchhiker-s-guide-to-quantization-method-choices"><a href="#_3-a-hitchhiker-s-guide-to-quantization-method-choices" class="header-anchor">#</a> <strong>3. A Hitchhiker’s Guide to Quantization Method Choices</strong></h3> <h4 id="_3-1-trade-offs-in-low-bit-quantization"><a href="#_3-1-trade-offs-in-low-bit-quantization" class="header-anchor">#</a> <strong>3.1 Trade-offs in Low-bit Quantization</strong></h4> <ol><li><strong>Range Clipping</strong> : Lower-bit quantization suffers from outlier effects, requiring range clipping or <strong>learnable scales</strong> .</li> <li><strong>Quantization Grids</strong> :</li></ol> <ul><li>Binary &amp; Ternary require balanced levels.</li> <li>2-bit prefers symmetric distribution*.</li> <li>3-bit and 4-bit benefit from including &quot;0&quot; in quantization levels.</li></ul> <h4 id="_3-2-introducing-paretoq"><a href="#_3-2-introducing-paretoq" class="header-anchor">#</a> <strong>3.2 Introducing ParetoQ</strong></h4> <ul><li><strong>Combines the best quantization functions per bit-width</strong> :
<ul><li><strong>1-bit</strong> : Elastic Binarization.</li> <li><strong>1.58-bit, 2-bit</strong> : <strong>Stretched Elastic Quant (SEQ)</strong> .</li> <li><strong>3-bit, 4-bit</strong> : <strong>Learned Step Size Quantization (LSQ)</strong> .</li></ul></li> <li><strong>Finding-3:</strong>  No single best function for all bit-widths.
<strong>Learnable range settings outperform fixed statistical methods</strong> , especially for <strong>sub-4-bit quantization</strong> .</li></ul> <h3 id="_4-pareto-optimality-of-extremely-low-bit-llms"><a href="#_4-pareto-optimality-of-extremely-low-bit-llms" class="header-anchor">#</a> <strong>4. Pareto-Optimality of Extremely Low-Bit LLMs</strong></h3> <h4 id="_4-1-accuracy-compression-trade-off"><a href="#_4-1-accuracy-compression-trade-off" class="header-anchor">#</a> <strong>4.1 Accuracy-Compression Trade-off</strong></h4> <ul><li>Ternary (1.58-bit), 2-bit, and 3-bit outperform 4-bit in accuracy-size efficiency.</li> <li>2-bit and ternary quantization sit on the Pareto frontier.</li></ul> <h4 id="_4-2-hardware-constraints"><a href="#_4-2-hardware-constraints" class="header-anchor">#</a> <strong>4.2 Hardware Constraints</strong></h4> <ul><li>Ternary (1.58-bit) appears efficient but is difficult to implement due to indexing overhead.</li> <li>2-bit is more hardware-friendly**  due to **simpler storage and arithmetic operations.</li></ul> <h4 id="_4-3-accuracy-speed-trade-off"><a href="#_4-3-accuracy-speed-trade-off" class="header-anchor">#</a> <strong>4.3 Accuracy-Speed Trade-off</strong></h4> <ul><li>2-bit achieves higher speed at the same accuracy as 4-bit.</li> <li>2-bit kernels are significantly faster than 4-bit kernels in large matrix multiplications.</li></ul> <h3 id="_6-related-work"><a href="#_6-related-work" class="header-anchor">#</a> <strong>6. Related Work</strong></h3> <ul><li><strong>Early quantization research</strong>  focused on <strong>8-bit and 4-bit LLMs</strong> .</li> <li>Recent <strong>sub-4-bit research</strong>  explored <strong>ternary, 2-bit, and 1-bit models</strong> , but lacked a <strong>unified comparison framework</strong> .</li> <li><strong>ParetoQ is the first study to systematically compare sub-4-bit quantization schemes</strong> .</li></ul> <p><strong>7. Conclusions</strong></p> <ul><li>ParetoQ unifies training and quantization schemes across five bit-widths.</li> <li>Ternary (1.58-bit), 2-bit, and 3-bit quantization outperform 4-bit in the accuracy-size trade-off.</li> <li>2-bit is the most practical choice due to its hardware efficiency.</li> <li>First framework that ensures fair comparisons across different quantization methods.</li></ul> <p><strong>Key Takeaways (3 Sentences)</strong></p> <ol><li>ParetoQ introduces a unified framework for extreme low-bit quantization, identifying 2-bit as the most efficient trade-off between accuracy, memory, and computational speed.</li> <li>A clear transition exists between 2-bit and 3-bit models, where lower-bit quantization requires substantial fine-tuning to compensate for drastic weight transformations.</li> <li>With its optimized quantization functions and training schemes, ParetoQ achieves state-of-the-art performance across all bit-widths, surpassing previous specialized methods.</li></ol> <hr> <h2 id="_47-microscaling-data-formats-for-deep-learning"><a href="#_47-microscaling-data-formats-for-deep-learning" class="header-anchor">#</a> [47] Microscaling Data Formats for Deep Learning</h2> <p><img src="https://github.com/user-attachments/assets/e1533c6c-54ca-47f8-946a-2d6fe7d08aef" alt="image"></p> <h3 id="introduction-2"><a href="#introduction-2" class="header-anchor">#</a> <strong>Introduction</strong></h3> <p>The rapid advancement of deep learning models has led to increased computational and storage costs.<br>
One approach to mitigating these costs is reducing bit-width precision in data formats, moving beyond traditional <strong>FP32</strong>  to lower-bit formats such as <strong>FP16, BFloat16, FP8, and INT8</strong>.<br>
However, per-tensor scaling in low-bit-width formats struggles with dynamic range limitations.<br> <strong>Microscaling (MX) data formats</strong>  introduce <strong>per-block scaling factors</strong>  to enhance efficiency, maintain accuracy, and ease deployment in AI hardware.</p> <h3 id="microscaling-mx-data-formats"><a href="#microscaling-mx-data-formats" class="header-anchor">#</a> <strong>Microscaling (MX) Data Formats</strong></h3> <p>MX formats encode numerical values in <strong>fixed-size blocks</strong> , where each block consists of:</p> <ul><li>A <strong>shared scaling factor (X)</strong></li> <li>Multiple <strong>narrow bit-width elements (Pi)</strong></li></ul> <p>This approach <strong>extends the dynamic range</strong>  beyond what per-tensor scaling allows, making sub-8-bit computations feasible.<br>
MX formats are <strong>hardware-efficient</strong>  while minimizing accuracy loss and <strong>ensuring seamless adoption in existing AI frameworks</strong>.</p> <h3 id="concrete-mx-formats"><a href="#concrete-mx-formats" class="header-anchor">#</a> <strong>Concrete MX Formats</strong></h3> <p>MX formats are categorized based on <strong>block size, scale format, and element bit-width</strong>.</p> <table><thead><tr><th>Format</th> <th>Block Size</th> <th>Scale Data Format</th> <th>Scale Bits</th> <th>Element Format</th> <th>Element Bit-width</th></tr></thead> <tbody><tr><td>MXFP8</td> <td>32</td> <td>E8M0</td> <td>8</td> <td>FP8 (E4M3/E5M2)</td> <td>8</td></tr> <tr><td>MXFP6</td> <td>32</td> <td>E8M0</td> <td>8</td> <td>FP6 (E2M3/E3M2)</td> <td>6</td></tr> <tr><td>MXFP4</td> <td>32</td> <td>E8M0</td> <td>8</td> <td>FP4 (E2M1)</td> <td>4</td></tr> <tr><td>MXINT8</td> <td>32</td> <td>E8M0</td> <td>8</td> <td>INT8</td> <td>8</td></tr></tbody></table> <h3 id="scalar-float-to-mx-format-conversion"><a href="#scalar-float-to-mx-format-conversion" class="header-anchor">#</a> <strong>Scalar Float to MX Format Conversion</strong></h3> <p>To convert floating-point data to an MX format, the <strong>shared scaling factor (X)</strong>  is computed based on the largest absolute value in a block.<br>
Each element is then <strong>normalized using X and quantized</strong>  to the desired format. The conversion follows a <strong>quantization algorithm</strong>  that:</p> <ol><li><strong>Determines the scaling exponent</strong>  from the maximum value in the block.</li> <li><strong>Computes X as a power of two</strong> .</li> <li><strong>Quantizes elements (Pi) based on X</strong> .</li></ol> <h3 id="compute-flow-and-training-pipeline"><a href="#compute-flow-and-training-pipeline" class="header-anchor">#</a> <strong>Compute Flow and Training Pipeline</strong></h3> <p>For deep learning workloads, <strong>dot-product operations (e.g., matrix multiplication, convolutions) are performed in MX formats</strong> , while <strong>non-dot operations (e.g., activations, normalization, residual add) remain in higher-precision formats like FP32 or BFloat16</strong> .<br>
Training involves keeping a <strong>master FP32 copy of weights</strong>  while performing compute-intensive operations in MX formats.<br> <strong>Quantization-aware fine-tuning</strong>  is often required for best accuracy, particularly for lower-bit formats like MXFP6 and MXFP4.</p> <p><strong>Experimental Results</strong> <strong>Inference</strong> MX data formats were tested across <strong>language models, vision transformers, speech recognition, and recommendation models</strong>.</p> <p><strong>Direct-Cast Inference (No Fine-Tuning)</strong></p> <ul><li><strong>MXINT8 performs nearly identically to FP32</strong>  across all tasks, making it a <strong>drop-in replacement</strong> .</li> <li><strong>MXFP8 and MXFP6 maintain good accuracy</strong> , but MXFP6 requires fine-tuning for best results.</li> <li><strong>MXFP4 suffers from significant accuracy loss</strong> , especially in complex models.</li></ul> <p><strong>Post-Training Quantization (PTQ) with Error Diffusion</strong></p> <ul><li>Error diffusion (similar to <strong>GPFQ-based post-training quantization</strong> ) helps recover accuracy.</li> <li><strong>MXFP6 achieves results close to FP32</strong>  after error diffusion.</li> <li><strong>MXFP4 remains significantly worse than FP32, limiting its practical use in inference</strong> .</li></ul> <p><strong>Finetuned Inference</strong></p> <ul><li><strong>MXFP6 achieves FP32-level accuracy after fine-tuning</strong> .</li> <li><strong>MXFP4 improves slightly but still lags behind</strong> .</li> <li>MXINT8 continues to serve as the most effective <strong>low-friction alternative to FP32</strong> .</li></ul> <p><strong>Generative Model Inference (GPT-3, LLaMA)</strong></p> <ul><li><strong>MXINT8 closely matches FP32 performance</strong>  on GPT-3 and LLaMA.</li> <li><strong>MXFP6 and MXFP8 perform well in most tasks</strong> , but some degradation is observed in complex benchmarks.</li> <li><strong>MXFP4 shows noticeable loss</strong> , especially in <strong>zero-shot settings</strong> .</li></ul> <p><strong>Training with MX Formats</strong>
For the <strong>first time, MX formats enable sub-8-bit training of large-scale transformers</strong>  with minimal accuracy loss.</p> <p><strong>Training with MXFP6</strong></p> <ul><li><strong>MXFP6 (E3M2) trains models with no accuracy drop compared to FP32</strong> .</li> <li>This represents the <strong>first demonstration of 6-bit training for large transformer models</strong>  without modifications to the training recipe.</li> <li><strong>Hyperparameters remain unchanged from FP32 training</strong>, making MXFP6 a practical choice.</li></ul> <p><strong>Training with MXFP4 + MXFP6</strong></p> <ul><li><strong>MXFP4 weights combined with MXFP6 activations/gradients</strong>  yield slightly worse performance but remain viable for training.</li> <li><strong>Loss curves show only a minor increase in training loss</strong> , proving feasibility.</li></ul> <h3 id="conclusion-2"><a href="#conclusion-2" class="header-anchor">#</a> <strong>Conclusion</strong></h3> <p>Microscaling (MX) data formats introduce <strong>per-block scaling</strong>  to <strong>reduce bit-width</strong>  while maintaining <strong>high accuracy, hardware efficiency, and seamless integration</strong> .</p> <h3 id="key-findings"><a href="#key-findings" class="header-anchor">#</a> Key findings:</h3> <ol><li>MXINT8 is an effective drop-in replacement for FP32 inference.</li> <li>MXFP6 enables sub-8-bit inference and training with minimal accuracy loss.</li> <li>MXFP4 combined with MXFP6 remains viable for training but suffers in inference.</li> <li>First-ever demonstration of training large generative models using 6-bit weights, activations, and gradients** .
MX formats offer a compelling path toward <strong>lower precision deep learning</strong>  without sacrificing model quality.</li></ol> <h3 id="three-sentence-summary"><a href="#three-sentence-summary" class="header-anchor">#</a> <strong>Three-Sentence Summary</strong></h3> <p>Microscaling (MX) data formats introduce <strong>per-block scaling factors</strong> , improving the efficiency and accuracy of sub-8-bit deep learning computations.</p> <p><strong>MXINT8 serves as a near-lossless drop-in replacement for FP32 inference</strong> , while <strong>MXFP6 enables large-scale deep learning models to be trained with sub-8-bit precision without altering training recipes</strong> .</p> <p>The results demonstrate that <strong>MX formats significantly reduce computational and storage costs while maintaining model performance</strong> , making them a strong alternative to traditional floating-point formats.</p> <hr> <h2 id="_15-35-fp8-versus-int8-for-efficient-deep-learning-inference"><a href="#_15-35-fp8-versus-int8-for-efficient-deep-learning-inference" class="header-anchor">#</a> 15. [35] FP8 versus INT8 for efficient deep learning inference</h2> <p>This paperexplores the efficiency and accuracy of using FP8 (8-bit floating point) and INT8 (8-bit integer) formats for deep learning inference, particularly on edge devices.</p> <p>The key points and findings of the paper are summarized as follows:</p> <h3 id="_1-introduction-and-motivation"><a href="#_1-introduction-and-motivation" class="header-anchor">#</a> 1. Introduction and Motivation</h3> <p>The paper discusses the growing interest in using FP8 for neural network training, especially with Nvidia's introduction of FP8 in their Hopper architecture GPUs.<br>
While FP8 is being considered for training, the paper focuses on its implications for efficient inference on edge devices, where INT8 is commonly used due to its efficiency.<br>
The authors question whether training networks in FP8 and deploying them in the same format could bypass the need for quantization, which is currently required when converting FP32/FP16 models to INT8.</p> <h3 id="_2-hardware-considerations"><a href="#_2-hardware-considerations" class="header-anchor">#</a> 2. Hardware Considerations</h3> <p>The paper argues that FP8 is less efficient than INT8 in terms of hardware area and energy consumption. FP8 requires 50% more area and energy compared to INT8, making it less suitable for efficient inference.<br>
Floating-point operations are inherently more complex and costly in hardware compared to integer operations, especially when considering the need for floating-point accumulators.<br>
The authors highlight that FP8 implementations often involve mixed precision (e.g., FP16 for activations), which can lead to inefficiencies, particularly in networks with large activation tensors.</p> <h3 id="_3-accuracy-comparison"><a href="#_3-accuracy-comparison" class="header-anchor">#</a> 3. Accuracy Comparison</h3> <p>The paper provides a theoretical and empirical comparison of FP8 and INT8 formats in terms of network accuracy.<br>
The key difference between FP8 and INT8 lies in their ability to handle outliers. FP8, with its exponent bits, can better represent outliers, while INT8 is more efficient for well-behaved, Gaussian-like distributions.<br>
In post-training quantization (PTQ), INT8 generally performs better for networks without significant outliers, while FP8-E4 (4 exponent bits) is better for networks with outliers, such as transformers.<br>
In quantization-aware training (QAT), INT8 often outperforms FP8, as training can reduce the impact of outliers, making INT8 more accurate and efficient.</p> <h3 id="_4-transformer-networks"><a href="#_4-transformer-networks" class="header-anchor">#</a> 4. Transformer Networks</h3> <p>Transformer networks, particularly BERT, exhibit significant outliers in certain layers, making FP8-E4 more accurate in PTQ settings.</p> <p>However, the paper argues that these outlier issues can be mitigated with techniques like mixed precision (W8A16) or quantization-aware training, allowing INT8 to achieve similar accuracy without the hardware inefficiencies of FP8.</p> <h3 id="_5-comparison-to-other-work"><a href="#_5-comparison-to-other-work" class="header-anchor">#</a> 5. Comparison to Other Work</h3> <p>The authors compare their findings with other works, such as those from Nvidia, Arm, and Intel, and Graphcore, which also explore FP8 for training.</p> <p>They find that their results are consistent with these works but provide a more comprehensive comparison between FP8 and INT8.</p> <p>The paper highlights that other works often omit critical comparisons, such as the hardware efficiency of FP8 versus INT8, and the impact of mixed precision on inference performance.</p> <h3 id="_6-fp8-to-int8-conversion"><a href="#_6-fp8-to-int8-conversion" class="header-anchor">#</a> 6. FP8 to INT8 Conversion</h3> <p>The paper explores the feasibility of converting FP8-trained networks to INT8.</p> <p>For networks without significant outliers, the conversion is straightforward and can even improve accuracy.</p> <p><strong>For networks with outliers, such as transformers, the conversion to INT8 may degrade accuracy, but this can be mitigated with quantization-aware training.</strong></p> <h3 id="_7-int-quantization-paradigm"><a href="#_7-int-quantization-paradigm" class="header-anchor">#</a> 7. INT Quantization Paradigm</h3> <p>The authors advocate for the use of INT8 and INT4 formats for efficient inference, as they offer better hardware efficiency and accuracy for most networks.</p> <p>They present a quantization paradigm where INT16 is used for high accuracy, INT8 for most networks, and INT4 for further efficiency, especially in weight-bound networks like large language models.</p> <h3 id="_8-conclusion"><a href="#_8-conclusion" class="header-anchor">#</a> 8. Conclusion</h3> <p>The paper concludes that FP8 is not a suitable replacement for INT8 in efficient deep learning inference.</p> <p>While FP8 can handle outliers better in certain cases, the hardware inefficiencies and the availability of techniques to mitigate outlier issues in INT8 make it a less attractive option.</p> <p>The authors recommend using INT8 and INT4 formats for efficient on-device inference, as they provide the best trade-off between accuracy and efficiency.</p> <h3 id="key-takeaways"><a href="#key-takeaways" class="header-anchor">#</a> Key Takeaways:</h3> <p>FP8 is less efficient than INT8 in terms of hardware area and energy consumption.</p> <p>INT8 is more accurate for most networks, especially after quantization-aware training, which can reduce the impact of outliers.</p> <p><strong>FP8 is only beneficial in specific cases, such as transformer networks with significant outliers, but these issues can be addressed with INT8 using mixed precision or QAT.</strong></p> <p>INT4 and INT8 are recommended for efficient inference, offering a better balance of accuracy and hardware efficiency compared to FP8.</p> <p>Overall, the paper provides a comprehensive analysis of the trade-offs between FP8 and INT8, concluding that INT8 remains the superior choice for efficient deep learning inference on edge devices.</p> <hr> <h2 id="_16-28-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models"><a href="#_16-28-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models" class="header-anchor">#</a> 16. [28] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</h2> <p>This paper, explores the use of low-bit quantization techniques to reduce the size and computational cost of Large Language Models (LLMs) during inference.</p> <p>The authors investigate the effectiveness of both integer (INT) and floating-point (FP) formats for quantization and propose a novel method called Mixture of Formats Quantization (MoFQ) that selects the optimal format (INT or FP) for each layer of the model.</p> <p>The key contributions and findings of the paper are summarized below:</p> <h3 id="_1-introduction-and-motivation-2"><a href="#_1-introduction-and-motivation-2" class="header-anchor">#</a> 1. Introduction and Motivation</h3> <p>Large Language Models (LLMs) are widely used in multimedia applications, but their deployment is challenging due to their large size and high computational cost.</p> <p>Low-bit quantization (e.g., INT8, INT4, FP8, FP4) is a common technique to reduce model size and inference cost.</p> <p>Traditionally, integer formats have been used, but as LLMs grow larger, integer quantization faces challenges in maintaining accuracy.</p> <p>Floating-point formats (e.g., FP8, FP4) are emerging as alternatives, supported by advanced hardware like NVIDIA's H100 GPU.</p> <p>FP formats offer a larger dynamic range and better precision for small values but may have higher hardware costs.</p> <p>The paper aims to compare INT and FP formats for LLM quantization and propose a method to leverage the strengths of both formats.</p> <h3 id="_2-comparative-analysis-of-int-and-fp-formats"><a href="#_2-comparative-analysis-of-int-and-fp-formats" class="header-anchor">#</a> 2. Comparative Analysis of INT and FP Formats</h3> <p>Hardware Efficiency: The authors compare the hardware cost of INT and FP multiply-accumulate (MAC) units across different bit-widths.</p> <p>They find that at 8-bit, the hardware cost of FP8 and INT8 MAC units is <strong>almost identical</strong>, making FP8 a viable alternative.</p> <p>Quantization Error: The authors analyze the quantization error of INT and FP formats on weight and activation tensors from the LLaMA-65B model. They find that:</p> <p>For 4-bit weight quantization, some layers prefer INT4, while others prefer FP4.</p> <p>For 8-bit weight and activation quantization, INT8 is better for weights, while FP8 is better for activations due to its robustness to dynamic ranges.</p> <h3 id="_3-mixture-of-formats-quantization-mofq"><a href="#_3-mixture-of-formats-quantization-mofq" class="header-anchor">#</a> 3. Mixture of Formats Quantization (MoFQ)</h3> <p>Based on the observation that different layers prefer different formats, the authors propose MoFQ, a method that selects the optimal format (INT or FP) for each layer based on quantization error.</p> <p>MoFQ is applicable to both weight-only (W-only) and weight-activation (WA) quantization scenarios:</p> <p>For W-only quantization, MoFQ selects the best format for weight tensors.</p> <p>For WA quantization, the same format is used for both weights and activations in each layer, as current hardware does not support mixed INT8 and FP8 operations.</p> <p>The authors also improve the FP4 format by reallocating NaN and Inf representations to increase precision, reducing quantization errors by about 35%.</p> <h3 id="_4-implementation-and-experiments"><a href="#_4-implementation-and-experiments" class="header-anchor">#</a> 4. Implementation and Experiments</h3> <p>The authors implement an FP/MoFQ-based inference system for W-only quantization, which maintains consistent inference speed compared to INT-based systems.</p> <p>Experiments are conducted on LLaMA and OPT models using datasets like LAMBADA, PIQA, HellaSwag, and WikiText-2.</p> <p>Results:</p> <ul><li>For 8-bit WA quantization, MoFQ8 (a mixture of FP8 and INT8) achieves accuracy close to FP16 models, outperforming both INT8 and FP8 quantization.</li> <li>For 4-bit W-only quantization, MoFQ4 (a mixture of FP4 and INT4) improves quantization accuracy, with most layers preferring FP4.</li> <li>MoFQ-based quantization is faster than traditional methods like GPTQ and AWQ, with no additional hardware overhead.</li></ul> <h3 id="_5-key-contributions"><a href="#_5-key-contributions" class="header-anchor">#</a> 5. Key Contributions</h3> <p>The paper provides a comparative analysis of INT and FP formats for LLM quantization, offering insights into their hardware efficiency and quantization error.</p> <p>It proposes MoFQ, a <strong>layer-wise format selection method that leverages the complementary advantages of INT and FP formats</strong>, achieving state-of-the-art results in both 4-bit W-only and 8-bit WA quantization.</p> <p>The authors implement an FP/MoFQ-based inference system that maintains consistent inference speed with INT-based systems, demonstrating the practicality of their approach.</p> <h3 id="_6-conclusion"><a href="#_6-conclusion" class="header-anchor">#</a> 6. Conclusion</h3> <p>The paper demonstrates that floating-point formats can significantly improve LLM quantization, especially when combined with integer formats through the MoFQ method.</p> <p>The proposed method achieves better or comparable results to existing quantization techniques, with no additional hardware overhead, making it a promising approach for efficient LLM deployment.</p> <p>In summary, this paper introduces a novel quantization method that combines the strengths of integer and floating-point formats, offering a practical solution for reducing the computational cost of large language models while maintaining high accuracy.</p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/15.llm_quant.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/02/10, 06:40:07</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7049/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Reasoning in LLM</div></a> <a href="/qishao-notes/pages/dc7051/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">LLM Sparsity</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7049/" class="prev">Reasoning in LLM</a></span> <span class="next"><a href="/qishao-notes/pages/dc7051/">LLM Sparsity</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.6c40634c.js" defer></script><script src="/qishao-notes/assets/js/2.7441cbb4.js" defer></script><script src="/qishao-notes/assets/js/89.1f746f3e.js" defer></script>
  </body>
</html>
