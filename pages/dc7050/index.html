<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LLM Quantization &amp; Outlier | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.07bb8990.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.7441cbb4.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/89.27e84444.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.c53d023c.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.fc2abc73.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.08c3a542.js"><link rel="prefetch" href="/qishao-notes/assets/js/102.881cf2a1.js"><link rel="prefetch" href="/qishao-notes/assets/js/103.b0057dfd.js"><link rel="prefetch" href="/qishao-notes/assets/js/104.79e52b4d.js"><link rel="prefetch" href="/qishao-notes/assets/js/105.a14c0b58.js"><link rel="prefetch" href="/qishao-notes/assets/js/106.68b02da3.js"><link rel="prefetch" href="/qishao-notes/assets/js/107.f8c2c5fa.js"><link rel="prefetch" href="/qishao-notes/assets/js/108.6daf8339.js"><link rel="prefetch" href="/qishao-notes/assets/js/109.bf4aac26.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.b6f7d42d.js"><link rel="prefetch" href="/qishao-notes/assets/js/110.ddda0100.js"><link rel="prefetch" href="/qishao-notes/assets/js/111.fa3320d8.js"><link rel="prefetch" href="/qishao-notes/assets/js/112.85a5704d.js"><link rel="prefetch" href="/qishao-notes/assets/js/113.875270e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.dc4136f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.a28834fc.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.bfce2e69.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.936b94c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.1c803b2c.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.b688542f.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.ba9d4baf.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.42f21f4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.a7b3ac76.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.1adc4cd7.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.d47318cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.53082807.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.9bd10ddd.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.db192854.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.494aa23f.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.a48cd2d6.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.c0382aef.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.a9c55c08.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.b9f822cc.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.b7a33d18.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.219afea4.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.9db33bb5.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.5feb66b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.84f10398.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.dae4dbf8.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.2bbb98ee.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.ce927bde.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.feae3c28.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.eb741f11.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.1c8ad3d7.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.a4f72e4e.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.c5f2edca.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.7bd3cb20.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.55d450a9.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.5cec2b03.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.df5a29a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.3ce5b7fb.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.8cfabb38.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.f48930bf.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.8e889fd4.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.2b0ef473.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.024560b0.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.710ec0c8.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.2c439da7.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.75040c9a.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.4fddacf2.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.8cbd18e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.e66e714b.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.3abddccb.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.942dd3d6.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.b181d359.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.ddcf1d19.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.fe0350ce.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.3892b158.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.b54aecd8.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.02c5e9c0.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.c1fd8b9c.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.440d8122.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.6e3b44f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.4bfd230c.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.a6e7b8aa.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.6936f79c.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.1e07ff8f.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.b9fa42d0.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.26cb8349.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.13410603.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.a9920e67.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.085d30eb.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.06057897.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.25811434.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.71555750.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.20f35103.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.8c524a54.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.08df02ed.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.37d8e037.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.c7815d38.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.974bd739.js"><link rel="prefetch" href="/qishao-notes/assets/js/83.8503f2c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.9b3fe0ac.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.41b95816.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.6439298a.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.8f9fd9a3.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.e60bd10c.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.7d0e7d9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.a2bc6498.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.a6222cb8.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.1d3da893.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.779c9e29.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.3c6e339b.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.624f71eb.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.693af351.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.09b58e20.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.8751a37c.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.c2b1a1d3.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7043/" class="sidebar-link">Efficient LLM</a></li><li><a href="/qishao-notes/pages/dc7045/" class="sidebar-link">Estimation of LLM</a></li><li><a href="/qishao-notes/pages/dc7046/" class="sidebar-link">Summery of Inner Workings of LLM</a></li><li><a href="/qishao-notes/pages/dc7047/" class="sidebar-link">List of LLM Optimization Techniques</a></li><li><a href="/qishao-notes/pages/dc7048/" class="sidebar-link">Memory Optimizations in LLM</a></li><li><a href="/qishao-notes/pages/dc7049/" class="sidebar-link">Reasoning in LLM</a></li><li><a href="/qishao-notes/pages/dc7050/" aria-current="page" class="active sidebar-link">LLM Quantization &amp; Outlier</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization" class="sidebar-link">1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#problems" class="sidebar-link">Problems</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#solutions" class="sidebar-link">Solutions</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models" class="sidebar-link">2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways-in-three-sentences" class="sidebar-link">Key Takeaways in Three Sentences</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#abstract" class="sidebar-link">Abstract</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#introduction" class="sidebar-link">Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#background-and-related-works" class="sidebar-link">Background and Related Works</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#comparative-analysis-of-int-and-fp-formats" class="sidebar-link">Comparative Analysis of INT and FP Formats</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#exploiting-int-and-fp-complementarity" class="sidebar-link">Exploiting INT and FP Complementarity</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#conclusion" class="sidebar-link">Conclusion</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_26-fp8-lm-training-fp8-large-language-models" class="sidebar-link">[26] FP8-LM: Training FP8 Large Language Models</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#abstract-2" class="sidebar-link">Abstract</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction" class="sidebar-link">1. Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-fp8-llm-training" class="sidebar-link">2. FP8 LLM Training</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-experimentation" class="sidebar-link">3. Experimentation</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-related-work" class="sidebar-link">4. Related Work</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-conclusion" class="sidebar-link">5. Conclusion</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-summary-in-3-sentences" class="sidebar-link">Key Summary in 3 Sentences*</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7050/#_139-fp8-formats-for-deep-learning" class="sidebar-link">[139] FP8 Formats for Deep Learning</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_1-introduction-2" class="sidebar-link">1. Introduction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-contributions" class="sidebar-link">Key contributions:</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_2-aspects-of-fp8-usage-in-deep-learning" class="sidebar-link">2. Aspects of FP8 Usage in Deep Learning</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_3-fp8-binary-interchange-format" class="sidebar-link">3. FP8 Binary Interchange Format</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_4-empirical-results" class="sidebar-link">4. Empirical Results</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#_5-conclusions" class="sidebar-link">5. Conclusions</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7050/#key-takeaways-in-three-sentences-2" class="sidebar-link">Key Takeaways in Three Sentences</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/dc7051/" class="sidebar-link">LLM Sparsity</a></li><li><a href="/qishao-notes/pages/dc7052/" class="sidebar-link">LLM Scaling Law</a></li><li><a href="/qishao-notes/pages/dc7055/" class="sidebar-link">LLM Attention</a></li><li><a href="/qishao-notes/pages/dc7056/" class="sidebar-link">LLM KV Cache Management</a></li><li><a href="/qishao-notes/pages/dc7057/" class="sidebar-link">LLM Distributed Machine Learning</a></li><li><a href="/qishao-notes/pages/dc7058/" class="sidebar-link">LLM Posttraining/Finetuning</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-01-27</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">LLM Quantization &amp; Outlier<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>[139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization</li> <li>[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</li> <li>[26] FP8-LM: Training FP8 Large Language Models 👍 from Microsoft</li> <li>[139] FP8 Formats for Deep Learning</li> <li>[41] With Shared Microexponents, A Little Shifting Goes a Long Way 👍 from Meta, Microsoft 👍</li> <li>[34] Stable and low-precision training for large-scale vision-language models 👍
<em>mentioned in Deepseek paper mixed precision training section. Not read yet.</em></li></ol> <hr> <h2 id="_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization"><a href="#_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization" class="header-anchor">#</a> 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization</h2> <h3 id="problems"><a href="#problems" class="header-anchor">#</a> Problems</h3> <p>The problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:</p> <ul><li>High Dynamic Range of Activations
<ul><li>The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.</li> <li>This means that the values within these tensors vary significantly in magnitude.</li> <li>Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.</li> <li>Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.</li></ul></li> <li>Presence of Structured Outliers
<ul><li>The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).</li> <li>These outliers are not random; they appear to be correlated with specific input tokens and embedding dimensions.</li> <li>Further analysis revealed that these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP]).</li> <li>While this attention behavior might be beneficial for the model's performance, the outliers that cause it also create challenges for quantization.</li></ul></li> <li>Sensitivity to Quantization Noise
<ul><li>Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.</li> <li>Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.</li> <li>This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.</li></ul></li></ul> <h3 id="solutions"><a href="#solutions" class="header-anchor">#</a> Solutions</h3> <p>solutions proposed in the paper:</p> <ul><li><p>Mixed-precision PTQ</p> <ul><li>The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.</li> <li>To address this, they proposed using a higher bit-width (16-bit) for the more sensitive activation tensors, particularly the residual sum after the feed-forward network (FFN).</li> <li>This higher bit-width allows for more accurate representation of both the FFN's input and output, minimizing potential errors.</li> <li>Additionally, they explored using low-bit (2-4) quantization for weights and token embeddings, which can significantly reduce model size without much accuracy loss.</li></ul></li> <li><p>Per-embedding-group PTQ</p> <ul><li>The authors identified that outliers in the activation tensors primarily reside in a few specific embedding dimensions.</li> <li>To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.</li> <li>This method involves splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group.</li> <li>To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.</li> <li>This approach effectively handles outliers without significantly increasing computational overhead.</li></ul></li> <li><p>Quantization-aware training (QAT)</p> <ul><li>The authors also explored QAT, where the model is trained with simulated quantization operations.</li> <li>This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.</li> <li>During QAT, they used learnable ranges for both weights and activations, further enhancing the model's adaptability to quantization.</li></ul></li></ul> <hr> <h2 id="_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models"><a href="#_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models" class="header-anchor">#</a> 2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models</h2> <h3 id="key-takeaways-in-three-sentences"><a href="#key-takeaways-in-three-sentences" class="header-anchor">#</a> <strong>Key Takeaways in Three Sentences</strong></h3> <ol><li>The study demonstrates that <strong>low-bit floating-point formats, particularly FP8, provide superior quantization accuracy for LLMs compared to INT8</strong> , with comparable hardware efficiency at 8-bit precision.</li> <li>The <strong>Mixture of Formats Quantization (MoFQ) approach optimally selects between INT and FP per layer</strong> , improving accuracy without increasing computational overhead.</li> <li>MoFQ achieves <strong>state-of-the-art results in both W4-only and W8A8 quantization</strong> , outperforming existing methods like GPTQ, AWQ, LLM.int8(), and SmoothQuant while maintaining <strong>efficient inference speed</strong> .</li></ol> <h3 id="abstract"><a href="#abstract" class="header-anchor">#</a> <strong>Abstract</strong></h3> <p>The study finds that optimal quantization formats vary across layers in LLMs, leading to the <strong>Mixture of Formats Quantization (MoFQ)</strong>  approach, which selects the best format per layer.<br>
MoFQ achieves superior or comparable performance to current quantization methods for weight-only (W-only) and weight-activation (WA) quantization without additional hardware overhead.</p> <h3 id="introduction"><a href="#introduction" class="header-anchor">#</a> <strong>Introduction</strong></h3> <p>Quantization minimizes LLMs' size and inference costs, with prior work focusing on low-bit integer formats.<br>
However, as LLMs grow, integer quantization becomes less effective, requiring optimizations or alternatives. Low-bit floating-point formats have emerged as viable alternatives, with FP8 already supported in NVIDIA’s H100 GPUs.</p> <p>The study:</p> <ol><li>Compares INT and FP formats in terms of hardware efficiency and quantization error.</li> <li>Proposes <strong>Mixture of Formats Quantization (MoFQ)</strong> , selecting the best format per layer.</li> <li>Implements an inference system for W-only quantization, maintaining performance parity with INT-based systems.</li></ol> <h3 id="background-and-related-works"><a href="#background-and-related-works" class="header-anchor">#</a> <strong>Background and Related Works</strong></h3> <p><strong>Integer vs. Floating-Point Formats</strong></p> <ul><li><strong>Integer (INT)</strong> : Uniformly distributed values.</li> <li><strong>Floating-Point (FP)</strong> : Non-uniform distribution, allowing higher precision for small values but reduced precision for large values.</li> <li><strong>Hardware efficiency</strong> : FP operations typically cost more than INT, but at 8-bit, FP8 and INT8 MAC (Multiply-Accumulate) units have nearly identical area costs.</li></ul> <p><strong>Post-Training Quantization (PTQ) for LLMs</strong></p> <p>Two main PTQ strategies:</p> <ol><li><strong>Weight-Only (W-only) Quantization:</strong>  Applies to weights only, e.g., W4A16.</li> <li><strong>Weight-Activation (WA) Quantization:</strong>  Quantizes both weights and activations, e.g., W8A8.</li></ol> <p>State-of-the-art (SOTA) methods:</p> <ul><li><strong>LLM.int8()</strong> : Uses mixed precision (INT8+FP16).</li> <li><strong>SmoothQuant</strong> : Redistributes quantization difficulty from activations to weights.</li> <li><strong>GPTQ &amp; AWQ</strong> : Use second-order information and pre-scaling techniques to improve quantization.</li></ul> <h3 id="comparative-analysis-of-int-and-fp-formats"><a href="#comparative-analysis-of-int-and-fp-formats" class="header-anchor">#</a> <strong>Comparative Analysis of INT and FP Formats</strong></h3> <p><strong>A. Hardware Cost of INT vs. FP MAC Units</strong></p> <ul><li><strong>At 8-bit precision, FP8 and INT8 MACs require nearly the same hardware area</strong> , aligning with H100 GPU capabilities.</li></ul> <p><strong>B. Quantization Error Comparison</strong></p> <ol><li><strong>4-bit Weight-Only (W4) Quantization</strong>  (LLaMA-65B model):</li></ol> <ul><li>Some layers perform better with INT4, while others favor FP4, indicating <strong>layer-dependent format preference</strong> .</li></ul> <p><img src="https://github.com/user-attachments/assets/97be028e-fdb3-4883-a92e-b784d1ff1f87" alt="image"></p> <ol start="2"><li><strong>8-bit Weight-Activation (W8A8) Quantization</strong> :</li></ol> <ul><li><strong>Weights</strong> : INT8 generally has lower quantization error.</li> <li><strong>Activations</strong> : FP8 shows <strong>better robustness</strong>  for dynamic activation tensors.</li> <li>Best choice: INT8 for weights, FP8 for activations—but hardware constraints necessitate using the same format per layer.</li></ul> <p><img src="https://github.com/user-attachments/assets/7af41adf-5821-4c0b-8599-1385c1982c87" alt="image"></p> <h3 id="exploiting-int-and-fp-complementarity"><a href="#exploiting-int-and-fp-complementarity" class="header-anchor">#</a> <strong>Exploiting INT and FP Complementarity</strong></h3> <p><strong>A. Improved Low-Bit FP4 Format</strong></p> <ul><li>IEEE floating-point format reserves exponent values for NaN and Inf.</li> <li><strong>Reallocating NaN &amp; Inf to normalized numbers improves FP4 precision</strong>  by 35%.</li></ul> <p><strong>B. Mixture of Formats Quantization (MoFQ)</strong></p> <ul><li>Selects the best quantization format (INT or FP) <strong>per layer</strong>  based on quantization error.</li> <li>Works for both <strong>W-only and WA quantization</strong> .</li> <li><strong>Algorithm</strong> : Iterates through layers, computes quantization error for INT and FP, and selects the lower-error format.</li></ul> <p><strong>C. Low-Bit W-Only Inference System</strong></p> <ul><li><strong>INT4 and FP4 require conversion to FP16 before computation</strong>  due to FP16 activations.</li> <li><strong>W8A8 quantization</strong> : FP16 activations are converted to FP8 or INT8 based on next-layer format selection.</li> <li><strong>No additional hardware overhead for FP-based or MoFQ-based inference</strong>  compared to INT-based quantization.</li></ul> <p><img src="https://github.com/user-attachments/assets/7d5e0e92-deda-4e40-8775-b428fff2ca0c" alt="image"></p> <h3 id="conclusion"><a href="#conclusion" class="header-anchor">#</a> <strong>Conclusion</strong></h3> <ul><li><strong>Comparative study</strong> : INT and FP formats have complementary strengths.</li> <li><strong>Key finding</strong> : <strong>FP8 and INT8 MAC units have similar hardware costs at low-bit quantization</strong> .</li> <li><strong>MoFQ method</strong> :
<ul><li>Selects the best quantization format <strong>per layer</strong> .</li> <li><strong>Achieves state-of-the-art accuracy</strong>  in W4-only and W8A8 quantization.</li> <li><strong>No additional inference latency or hardware overhead</strong> .</li></ul></li></ul> <hr> <h2 id="_26-fp8-lm-training-fp8-large-language-models"><a href="#_26-fp8-lm-training-fp8-large-language-models" class="header-anchor">#</a> [26] FP8-LM: Training FP8 Large Language Models</h2> <h3 id="abstract-2"><a href="#abstract-2" class="header-anchor">#</a> <strong>Abstract</strong></h3> <p>The paper explores <strong>FP8 low-bit data formats</strong>  for training large language models (LLMs), significantly reducing <strong>memory usage and computation costs</strong>  while maintaining accuracy.</p> <p>The authors introduce an <strong>FP8 automatic mixed-precision training framework</strong>  with three levels of FP8 utilization, improving mixed-precision and distributed parallel training. <strong>Key results</strong>  show that training the <strong>GPT-175B model on an H100 GPU platform</strong>  using FP8:</p> <ul><li><strong>Reduces memory usage by 39%</strong></li> <li><strong>Speeds up training by 75% compared to BF16 (Megatron-LM)</strong></li> <li><strong>Outperforms Nvidia Transformer Engine by 37%</strong>
The <strong>FP8 training methodology is generalizable</strong>  to fine-tuning, instruction tuning, and reinforcement learning with human feedback (RLHF). The framework is <strong>open-sourced</strong>  at <a href="https://github.com/Azure/MS-AMP" target="_blank" rel="noopener noreferrer">aka.ms/MS.AMP<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> .</li></ul> <h3 id="_1-introduction"><a href="#_1-introduction" class="header-anchor">#</a> <strong>1. Introduction</strong></h3> <p>LLMs have demonstrated exceptional performance in various domains but are extremely expensive to train.<br>
The cost of training models like <strong>GPT-3 (175B) or PaLM (540B)</strong>  is enormous, requiring <strong>thousands of GPUs or TPUs</strong> .<br>
Low-precision training is a <strong>promising solution</strong>  as it:</p> <ul><li><strong>Increases speed</strong></li> <li><strong>Reduces memory usage</strong></li> <li><strong>Minimizes communication overhead</strong></li></ul> <p>Most existing frameworks, such as <strong>Megatron-LM, MetaSeq, and Colossal-AI</strong> , use <strong>FP32, FP16, or BF16 mixed-precision training</strong> , but <strong>FP8 offers significant efficiency gains</strong> :</p> <ul><li><strong>2× speed-up</strong></li> <li><strong>50%-75% memory and communication savings</strong></li></ul> <h4 id="challenges-of-fp8-training"><a href="#challenges-of-fp8-training" class="header-anchor">#</a> <strong>Challenges of FP8 Training</strong></h4> <ol><li><strong>Data underflow/overflow issues</strong>  due to FP8’s limited dynamic range.</li> <li><strong>Numerical instabilities and divergence</strong>  during training.</li></ol> <h4 id="proposed-fp8-mixed-precision-framework"><a href="#proposed-fp8-mixed-precision-framework" class="header-anchor">#</a> <strong>Proposed FP8 Mixed-Precision Framework</strong></h4> <ul><li>Introduces <strong>three levels of FP8 utilization</strong>  (gradients, optimizer states, and distributed learning).</li> <li>Uses <strong>precision decoupling</strong>  and <strong>automatic scaling</strong>  to mitigate numerical instability.</li> <li>Achieves <strong>29%-39% memory savings</strong>  and <strong>63%-65% communication cost reductions</strong> .</li></ul> <h3 id="_2-fp8-llm-training"><a href="#_2-fp8-llm-training" class="header-anchor">#</a> <strong>2. FP8 LLM Training</strong></h3> <h4 id="_2-1-fp8-gradient-and-all-reduce-communication"><a href="#_2-1-fp8-gradient-and-all-reduce-communication" class="header-anchor">#</a> <strong>2.1 FP8 Gradient and All-Reduce Communication</strong></h4> <ul><li>Traditional mixed-precision training uses <strong>FP16/FP32 for gradients</strong> , leading to high communication costs.</li> <li>Applying <strong>FP8 directly to gradients</strong>  results in <strong>loss of accuracy</strong>  due to underflow/overflow.</li> <li>The paper proposes an <strong>automatic scaling technique</strong>  to adapt scaling factors dynamically, preventing numerical instability.</li></ul> <h4 id="_2-2-fp8-optimizer"><a href="#_2-2-fp8-optimizer" class="header-anchor">#</a> <strong>2.2 FP8 Optimizer</strong></h4> <ul><li>The <strong>Adam optimizer</strong>  typically consumes <strong>16 bytes per parameter</strong>  due to high-precision storage of gradients and optimizer states.</li> <li>The proposed <strong>FP8 optimizer</strong>  stores:
<ul><li>FP8 first-order moment</li> <li>FP16 master weights (with tensor scaling)</li> <li>FP16 second-order moment</li></ul></li> <li>This reduces <strong>memory consumption from 16 bytes to 6 bytes per parameter</strong>  (2.6× savings).</li></ul> <blockquote><p>My main takeaway is that direction of gradient matters, instead of magnitude.</p></blockquote> <p><img src="https://github.com/user-attachments/assets/aa8bdd9a-c5ea-4910-bfdf-836eca32f9b5" alt="image"></p> <h4 id="_2-3-fp8-distributed-parallel-training"><a href="#_2-3-fp8-distributed-parallel-training" class="header-anchor">#</a> <strong>2.3 FP8 Distributed Parallel Training</strong></h4> <ul><li><strong>Tensor Parallelism</strong> : Uses <strong>FP8 for weight and activation tensors</strong> , reducing compute and communication overhead.</li> <li><strong>Sequence Parallelism</strong> : Converts activation tensors to <strong>FP8 before communication</strong> , reducing costs.</li> <li><strong>ZeRO (Zero Redundancy Optimizer) Support</strong> : Distributes <strong>full tensors</strong>  across devices while preserving <strong>FP8 scaling factors</strong> .</li></ul> <h3 id="_3-experimentation"><a href="#_3-experimentation" class="header-anchor">#</a> <strong>3. Experimentation</strong></h3> <h4 id="_3-1-experimental-setup"><a href="#_3-1-experimental-setup" class="header-anchor">#</a> <strong>3.1 Experimental Setup</strong></h4> <ul><li><strong>Training Dataset</strong> : Collected from <strong>CommonCrawl, The Pile, C4, OpenWebText, Wikipedia, RedPajama</strong> , and other curated sources.</li> <li><strong>Model Configuration</strong> : Uses a <strong>decoder-only Transformer</strong>  architecture (like GPT-3), with <strong>RoPE embeddings and Flash Attention</strong> .</li></ul> <h4 id="_3-2-main-results"><a href="#_3-2-main-results" class="header-anchor">#</a> <strong>3.2 Main Results</strong></h4> <h5 id="model-performance"><a href="#model-performance" class="header-anchor">#</a> <strong>Model Performance</strong></h5> <ul><li><strong>Loss curves of FP8 models match BF16 models</strong> , confirming <strong>accuracy preservation</strong></li> <li><strong>Zero-shot evaluations</strong>  on <strong>Lambada, HellaSwag, BoolQ, PIQA, COPA</strong>  show <strong>comparable performance between FP8 and BF16</strong> .</li> <li><strong>Fine-tuning (SFT &amp; RLHF)</strong> : FP8 achieves:
<ul><li>27% faster fine-tuning</li> <li>32% reduction in model weight memory</li> <li>62% optimizer state memory savings</li></ul></li></ul> <p><strong>System Performance</strong></p> <ul><li><strong>Memory reduction</strong> : FP8 achieves <strong>28%-39% lower memory usage</strong>  than BF16.</li> <li><strong>Training speed improvement</strong> :
<ul><li>75% faster training for GPT-175B</li> <li>37% faster than Nvidia Transformer Engine</li></ul></li> <li><strong>Communication efficiency</strong> :
<ul><li>63%-65% reduction in weight gradient communication</li> <li>34% lower activation-related communication costs</li></ul></li></ul> <h4 id="_3-3-ablation-study"><a href="#_3-3-ablation-study" class="header-anchor">#</a> <strong>3.3 Ablation Study</strong></h4> <ul><li><strong>Gradient Scaling</strong> : <strong>Automatic scaling</strong>  reduces <strong>underflow/overflow errors</strong> , improving training stability.</li> <li><strong>Optimizer Precision</strong> :
<ul><li>FP16 master weights outperform FP8 master weights in accuracy preservation.</li> <li>FP8 first-order gradient moment is viable, but FP8 second-order moment leads to divergence.</li></ul></li> <li><strong>Parallelism Optimization</strong> :
<ul><li><strong>FP8 sequence and tensor parallelism</strong>  reduce communication costs by <strong>34%</strong> .</li> <li><strong>FP8 ZeRO</strong>  maintains a balanced GPU memory load while saving memory.</li></ul></li></ul> <h3 id="_4-related-work"><a href="#_4-related-work" class="header-anchor">#</a> <strong>4. Related Work</strong></h3> <ul><li><strong>Mixed-Precision Training</strong> : Prior work focused on <strong>FP16/BF16</strong> , but <strong>FP8 remains underexplored</strong> .</li> <li><strong>Low-Precision LLM Training</strong> :
<ul><li><strong>OPT, Bloom, Gopher, Chinchilla</strong>  used <strong>BF16</strong>  for better numerical stability.</li> <li>FP8 support was limited before Nvidia Hopper GPUs.</li> <li>This work provides the <strong>first systematic FP8 training framework</strong>  for <strong>pre-training and fine-tuning LLMs</strong> .</li></ul></li></ul> <h3 id="_5-conclusion"><a href="#_5-conclusion" class="header-anchor">#</a> <strong>5. Conclusion</strong></h3> <ul><li>Introduces a <strong>new FP8 mixed-precision training framework</strong>  with <strong>automatic scaling</strong>  and <strong>precision decoupling</strong> .</li> <li>Achieves <strong>significant reductions in memory, compute, and communication costs</strong> .</li> <li><strong>Maintains model accuracy</strong>  across <strong>GPT models from 125M to 175B parameters</strong> .</li> <li>Demonstrates <strong>versatility</strong>  in pre-training, instruction tuning, and RLHF.</li> <li><strong>Future work</strong>  includes scaling to even larger models, training multi-modal models, and deploying FP8 LLMs on edge devices.</li></ul> <h3 id="key-summary-in-3-sentences"><a href="#key-summary-in-3-sentences" class="header-anchor">#</a> <em>Key Summary in 3 Sentences</em>*</h3> <p>This paper introduces an <strong>FP8 mixed-precision training framework</strong>  that reduces memory consumption by <strong>39%</strong> , speeds up training by <strong>75%</strong> , and <strong>outperforms Nvidia Transformer Engine by 37%</strong>  while maintaining LLM accuracy.<br>
The framework uses <strong>automatic scaling and precision decoupling</strong>  to stabilize training, supports <strong>FP8 optimizers and distributed training</strong> , and generalizes to <strong>fine-tuning and reinforcement learning with human feedback (RLHF)</strong> .<br>
These findings establish <strong>FP8 as the next-generation precision format for training LLMs</strong> , significantly lowering costs while preserving model performance.</p> <hr> <h2 id="_139-fp8-formats-for-deep-learning"><a href="#_139-fp8-formats-for-deep-learning" class="header-anchor">#</a> [139] FP8 Formats for Deep Learning</h2> <h3 id="_1-introduction-2"><a href="#_1-introduction-2" class="header-anchor">#</a> <strong>1. Introduction</strong></h3> <p>Deep learning models require increasing computational resources, necessitating lower-precision formats for efficiency.</p> <p>FP8 is a <strong>natural evolution</strong>  from FP16 and BF16, reducing <strong>compute and memory costs</strong>  while maintaining <strong>accuracy comparable to FP16</strong> .</p> <h3 id="key-contributions"><a href="#key-contributions" class="header-anchor">#</a> Key contributions:</h3> <ul><li><p><strong>Two FP8 formats:</strong></p> <ul><li><strong>E4M3</strong> : 4-bit exponent, 3-bit mantissa (for weights and activations).</li> <li><strong>E5M2</strong> : 5-bit exponent, 2-bit mantissa (for gradients).</li></ul></li> <li><p><strong>Training and inference in FP8</strong>  match FP16/BF16 accuracy across CNNs, RNNs, and Transformers.</p></li> <li><p><strong>Post-training quantization (PTQ)</strong>  using FP8 <strong>outperforms int8</strong>  while preserving model accuracy.</p></li></ul> <h3 id="_2-aspects-of-fp8-usage-in-deep-learning"><a href="#_2-aspects-of-fp8-usage-in-deep-learning" class="header-anchor">#</a> <strong>2. Aspects of FP8 Usage in Deep Learning</strong></h3> <ul><li><strong>FP8 computations</strong>  will be performed in <strong>higher precision (FP16/FP32)</strong> , with final results cast back to FP8.</li> <li><strong>Scaling factors</strong>  are applied to <strong>optimize FP8 precision</strong> , similar to <strong>loss-scaling in FP16 mixed precision</strong> .</li> <li><strong>Handling of special values (NaNs, Infs) is modified in E4M3</strong>  to increase dynamic range.</li></ul> <h3 id="_3-fp8-binary-interchange-format"><a href="#_3-fp8-binary-interchange-format" class="header-anchor">#</a> <strong>3. FP8 Binary Interchange Format</strong></h3> <p><img src="https://github.com/user-attachments/assets/dc054ba5-6892-4c63-888e-ef24c9789d8a" alt="image"></p> <p>FP8 includes <strong>two encodings</strong> :</p> <ul><li><p><strong>E4M3</strong> :</p> <ul><li><strong>Used for weights and activations</strong> .</li> <li><strong>No representation for infinities</strong>  (max value: <strong>448</strong> ).</li> <li><strong>Single NaN representation to extend range</strong> .</li></ul></li> <li><p><strong>E5M2</strong> :</p> <ul><li><strong>Used for gradients</strong> .</li> <li><strong>Standard IEEE-like format</strong> , supporting <strong>NaNs and infinities</strong> .</li> <li>Larger range (up to <strong>57,344</strong> ).</li></ul></li></ul> <p><img src="https://github.com/user-attachments/assets/ac2dbf07-fd59-4902-a55b-0bb4b0385408" alt="image"></p> <h4 id="_3-1-special-value-representations"><a href="#_3-1-special-value-representations" class="header-anchor">#</a> <strong>3.1 Special Value Representations</strong></h4> <ul><li><strong>E4M3 removes infinities</strong>  and limits NaNs to a <strong>single pattern</strong> , extending its <strong>dynamic range</strong> .</li> <li><strong>E5M2 follows IEEE-754</strong> , allowing <strong>straightforward conversion from FP16</strong> .</li></ul> <h4 id="_3-2-exponent-bias"><a href="#_3-2-exponent-bias" class="header-anchor">#</a> <strong>3.2 Exponent Bias</strong></h4> <ul><li><strong>E4M3 bias = 7, E5M2 bias = 15</strong>  (matching IEEE-style representation).</li> <li>Some models require <strong>per-tensor scaling</strong>  rather than a fixed exponent bias (Figure 2).</li></ul> <h3 id="_4-empirical-results"><a href="#_4-empirical-results" class="header-anchor">#</a> <strong>4. Empirical Results</strong></h3> <h4 id="_4-1-training"><a href="#_4-1-training" class="header-anchor">#</a> <strong>4.1 Training</strong></h4> <ul><li>FP8 training achieves <strong>accuracy comparable to FP16/BF16</strong>  across CNNs, RNNs, and Transformers.</li> <li><strong>Image Classification</strong> :
<ul><li>FP8 accuracy is <strong>within statistical variation</strong>  of FP16 for most CNNs (ResNet, MobileNet, VGG, etc.).</li></ul></li> <li><strong>Language Translation</strong> :
<ul><li>FP8 BLEU scores <strong>match FP16</strong>  for Transformer and GNMT models.</li></ul></li> <li><strong>NLP Models (Table 4, Figure 1)</strong> :
<ul><li>GPT models (126M to 175B parameters) trained in FP8 <strong>match FP16 in perplexity</strong> .</li></ul></li></ul> <h4 id="_4-2-inference"><a href="#_4-2-inference" class="header-anchor">#</a> <strong>4.2 Inference</strong></h4> <ul><li><strong>FP8 post-training quantization (PTQ) outperforms int8</strong> , retaining <strong>full precision accuracy</strong>  for:
<ul><li>BERT (F1 score on SQuAD).</li> <li>GPT-3 (perplexity on Wikitext103).</li></ul></li> <li><strong>FP8-trained models require no additional quantization steps</strong> , simplifying deployment.</li></ul> <h4 id="_4-3-per-tensor-scaling"><a href="#_4-3-per-tensor-scaling" class="header-anchor">#</a> <strong>4.3 Per-Tensor Scaling</strong></h4> <ul><li><strong>Fixed exponent bias fails</strong>  when additional tensors (e.g., residuals) are stored in FP8.</li> <li><strong>Per-tensor scaling maintains accuracy</strong> , making FP8 viable for <strong>expanded use beyond GEMMs</strong> .</li></ul> <h3 id="_5-conclusions"><a href="#_5-conclusions" class="header-anchor">#</a> <strong>5. Conclusions</strong></h3> <ul><li><strong>FP8 formats (E4M3, E5M2) efficiently reduce training and inference costs while maintaining accuracy</strong> .</li> <li><strong>FP8 training is on par with FP16/BF16</strong> , without hyperparameter changes.</li> <li><strong>FP8 simplifies inference</strong>  by eliminating the need for quantization-aware training (QAT) required for int8.</li> <li><strong>Future work</strong> : Expanding FP8 usage to <strong>more tensor types and operations</strong>  beyond matrix multiplications.</li></ul> <h3 id="key-takeaways-in-three-sentences-2"><a href="#key-takeaways-in-three-sentences-2" class="header-anchor">#</a> <strong>Key Takeaways in Three Sentences</strong></h3> <p>FP8 formats (E4M3 for weights/activations, E5M2 for gradients) <strong>significantly reduce computation and memory overhead</strong>  while maintaining <strong>accuracy equivalent to FP16/BF16</strong>  across CNNs, RNNs, and Transformer models.</p> <p><strong>Post-training quantization (PTQ) with FP8 outperforms int8</strong> , allowing for <strong>simpler and more effective deployment</strong>  of trained models. The study <strong>validates FP8 training up to 175B parameters</strong> , proving its scalability for large-scale deep learning applications.</p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/15.llm_quant.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/02/10, 00:33:56</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7049/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Reasoning in LLM</div></a> <a href="/qishao-notes/pages/dc7051/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">LLM Sparsity</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7049/" class="prev">Reasoning in LLM</a></span> <span class="next"><a href="/qishao-notes/pages/dc7051/">LLM Sparsity</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.07bb8990.js" defer></script><script src="/qishao-notes/assets/js/2.7441cbb4.js" defer></script><script src="/qishao-notes/assets/js/89.27e84444.js" defer></script>
  </body>
</html>
