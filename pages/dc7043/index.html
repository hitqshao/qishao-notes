<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Efficient LLM | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.b52864fe.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.9488d546.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/83.62c97a3d.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.b0e5169f.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.425a1c28.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.4d47c8ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.475a6daa.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.114c3ed2.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.ac13e421.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.3b56407f.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.08e9a735.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.3922e5ea.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.549c09b6.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.9f16959c.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.7e55c536.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.76601c10.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.0a7a876b.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.5653cd65.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.ac0fae2c.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.d4e09f9d.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.c1f4fb62.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.d98693f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.efac147b.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.c36d3ee8.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.30147d00.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.43109997.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.de0210de.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.70fc9fb5.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.bbebc864.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.b49e195c.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.ecd3ee66.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.f9ea95af.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.c05c8ede.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.72bddc0c.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.e170c40e.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.fcb12c0c.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.43d9d83d.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.e38549c5.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.bbe65f24.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.9a84b4fe.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.dd61f910.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.65f0324a.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.d10bc3f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.5ba57bf3.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.33df5712.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.ed2999ed.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.893f4076.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.e730adaa.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.8bd1b68c.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.8c78866f.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.93caeab2.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.e9564fbc.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.8a9d6a33.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.df460984.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.58914c2d.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.e17d8083.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.3bc7fb86.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.1a68e511.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.d42de716.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.bfc85820.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.4d8772e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.ca98f9ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.fbb2c224.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.f5b036f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.4316ab04.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.500d574e.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.eaf66463.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.97735d38.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.fce6e2f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.7dc4a3e5.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.61cf9adf.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.101a9714.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.7a75537c.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.b0a99f9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.ade5b2af.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.a9212cbe.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.c8d54deb.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.d759c239.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.7a556637.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.9025003e.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.a41e4f59.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.2cb14f3d.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.34ad79b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.05d0da2f.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.7305a071.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.59a9045c.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.04abffc4.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.313d3c2a.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.1367bb99.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.921bfa11.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.ce3d6321.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.a842517c.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.0182f571.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.c253cf2e.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.a1df2f10.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.ceb6f963.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.2e01f6c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.768dbdf6.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.0f2127b2.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.e059802b.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.8e7cada3.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7043/" aria-current="page" class="active sidebar-link">Efficient LLM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7043/#_1-resource-efficient-architectures" class="sidebar-link">1. Resource-Efficient Architectures</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-1-efficient-attention" class="sidebar-link">1.1 Efficient Attention</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-2-dynamic-neural-network" class="sidebar-link">1.2 Dynamic Neural Network</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-3-diffusion-specific-optimization" class="sidebar-link">1.3 Diffusion-Specific Optimization</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-4-vit-specific-optimizations" class="sidebar-link">1.4 ViT-Specific Optimizations</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7043/#_2-resource-efficient-algorithms" class="sidebar-link">2. Resource-Efficient Algorithms</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-1-pre-training-algorithms" class="sidebar-link">2.1 Pre-Training Algorithms</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-2-fine-tuning-algorithms" class="sidebar-link">2.2 Fine-Tuning Algorithms</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-3-inference-algorithms" class="sidebar-link">2.3 Inference Algorithms</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-4-model-compression" class="sidebar-link">2.4 Model Compression</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7043/#_3-resource-efficient-systems" class="sidebar-link">3. Resource-Efficient Systems</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-1-distributed-training" class="sidebar-link">3.1 Distributed Training</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-2-hardware-aware-optimizations" class="sidebar-link">3.2 Hardware-Aware Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-3-serving-on-cloud" class="sidebar-link">3.3 Serving on Cloud</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-4-serving-on-edge" class="sidebar-link">3.4 Serving on Edge</a></li></ul></li></ul></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-01-20</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">Efficient LLM<!----></h1> <!----> <div class="theme-vdoing-content content__default"><p>[1] Resource-efficient Algorithms and Systems of Foundation Models: A Survey</p> <hr> <p><img src="https://github.com/user-attachments/assets/539f3a36-1e67-486a-a1af-d7b637cfaf8e" alt="image"> <em>Source: Resource-efficient</em></p> <p>Computation complexity of attention is O(T 2D), whereas that of the FFN is O(TD2), where T represents the sequence length and D the hidden state dimension of the model.<br>
The FFN layer is the most computationally intensive component.</p> <h2 id="_1-resource-efficient-architectures"><a href="#_1-resource-efficient-architectures" class="header-anchor">#</a> 1. Resource-Efficient Architectures</h2> <h3 id="_1-1-efficient-attention"><a href="#_1-1-efficient-attention" class="header-anchor">#</a> 1.1 Efficient Attention</h3> <p><img src="https://github.com/user-attachments/assets/d87f3dae-6a64-46ed-af28-51b4f2c16f3d" alt="image"></p> <ul><li><p><strong>Sparse Attention</strong>: Reduces complexity (e.g., Longformer, BigBird).<br>
Motivated by graph sparsification, sparse attention aims to build a sparse attention matrix.<br>
This approach aims to retain the empirical advantages of a fully quadratic self-attention scheme while employing a reduced number of inner products.</p></li> <li><p><strong>Approximate Attention</strong>: Low-rank approximations (e.g., Linformer, Reformer).
Approximate attention mainly includes low-rank approximations of the self-attention matrix and innovative reformulations of the self-attention.</p></li> <li><p><strong>Attention-Free Approaches</strong>: Alternatives like Hyena, Mamba.<br>
Despite the dominance of attention-based transformer architectures in large FMs, several works have put forth innovative architectures that hold the potential to replace the traditional transformer model.</p></li></ul> <h3 id="_1-2-dynamic-neural-network"><a href="#_1-2-dynamic-neural-network" class="header-anchor">#</a> 1.2 Dynamic Neural Network</h3> <p><img src="https://github.com/user-attachments/assets/baefb07e-0c3b-46c2-a728-5cb2e94a21b5" alt="image"></p> <ul><li><p><strong>Mixture of Experts (MoE)</strong>: (e.g., Switch Transformer, GLaM, MoEfication, FFF).</p></li> <li><p><strong>Early Exiting</strong>: Premature termination based on confidence (e.g., FREE, SkipDecode,DeeBERT, PABEE).
early-exiting optimization is a strategy that allows a model to terminate its computational process prematurely when it attains high confidence in the prediction or encounters resource constraints.</p></li></ul> <h3 id="_1-3-diffusion-specific-optimization"><a href="#_1-3-diffusion-specific-optimization" class="header-anchor">#</a> 1.3 Diffusion-Specific Optimization</h3> <ul><li><strong>Efficient Sampling</strong></li> <li><strong>Diffusion in Latent Space</strong></li> <li><strong>Diffusion Architecture Variants</strong></li></ul> <h3 id="_1-4-vit-specific-optimizations"><a href="#_1-4-vit-specific-optimizations" class="header-anchor">#</a> 1.4 ViT-Specific Optimizations</h3> <ul><li><strong>Efficient ViT Variants</strong>: MobileViT, EfficientFormer, EdgeViT.</li></ul> <h2 id="_2-resource-efficient-algorithms"><a href="#_2-resource-efficient-algorithms" class="header-anchor">#</a> 2. Resource-Efficient Algorithms</h2> <h3 id="_2-1-pre-training-algorithms"><a href="#_2-1-pre-training-algorithms" class="header-anchor">#</a> 2.1 Pre-Training Algorithms</h3> <ul><li><strong>Training Data Quality Control</strong>: DataComp, DFN.<br>
A portion of work focus on controlling the quality of training data.</li> <li><strong>Training Data Reduction</strong>: Deduplication, image patch removal.<br>
Pre-training for large FMs needs a dataset at the trillion scale, exemplified by 0.3 trillion tokens for GPT-3-175B [25] and 2 trillion tokens for LLaMa-2-70B [238].<br>
prior literature resorts to reduce vast training data through two aspects: deduplicating text datasets and image patch removal.</li> <li><strong>Progressive Learning</strong>: StackingBERT, CompoundGrow.
Progressive learning is a training strategy that begins by training a small model and then gradually increases the model size, throughout the training process.</li> <li><strong>Mixed Precision Training</strong>: Mesa, GACT.
Mixed precision training often utilizes half-precision floating-point data representation instead of single precision. This approach significantly reduces memory
requirements, approximately halving the storage space needed for weights, activations, and gradients.</li></ul> <h3 id="_2-2-fine-tuning-algorithms"><a href="#_2-2-fine-tuning-algorithms" class="header-anchor">#</a> 2.2 Fine-Tuning Algorithms</h3> <p><img src="https://github.com/user-attachments/assets/1ed489c1-3451-4874-8f36-fa99fc65e0ca" alt="image"></p> <ul><li><strong>Additive Tuning</strong>:
<ul><li><em>Adapter tuning</em> aims to reduce training costs by introducing adapter modules to specific layers (or all layers) of pre-trained large FMs.
During tuning, the backbone of the pre-trained model remains frozen, and adapter modules are utilized to acquire task-specific knowledge.</li> <li><em>prompt tuning</em> involves designing a task-specific prompt for each task, with the aim of replacing the traditional fine-tuning of pre-trained large FMs parameters.<br>
By tuning the input prompts instead, this method significantly reduces the resources and time required for the fine-tuning.</li> <li><em>prefix tuning</em> introduces a trainable, task-specific prefix part to each layer of large FMs.
This technique aims to reduce the tuning cost by limiting the updates to the parameters in this prefix.</li></ul></li> <li><strong>Selective Tuning</strong>: Freezing most parameters, selective updates.</li> <li><strong>Re-parameter Tuning</strong>: Low-rank adaptation (e.g., LoRA, Delta-LoRA).</li></ul> <h3 id="_2-3-inference-algorithms"><a href="#_2-3-inference-algorithms" class="header-anchor">#</a> 2.3 Inference Algorithms</h3> <ul><li><strong>Opportunistic Decoding</strong>: Speculative decoding, look-ahead decoding.</li> <li><strong>Input Filtering and Compression</strong>: Prompt compression, token pruning.</li> <li><strong>KV Cache Optimization</strong>: Memory-efficient sparse attention.</li> <li><strong>Long Context Handling</strong>: LM-Infinite, StreamingLLM.</li></ul> <h3 id="_2-4-model-compression"><a href="#_2-4-model-compression" class="header-anchor">#</a> 2.4 Model Compression</h3> <ul><li><strong>Pruning</strong>: Structured, unstructured, and contextual pruning.</li> <li><strong>Knowledge Distillation</strong>: Black-box and white-box KD.</li> <li><strong>Quantization</strong>: Quantization-aware training (QAT), post-training quantization (PTQ).</li> <li><strong>Low-Rank Decomposition (LoRD)</strong>: TensorGPT, LoSparse.</li></ul> <h2 id="_3-resource-efficient-systems"><a href="#_3-resource-efficient-systems" class="header-anchor">#</a> 3. Resource-Efficient Systems</h2> <h3 id="_3-1-distributed-training"><a href="#_3-1-distributed-training" class="header-anchor">#</a> 3.1 Distributed Training</h3> <ul><li><strong>Resilience</strong>: Checkpointing, redundant computation.</li> <li><strong>Parallelism</strong>: Data, model, and sequence parallelism.</li> <li><strong>Communication</strong>: Compression, overlapping with computation.</li> <li><strong>Storage</strong>: Offloading, heterogeneous GPUs.</li> <li><strong>MoE Optimization</strong>: MegaBlocks, Tutel.</li></ul> <h3 id="_3-2-hardware-aware-optimizations"><a href="#_3-2-hardware-aware-optimizations" class="header-anchor">#</a> 3.2 Hardware-Aware Optimizations</h3> <ul><li><strong>EdgeBERT</strong>: Latency-aware energy optimization.</li> <li><strong>FlightLLM</strong>: FPGA-based LLM inference.</li> <li><strong>SpAtten</strong>: Sparse attention with cascade token pruning.</li></ul> <h3 id="_3-3-serving-on-cloud"><a href="#_3-3-serving-on-cloud" class="header-anchor">#</a> 3.3 Serving on Cloud</h3> <ul><li><strong>Inference Accelerating</strong>: Kernel optimization, request batching.</li> <li><strong>Memory Saving</strong>: KV cache management, offloading.</li> <li><strong>Emerging Platforms</strong>: Spot instances, heterogeneous GPUs.</li></ul> <h3 id="_3-4-serving-on-edge"><a href="#_3-4-serving-on-edge" class="header-anchor">#</a> 3.4 Serving on Edge</h3> <ul><li><strong>Edge-Cloud Collaboration</strong>: EdgeFM.</li> <li><strong>On-Device MoE</strong>: EdgeMoE, PC-MoE.</li> <li><strong>Memory Optimization</strong>: LLMCad, PowerInfer.</li> <li><strong>I/O Optimization</strong>: STI, LLM in a flash.</li> <li><strong>Kernel Optimization</strong>: Integer-based edge kernels.</li></ul></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/09.eff_llm.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/01/20, 17:28:49</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7042/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">LLM Internals</div></a> <!----></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7042/" class="prev">LLM Internals</a></span> <!----></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.b52864fe.js" defer></script><script src="/qishao-notes/assets/js/2.9488d546.js" defer></script><script src="/qishao-notes/assets/js/83.62c97a3d.js" defer></script>
  </body>
</html>
