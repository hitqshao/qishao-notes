<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Efficient LLM | CPU &amp; GPU Microarch. Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="google-site-verification" content="66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.922e50b3.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.9f2055b9.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.9488d546.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/83.634aa937.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.b0e5169f.js"><link rel="prefetch" href="/qishao-notes/assets/js/100.566d03c1.js"><link rel="prefetch" href="/qishao-notes/assets/js/101.4d47c8ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.5d7c26fc.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.0eb81a31.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.02751c9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.398474f0.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.08e9a735.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.95175d40.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.a555feb0.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.9f16959c.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.7e55c536.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.76601c10.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.5c19aa00.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.5653cd65.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.8faabb81.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.f16d18ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.c1f4fb62.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.ea51e514.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.ac0f6a8b.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.c36d3ee8.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.30147d00.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.43109997.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.33b24905.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.4e37df24.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.fc95fc95.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.f721dec8.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.eb59f7bb.js"><link rel="prefetch" href="/qishao-notes/assets/js/35.82d60c42.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.9ba0ca67.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.0cc8968e.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.e170c40e.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.fcb12c0c.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.43d9d83d.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.e38549c5.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.bbe65f24.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.9a84b4fe.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.dd61f910.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.9f340b2f.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.e3f783b7.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.5ba57bf3.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.33df5712.js"><link rel="prefetch" href="/qishao-notes/assets/js/48.ed2999ed.js"><link rel="prefetch" href="/qishao-notes/assets/js/49.893f4076.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.e730adaa.js"><link rel="prefetch" href="/qishao-notes/assets/js/50.8bd1b68c.js"><link rel="prefetch" href="/qishao-notes/assets/js/51.f3ff2487.js"><link rel="prefetch" href="/qishao-notes/assets/js/52.2b74b357.js"><link rel="prefetch" href="/qishao-notes/assets/js/53.35cf9a4f.js"><link rel="prefetch" href="/qishao-notes/assets/js/54.2934a37c.js"><link rel="prefetch" href="/qishao-notes/assets/js/55.dceb53a2.js"><link rel="prefetch" href="/qishao-notes/assets/js/56.32e8b200.js"><link rel="prefetch" href="/qishao-notes/assets/js/57.814992b8.js"><link rel="prefetch" href="/qishao-notes/assets/js/58.5f83350d.js"><link rel="prefetch" href="/qishao-notes/assets/js/59.e3c782e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.fa21d4a5.js"><link rel="prefetch" href="/qishao-notes/assets/js/60.bfc85820.js"><link rel="prefetch" href="/qishao-notes/assets/js/61.4d8772e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/62.5428035f.js"><link rel="prefetch" href="/qishao-notes/assets/js/63.b7b6a76d.js"><link rel="prefetch" href="/qishao-notes/assets/js/64.12616bc1.js"><link rel="prefetch" href="/qishao-notes/assets/js/65.c26db4ef.js"><link rel="prefetch" href="/qishao-notes/assets/js/66.a8d0be31.js"><link rel="prefetch" href="/qishao-notes/assets/js/67.9a6ec4f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/68.70e07188.js"><link rel="prefetch" href="/qishao-notes/assets/js/69.fce6e2f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.1cb9872e.js"><link rel="prefetch" href="/qishao-notes/assets/js/70.61cf9adf.js"><link rel="prefetch" href="/qishao-notes/assets/js/71.a2a94854.js"><link rel="prefetch" href="/qishao-notes/assets/js/72.1e9165fd.js"><link rel="prefetch" href="/qishao-notes/assets/js/73.b0a99f9f.js"><link rel="prefetch" href="/qishao-notes/assets/js/74.ade5b2af.js"><link rel="prefetch" href="/qishao-notes/assets/js/75.a9212cbe.js"><link rel="prefetch" href="/qishao-notes/assets/js/76.a503f514.js"><link rel="prefetch" href="/qishao-notes/assets/js/77.1224294e.js"><link rel="prefetch" href="/qishao-notes/assets/js/78.7a556637.js"><link rel="prefetch" href="/qishao-notes/assets/js/79.9025003e.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.cd705a28.js"><link rel="prefetch" href="/qishao-notes/assets/js/80.2cb14f3d.js"><link rel="prefetch" href="/qishao-notes/assets/js/81.34ad79b1.js"><link rel="prefetch" href="/qishao-notes/assets/js/82.05d0da2f.js"><link rel="prefetch" href="/qishao-notes/assets/js/84.496742c9.js"><link rel="prefetch" href="/qishao-notes/assets/js/85.c46b2168.js"><link rel="prefetch" href="/qishao-notes/assets/js/86.16c80b7f.js"><link rel="prefetch" href="/qishao-notes/assets/js/87.1b7a6186.js"><link rel="prefetch" href="/qishao-notes/assets/js/88.869f6390.js"><link rel="prefetch" href="/qishao-notes/assets/js/89.df9c7490.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.69c83650.js"><link rel="prefetch" href="/qishao-notes/assets/js/90.6274befb.js"><link rel="prefetch" href="/qishao-notes/assets/js/91.0182f571.js"><link rel="prefetch" href="/qishao-notes/assets/js/92.ec1cd9e4.js"><link rel="prefetch" href="/qishao-notes/assets/js/93.a1df2f10.js"><link rel="prefetch" href="/qishao-notes/assets/js/94.ea759c7f.js"><link rel="prefetch" href="/qishao-notes/assets/js/95.47224156.js"><link rel="prefetch" href="/qishao-notes/assets/js/96.768dbdf6.js"><link rel="prefetch" href="/qishao-notes/assets/js/97.13fab065.js"><link rel="prefetch" href="/qishao-notes/assets/js/98.f2d0a8f6.js"><link rel="prefetch" href="/qishao-notes/assets/js/99.22aaceeb.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.922e50b3.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">CPU &amp; GPU Microarch. Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">ml&amp;llm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/mix/" class="nav-link">program</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/dc7035/" class="sidebar-link">how LLM works</a></li><li><a href="/qishao-notes/pages/dc7036/" class="sidebar-link">LLM Hardware Optimization</a></li><li><a href="/qishao-notes/pages/dc7037/" class="sidebar-link">How to run llama.cpp with gem5</a></li><li><a href="/qishao-notes/pages/dc7038/" class="sidebar-link">Memory Usage in Training LLM</a></li><li><a href="/qishao-notes/pages/dc7039/" class="sidebar-link">LLM optimizations</a></li><li><a href="/qishao-notes/pages/dc7040/" class="sidebar-link">LLM flash algorthms</a></li><li><a href="/qishao-notes/pages/dc7041/" class="sidebar-link">LLM compute &amp; memory bound</a></li><li><a href="/qishao-notes/pages/dc7042/" class="sidebar-link">LLM Internals</a></li><li><a href="/qishao-notes/pages/dc7043/" aria-current="page" class="active sidebar-link">Efficient LLM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7043/#_1-resource-efficient-architectures" class="sidebar-link">1. Resource-Efficient Architectures</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-1-efficient-attention" class="sidebar-link">1.1 Efficient Attention</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-2-dynamic-neural-network" class="sidebar-link">1.2 Dynamic Neural Network</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-3-diffusion-specific-optimization" class="sidebar-link">1.3 Diffusion-Specific Optimization</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_1-4-vit-specific-optimizations" class="sidebar-link">1.4 ViT-Specific Optimizations</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7043/#_2-resource-efficient-algorithms" class="sidebar-link">2. Resource-Efficient Algorithms</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-1-pre-training-algorithms" class="sidebar-link">2.1 Pre-Training Algorithms</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-2-fine-tuning-algorithms" class="sidebar-link">2.2 Fine-Tuning Algorithms</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-3-inference-algorithms" class="sidebar-link">2.3 Inference Algorithms</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_2-4-model-compression" class="sidebar-link">2.4 Model Compression</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/dc7043/#_3-resource-efficient-systems" class="sidebar-link">3. Resource-Efficient Systems</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-1-distributed-training" class="sidebar-link">3.1 Distributed Training</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-2-hardware-aware-optimizations" class="sidebar-link">3.2 Hardware-Aware Optimizations</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-3-serving-on-cloud" class="sidebar-link">3.3 Serving on Cloud</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/dc7043/#_3-4-serving-on-edge" class="sidebar-link">3.4 Serving on Edge</a></li></ul></li></ul></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-6"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/llm/#llm" data-v-06225672>llm</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-01-20</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">Efficient LLM<!----></h1> <!----> <div class="theme-vdoing-content content__default"><p>[1] Resource-efficient Algorithms and Systems of Foundation Models: A Survey</p> <hr> <p><img src="https://github.com/user-attachments/assets/539f3a36-1e67-486a-a1af-d7b637cfaf8e" alt="image"> <em>Source: Resource-efficient</em></p> <p>Computation complexity of attention is O(T 2D), whereas that of the FFN is O(TD2), where T represents the sequence length and D the hidden state dimension of the model.<br>
The FFN layer is the most computationally intensive component.</p> <h2 id="_1-resource-efficient-architectures"><a href="#_1-resource-efficient-architectures" class="header-anchor">#</a> 1. Resource-Efficient Architectures</h2> <h3 id="_1-1-efficient-attention"><a href="#_1-1-efficient-attention" class="header-anchor">#</a> 1.1 Efficient Attention</h3> <p><img src="https://github.com/user-attachments/assets/d87f3dae-6a64-46ed-af28-51b4f2c16f3d" alt="image"></p> <ul><li><p><strong>Sparse Attention</strong>: Reduces complexity (e.g., Longformer, BigBird).<br>
Motivated by graph sparsification, sparse attention aims to build a sparse attention matrix.<br>
This approach aims to retain the empirical advantages of a fully quadratic self-attention scheme while employing a reduced number of inner products.</p></li> <li><p><strong>Approximate Attention</strong>: Low-rank approximations (e.g., Linformer, Reformer).
Approximate attention mainly includes low-rank approximations of the self-attention matrix and innovative reformulations of the self-attention.</p></li> <li><p><strong>Attention-Free Approaches</strong>: Alternatives like Hyena, Mamba.<br>
Despite the dominance of attention-based transformer architectures in large FMs, several works have put forth innovative architectures that hold the potential to replace the traditional transformer model.</p></li></ul> <h3 id="_1-2-dynamic-neural-network"><a href="#_1-2-dynamic-neural-network" class="header-anchor">#</a> 1.2 Dynamic Neural Network</h3> <p><img src="https://github.com/user-attachments/assets/baefb07e-0c3b-46c2-a728-5cb2e94a21b5" alt="image"></p> <ul><li><p><strong>Mixture of Experts (MoE)</strong>: (e.g., Switch Transformer, GLaM, MoEfication, FFF).</p></li> <li><p><strong>Early Exiting</strong>: Premature termination based on confidence (e.g., FREE, SkipDecode,DeeBERT, PABEE).
early-exiting optimization is a strategy that allows a model to terminate its computational process prematurely when it attains high confidence in the prediction or encounters resource constraints.</p></li></ul> <h3 id="_1-3-diffusion-specific-optimization"><a href="#_1-3-diffusion-specific-optimization" class="header-anchor">#</a> 1.3 Diffusion-Specific Optimization</h3> <ul><li><strong>Efficient Sampling</strong></li> <li><strong>Diffusion in Latent Space</strong></li> <li><strong>Diffusion Architecture Variants</strong></li></ul> <h3 id="_1-4-vit-specific-optimizations"><a href="#_1-4-vit-specific-optimizations" class="header-anchor">#</a> 1.4 ViT-Specific Optimizations</h3> <ul><li><strong>Efficient ViT Variants</strong>: MobileViT, EfficientFormer, EdgeViT.</li></ul> <h2 id="_2-resource-efficient-algorithms"><a href="#_2-resource-efficient-algorithms" class="header-anchor">#</a> 2. Resource-Efficient Algorithms</h2> <h3 id="_2-1-pre-training-algorithms"><a href="#_2-1-pre-training-algorithms" class="header-anchor">#</a> 2.1 Pre-Training Algorithms</h3> <ul><li><strong>Training Data Quality Control</strong>: DataComp, DFN.<br>
A portion of work focus on controlling the quality of training data.</li> <li><strong>Training Data Reduction</strong>: Deduplication, image patch removal.<br>
Pre-training for large FMs needs a dataset at the trillion scale, exemplified by 0.3 trillion tokens for GPT-3-175B [25] and 2 trillion tokens for LLaMa-2-70B [238].<br>
prior literature resorts to reduce vast training data through two aspects: deduplicating text datasets and image patch removal.</li> <li><strong>Progressive Learning</strong>: StackingBERT, CompoundGrow.
Progressive learning is a training strategy that begins by training a small model and then gradually increases the model size, throughout the training process.</li> <li><strong>Mixed Precision Training</strong>: Mesa, GACT.
Mixed precision training often utilizes half-precision floating-point data representation instead of single precision. This approach significantly reduces memory
requirements, approximately halving the storage space needed for weights, activations, and gradients.</li></ul> <h3 id="_2-2-fine-tuning-algorithms"><a href="#_2-2-fine-tuning-algorithms" class="header-anchor">#</a> 2.2 Fine-Tuning Algorithms</h3> <p><img src="https://github.com/user-attachments/assets/1ed489c1-3451-4874-8f36-fa99fc65e0ca" alt="image"></p> <ul><li><p><strong>Additive Tuning</strong>:</p> <ul><li><em>Adapter tuning</em> aims to reduce training costs by introducing adapter modules to specific layers (or all layers) of pre-trained large FMs.
During tuning, the backbone of the pre-trained model remains frozen, and adapter modules are utilized to acquire task-specific knowledge.</li> <li><em>prompt tuning</em> involves designing a task-specific prompt for each task, with the aim of replacing the traditional fine-tuning of pre-trained large FMs parameters.<br>
By tuning the input prompts instead, this method significantly reduces the resources and time required for the fine-tuning.</li> <li><em>prefix tuning</em> introduces a trainable, task-specific prefix part to each layer of large FMs.
This technique aims to reduce the tuning cost by limiting the updates to the parameters in this prefix.</li></ul></li> <li><p><strong>Selective Tuning</strong>: Freezing most parameters, selective updates.
Selective tuning aims to maintain high performance on new tasks with low training costs by freezing the majority of parameters in large FMs and selectively updating only a small portion of the parameters.</p></li> <li><p><strong>Re-parameter Tuning</strong>: Low-rank adaptation (e.g., <strong>LoRA</strong>, Delta-LoRA).
<img src="https://github.com/user-attachments/assets/920fd758-54f1-492c-bf1e-0a5b4209f2b4" alt="image"></p> <p>Re-parameter tuning adapts large FMs by targeting a significantly smaller subspace than the original, expansive training space.<br>
This approach involves fine-tuning low-rank matrix parameters, a technique that effectively reduces the overall training cost.</p></li></ul> <h3 id="_2-3-inference-algorithms"><a href="#_2-3-inference-algorithms" class="header-anchor">#</a> 2.3 Inference Algorithms</h3> <ul><li><p><strong>Opportunistic Decoding</strong>:</p> <ul><li>Speculative decoding (<em>SpecInfer, LLMCad</em>) generating sequences autoregressively with a cost-efficient small model, followed by parallel token verification using a larger model.</li> <li>Look-ahead decoding accelerates inference in large FMs without relying on a draft model or data store, reducing decoding steps in proportion to log(FLOPs).</li></ul></li> <li><p><strong>Input Filtering and Compression</strong>:</p> <ul><li>Prompt compression(LLMLingua,LLMZip,ICAE,COT-Max)
LLMZip [241] employs LLaMA-7B for compressing natural language. Experimental results demonstrate that LLMZip outperforms cutting-edge text compression methods, including BSC, ZPAQ, and paq8h.</li> <li>Token pruning Pruning of input sequences for transformers, often involving the incremental removal of less important tokens during inference.</li></ul></li> <li><p><strong>KV Cache Optimization</strong>: Memory-efficient sparse attention.
most sparse attention designs, which primarily target the reduction of computational complexity [24, 294], do not necessarily lead to a reduction in KV cache memory consumption.<br>
This is because achieving a reduced memory footprint for the KV cache necessitates a more stringent sparsity pattern.</p> <ul><li><em>H2O</em> KV cache eviction stragegy: employs attention scores to identify and select the least important KV cache tokens in the current state for eviction</li> <li><em>Dynamic Context Pruning</em> learns a memory-efficient KV cache eviction strategy during the pre-training phase.</li> <li><em>Scissorhands</em>: innovative compact KV cache</li> <li><em>Landmark Attention</em> enables the storage of most KV caches in a slower but larger capacity memory</li></ul></li> <li><p><strong>Long Context Handling</strong>: LM-Infinite, StreamingLLM. Due to the quadratic computational cost associated with attention mechanisms, various resource-efficient optimizations have been proposed to handle long inputs.</p> <ul><li>LM-Infinite introduces a Λ-shaped attention mechanism to handle long contexts efficiently.</li> <li>StreamingLLM facilitates large FMs trained with a finite-length attention window to generalize to infinite stream decoding without the need for any fine-tuning.</li></ul></li></ul> <h3 id="_2-4-model-compression"><a href="#_2-4-model-compression" class="header-anchor">#</a> 2.4 Model Compression</h3> <p><img src="https://github.com/user-attachments/assets/d3d1214d-8f83-4553-97bd-467a2b914dd4" alt="image"></p> <ul><li><strong>Pruning</strong> <ul><li>Structured pruning eliminates entire structural components, such as groups of consecutive parameters or hierarchical structures<br>
LLM Pruner selectively removes non-essential model structures based on gradient information and incorporates LoRA to recover the model’s accuracy after pruning.<br>
Structured pruning is also employed in training.<br> <em>Sheared LLaMA</em> adopts an end-to-end to remove channels, encompassing layers, attention heads, intermediate layers, and hidden layers.<br> <em>AdaPrune</em> accelerates neural network training using transposable masks.<br> <em>GUM</em> considers neuron specificity and introduces pruning through network component-based global mobility and local uniqueness scores.<br> <em>PLATON</em> tackles the uncertainty in importance scores during pruning by employing the upper confidence bound of importance estimation.</li> <li>Unstructred pruning It removes neurons with weights below a threshold, thereby compressing the model.
<em>SparseGPT</em> sparse regression solver
<em>Wanda</em> prunes weights with smallest magnitude multiplied by corresponding input activations.
<em>SIGE</em> converts computation reduction into latency reduction.</li> <li><strong>Contextual pruning</strong> selects the sparse state of each layer.
<em>Deja Vu</em> dynamically predicts the sparsity of the next layer using the activations of the previous layer. It determines which neurons of MLP blocks and the heads of attention blocks need to be retained. To mitigate the overhead of this predictor, Deja Vu asynchronously predicts the next layer.
<em>PowerInfer</em> utilizes the sparsity of activation to dynamically predict the hotactivated neurons of the next layer and computes them on the GPU, whereas other cold-activated neurons are computed on the CPU.</li></ul></li> <li><strong>Knowledge Distillation</strong>: Black-box and white-box KD.</li> <li><strong>Quantization</strong>: Quantization-aware training (QAT), post-training quantization (PTQ).</li> <li><strong>Low-Rank Decomposition (LoRD)</strong>: TensorGPT, LoSparse.</li></ul> <h2 id="_3-resource-efficient-systems"><a href="#_3-resource-efficient-systems" class="header-anchor">#</a> 3. Resource-Efficient Systems</h2> <h3 id="_3-1-distributed-training"><a href="#_3-1-distributed-training" class="header-anchor">#</a> 3.1 Distributed Training</h3> <ul><li><strong>Resilience</strong>: Checkpointing, redundant computation.</li> <li><strong>Parallelism</strong>: Data, model, and sequence parallelism.</li> <li><strong>Communication</strong>: Compression, overlapping with computation.</li> <li><strong>Storage</strong>: Offloading, heterogeneous GPUs.</li> <li><strong>MoE Optimization</strong>: MegaBlocks, Tutel.</li></ul> <h3 id="_3-2-hardware-aware-optimizations"><a href="#_3-2-hardware-aware-optimizations" class="header-anchor">#</a> 3.2 Hardware-Aware Optimizations</h3> <ul><li><strong>EdgeBERT</strong>: Latency-aware energy optimization.</li> <li><strong>FlightLLM</strong>: FPGA-based LLM inference.</li> <li><strong>SpAtten</strong>: Sparse attention with cascade token pruning.</li></ul> <h3 id="_3-3-serving-on-cloud"><a href="#_3-3-serving-on-cloud" class="header-anchor">#</a> 3.3 Serving on Cloud</h3> <ul><li><strong>Inference Accelerating</strong>: Kernel optimization, request batching.</li> <li><strong>Memory Saving</strong>: KV cache management, offloading.</li> <li><strong>Emerging Platforms</strong>: Spot instances, heterogeneous GPUs.</li></ul> <h3 id="_3-4-serving-on-edge"><a href="#_3-4-serving-on-edge" class="header-anchor">#</a> 3.4 Serving on Edge</h3> <ul><li><strong>Edge-Cloud Collaboration</strong>: EdgeFM.</li> <li><strong>On-Device MoE</strong>: EdgeMoE, PC-MoE.</li> <li><strong>Memory Optimization</strong>: LLMCad, PowerInfer.</li> <li><strong>I/O Optimization</strong>: STI, LLM in a flash.</li> <li><strong>Kernel Optimization</strong>: Integer-based edge kernels.</li></ul></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/05.llm/09.eff_llm.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/01/20, 18:58:59</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/dc7042/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">LLM Internals</div></a> <!----></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/dc7042/" class="prev">LLM Internals</a></span> <!----></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.9f2055b9.js" defer></script><script src="/qishao-notes/assets/js/2.9488d546.js" defer></script><script src="/qishao-notes/assets/js/83.634aa937.js" defer></script>
  </body>
</html>
