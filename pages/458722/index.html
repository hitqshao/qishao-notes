<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Architectural Survey | Qi Shao</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="stylesheet" href="custom.css">
    <script language="javascript" type="text/javascript" src="/qishao-notes/js/pgmanor-self.js"></script>
    <meta name="description" content="Computer System">
    <meta name="keywords" content="Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/qishao-notes/assets/css/0.styles.ef74cbd5.css" as="style"><link rel="preload" href="/qishao-notes/assets/js/app.9af4ddd9.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/2.409f7334.js" as="script"><link rel="preload" href="/qishao-notes/assets/js/35.550dd610.js" as="script"><link rel="prefetch" href="/qishao-notes/assets/js/10.ff60f5f4.js"><link rel="prefetch" href="/qishao-notes/assets/js/11.a5d0c4e8.js"><link rel="prefetch" href="/qishao-notes/assets/js/12.9dd6ac7e.js"><link rel="prefetch" href="/qishao-notes/assets/js/13.2cf0e078.js"><link rel="prefetch" href="/qishao-notes/assets/js/14.c5d0839f.js"><link rel="prefetch" href="/qishao-notes/assets/js/15.00157499.js"><link rel="prefetch" href="/qishao-notes/assets/js/16.59d320b3.js"><link rel="prefetch" href="/qishao-notes/assets/js/17.7ba063e3.js"><link rel="prefetch" href="/qishao-notes/assets/js/18.2ebec85b.js"><link rel="prefetch" href="/qishao-notes/assets/js/19.aa8ac1ae.js"><link rel="prefetch" href="/qishao-notes/assets/js/20.c8eb0ba5.js"><link rel="prefetch" href="/qishao-notes/assets/js/21.824b6fe1.js"><link rel="prefetch" href="/qishao-notes/assets/js/22.eff57bac.js"><link rel="prefetch" href="/qishao-notes/assets/js/23.02fa9681.js"><link rel="prefetch" href="/qishao-notes/assets/js/24.5b16f3ad.js"><link rel="prefetch" href="/qishao-notes/assets/js/25.3b8cec93.js"><link rel="prefetch" href="/qishao-notes/assets/js/26.551b04e9.js"><link rel="prefetch" href="/qishao-notes/assets/js/27.a9f79b9d.js"><link rel="prefetch" href="/qishao-notes/assets/js/28.f5050d57.js"><link rel="prefetch" href="/qishao-notes/assets/js/29.3fc6e179.js"><link rel="prefetch" href="/qishao-notes/assets/js/3.618d3986.js"><link rel="prefetch" href="/qishao-notes/assets/js/30.2fa63669.js"><link rel="prefetch" href="/qishao-notes/assets/js/31.93f5f633.js"><link rel="prefetch" href="/qishao-notes/assets/js/32.045f46c4.js"><link rel="prefetch" href="/qishao-notes/assets/js/33.8dd38d98.js"><link rel="prefetch" href="/qishao-notes/assets/js/34.e86874d0.js"><link rel="prefetch" href="/qishao-notes/assets/js/36.577530f3.js"><link rel="prefetch" href="/qishao-notes/assets/js/37.c5acffa0.js"><link rel="prefetch" href="/qishao-notes/assets/js/38.372b2a3c.js"><link rel="prefetch" href="/qishao-notes/assets/js/39.f31e3446.js"><link rel="prefetch" href="/qishao-notes/assets/js/4.8016aaad.js"><link rel="prefetch" href="/qishao-notes/assets/js/40.3e8d6eb0.js"><link rel="prefetch" href="/qishao-notes/assets/js/41.0fd130a0.js"><link rel="prefetch" href="/qishao-notes/assets/js/42.8b22fe60.js"><link rel="prefetch" href="/qishao-notes/assets/js/43.e1153cf2.js"><link rel="prefetch" href="/qishao-notes/assets/js/44.18ca1d12.js"><link rel="prefetch" href="/qishao-notes/assets/js/45.69e41a95.js"><link rel="prefetch" href="/qishao-notes/assets/js/46.06787bc4.js"><link rel="prefetch" href="/qishao-notes/assets/js/47.ee92ddc4.js"><link rel="prefetch" href="/qishao-notes/assets/js/5.71cfab20.js"><link rel="prefetch" href="/qishao-notes/assets/js/6.938d7909.js"><link rel="prefetch" href="/qishao-notes/assets/js/7.06c60e57.js"><link rel="prefetch" href="/qishao-notes/assets/js/8.9d9bbbd6.js"><link rel="prefetch" href="/qishao-notes/assets/js/9.95b204c9.js">
    <link rel="stylesheet" href="/qishao-notes/assets/css/0.styles.ef74cbd5.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/qishao-notes/" class="home-link router-link-active"><!----> <span class="site-name">Qi Shao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">llm</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="/qishao-notes/message-board/" class="nav-link">BBS</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/qishao-notes/" class="nav-link">Home</a></div><div class="nav-item"><a href="/qishao-notes/hbm/" class="nav-link">hbm</a></div><div class="nav-item"><a href="/qishao-notes/compiler/" class="nav-link">compiler</a></div><div class="nav-item"><a href="/qishao-notes/gpu/" class="nav-link">gpu</a></div><div class="nav-item"><a href="/qishao-notes/cpu/" class="nav-link">cpu</a></div><div class="nav-item"><a href="/qishao-notes/llm/" class="nav-link">llm</a></div><div class="nav-item"><a href="/qishao-notes/unix/" class="nav-link">unix</a></div><div class="nav-item"><a href="/qishao-notes/message-board/" class="nav-link">BBS</a></div><div class="nav-item"><a href="https://blog.csdn.net/hit_shaoqi" target="_blank" rel="noopener noreferrer" class="nav-link external">
  CSDN
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a href="https://github.com/hitqshao/qishao-notes" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><a href="/qishao-notes/pages/cc7034/" class="sidebar-link">operand-collector</a></li><li><a href="/qishao-notes/pages/2476ae/" class="sidebar-link">GPU WARP Scheduler</a></li><li><a href="/qishao-notes/pages/14769f/" class="sidebar-link">Precision Exception</a></li><li><a href="/qishao-notes/pages/44771e/" class="sidebar-link">Unified Memory Paper List</a></li><li><a href="/qishao-notes/pages/44871e/" class="sidebar-link">TensorCore Paper List</a></li><li><a href="/qishao-notes/pages/45871e/" class="sidebar-link">Memory Behaviour Paper List</a></li><li><a href="/qishao-notes/pages/45871f/" class="sidebar-link">GPU Virtualization Paper List</a></li><li><a href="/qishao-notes/pages/458720/" class="sidebar-link">Large Language Model Paper List</a></li><li><a href="/qishao-notes/pages/458721/" class="sidebar-link">GPU Simulator</a></li><li><a href="/qishao-notes/pages/458722/" aria-current="page" class="active sidebar-link">Architectural Survey</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/458722/#control-flow-divergence" class="sidebar-link">Control flow divergence</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/458722/#_1-regrouping-divergent-warps" class="sidebar-link">1. Regrouping Divergent warps</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/458722/#_2-large-warp-cta-compaction" class="sidebar-link">2.  Large Warp/CTA compaction</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/458722/#_3-multi-path-execution" class="sidebar-link">3. Multi-path execution</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/458722/#_4-mimd-like-architecture" class="sidebar-link">4. MIMD-like architecture</a></li><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/458722/#_5-dynamic-kernels-threads" class="sidebar-link">5. Dynamic kernels/threads</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/458722/#cuda-enables-dynamic-parallsim-creating-subkernels-from-each-thread" class="sidebar-link">CUDA enables dynamic parallsim, creating subkernels from each thread.</a></li><li class="sidebar-sub-header level2"><a href="/qishao-notes/pages/458722/#efficient-utilization-of-memory-bandwidth" class="sidebar-link">Efficient utilization of memory bandwidth</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/qishao-notes/pages/458722/#alleviating-cache-thrashing-and-resource-contention" class="sidebar-link">Alleviating cache thrashing, and resource contention</a></li></ul></li></ul></li><li><a href="/qishao-notes/pages/47871e/" class="sidebar-link">TO READ</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/qishao-notes/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/qishao-notes/gpu/#gpu" data-v-06225672>gpu</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/hitqshao" target="_blank" title="作者" class="beLink" data-v-06225672>hitqishao</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2024-03-30</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">Architectural Survey<!----></h1> <!----> <div class="theme-vdoing-content content__default"><ol><li>A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity</li></ol> <hr> <h1 id="_1-a-survey-of-architectural-approaches-for-improving-gpgpu-performance-programmability-and-heterogeneity"><a href="#_1-a-survey-of-architectural-approaches-for-improving-gpgpu-performance-programmability-and-heterogeneity" class="header-anchor">#</a> 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity</h1> <p>Four major improvement</p> <ul><li>mitigating the impact of control flow divergence</li> <li>alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main
memory</li> <li>increasing the available parallelism and concurrency</li> <li>improving pipeline execution and exploiting scalarization opportunities.</li></ul> <p><img src="https://github.com/hitqshao/qishao-notes/assets/23403286/d9c6b47b-d469-4154-9dc5-b0b0f4168d70" alt="image"></p> <h2 id="control-flow-divergence"><a href="#control-flow-divergence" class="header-anchor">#</a> <strong>Control flow divergence</strong></h2> <ol><li>First, GPUs employ PDOM stack-based mechanism that serializes the execution of divergent paths. This serialization of divergent paths reduces the available thread level parallelism
(i.e., the number of active warps at a time) which limits the ability of GPUs to hide long memory instruction latency.</li> <li>Control divergence limits the number of active threads in the running warps. As a result, SIMD execution units are not efficiently utilized when a diverged warp is executed.</li> <li>Control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. Memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.</li> <li>Irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per CTAs) to some GPU cores are larger than others.</li></ol> <p><img src="https://github.com/hitqshao/qishao-notes/assets/23403286/f9f7621e-ba28-4261-84bf-73b7ba00d9c4" alt="image"></p> <h3 id="_1-regrouping-divergent-warps"><a href="#_1-regrouping-divergent-warps" class="header-anchor">#</a> 1. <strong>Regrouping Divergent warps</strong><br></h3> <p>Instead, DWF dynamically re-forms divergent warps into new non-divergent warps on the fly.<br>
Moreover, DWF does not reconverge diverged warp at IPDOM in order to amortize coalesced memory address of converged warps.</p> <p><img src="https://github.com/hitqshao/qishao-notes/assets/23403286/c19066f6-bbe5-4589-9ae5-58be982e465b" alt="image"></p> <p><img src="https://github.com/hitqshao/qishao-notes/assets/23403286/bae42a54-ea88-477b-8103-61fd313f03c1" alt="image"></p> <h3 id="_2-large-warp-cta-compaction"><a href="#_2-large-warp-cta-compaction" class="header-anchor">#</a> 2.  <strong>Large Warp/CTA compaction</strong></h3> <ul><li><p>Thread Block Compaction (TBC)<br>
Allows a group of warps, that belong to the same thread block, to share the same PDOM stack.<br> <strong>However, TBC stalls all warps within a CTA on any potentially divergent branch until all warps reach the branch point.</strong></p> <p><img src="https://github.com/hitqshao/qishao-notes/assets/23403286/0f41fa75-67e0-40e3-9f3b-fe1e55ca9d3e" alt="image"></p></li></ul> <p>The major difference between 1) and TBC is that 1) can only merge threads in a warp when they are ready in a queue. Thus it miss some potentials.</p> <p>TBC replace per-warp convergence stack with in-threadblock stack.</p> <ul><li><p>CAPRI<br>
CAPRI dynamically identifies the compaction effectiveness of a branch and only stalls threads that are predicted to benefit from compaction.<br></p></li> <li><p>SLP
proposed SIMD lane permutation (SLP) as an optimization to expand the applicability of compaction in case of conventional compaction technique is ineffective.</p> <p><img src="https://github.com/hitqshao/qishao-notes/assets/23403286/5c3a5f0d-40c5-4b17-9d73-e73d064fa170" alt="image"></p></li></ul> <h3 id="_3-multi-path-execution"><a href="#_3-multi-path-execution" class="header-anchor">#</a> 3. <strong>Multi-path execution</strong><br></h3> <ul><li>DPS<br>
Dual-path Stack<br></li> <li>Multi-path Execution<br></li></ul> <h3 id="_4-mimd-like-architecture"><a href="#_4-mimd-like-architecture" class="header-anchor">#</a> 4. <strong>MIMD-like architecture</strong><br></h3> <p>Rogers et al. [194] observed that regular applications perform better with a wider warp size, whereas divergent applications achieve better performance with a  smaller warp size. <br>
VWS groups sets of these smaller warps together by ganging their execution in the warp scheduler and thus amortizing the energy consumed by fetch, decode, and warp scheduling across more threads.</p> <h3 id="_5-dynamic-kernels-threads"><a href="#_5-dynamic-kernels-threads" class="header-anchor">#</a> 5. <strong>Dynamic kernels/threads</strong> <br></h3> <p>Related Paper:
<strong>Characterization and Analysis of Dynamic Parallelism in Unstructured GPU Applications.</strong> [108] <br> <strong>Dynamic Thread Block Launch: A Lightweight Execution Mechanism to Support Irregular Applications on GPUs</strong>. [85] <br>
By wang jing NVIDIA</p> <p>👍 👍 👍
<strong>These two paper has very thorough explanation of how kernels are launched, kernel parameters are gained and how thread create subkernel.</strong></p> <p><img src="https://github.com/hitqshao/qishao-notes/assets/23403286/6e30803e-5aaf-4bad-b626-f301c3487c1f" alt="image"></p> <p><img src="https://github.com/hitqshao/qishao-notes/assets/23403286/eba31c47-1954-4251-bd72-ab05c1f9ce64" alt="image"></p> <h2 id="cuda-enables-dynamic-parallsim-creating-subkernels-from-each-thread"><a href="#cuda-enables-dynamic-parallsim-creating-subkernels-from-each-thread" class="header-anchor">#</a> CUDA enables dynamic parallsim, creating subkernels from each thread.</h2> <p><strong>Copied from &quot;Characterization&quot;</strong></p> <p>When a child kernel is launched, the parameter buffer pointer of the kernel is retrieved through the device runtime API cudaGetParameterBuffer.
Then the argument values are stored in the parameter buffer and the kernel is launched by calling cudaLaunchDevice.
After that, the device runtime manager appends the child kernels to an execution queue and dispatches the kernel to SMXs according to a certain scheduling policy.
The CDP kernel launching overhead comprises of kernel parameter parsing, calling cudaGetParameterBuffer and cudaLaunchDevice, as well as the process that device runtime manager setups,enqueues, manages and dispatches the child kernels.</p> <hr> <p>however, the huge kernel launching overhead could negate the performance benefit of DFP. The overhead is due to the large number of launched kernels, the associated memory footprint and the low number of running warps per core.The CPU launches GPU kernels by dispatching kernel launching commands. Kernel parameters are passed from CPU to
GPU at the kernel launching time and stored in the GPU global</p> <p>Wang et al. [236] proposed new mechanism, called Dynamic Thread Block Launch (DTBL), that employs light-weight thread block rather than heavy-weight device kernel for DFP.</p> <hr> <h2 id="efficient-utilization-of-memory-bandwidth"><a href="#efficient-utilization-of-memory-bandwidth" class="header-anchor">#</a> <strong>Efficient utilization of memory bandwidth</strong></h2> <h3 id="alleviating-cache-thrashing-and-resource-contention"><a href="#alleviating-cache-thrashing-and-resource-contention" class="header-anchor">#</a> <strong>Alleviating cache thrashing, and resource contention</strong></h3> <h4 id="_1-two-level-warp-scheduling"><a href="#_1-two-level-warp-scheduling" class="header-anchor">#</a> 1. <strong>Two-level warp scheduling</strong></h4> <ul><li>TLRR<br> <img src="https://github.com/hitqshao/qishao-notes/assets/23403286/586e0677-70fd-4623-b4ee-beaf001ad3dc" alt="image"></li></ul> <p>They proposed two-level round-robin warp scheduling (TL-RR), in which the warps are split into fetch groups.<br>
TL-RR executes only one fetch group at a time and it schedules warps from the same fetch group in a round-robin fashion.<br>
When the running warps reach a long latency operation, then the next fetch group is prioritized.<br>
They try to alleviate the issue of threads in all warps <strong>arrive the same memory latency instruction at the same time</strong>.<br></p> <div class="language- extra-class"><pre><code>![image](https://github.com/hitqshao/qishao-notes/assets/23403286/6851655b-1a31-45ec-bc79-90cf4b97435d)

![image](https://github.com/hitqshao/qishao-notes/assets/23403286/2ff9dbbb-d756-42e1-b821-bc698417a9a0)
</code></pre></div><ul><li>OWL
OWL augments the TL-RR with CTA-awareness, such that warps are split into groups of CTAs basis rather than warps basis, resulting in increased intra-CTA locality. <br>
OWL gives a group of CTAs higher priority when their data exist at the L1 cache such that they get the opportunity to reuse it, therefore improving L1 hit rates and alleviating cache contention.<br></li></ul> <h4 id="_2-coarse-grained-cta-throttling"><a href="#_2-coarse-grained-cta-throttling" class="header-anchor">#</a> 2. <strong>Coarse-grained CTA throttling</strong></h4> <div class="language- extra-class"><pre><code>***DYNCTA** &lt;br&gt;
- Always executing the maximum possible number of CTAs on a GPU core (i.e., increasing TLP to the maximum) does not always lead to better performance.
- To alleviate resource contention, they proposed dynamic CTA scheduling mechanism (DYNCTA), which aims to allocate the optimal number of CTAs per GPU core that alleviate memory contention according to an application characteristics. 
- DYNCTA dynamically adjusts over sampling periods the number of active CTAs per GPU core that reduces the memory latency without sacrificing the available TLP.
***LCS** &lt;br&gt;
In contrast to DYNCTA that monitors the workload behavior for the entire kernel execution, LCS leverages GTO scheduler to find the optimal number of thread blocks at the early beginning of kernel execution.
</code></pre></div><h4 id="_3-fine-grained-warp-throttling"><a href="#_3-fine-grained-warp-throttling" class="header-anchor">#</a> 3. <strong>Fine-grained warp throttling</strong>  </h4> <p>due to the massive multithreading and the limited capacity of L1 cache, divergent GPGPU applications cause severe cache contention.<br></p> <ul><li><strong>CCWS</strong> <br>
uses a victim tag array, called lost locality detector, to detect warps that have lost locality due to thrashing. These warps are prioritized till they exploit their locality while other warps are descheduled.<br></li> <li><strong>DAWS</strong> <br>
DAWS is a divergence-based cache footprint predictor to calculate the amount of locality in loops required by each warp. <br>
DAWS uses these predictions to prioritize a group of warps such that the cache footprint of these warps do not exceed the capacity of the L1 cache. <br></li></ul> <h4 id="_4-fine-grained-warp-throttling"><a href="#_4-fine-grained-warp-throttling" class="header-anchor">#</a> 4. <strong>Fine-grained warp throttling</strong>  </h4> <p>previous CTA or warp throttling techniques leave memory bandwidth and other chip resources (L2 cache, interconnection and execution units) significantly underutilized.</p> <ul><li><strong>PCAL</strong> <br>
At the beginning of kernel execution, PCAL executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources.
Thus, PCAL reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.</li> <li><strong>CCA</strong> <br>
At the beginning of kernel execution, PCAL executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources. Thus, PCAL reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.</li></ul></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/hitqshao/qishao-notes/edit/main/docs/03.gpu/10. Architectural Survey.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2024/04/01, 03:21:09</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/qishao-notes/pages/458721/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">GPU Simulator</div></a> <a href="/qishao-notes/pages/47871e/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">TO READ</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/qishao-notes/pages/458721/" class="prev">GPU Simulator</a></span> <span class="next"><a href="/qishao-notes/pages/47871e/">TO READ</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/hitqshao" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:hitqshao@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://gitee.com/hitqshao" title="Gitee" target="_blank" class="iconfont icon-gitee"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2024
    <span>Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/qishao-notes/assets/js/app.9af4ddd9.js" defer></script><script src="/qishao-notes/assets/js/2.409f7334.js" defer></script><script src="/qishao-notes/assets/js/35.550dd610.js" defer></script>
  </body>
</html>
