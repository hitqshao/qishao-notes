(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(e){function n(n){for(var a,o,s=n[0],c=n[1],l=n[2],u=0,h=[];u<s.length;u++)o=s[u],Object.prototype.hasOwnProperty.call(r,o)&&r[o]&&h.push(r[o][0]),r[o]=0;for(a in c)Object.prototype.hasOwnProperty.call(c,a)&&(e[a]=c[a]);for(d&&d(n);h.length;)h.shift()();return i.push.apply(i,l||[]),t()}function t(){for(var e,n=0;n<i.length;n++){for(var t=i[n],a=!0,s=1;s<t.length;s++){var c=t[s];0!==r[c]&&(a=!1)}a&&(i.splice(n--,1),e=o(o.s=t[0]))}return e}var a={},r={1:0},i=[];function o(n){if(a[n])return a[n].exports;var t=a[n]={i:n,l:!1,exports:{}};return e[n].call(t.exports,t,t.exports,o),t.l=!0,t.exports}o.e=function(e){var n=[],t=r[e];if(0!==t)if(t)n.push(t[2]);else{var a=new Promise((function(n,a){t=r[e]=[n,a]}));n.push(t[2]=a);var i,s=document.createElement("script");s.charset="utf-8",s.timeout=120,o.nc&&s.setAttribute("nonce",o.nc),s.src=function(e){return o.p+"assets/js/"+({}[e]||e)+"."+{2:"409f7334",3:"0b6b096b",4:"8016aaad",5:"71cfab20",6:"13b293c1",7:"bd09a0eb",8:"0b8063cd",9:"95b204c9",10:"8b912c28",11:"68f10fb9",12:"e4357c8f",13:"96e9fb44",14:"12c35a25",15:"301a02ce",16:"a960ffff",17:"24801ac6",18:"2ebec85b",19:"6f5bbf6a",20:"51f7fa25",21:"af482f99",22:"517515c4",23:"35e983a9",24:"367bc938",25:"6892530d",26:"eaff0988",27:"e8d36ebd",28:"32259fdc",29:"f3a2b4f7",30:"a830cfc8",31:"0c998a26",32:"8bbccd2b",33:"492fe73d",34:"8abeb43d",35:"0fc62445",36:"613f8a8d",37:"b3043426",38:"aa57889a",39:"2c66f4ee",40:"93c0ac44",41:"daba784a",42:"76b5f2b0",43:"ad7848cd",44:"9e7fbae2",45:"56b15430",46:"e2ae9552",47:"c20947d6",48:"26fb3e4c",49:"47b54542",50:"b7713ed5",51:"3afe560b",52:"127a5190",53:"7367edf7"}[e]+".js"}(e);var c=new Error;i=function(n){s.onerror=s.onload=null,clearTimeout(l);var t=r[e];if(0!==t){if(t){var a=n&&("load"===n.type?"missing":n.type),i=n&&n.target&&n.target.src;c.message="Loading chunk "+e+" failed.\n("+a+": "+i+")",c.name="ChunkLoadError",c.type=a,c.request=i,t[1](c)}r[e]=void 0}};var l=setTimeout((function(){i({type:"timeout",target:s})}),12e4);s.onerror=s.onload=i,document.head.appendChild(s)}return Promise.all(n)},o.m=e,o.c=a,o.d=function(e,n,t){o.o(e,n)||Object.defineProperty(e,n,{enumerable:!0,get:t})},o.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},o.t=function(e,n){if(1&n&&(e=o(e)),8&n)return e;if(4&n&&"object"==typeof e&&e&&e.__esModule)return e;var t=Object.create(null);if(o.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:e}),2&n&&"string"!=typeof e)for(var a in e)o.d(t,a,function(n){return e[n]}.bind(null,a));return t},o.n=function(e){var n=e&&e.__esModule?function(){return e.default}:function(){return e};return o.d(n,"a",n),n},o.o=function(e,n){return Object.prototype.hasOwnProperty.call(e,n)},o.p="/qishao-notes/",o.oe=function(e){throw console.error(e),e};var s=window.webpackJsonp=window.webpackJsonp||[],c=s.push.bind(s);s.push=n,s=s.slice();for(var l=0;l<s.length;l++)n(s[l]);var d=c;i.push([129,0]),t()}([function(e,n,t){"use strict";var a=t(104),r=Object.prototype.toString;function i(e){return"[object Array]"===r.call(e)}function o(e){return void 0===e}function s(e){return null!==e&&"object"==typeof e}function c(e){if("[object Object]"!==r.call(e))return!1;var n=Object.getPrototypeOf(e);return null===n||n===Object.prototype}function l(e){return"[object Function]"===r.call(e)}function d(e,n){if(null!=e)if("object"!=typeof e&&(e=[e]),i(e))for(var t=0,a=e.length;t<a;t++)n.call(null,e[t],t,e);else for(var r in e)Object.prototype.hasOwnProperty.call(e,r)&&n.call(null,e[r],r,e)}e.exports={isArray:i,isArrayBuffer:function(e){return"[object ArrayBuffer]"===r.call(e)},isBuffer:function(e){return null!==e&&!o(e)&&null!==e.constructor&&!o(e.constructor)&&"function"==typeof e.constructor.isBuffer&&e.constructor.isBuffer(e)},isFormData:function(e){return"undefined"!=typeof FormData&&e instanceof FormData},isArrayBufferView:function(e){return"undefined"!=typeof ArrayBuffer&&ArrayBuffer.isView?ArrayBuffer.isView(e):e&&e.buffer&&e.buffer instanceof ArrayBuffer},isString:function(e){return"string"==typeof e},isNumber:function(e){return"number"==typeof e},isObject:s,isPlainObject:c,isUndefined:o,isDate:function(e){return"[object Date]"===r.call(e)},isFile:function(e){return"[object File]"===r.call(e)},isBlob:function(e){return"[object Blob]"===r.call(e)},isFunction:l,isStream:function(e){return s(e)&&l(e.pipe)},isURLSearchParams:function(e){return"undefined"!=typeof URLSearchParams&&e instanceof URLSearchParams},isStandardBrowserEnv:function(){return("undefined"==typeof navigator||"ReactNative"!==navigator.product&&"NativeScript"!==navigator.product&&"NS"!==navigator.product)&&("undefined"!=typeof window&&"undefined"!=typeof document)},forEach:d,merge:function e(){var n={};function t(t,a){c(n[a])&&c(t)?n[a]=e(n[a],t):c(t)?n[a]=e({},t):i(t)?n[a]=t.slice():n[a]=t}for(var a=0,r=arguments.length;a<r;a++)d(arguments[a],t);return n},extend:function(e,n,t){return d(n,(function(n,r){e[r]=t&&"function"==typeof n?a(n,t):n})),e},trim:function(e){return e.trim?e.trim():e.replace(/^\s+|\s+$/g,"")},stripBOM:function(e){return 65279===e.charCodeAt(0)&&(e=e.slice(1)),e}}},function(e,n,t){"use strict";var a=function(e){return e&&e.Math===Math&&e};e.exports=a("object"==typeof globalThis&&globalThis)||a("object"==typeof window&&window)||a("object"==typeof self&&self)||a("object"==typeof global&&global)||a("object"==typeof this&&this)||function(){return this}()||Function("return this")()},function(e,n,t){"use strict";var a="object"==typeof document&&document.all;e.exports=void 0===a&&void 0!==a?function(e){return"function"==typeof e||e===a}:function(e){return"function"==typeof e}},function(e,n,t){"use strict";var a=t(31),r=Function.prototype,i=r.call,o=a&&r.bind.bind(i,i);e.exports=a?o:function(e){return function(){return i.apply(e,arguments)}}},function(e,n,t){"use strict";e.exports=function(e){try{return!!e()}catch(e){return!0}}},function(e,n,t){"use strict";function a(e,n,t,a,r,i,o,s){var c,l="function"==typeof e?e.options:e;if(n&&(l.render=n,l.staticRenderFns=t,l._compiled=!0),a&&(l.functional=!0),i&&(l._scopeId="data-v-"+i),o?(c=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),r&&r.call(this,e),e&&e._registeredComponents&&e._registeredComponents.add(o)},l._ssrRegister=c):r&&(c=s?function(){r.call(this,(l.functional?this.parent:this).$root.$options.shadowRoot)}:r),c)if(l.functional){l._injectStyles=c;var d=l.render;l.render=function(e,n){return c.call(n),d(e,n)}}else{var u=l.beforeCreate;l.beforeCreate=u?[].concat(u,c):[c]}return{exports:e,options:l}}t.d(n,"a",(function(){return a}))},function(e,n,t){"use strict";var a=t(4);e.exports=!a((function(){return 7!==Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(e,n){var t=Array.isArray;e.exports=t},function(e,n,t){"use strict";var a=t(2);e.exports=function(e){return"object"==typeof e?null!==e:a(e)}},function(e,n,t){var a=t(78),r="object"==typeof self&&self&&self.Object===Object&&self,i=a||r||Function("return this")();e.exports=i},function(e,n,t){"use strict";var a=t(3),r=t(36),i=a({}.hasOwnProperty);e.exports=Object.hasOwn||function(e,n){return i(r(e),n)}},function(e,n,t){var a=t(188),r=t(191);e.exports=function(e,n){var t=r(e,n);return a(t)?t:void 0}},function(e,n,t){var a=t(291),r=t(102),i=/[T ]/,o=/:/,s=/^(\d{2})$/,c=[/^([+-]\d{2})$/,/^([+-]\d{3})$/,/^([+-]\d{4})$/],l=/^(\d{4})/,d=[/^([+-]\d{4})/,/^([+-]\d{5})/,/^([+-]\d{6})/],u=/^-(\d{2})$/,h=/^-?(\d{3})$/,p=/^-?(\d{2})-?(\d{2})$/,m=/^-?W(\d{2})$/,f=/^-?W(\d{2})-?(\d{1})$/,g=/^(\d{2}([.,]\d*)?)$/,y=/^(\d{2}):?(\d{2}([.,]\d*)?)$/,v=/^(\d{2}):?(\d{2}):?(\d{2}([.,]\d*)?)$/,b=/([Z+-].*)$/,w=/^(Z)$/,k=/^([+-])(\d{2})$/,_=/^([+-])(\d{2}):?(\d{2})$/;function x(e,n,t){n=n||0,t=t||0;var a=new Date(0);a.setUTCFullYear(e,0,4);var r=7*n+t+1-(a.getUTCDay()||7);return a.setUTCDate(a.getUTCDate()+r),a}e.exports=function(e,n){if(r(e))return new Date(e.getTime());if("string"!=typeof e)return new Date(e);var t=(n||{}).additionalDigits;t=null==t?2:Number(t);var C=function(e){var n,t={},a=e.split(i);o.test(a[0])?(t.date=null,n=a[0]):(t.date=a[0],n=a[1]);if(n){var r=b.exec(n);r?(t.time=n.replace(r[1],""),t.timezone=r[1]):t.time=n}return t}(e),A=function(e,n){var t,a=c[n],r=d[n];if(t=l.exec(e)||r.exec(e)){var i=t[1];return{year:parseInt(i,10),restDateString:e.slice(i.length)}}if(t=s.exec(e)||a.exec(e)){var o=t[1];return{year:100*parseInt(o,10),restDateString:e.slice(o.length)}}return{year:null}}(C.date,t),M=A.year,S=function(e,n){if(null===n)return null;var t,a,r,i;if(0===e.length)return(a=new Date(0)).setUTCFullYear(n),a;if(t=u.exec(e))return a=new Date(0),r=parseInt(t[1],10)-1,a.setUTCFullYear(n,r),a;if(t=h.exec(e)){a=new Date(0);var o=parseInt(t[1],10);return a.setUTCFullYear(n,0,o),a}if(t=p.exec(e)){a=new Date(0),r=parseInt(t[1],10)-1;var s=parseInt(t[2],10);return a.setUTCFullYear(n,r,s),a}if(t=m.exec(e))return i=parseInt(t[1],10)-1,x(n,i);if(t=f.exec(e)){i=parseInt(t[1],10)-1;var c=parseInt(t[2],10)-1;return x(n,i,c)}return null}(A.restDateString,M);if(S){var T,P=S.getTime(),I=0;if(C.time&&(I=function(e){var n,t,a;if(n=g.exec(e))return(t=parseFloat(n[1].replace(",",".")))%24*36e5;if(n=y.exec(e))return t=parseInt(n[1],10),a=parseFloat(n[2].replace(",",".")),t%24*36e5+6e4*a;if(n=v.exec(e)){t=parseInt(n[1],10),a=parseInt(n[2],10);var r=parseFloat(n[3].replace(",","."));return t%24*36e5+6e4*a+1e3*r}return null}(C.time)),C.timezone)T=6e4*function(e){var n,t;if(n=w.exec(e))return 0;if(n=k.exec(e))return t=60*parseInt(n[2],10),"+"===n[1]?-t:t;if(n=_.exec(e))return t=60*parseInt(n[2],10)+parseInt(n[3],10),"+"===n[1]?-t:t;return 0}(C.timezone);else{var D=P+I,L=new Date(D);T=a(L);var O=new Date(D);O.setDate(L.getDate()+1);var z=a(O)-a(L);z>0&&(T+=z)}return new Date(P+I+T)}return new Date(e)}},function(e,n,t){"use strict";t.d(n,"e",(function(){return a})),t.d(n,"b",(function(){return i})),t.d(n,"j",(function(){return o})),t.d(n,"g",(function(){return c})),t.d(n,"h",(function(){return l})),t.d(n,"i",(function(){return d})),t.d(n,"c",(function(){return u})),t.d(n,"f",(function(){return h})),t.d(n,"l",(function(){return p})),t.d(n,"m",(function(){return m})),t.d(n,"d",(function(){return g})),t.d(n,"k",(function(){return y})),t.d(n,"n",(function(){return v})),t.d(n,"a",(function(){return w}));t(29);const a=/#.*$/,r=/\.(md|html)$/,i=/\/$/,o=/^[a-z]+:/i;function s(e){return decodeURI(e).replace(a,"").replace(r,"")}function c(e){return o.test(e)}function l(e){return/^mailto:/.test(e)}function d(e){return/^tel:/.test(e)}function u(e){if(c(e))return e;if(!e)return"404";const n=e.match(a),t=n?n[0]:"",r=s(e);return i.test(r)?e:r+".html"+t}function h(e,n){const t=e.hash,r=function(e){const n=e&&e.match(a);if(n)return n[0]}(n);if(r&&t!==r)return!1;return s(e.path)===s(n)}function p(e,n,t){if(c(n))return{type:"external",path:n};t&&(n=function(e,n,t){const a=e.charAt(0);if("/"===a)return e;if("?"===a||"#"===a)return n+e;const r=n.split("/");t&&r[r.length-1]||r.pop();const i=e.replace(/^\//,"").split("/");for(let e=0;e<i.length;e++){const n=i[e];".."===n?r.pop():"."!==n&&r.push(n)}""!==r[0]&&r.unshift("");return r.join("/")}(n,t));const a=s(n);for(let n=0;n<e.length;n++)if(s(e[n].regularPath)===a)return Object.assign({},e[n],{type:"page",path:u(e[n].path)});return console.error(`[vuepress] No matching page found for sidebar item "${n}"`),{}}function m(e,n,t,a){const{pages:r,themeConfig:i}=t,o=a&&i.locales&&i.locales[a]||i;if("auto"===(e.frontmatter.sidebar||o.sidebar||i.sidebar))return f(e);const s=o.sidebar||i.sidebar;if(s){const{base:t,config:a}=function(e,n){if(Array.isArray(n))return{base:"/",config:n};for(const a in n)if(0===(t=e,/(\.html|\/)$/.test(t)?t:t+"/").indexOf(encodeURI(a)))return{base:a,config:n[a]};var t;return{}}(n,s);return"auto"===a?f(e):a?a.map(e=>function e(n,t,a,r=1){if("string"==typeof n)return p(t,n,a);if(Array.isArray(n))return Object.assign(p(t,n[0],a),{title:n[1]});{r>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const i=n.children||[];return 0===i.length&&n.path?Object.assign(p(t,n.path,a),{title:n.title}):{type:"group",path:n.path,title:n.title,sidebarDepth:n.sidebarDepth,initialOpenGroupIndex:n.initialOpenGroupIndex,children:i.map(n=>e(n,t,a,r+1)),collapsable:!1!==n.collapsable}}}(e,r,t)):[]}return[]}function f(e){const n=g(e.headers||[]);return[{type:"group",collapsable:!1,title:e.title,path:null,children:n.map(n=>({type:"auto",title:n.title,basePath:e.path,path:e.path+"#"+n.slug,children:n.children||[]}))}]}function g(e){let n;return(e=e.map(e=>Object.assign({},e))).forEach(e=>{2===e.level?n=e:n&&(n.children||(n.children=[])).push(e)}),e.filter(e=>2===e.level)}function y(e){return Object.assign(e,{type:e.items&&e.items.length?"links":"link"})}function v(e){return Object.prototype.toString.call(e).match(/\[object (.*?)\]/)[1].toLowerCase()}function b(e){let n=e.frontmatter.date||e.lastUpdated||new Date,t=new Date(n);return"Invalid Date"==t&&n&&(t=new Date(n.replace(/-/g,"/"))),t.getTime()}function w(e,n){return b(n)-b(e)}},function(e,n){e.exports=function(e){return null!=e&&"object"==typeof e}},function(e,n,t){"use strict";var a=t(272),r=t(273),i=t(274),o=t(275),s=t(99),c=t(20),l=t(276),d=Function,u=function(e){try{return d('"use strict"; return ('+e+").constructor;")()}catch(e){}},h=Object.getOwnPropertyDescriptor;if(h)try{h({},"")}catch(e){h=null}var p=function(){throw new c},m=h?function(){try{return p}catch(e){try{return h(arguments,"callee").get}catch(e){return p}}}():p,f=t(277)(),g=t(279)(),y=Object.getPrototypeOf||(g?function(e){return e.__proto__}:null),v={},b="undefined"!=typeof Uint8Array&&y?y(Uint8Array):void 0,w={__proto__:null,"%AggregateError%":"undefined"==typeof AggregateError?void 0:AggregateError,"%Array%":Array,"%ArrayBuffer%":"undefined"==typeof ArrayBuffer?void 0:ArrayBuffer,"%ArrayIteratorPrototype%":f&&y?y([][Symbol.iterator]()):void 0,"%AsyncFromSyncIteratorPrototype%":void 0,"%AsyncFunction%":v,"%AsyncGenerator%":v,"%AsyncGeneratorFunction%":v,"%AsyncIteratorPrototype%":v,"%Atomics%":"undefined"==typeof Atomics?void 0:Atomics,"%BigInt%":"undefined"==typeof BigInt?void 0:BigInt,"%BigInt64Array%":"undefined"==typeof BigInt64Array?void 0:BigInt64Array,"%BigUint64Array%":"undefined"==typeof BigUint64Array?void 0:BigUint64Array,"%Boolean%":Boolean,"%DataView%":"undefined"==typeof DataView?void 0:DataView,"%Date%":Date,"%decodeURI%":decodeURI,"%decodeURIComponent%":decodeURIComponent,"%encodeURI%":encodeURI,"%encodeURIComponent%":encodeURIComponent,"%Error%":a,"%eval%":eval,"%EvalError%":r,"%Float32Array%":"undefined"==typeof Float32Array?void 0:Float32Array,"%Float64Array%":"undefined"==typeof Float64Array?void 0:Float64Array,"%FinalizationRegistry%":"undefined"==typeof FinalizationRegistry?void 0:FinalizationRegistry,"%Function%":d,"%GeneratorFunction%":v,"%Int8Array%":"undefined"==typeof Int8Array?void 0:Int8Array,"%Int16Array%":"undefined"==typeof Int16Array?void 0:Int16Array,"%Int32Array%":"undefined"==typeof Int32Array?void 0:Int32Array,"%isFinite%":isFinite,"%isNaN%":isNaN,"%IteratorPrototype%":f&&y?y(y([][Symbol.iterator]())):void 0,"%JSON%":"object"==typeof JSON?JSON:void 0,"%Map%":"undefined"==typeof Map?void 0:Map,"%MapIteratorPrototype%":"undefined"!=typeof Map&&f&&y?y((new Map)[Symbol.iterator]()):void 0,"%Math%":Math,"%Number%":Number,"%Object%":Object,"%parseFloat%":parseFloat,"%parseInt%":parseInt,"%Promise%":"undefined"==typeof Promise?void 0:Promise,"%Proxy%":"undefined"==typeof Proxy?void 0:Proxy,"%RangeError%":i,"%ReferenceError%":o,"%Reflect%":"undefined"==typeof Reflect?void 0:Reflect,"%RegExp%":RegExp,"%Set%":"undefined"==typeof Set?void 0:Set,"%SetIteratorPrototype%":"undefined"!=typeof Set&&f&&y?y((new Set)[Symbol.iterator]()):void 0,"%SharedArrayBuffer%":"undefined"==typeof SharedArrayBuffer?void 0:SharedArrayBuffer,"%String%":String,"%StringIteratorPrototype%":f&&y?y(""[Symbol.iterator]()):void 0,"%Symbol%":f?Symbol:void 0,"%SyntaxError%":s,"%ThrowTypeError%":m,"%TypedArray%":b,"%TypeError%":c,"%Uint8Array%":"undefined"==typeof Uint8Array?void 0:Uint8Array,"%Uint8ClampedArray%":"undefined"==typeof Uint8ClampedArray?void 0:Uint8ClampedArray,"%Uint16Array%":"undefined"==typeof Uint16Array?void 0:Uint16Array,"%Uint32Array%":"undefined"==typeof Uint32Array?void 0:Uint32Array,"%URIError%":l,"%WeakMap%":"undefined"==typeof WeakMap?void 0:WeakMap,"%WeakRef%":"undefined"==typeof WeakRef?void 0:WeakRef,"%WeakSet%":"undefined"==typeof WeakSet?void 0:WeakSet};if(y)try{null.error}catch(e){var k=y(y(e));w["%Error.prototype%"]=k}var _={__proto__:null,"%ArrayBufferPrototype%":["ArrayBuffer","prototype"],"%ArrayPrototype%":["Array","prototype"],"%ArrayProto_entries%":["Array","prototype","entries"],"%ArrayProto_forEach%":["Array","prototype","forEach"],"%ArrayProto_keys%":["Array","prototype","keys"],"%ArrayProto_values%":["Array","prototype","values"],"%AsyncFunctionPrototype%":["AsyncFunction","prototype"],"%AsyncGenerator%":["AsyncGeneratorFunction","prototype"],"%AsyncGeneratorPrototype%":["AsyncGeneratorFunction","prototype","prototype"],"%BooleanPrototype%":["Boolean","prototype"],"%DataViewPrototype%":["DataView","prototype"],"%DatePrototype%":["Date","prototype"],"%ErrorPrototype%":["Error","prototype"],"%EvalErrorPrototype%":["EvalError","prototype"],"%Float32ArrayPrototype%":["Float32Array","prototype"],"%Float64ArrayPrototype%":["Float64Array","prototype"],"%FunctionPrototype%":["Function","prototype"],"%Generator%":["GeneratorFunction","prototype"],"%GeneratorPrototype%":["GeneratorFunction","prototype","prototype"],"%Int8ArrayPrototype%":["Int8Array","prototype"],"%Int16ArrayPrototype%":["Int16Array","prototype"],"%Int32ArrayPrototype%":["Int32Array","prototype"],"%JSONParse%":["JSON","parse"],"%JSONStringify%":["JSON","stringify"],"%MapPrototype%":["Map","prototype"],"%NumberPrototype%":["Number","prototype"],"%ObjectPrototype%":["Object","prototype"],"%ObjProto_toString%":["Object","prototype","toString"],"%ObjProto_valueOf%":["Object","prototype","valueOf"],"%PromisePrototype%":["Promise","prototype"],"%PromiseProto_then%":["Promise","prototype","then"],"%Promise_all%":["Promise","all"],"%Promise_reject%":["Promise","reject"],"%Promise_resolve%":["Promise","resolve"],"%RangeErrorPrototype%":["RangeError","prototype"],"%ReferenceErrorPrototype%":["ReferenceError","prototype"],"%RegExpPrototype%":["RegExp","prototype"],"%SetPrototype%":["Set","prototype"],"%SharedArrayBufferPrototype%":["SharedArrayBuffer","prototype"],"%StringPrototype%":["String","prototype"],"%SymbolPrototype%":["Symbol","prototype"],"%SyntaxErrorPrototype%":["SyntaxError","prototype"],"%TypedArrayPrototype%":["TypedArray","prototype"],"%TypeErrorPrototype%":["TypeError","prototype"],"%Uint8ArrayPrototype%":["Uint8Array","prototype"],"%Uint8ClampedArrayPrototype%":["Uint8ClampedArray","prototype"],"%Uint16ArrayPrototype%":["Uint16Array","prototype"],"%Uint32ArrayPrototype%":["Uint32Array","prototype"],"%URIErrorPrototype%":["URIError","prototype"],"%WeakMapPrototype%":["WeakMap","prototype"],"%WeakSetPrototype%":["WeakSet","prototype"]},x=t(52),C=t(281),A=x.call(Function.call,Array.prototype.concat),M=x.call(Function.apply,Array.prototype.splice),S=x.call(Function.call,String.prototype.replace),T=x.call(Function.call,String.prototype.slice),P=x.call(Function.call,RegExp.prototype.exec),I=/[^%.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|%$))/g,D=/\\(\\)?/g,L=function(e){var n=T(e,0,1),t=T(e,-1);if("%"===n&&"%"!==t)throw new s("invalid intrinsic syntax, expected closing `%`");if("%"===t&&"%"!==n)throw new s("invalid intrinsic syntax, expected opening `%`");var a=[];return S(e,I,(function(e,n,t,r){a[a.length]=t?S(r,D,"$1"):n||e})),a},O=function(e,n){var t,a=e;if(C(_,a)&&(a="%"+(t=_[a])[0]+"%"),C(w,a)){var r=w[a];if(r===v&&(r=function e(n){var t;if("%AsyncFunction%"===n)t=u("async function () {}");else if("%GeneratorFunction%"===n)t=u("function* () {}");else if("%AsyncGeneratorFunction%"===n)t=u("async function* () {}");else if("%AsyncGenerator%"===n){var a=e("%AsyncGeneratorFunction%");a&&(t=a.prototype)}else if("%AsyncIteratorPrototype%"===n){var r=e("%AsyncGenerator%");r&&y&&(t=y(r.prototype))}return w[n]=t,t}(a)),void 0===r&&!n)throw new c("intrinsic "+e+" exists, but is not available. Please file an issue!");return{alias:t,name:a,value:r}}throw new s("intrinsic "+e+" does not exist!")};e.exports=function(e,n){if("string"!=typeof e||0===e.length)throw new c("intrinsic name must be a non-empty string");if(arguments.length>1&&"boolean"!=typeof n)throw new c('"allowMissing" argument must be a boolean');if(null===P(/^%?[^%]*%?$/,e))throw new s("`%` may not be present anywhere but at the beginning and end of the intrinsic name");var t=L(e),a=t.length>0?t[0]:"",r=O("%"+a+"%",n),i=r.name,o=r.value,l=!1,d=r.alias;d&&(a=d[0],M(t,A([0,1],d)));for(var u=1,p=!0;u<t.length;u+=1){var m=t[u],f=T(m,0,1),g=T(m,-1);if(('"'===f||"'"===f||"`"===f||'"'===g||"'"===g||"`"===g)&&f!==g)throw new s("property names with quotes must have matching quotes");if("constructor"!==m&&p||(l=!0),C(w,i="%"+(a+="."+m)+"%"))o=w[i];else if(null!=o){if(!(m in o)){if(!n)throw new c("base intrinsic for "+e+" exists, but the property is not available.");return}if(h&&u+1>=t.length){var y=h(o,m);o=(p=!!y)&&"get"in y&&!("originalValue"in y.get)?y.get:o[m]}else p=C(o,m),o=o[m];p&&!l&&(w[i]=o)}}return o}},function(e,n,t){"use strict";var a=t(270),r=t(289),i=t(54);e.exports={formats:i,parse:r,stringify:a}},function(e,n,t){var a=t(19),r=t(173),i=t(174),o=a?a.toStringTag:void 0;e.exports=function(e){return null==e?void 0===e?"[object Undefined]":"[object Null]":o&&o in Object(e)?r(e):i(e)}},function(e,n,t){"use strict";var a=t(6),r=t(21),i=t(39);e.exports=a?function(e,n,t){return r.f(e,n,i(1,t))}:function(e,n,t){return e[n]=t,e}},function(e,n,t){var a=t(9).Symbol;e.exports=a},function(e,n,t){"use strict";e.exports=TypeError},function(e,n,t){"use strict";var a=t(6),r=t(73),i=t(124),o=t(57),s=t(64),c=TypeError,l=Object.defineProperty,d=Object.getOwnPropertyDescriptor;n.f=a?i?function(e,n,t){if(o(e),n=s(n),o(t),"function"==typeof e&&"prototype"===n&&"value"in t&&"writable"in t&&!t.writable){var a=d(e,n);a&&a.writable&&(e[n]=t.value,t={configurable:"configurable"in t?t.configurable:a.configurable,enumerable:"enumerable"in t?t.enumerable:a.enumerable,writable:!1})}return l(e,n,t)}:l:function(e,n,t){if(o(e),n=s(n),o(t),r)try{return l(e,n,t)}catch(e){}if("get"in t||"set"in t)throw new c("Accessors not supported");return"value"in t&&(e[n]=t.value),e}},function(e,n,t){"use strict";var a=t(3),r=a({}.toString),i=a("".slice);e.exports=function(e){return i(r(e),8,-1)}},function(e,n,t){var a=t(178),r=t(179),i=t(180),o=t(181),s=t(182);function c(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}c.prototype.clear=a,c.prototype.delete=r,c.prototype.get=i,c.prototype.has=o,c.prototype.set=s,e.exports=c},function(e,n,t){var a=t(80);e.exports=function(e,n){for(var t=e.length;t--;)if(a(e[t][0],n))return t;return-1}},function(e,n,t){var a=t(11)(Object,"create");e.exports=a},function(e,n,t){var a=t(200);e.exports=function(e,n){var t=e.__data__;return a(n)?t["string"==typeof n?"string":"hash"]:t.map}},function(e,n,t){var a=t(50);e.exports=function(e){if("string"==typeof e||a(e))return e;var n=e+"";return"0"==n&&1/e==-1/0?"-0":n}},function(e,n,t){var a,r;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(r="function"==typeof(a=function(){var e,n,t={version:"0.2.0"},a=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function r(e,n,t){return e<n?n:e>t?t:e}function i(e){return 100*(-1+e)}t.configure=function(e){var n,t;for(n in e)void 0!==(t=e[n])&&e.hasOwnProperty(n)&&(a[n]=t);return this},t.status=null,t.set=function(e){var n=t.isStarted();e=r(e,a.minimum,1),t.status=1===e?null:e;var c=t.render(!n),l=c.querySelector(a.barSelector),d=a.speed,u=a.easing;return c.offsetWidth,o((function(n){""===a.positionUsing&&(a.positionUsing=t.getPositioningCSS()),s(l,function(e,n,t){var r;return(r="translate3d"===a.positionUsing?{transform:"translate3d("+i(e)+"%,0,0)"}:"translate"===a.positionUsing?{transform:"translate("+i(e)+"%,0)"}:{"margin-left":i(e)+"%"}).transition="all "+n+"ms "+t,r}(e,d,u)),1===e?(s(c,{transition:"none",opacity:1}),c.offsetWidth,setTimeout((function(){s(c,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){t.remove(),n()}),d)}),d)):setTimeout(n,d)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var e=function(){setTimeout((function(){t.status&&(t.trickle(),e())}),a.trickleSpeed)};return a.trickle&&e(),this},t.done=function(e){return e||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(e){var n=t.status;return n?("number"!=typeof e&&(e=(1-n)*r(Math.random()*n,.1,.95)),n=r(n+e,0,.994),t.set(n)):t.start()},t.trickle=function(){return t.inc(Math.random()*a.trickleRate)},e=0,n=0,t.promise=function(a){return a&&"resolved"!==a.state()?(0===n&&t.start(),e++,n++,a.always((function(){0==--n?(e=0,t.done()):t.set((e-n)/e)})),this):this},t.render=function(e){if(t.isRendered())return document.getElementById("nprogress");l(document.documentElement,"nprogress-busy");var n=document.createElement("div");n.id="nprogress",n.innerHTML=a.template;var r,o=n.querySelector(a.barSelector),c=e?"-100":i(t.status||0),d=document.querySelector(a.parent);return s(o,{transition:"all 0 linear",transform:"translate3d("+c+"%,0,0)"}),a.showSpinner||(r=n.querySelector(a.spinnerSelector))&&h(r),d!=document.body&&l(d,"nprogress-custom-parent"),d.appendChild(n),n},t.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(a.parent),"nprogress-custom-parent");var e=document.getElementById("nprogress");e&&h(e)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var e=document.body.style,n="WebkitTransform"in e?"Webkit":"MozTransform"in e?"Moz":"msTransform"in e?"ms":"OTransform"in e?"O":"";return n+"Perspective"in e?"translate3d":n+"Transform"in e?"translate":"margin"};var o=function(){var e=[];function n(){var t=e.shift();t&&t(n)}return function(t){e.push(t),1==e.length&&n()}}(),s=function(){var e=["Webkit","O","Moz","ms"],n={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(e,n){return n.toUpperCase()})),n[t]||(n[t]=function(n){var t=document.body.style;if(n in t)return n;for(var a,r=e.length,i=n.charAt(0).toUpperCase()+n.slice(1);r--;)if((a=e[r]+i)in t)return a;return n}(t))}function a(e,n,a){n=t(n),e.style[n]=a}return function(e,n){var t,r,i=arguments;if(2==i.length)for(t in n)void 0!==(r=n[t])&&n.hasOwnProperty(t)&&a(e,t,r);else a(e,i[1],i[2])}}();function c(e,n){return("string"==typeof e?e:u(e)).indexOf(" "+n+" ")>=0}function l(e,n){var t=u(e),a=t+n;c(t,n)||(e.className=a.substring(1))}function d(e,n){var t,a=u(e);c(e,n)&&(t=a.replace(" "+n+" "," "),e.className=t.substring(1,t.length-1))}function u(e){return(" "+(e.className||"")+" ").replace(/\s+/gi," ")}function h(e){e&&e.parentNode&&e.parentNode.removeChild(e)}return t})?a.call(n,t,n,e):a)||(e.exports=r)},function(e,n,t){"use strict";var a=t(30),r=t(36),i=t(37),o=t(167),s=t(169);a({target:"Array",proto:!0,arity:1,forced:t(4)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(e){return e instanceof TypeError}}()},{push:function(e){var n=r(this),t=i(n),a=arguments.length;s(t+a);for(var c=0;c<a;c++)n[t]=arguments[c],t++;return o(n,t),t}})},function(e,n,t){"use strict";var a=t(1),r=t(62).f,i=t(18),o=t(120),s=t(42),c=t(74),l=t(148);e.exports=function(e,n){var t,d,u,h,p,m=e.target,f=e.global,g=e.stat;if(t=f?a:g?a[m]||s(m,{}):a[m]&&a[m].prototype)for(d in n){if(h=n[d],u=e.dontCallGetSet?(p=r(t,d))&&p.value:t[d],!l(f?d:m+(g?".":"#")+d,e.forced)&&void 0!==u){if(typeof h==typeof u)continue;c(h,u)}(e.sham||u&&u.sham)&&i(h,"sham",!0),o(t,d,h,e)}}},function(e,n,t){"use strict";var a=t(4);e.exports=!a((function(){var e=function(){}.bind();return"function"!=typeof e||e.hasOwnProperty("prototype")}))},function(e,n,t){"use strict";var a=t(58),r=t(40);e.exports=function(e){return a(r(e))}},function(e,n,t){"use strict";var a=t(1),r=t(2),i=function(e){return r(e)?e:void 0};e.exports=function(e,n){return arguments.length<2?i(a[e]):a[e]&&a[e][n]}},function(e,n,t){"use strict";var a=t(2),r=t(135),i=TypeError;e.exports=function(e){if(a(e))return e;throw new i(r(e)+" is not a function")}},function(e,n,t){"use strict";var a=t(1),r=t(70),i=t(10),o=t(72),s=t(68),c=t(67),l=a.Symbol,d=r("wks"),u=c?l.for||l:l&&l.withoutSetter||o;e.exports=function(e){return i(d,e)||(d[e]=s&&i(l,e)?l[e]:u("Symbol."+e)),d[e]}},function(e,n,t){"use strict";var a=t(40),r=Object;e.exports=function(e){return r(a(e))}},function(e,n,t){"use strict";var a=t(146);e.exports=function(e){return a(e.length)}},function(e,n,t){"use strict";var a=t(31),r=Function.prototype.call;e.exports=a?r.bind(r):function(){return r.apply(r,arguments)}},function(e,n,t){"use strict";e.exports=function(e,n){return{enumerable:!(1&e),configurable:!(2&e),writable:!(4&e),value:n}}},function(e,n,t){"use strict";var a=t(63),r=TypeError;e.exports=function(e){if(a(e))throw new r("Can't call method on "+e);return e}},function(e,n,t){"use strict";var a=t(71),r=t(1),i=t(42),o=e.exports=r["__core-js_shared__"]||i("__core-js_shared__",{});(o.versions||(o.versions=[])).push({version:"3.37.1",mode:a?"pure":"global",copyright:"© 2014-2024 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.37.1/LICENSE",source:"https://github.com/zloirock/core-js"})},function(e,n,t){"use strict";var a=t(1),r=Object.defineProperty;e.exports=function(e,n){try{r(a,e,{value:n,configurable:!0,writable:!0})}catch(t){a[e]=n}return n}},function(e,n,t){var a=t(172),r=t(14),i=Object.prototype,o=i.hasOwnProperty,s=i.propertyIsEnumerable,c=a(function(){return arguments}())?a:function(e){return r(e)&&o.call(e,"callee")&&!s.call(e,"callee")};e.exports=c},function(e,n,t){var a=t(11)(t(9),"Map");e.exports=a},function(e,n){e.exports=function(e){var n=typeof e;return null!=e&&("object"==n||"function"==n)}},function(e,n,t){var a=t(192),r=t(199),i=t(201),o=t(202),s=t(203);function c(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}c.prototype.clear=a,c.prototype.delete=r,c.prototype.get=i,c.prototype.has=o,c.prototype.set=s,e.exports=c},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e){t[++n]=e})),t}},function(e,n){e.exports=function(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=9007199254740991}},function(e,n,t){var a=t(7),r=t(50),i=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,o=/^\w*$/;e.exports=function(e,n){if(a(e))return!1;var t=typeof e;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=e&&!r(e))||(o.test(e)||!i.test(e)||null!=n&&e in Object(n))}},function(e,n,t){var a=t(17),r=t(14);e.exports=function(e){return"symbol"==typeof e||r(e)&&"[object Symbol]"==a(e)}},function(e,n){e.exports=function(e){return e}},function(e,n,t){"use strict";var a=t(280);e.exports=Function.prototype.bind||a},function(e,n,t){"use strict";var a=t(15)("%Object.defineProperty%",!0)||!1;if(a)try{a({},"a",{value:1})}catch(e){a=!1}e.exports=a},function(e,n,t){"use strict";var a=String.prototype.replace,r=/%20/g,i="RFC1738",o="RFC3986";e.exports={default:o,formatters:{RFC1738:function(e){return a.call(e,r,"+")},RFC3986:function(e){return String(e)}},RFC1738:i,RFC3986:o}},function(e,n,t){var a=t(296);e.exports=function(e){return a(e,{weekStartsOn:1})}},function(e,n,t){"use strict";var a=t(0),r=t(308),i=t(106),o={"Content-Type":"application/x-www-form-urlencoded"};function s(e,n){!a.isUndefined(e)&&a.isUndefined(e["Content-Type"])&&(e["Content-Type"]=n)}var c,l={transitional:{silentJSONParsing:!0,forcedJSONParsing:!0,clarifyTimeoutError:!1},adapter:(("undefined"!=typeof XMLHttpRequest||"undefined"!=typeof process&&"[object process]"===Object.prototype.toString.call(process))&&(c=t(107)),c),transformRequest:[function(e,n){return r(n,"Accept"),r(n,"Content-Type"),a.isFormData(e)||a.isArrayBuffer(e)||a.isBuffer(e)||a.isStream(e)||a.isFile(e)||a.isBlob(e)?e:a.isArrayBufferView(e)?e.buffer:a.isURLSearchParams(e)?(s(n,"application/x-www-form-urlencoded;charset=utf-8"),e.toString()):a.isObject(e)||n&&"application/json"===n["Content-Type"]?(s(n,"application/json"),function(e,n,t){if(a.isString(e))try{return(n||JSON.parse)(e),a.trim(e)}catch(e){if("SyntaxError"!==e.name)throw e}return(t||JSON.stringify)(e)}(e)):e}],transformResponse:[function(e){var n=this.transitional,t=n&&n.silentJSONParsing,r=n&&n.forcedJSONParsing,o=!t&&"json"===this.responseType;if(o||r&&a.isString(e)&&e.length)try{return JSON.parse(e)}catch(e){if(o){if("SyntaxError"===e.name)throw i(e,this,"E_JSON_PARSE");throw e}}return e}],timeout:0,xsrfCookieName:"XSRF-TOKEN",xsrfHeaderName:"X-XSRF-TOKEN",maxContentLength:-1,maxBodyLength:-1,validateStatus:function(e){return e>=200&&e<300}};l.headers={common:{Accept:"application/json, text/plain, */*"}},a.forEach(["delete","get","head"],(function(e){l.headers[e]={}})),a.forEach(["post","put","patch"],(function(e){l.headers[e]=a.merge(o)})),e.exports=l},function(e,n,t){"use strict";var a=t(8),r=String,i=TypeError;e.exports=function(e){if(a(e))return e;throw new i(r(e)+" is not an object")}},function(e,n,t){"use strict";var a=t(3),r=t(4),i=t(22),o=Object,s=a("".split);e.exports=r((function(){return!o("z").propertyIsEnumerable(0)}))?function(e){return"String"===i(e)?s(e,""):o(e)}:o},function(e,n,t){"use strict";e.exports={}},function(e,n){e.exports=function(e){return e.webpackPolyfill||(e.deprecate=function(){},e.paths=[],e.children||(e.children=[]),Object.defineProperty(e,"loaded",{enumerable:!0,get:function(){return e.l}}),Object.defineProperty(e,"id",{enumerable:!0,get:function(){return e.i}}),e.webpackPolyfill=1),e}},function(e,n){var t=/^\s+|\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,r=/^0b[01]+$/i,i=/^0o[0-7]+$/i,o=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,c="object"==typeof self&&self&&self.Object===Object&&self,l=s||c||Function("return this")(),d=Object.prototype.toString,u=Math.max,h=Math.min,p=function(){return l.Date.now()};function m(e){var n=typeof e;return!!e&&("object"==n||"function"==n)}function f(e){if("number"==typeof e)return e;if(function(e){return"symbol"==typeof e||function(e){return!!e&&"object"==typeof e}(e)&&"[object Symbol]"==d.call(e)}(e))return NaN;if(m(e)){var n="function"==typeof e.valueOf?e.valueOf():e;e=m(n)?n+"":n}if("string"!=typeof e)return 0===e?e:+e;e=e.replace(t,"");var s=r.test(e);return s||i.test(e)?o(e.slice(2),s?2:8):a.test(e)?NaN:+e}e.exports=function(e,n,t){var a,r,i,o,s,c,l=0,d=!1,g=!1,y=!0;if("function"!=typeof e)throw new TypeError("Expected a function");function v(n){var t=a,i=r;return a=r=void 0,l=n,o=e.apply(i,t)}function b(e){return l=e,s=setTimeout(k,n),d?v(e):o}function w(e){var t=e-c;return void 0===c||t>=n||t<0||g&&e-l>=i}function k(){var e=p();if(w(e))return _(e);s=setTimeout(k,function(e){var t=n-(e-c);return g?h(t,i-(e-l)):t}(e))}function _(e){return s=void 0,y&&a?v(e):(a=r=void 0,o)}function x(){var e=p(),t=w(e);if(a=arguments,r=this,c=e,t){if(void 0===s)return b(c);if(g)return s=setTimeout(k,n),v(c)}return void 0===s&&(s=setTimeout(k,n)),o}return n=f(n)||0,m(t)&&(d=!!t.leading,i=(g="maxWait"in t)?u(f(t.maxWait)||0,n):i,y="trailing"in t?!!t.trailing:y),x.cancel=function(){void 0!==s&&clearTimeout(s),l=0,a=c=r=s=void 0},x.flush=function(){return void 0===s?o:_(p())},x}},function(e,n,t){"use strict";var a=t(6),r=t(38),i=t(131),o=t(39),s=t(32),c=t(64),l=t(10),d=t(73),u=Object.getOwnPropertyDescriptor;n.f=a?u:function(e,n){if(e=s(e),n=c(n),d)try{return u(e,n)}catch(e){}if(l(e,n))return o(!r(i.f,e,n),e[n])}},function(e,n,t){"use strict";e.exports=function(e){return null==e}},function(e,n,t){"use strict";var a=t(132),r=t(65);e.exports=function(e){var n=a(e,"string");return r(n)?n:n+""}},function(e,n,t){"use strict";var a=t(33),r=t(2),i=t(66),o=t(67),s=Object;e.exports=o?function(e){return"symbol"==typeof e}:function(e){var n=a("Symbol");return r(n)&&i(n.prototype,s(e))}},function(e,n,t){"use strict";var a=t(3);e.exports=a({}.isPrototypeOf)},function(e,n,t){"use strict";var a=t(68);e.exports=a&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(e,n,t){"use strict";var a=t(69),r=t(4),i=t(1).String;e.exports=!!Object.getOwnPropertySymbols&&!r((function(){var e=Symbol("symbol detection");return!i(e)||!(Object(e)instanceof Symbol)||!Symbol.sham&&a&&a<41}))},function(e,n,t){"use strict";var a,r,i=t(1),o=t(133),s=i.process,c=i.Deno,l=s&&s.versions||c&&c.version,d=l&&l.v8;d&&(r=(a=d.split("."))[0]>0&&a[0]<4?1:+(a[0]+a[1])),!r&&o&&(!(a=o.match(/Edge\/(\d+)/))||a[1]>=74)&&(a=o.match(/Chrome\/(\d+)/))&&(r=+a[1]),e.exports=r},function(e,n,t){"use strict";var a=t(41);e.exports=function(e,n){return a[e]||(a[e]=n||{})}},function(e,n,t){"use strict";e.exports=!1},function(e,n,t){"use strict";var a=t(3),r=0,i=Math.random(),o=a(1..toString);e.exports=function(e){return"Symbol("+(void 0===e?"":e)+")_"+o(++r+i,36)}},function(e,n,t){"use strict";var a=t(6),r=t(4),i=t(123);e.exports=!a&&!r((function(){return 7!==Object.defineProperty(i("div"),"a",{get:function(){return 7}}).a}))},function(e,n,t){"use strict";var a=t(10),r=t(141),i=t(62),o=t(21);e.exports=function(e,n,t){for(var s=r(n),c=o.f,l=i.f,d=0;d<s.length;d++){var u=s[d];a(e,u)||t&&a(t,u)||c(e,u,l(n,u))}}},function(e,n,t){"use strict";var a=t(145);e.exports=function(e){var n=+e;return n!=n||0===n?0:a(n)}},function(e,n,t){"use strict";var a=t(155),r=t(8),i=t(40),o=t(156);e.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var e,n=!1,t={};try{(e=a(Object.prototype,"__proto__","set"))(t,[]),n=t instanceof Array}catch(e){}return function(t,a){return i(t),o(a),r(t)?(n?e(t,a):t.__proto__=a,t):t}}():void 0)},function(e,n){e.exports=function(e,n){for(var t=-1,a=n.length,r=e.length;++t<a;)e[r+t]=n[t];return e}},function(e,n){var t="object"==typeof global&&global&&global.Object===Object&&global;e.exports=t},function(e,n,t){var a=t(23),r=t(183),i=t(184),o=t(185),s=t(186),c=t(187);function l(e){var n=this.__data__=new a(e);this.size=n.size}l.prototype.clear=r,l.prototype.delete=i,l.prototype.get=o,l.prototype.has=s,l.prototype.set=c,e.exports=l},function(e,n){e.exports=function(e,n){return e===n||e!=e&&n!=n}},function(e,n,t){var a=t(17),r=t(45);e.exports=function(e){if(!r(e))return!1;var n=a(e);return"[object Function]"==n||"[object GeneratorFunction]"==n||"[object AsyncFunction]"==n||"[object Proxy]"==n}},function(e,n){var t=Function.prototype.toString;e.exports=function(e){if(null!=e){try{return t.call(e)}catch(e){}try{return e+""}catch(e){}}return""}},function(e,n,t){var a=t(204),r=t(14);e.exports=function e(n,t,i,o,s){return n===t||(null==n||null==t||!r(n)&&!r(t)?n!=n&&t!=t:a(n,t,i,o,e,s))}},function(e,n,t){var a=t(85),r=t(207),i=t(86);e.exports=function(e,n,t,o,s,c){var l=1&t,d=e.length,u=n.length;if(d!=u&&!(l&&u>d))return!1;var h=c.get(e),p=c.get(n);if(h&&p)return h==n&&p==e;var m=-1,f=!0,g=2&t?new a:void 0;for(c.set(e,n),c.set(n,e);++m<d;){var y=e[m],v=n[m];if(o)var b=l?o(v,y,m,n,e,c):o(y,v,m,e,n,c);if(void 0!==b){if(b)continue;f=!1;break}if(g){if(!r(n,(function(e,n){if(!i(g,n)&&(y===e||s(y,e,t,o,c)))return g.push(n)}))){f=!1;break}}else if(y!==v&&!s(y,v,t,o,c)){f=!1;break}}return c.delete(e),c.delete(n),f}},function(e,n,t){var a=t(46),r=t(205),i=t(206);function o(e){var n=-1,t=null==e?0:e.length;for(this.__data__=new a;++n<t;)this.add(e[n])}o.prototype.add=o.prototype.push=r,o.prototype.has=i,e.exports=o},function(e,n){e.exports=function(e,n){return e.has(n)}},function(e,n,t){var a=t(217),r=t(223),i=t(91);e.exports=function(e){return i(e)?a(e):r(e)}},function(e,n,t){(function(e){var a=t(9),r=t(219),i=n&&!n.nodeType&&n,o=i&&"object"==typeof e&&e&&!e.nodeType&&e,s=o&&o.exports===i?a.Buffer:void 0,c=(s?s.isBuffer:void 0)||r;e.exports=c}).call(this,t(60)(e))},function(e,n){var t=/^(?:0|[1-9]\d*)$/;e.exports=function(e,n){var a=typeof e;return!!(n=null==n?9007199254740991:n)&&("number"==a||"symbol"!=a&&t.test(e))&&e>-1&&e%1==0&&e<n}},function(e,n,t){var a=t(220),r=t(221),i=t(222),o=i&&i.isTypedArray,s=o?r(o):a;e.exports=s},function(e,n,t){var a=t(81),r=t(48);e.exports=function(e){return null!=e&&r(e.length)&&!a(e)}},function(e,n,t){var a=t(11)(t(9),"Set");e.exports=a},function(e,n,t){var a=t(45);e.exports=function(e){return e==e&&!a(e)}},function(e,n){e.exports=function(e,n){return function(t){return null!=t&&(t[e]===n&&(void 0!==n||e in Object(t)))}}},function(e,n,t){var a=t(96),r=t(27);e.exports=function(e,n){for(var t=0,i=(n=a(n,e)).length;null!=e&&t<i;)e=e[r(n[t++])];return t&&t==i?e:void 0}},function(e,n,t){var a=t(7),r=t(49),i=t(234),o=t(237);e.exports=function(e,n){return a(e)?e:r(e,n)?[e]:i(o(e))}},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";e.exports=SyntaxError},function(e,n,t){"use strict";var a=t(15)("%Object.getOwnPropertyDescriptor%",!0);if(a)try{a([],"length")}catch(e){a=null}e.exports=a},function(e,n,t){"use strict";var a=t(54),r=Object.prototype.hasOwnProperty,i=Array.isArray,o=function(){for(var e=[],n=0;n<256;++n)e.push("%"+((n<16?"0":"")+n.toString(16)).toUpperCase());return e}(),s=function(e,n){for(var t=n&&n.plainObjects?Object.create(null):{},a=0;a<e.length;++a)void 0!==e[a]&&(t[a]=e[a]);return t};e.exports={arrayToObject:s,assign:function(e,n){return Object.keys(n).reduce((function(e,t){return e[t]=n[t],e}),e)},combine:function(e,n){return[].concat(e,n)},compact:function(e){for(var n=[{obj:{o:e},prop:"o"}],t=[],a=0;a<n.length;++a)for(var r=n[a],o=r.obj[r.prop],s=Object.keys(o),c=0;c<s.length;++c){var l=s[c],d=o[l];"object"==typeof d&&null!==d&&-1===t.indexOf(d)&&(n.push({obj:o,prop:l}),t.push(d))}return function(e){for(;e.length>1;){var n=e.pop(),t=n.obj[n.prop];if(i(t)){for(var a=[],r=0;r<t.length;++r)void 0!==t[r]&&a.push(t[r]);n.obj[n.prop]=a}}}(n),e},decode:function(e,n,t){var a=e.replace(/\+/g," ");if("iso-8859-1"===t)return a.replace(/%[0-9a-f]{2}/gi,unescape);try{return decodeURIComponent(a)}catch(e){return a}},encode:function(e,n,t,r,i){if(0===e.length)return e;var s=e;if("symbol"==typeof e?s=Symbol.prototype.toString.call(e):"string"!=typeof e&&(s=String(e)),"iso-8859-1"===t)return escape(s).replace(/%u[0-9a-f]{4}/gi,(function(e){return"%26%23"+parseInt(e.slice(2),16)+"%3B"}));for(var c="",l=0;l<s.length;l+=1024){for(var d=s.length>=1024?s.slice(l,l+1024):s,u=[],h=0;h<d.length;++h){var p=d.charCodeAt(h);45===p||46===p||95===p||126===p||p>=48&&p<=57||p>=65&&p<=90||p>=97&&p<=122||i===a.RFC1738&&(40===p||41===p)?u[u.length]=d.charAt(h):p<128?u[u.length]=o[p]:p<2048?u[u.length]=o[192|p>>6]+o[128|63&p]:p<55296||p>=57344?u[u.length]=o[224|p>>12]+o[128|p>>6&63]+o[128|63&p]:(h+=1,p=65536+((1023&p)<<10|1023&d.charCodeAt(h)),u[u.length]=o[240|p>>18]+o[128|p>>12&63]+o[128|p>>6&63]+o[128|63&p])}c+=u.join("")}return c},isBuffer:function(e){return!(!e||"object"!=typeof e)&&!!(e.constructor&&e.constructor.isBuffer&&e.constructor.isBuffer(e))},isRegExp:function(e){return"[object RegExp]"===Object.prototype.toString.call(e)},maybeMap:function(e,n){if(i(e)){for(var t=[],a=0;a<e.length;a+=1)t.push(n(e[a]));return t}return n(e)},merge:function e(n,t,a){if(!t)return n;if("object"!=typeof t){if(i(n))n.push(t);else{if(!n||"object"!=typeof n)return[n,t];(a&&(a.plainObjects||a.allowPrototypes)||!r.call(Object.prototype,t))&&(n[t]=!0)}return n}if(!n||"object"!=typeof n)return[n].concat(t);var o=n;return i(n)&&!i(t)&&(o=s(n,a)),i(n)&&i(t)?(t.forEach((function(t,i){if(r.call(n,i)){var o=n[i];o&&"object"==typeof o&&t&&"object"==typeof t?n[i]=e(o,t,a):n.push(t)}else n[i]=t})),n):Object.keys(t).reduce((function(n,i){var o=t[i];return r.call(n,i)?n[i]=e(n[i],o,a):n[i]=o,n}),o)}}},function(e,n){e.exports=function(e){return e instanceof Date}},function(e,n,t){var a=t(12),r=t(55);e.exports=function(e){var n=a(e),t=n.getFullYear(),i=new Date(0);i.setFullYear(t+1,0,4),i.setHours(0,0,0,0);var o=r(i),s=new Date(0);s.setFullYear(t,0,4),s.setHours(0,0,0,0);var c=r(s);return n.getTime()>=o.getTime()?t+1:n.getTime()>=c.getTime()?t:t-1}},function(e,n,t){"use strict";e.exports=function(e,n){return function(){for(var t=new Array(arguments.length),a=0;a<t.length;a++)t[a]=arguments[a];return e.apply(n,t)}}},function(e,n,t){"use strict";var a=t(0);function r(e){return encodeURIComponent(e).replace(/%3A/gi,":").replace(/%24/g,"$").replace(/%2C/gi,",").replace(/%20/g,"+").replace(/%5B/gi,"[").replace(/%5D/gi,"]")}e.exports=function(e,n,t){if(!n)return e;var i;if(t)i=t(n);else if(a.isURLSearchParams(n))i=n.toString();else{var o=[];a.forEach(n,(function(e,n){null!=e&&(a.isArray(e)?n+="[]":e=[e],a.forEach(e,(function(e){a.isDate(e)?e=e.toISOString():a.isObject(e)&&(e=JSON.stringify(e)),o.push(r(n)+"="+r(e))})))})),i=o.join("&")}if(i){var s=e.indexOf("#");-1!==s&&(e=e.slice(0,s)),e+=(-1===e.indexOf("?")?"?":"&")+i}return e}},function(e,n,t){"use strict";e.exports=function(e,n,t,a,r){return e.config=n,t&&(e.code=t),e.request=a,e.response=r,e.isAxiosError=!0,e.toJSON=function(){return{message:this.message,name:this.name,description:this.description,number:this.number,fileName:this.fileName,lineNumber:this.lineNumber,columnNumber:this.columnNumber,stack:this.stack,config:this.config,code:this.code}},e}},function(e,n,t){"use strict";var a=t(0),r=t(309),i=t(310),o=t(105),s=t(311),c=t(314),l=t(315),d=t(108);e.exports=function(e){return new Promise((function(n,t){var u=e.data,h=e.headers,p=e.responseType;a.isFormData(u)&&delete h["Content-Type"];var m=new XMLHttpRequest;if(e.auth){var f=e.auth.username||"",g=e.auth.password?unescape(encodeURIComponent(e.auth.password)):"";h.Authorization="Basic "+btoa(f+":"+g)}var y=s(e.baseURL,e.url);function v(){if(m){var a="getAllResponseHeaders"in m?c(m.getAllResponseHeaders()):null,i={data:p&&"text"!==p&&"json"!==p?m.response:m.responseText,status:m.status,statusText:m.statusText,headers:a,config:e,request:m};r(n,t,i),m=null}}if(m.open(e.method.toUpperCase(),o(y,e.params,e.paramsSerializer),!0),m.timeout=e.timeout,"onloadend"in m?m.onloadend=v:m.onreadystatechange=function(){m&&4===m.readyState&&(0!==m.status||m.responseURL&&0===m.responseURL.indexOf("file:"))&&setTimeout(v)},m.onabort=function(){m&&(t(d("Request aborted",e,"ECONNABORTED",m)),m=null)},m.onerror=function(){t(d("Network Error",e,null,m)),m=null},m.ontimeout=function(){var n="timeout of "+e.timeout+"ms exceeded";e.timeoutErrorMessage&&(n=e.timeoutErrorMessage),t(d(n,e,e.transitional&&e.transitional.clarifyTimeoutError?"ETIMEDOUT":"ECONNABORTED",m)),m=null},a.isStandardBrowserEnv()){var b=(e.withCredentials||l(y))&&e.xsrfCookieName?i.read(e.xsrfCookieName):void 0;b&&(h[e.xsrfHeaderName]=b)}"setRequestHeader"in m&&a.forEach(h,(function(e,n){void 0===u&&"content-type"===n.toLowerCase()?delete h[n]:m.setRequestHeader(n,e)})),a.isUndefined(e.withCredentials)||(m.withCredentials=!!e.withCredentials),p&&"json"!==p&&(m.responseType=e.responseType),"function"==typeof e.onDownloadProgress&&m.addEventListener("progress",e.onDownloadProgress),"function"==typeof e.onUploadProgress&&m.upload&&m.upload.addEventListener("progress",e.onUploadProgress),e.cancelToken&&e.cancelToken.promise.then((function(e){m&&(m.abort(),t(e),m=null)})),u||(u=null),m.send(u)}))}},function(e,n,t){"use strict";var a=t(106);e.exports=function(e,n,t,r,i){var o=new Error(e);return a(o,n,t,r,i)}},function(e,n,t){"use strict";e.exports=function(e){return!(!e||!e.__CANCEL__)}},function(e,n,t){"use strict";var a=t(0);e.exports=function(e,n){n=n||{};var t={},r=["url","method","data"],i=["headers","auth","proxy","params"],o=["baseURL","transformRequest","transformResponse","paramsSerializer","timeout","timeoutMessage","withCredentials","adapter","responseType","xsrfCookieName","xsrfHeaderName","onUploadProgress","onDownloadProgress","decompress","maxContentLength","maxBodyLength","maxRedirects","transport","httpAgent","httpsAgent","cancelToken","socketPath","responseEncoding"],s=["validateStatus"];function c(e,n){return a.isPlainObject(e)&&a.isPlainObject(n)?a.merge(e,n):a.isPlainObject(n)?a.merge({},n):a.isArray(n)?n.slice():n}function l(r){a.isUndefined(n[r])?a.isUndefined(e[r])||(t[r]=c(void 0,e[r])):t[r]=c(e[r],n[r])}a.forEach(r,(function(e){a.isUndefined(n[e])||(t[e]=c(void 0,n[e]))})),a.forEach(i,l),a.forEach(o,(function(r){a.isUndefined(n[r])?a.isUndefined(e[r])||(t[r]=c(void 0,e[r])):t[r]=c(void 0,n[r])})),a.forEach(s,(function(a){a in n?t[a]=c(e[a],n[a]):a in e&&(t[a]=c(void 0,e[a]))}));var d=r.concat(i).concat(o).concat(s),u=Object.keys(e).concat(Object.keys(n)).filter((function(e){return-1===d.indexOf(e)}));return a.forEach(u,l),t}},function(e,n,t){"use strict";function a(e){this.message=e}a.prototype.toString=function(){return"Cancel"+(this.message?": "+this.message:"")},a.prototype.__CANCEL__=!0,e.exports=a},function(e,n,t){},function(e,n,t){},function(e,n,t){var a=t(170),r=t(175),i=t(246),o=t(254),s=t(263),c=t(128),l=i((function(e){var n=c(e);return s(n)&&(n=void 0),o(a(e,1,s,!0),r(n,2))}));e.exports=l},function(e,n,t){var a=t(290),r=t(295),i=t(103),o=t(12),s=t(298),c=t(299);var l={M:function(e){return e.getMonth()+1},MM:function(e){return h(e.getMonth()+1,2)},Q:function(e){return Math.ceil((e.getMonth()+1)/3)},D:function(e){return e.getDate()},DD:function(e){return h(e.getDate(),2)},DDD:function(e){return a(e)},DDDD:function(e){return h(a(e),3)},d:function(e){return e.getDay()},E:function(e){return e.getDay()||7},W:function(e){return r(e)},WW:function(e){return h(r(e),2)},YY:function(e){return h(e.getFullYear(),4).substr(2)},YYYY:function(e){return h(e.getFullYear(),4)},GG:function(e){return String(i(e)).substr(2)},GGGG:function(e){return i(e)},H:function(e){return e.getHours()},HH:function(e){return h(e.getHours(),2)},h:function(e){var n=e.getHours();return 0===n?12:n>12?n%12:n},hh:function(e){return h(l.h(e),2)},m:function(e){return e.getMinutes()},mm:function(e){return h(e.getMinutes(),2)},s:function(e){return e.getSeconds()},ss:function(e){return h(e.getSeconds(),2)},S:function(e){return Math.floor(e.getMilliseconds()/100)},SS:function(e){return h(Math.floor(e.getMilliseconds()/10),2)},SSS:function(e){return h(e.getMilliseconds(),3)},Z:function(e){return u(e.getTimezoneOffset(),":")},ZZ:function(e){return u(e.getTimezoneOffset())},X:function(e){return Math.floor(e.getTime()/1e3)},x:function(e){return e.getTime()}};function d(e){return e.match(/\[[\s\S]/)?e.replace(/^\[|]$/g,""):e.replace(/\\/g,"")}function u(e,n){n=n||"";var t=e>0?"-":"+",a=Math.abs(e),r=a%60;return t+h(Math.floor(a/60),2)+n+h(r,2)}function h(e,n){for(var t=Math.abs(e).toString();t.length<n;)t="0"+t;return t}e.exports=function(e,n,t){var a=n?String(n):"YYYY-MM-DDTHH:mm:ss.SSSZ",r=(t||{}).locale,i=c.format.formatters,u=c.format.formattingTokensRegExp;r&&r.format&&r.format.formatters&&(i=r.format.formatters,r.format.formattingTokensRegExp&&(u=r.format.formattingTokensRegExp));var h=o(e);return s(h)?function(e,n,t){var a,r,i=e.match(t),o=i.length;for(a=0;a<o;a++)r=n[i[a]]||l[i[a]],i[a]=r||d(i[a]);return function(e){for(var n="",t=0;t<o;t++)i[t]instanceof Function?n+=i[t](e,l):n+=i[t];return n}}(a,i,u)(h):"Invalid Date"}},function(e,n,t){e.exports=t(303)},function(e,n,t){"use strict";
/**
 * @file Embedded JavaScript templating engine. {@link http://ejs.co}
 * @author Matthew Eernisse <mde@fleegix.org>
 * @author Tiancheng "Timothy" Gu <timothygu99@gmail.com>
 * @project EJS
 * @license {@link http://www.apache.org/licenses/LICENSE-2.0 Apache License, Version 2.0}
 */var a=t(321),r=t(322),i=t(323),o=!1,s=t(324).version,c=["delimiter","scope","context","debug","compileDebug","client","_with","rmWhitespace","strict","filename","async"],l=c.concat("cache"),d=/^\uFEFF/,u=/^[a-zA-Z_$][0-9a-zA-Z_$]*$/;function h(e,t){var r;if(t.some((function(t){return r=n.resolveInclude(e,t,!0),a.existsSync(r)})))return r}function p(e,t){var a,r=e.filename,i=arguments.length>1;if(e.cache){if(!r)throw new Error("cache option requires a filename");if(a=n.cache.get(r))return a;i||(t=f(r).toString().replace(d,""))}else if(!i){if(!r)throw new Error("Internal EJS error: no file name or template provided");t=f(r).toString().replace(d,"")}return a=n.compile(t,e),e.cache&&n.cache.set(r,a),a}function m(e,t,a){var r;if(!a){if("function"==typeof n.promiseImpl)return new n.promiseImpl((function(n,a){try{n(r=p(e)(t))}catch(e){a(e)}}));throw new Error("Please provide a callback function")}try{r=p(e)(t)}catch(e){return a(e)}a(null,r)}function f(e){return n.fileLoader(e)}function g(e,t){var r=i.shallowCopy(i.createNullProtoObjWherePossible(),t);if(r.filename=function(e,t){var r,i,o=t.views,s=/^[A-Za-z]+:\\|^\//.exec(e);if(s&&s.length)e=e.replace(/^\/*/,""),r=Array.isArray(t.root)?h(e,t.root):n.resolveInclude(e,t.root||"/",!0);else if(t.filename&&(i=n.resolveInclude(e,t.filename),a.existsSync(i)&&(r=i)),!r&&Array.isArray(o)&&(r=h(e,o)),!r&&"function"!=typeof t.includer)throw new Error('Could not find the include file "'+t.escapeFunction(e)+'"');return r}(e,r),"function"==typeof t.includer){var o=t.includer(e,r.filename);if(o&&(o.filename&&(r.filename=o.filename),o.template))return p(r,o.template)}return p(r)}function y(e,n,t,a,r){var i=n.split("\n"),o=Math.max(a-3,0),s=Math.min(i.length,a+3),c=r(t),l=i.slice(o,s).map((function(e,n){var t=n+o+1;return(t==a?" >> ":"    ")+t+"| "+e})).join("\n");throw e.path=c,e.message=(c||"ejs")+":"+a+"\n"+l+"\n\n"+e.message,e}function v(e){return e.replace(/;(\s*$)/,"$1")}function b(e,t){var a=i.hasOwnOnlyObject(t),r=i.createNullProtoObjWherePossible();this.templateText=e,this.mode=null,this.truncate=!1,this.currentLine=1,this.source="",r.client=a.client||!1,r.escapeFunction=a.escape||a.escapeFunction||i.escapeXML,r.compileDebug=!1!==a.compileDebug,r.debug=!!a.debug,r.filename=a.filename,r.openDelimiter=a.openDelimiter||n.openDelimiter||"<",r.closeDelimiter=a.closeDelimiter||n.closeDelimiter||">",r.delimiter=a.delimiter||n.delimiter||"%",r.strict=a.strict||!1,r.context=a.context,r.cache=a.cache||!1,r.rmWhitespace=a.rmWhitespace,r.root=a.root,r.includer=a.includer,r.outputFunctionName=a.outputFunctionName,r.localsName=a.localsName||n.localsName||"locals",r.views=a.views,r.async=a.async,r.destructuredLocals=a.destructuredLocals,r.legacyInclude=void 0===a.legacyInclude||!!a.legacyInclude,r.strict?r._with=!1:r._with=void 0===a._with||a._with,this.opts=r,this.regex=this.createRegex()}n.cache=i.cache,n.fileLoader=a.readFileSync,n.localsName="locals",n.promiseImpl=new Function("return this;")().Promise,n.resolveInclude=function(e,n,t){var a=r.dirname,i=r.extname,o=(0,r.resolve)(t?n:a(n),e);return i(e)||(o+=".ejs"),o},n.compile=function(e,n){return n&&n.scope&&(o||(console.warn("`scope` option is deprecated and will be removed in EJS 3"),o=!0),n.context||(n.context=n.scope),delete n.scope),new b(e,n).compile()},n.render=function(e,n,t){var a=n||i.createNullProtoObjWherePossible(),r=t||i.createNullProtoObjWherePossible();return 2==arguments.length&&i.shallowCopyFromList(r,a,c),p(r,e)(a)},n.renderFile=function(){var e,n,t,a=Array.prototype.slice.call(arguments),r=a.shift(),o={filename:r};return"function"==typeof arguments[arguments.length-1]&&(e=a.pop()),a.length?(n=a.shift(),a.length?i.shallowCopy(o,a.pop()):(n.settings&&(n.settings.views&&(o.views=n.settings.views),n.settings["view cache"]&&(o.cache=!0),(t=n.settings["view options"])&&i.shallowCopy(o,t)),i.shallowCopyFromList(o,n,l)),o.filename=r):n=i.createNullProtoObjWherePossible(),m(o,n,e)},n.Template=b,n.clearCache=function(){n.cache.reset()},b.modes={EVAL:"eval",ESCAPED:"escaped",RAW:"raw",COMMENT:"comment",LITERAL:"literal"},b.prototype={createRegex:function(){var e="(<%%|%%>|<%=|<%-|<%_|<%#|<%|%>|-%>|_%>)",n=i.escapeRegExpChars(this.opts.delimiter),t=i.escapeRegExpChars(this.opts.openDelimiter),a=i.escapeRegExpChars(this.opts.closeDelimiter);return e=e.replace(/%/g,n).replace(/</g,t).replace(/>/g,a),new RegExp(e)},compile:function(){var e,n,t,a=this.opts,o="",s="",c=a.escapeFunction,l=a.filename?JSON.stringify(a.filename):"undefined";if(!this.source){if(this.generateSource(),o+='  var __output = "";\n  function __append(s) { if (s !== undefined && s !== null) __output += s }\n',a.outputFunctionName){if(!u.test(a.outputFunctionName))throw new Error("outputFunctionName is not a valid JS identifier.");o+="  var "+a.outputFunctionName+" = __append;\n"}if(a.localsName&&!u.test(a.localsName))throw new Error("localsName is not a valid JS identifier.");if(a.destructuredLocals&&a.destructuredLocals.length){for(var d="  var __locals = ("+a.localsName+" || {}),\n",h=0;h<a.destructuredLocals.length;h++){var p=a.destructuredLocals[h];if(!u.test(p))throw new Error("destructuredLocals["+h+"] is not a valid JS identifier.");h>0&&(d+=",\n  "),d+=p+" = __locals."+p}o+=d+";\n"}!1!==a._with&&(o+="  with ("+a.localsName+" || {}) {\n",s+="  }\n"),s+="  return __output;\n",this.source=o+this.source+s}e=a.compileDebug?"var __line = 1\n  , __lines = "+JSON.stringify(this.templateText)+"\n  , __filename = "+l+";\ntry {\n"+this.source+"} catch (e) {\n  rethrow(e, __lines, __filename, __line, escapeFn);\n}\n":this.source,a.client&&(e="escapeFn = escapeFn || "+c.toString()+";\n"+e,a.compileDebug&&(e="rethrow = rethrow || "+y.toString()+";\n"+e)),a.strict&&(e='"use strict";\n'+e),a.debug&&console.log(e),a.compileDebug&&a.filename&&(e=e+"\n//# sourceURL="+l+"\n");try{if(a.async)try{t=new Function("return (async function(){}).constructor;")()}catch(e){throw e instanceof SyntaxError?new Error("This environment does not support async/await"):e}else t=Function;n=new t(a.localsName+", escapeFn, include, rethrow",e)}catch(e){throw e instanceof SyntaxError&&(a.filename&&(e.message+=" in "+a.filename),e.message+=" while compiling ejs\n\n",e.message+="If the above error is not helpful, you may want to try EJS-Lint:\n",e.message+="https://github.com/RyanZim/EJS-Lint",a.async||(e.message+="\n",e.message+="Or, if you meant to create an async function, pass `async: true` as an option.")),e}var m=a.client?n:function(e){return n.apply(a.context,[e||i.createNullProtoObjWherePossible(),c,function(n,t){var r=i.shallowCopy(i.createNullProtoObjWherePossible(),e);return t&&(r=i.shallowCopy(r,t)),g(n,a)(r)},y])};if(a.filename&&"function"==typeof Object.defineProperty){var f=a.filename,v=r.basename(f,r.extname(f));try{Object.defineProperty(m,"name",{value:v,writable:!1,enumerable:!1,configurable:!0})}catch(e){}}return m},generateSource:function(){this.opts.rmWhitespace&&(this.templateText=this.templateText.replace(/[\r\n]+/g,"\n").replace(/^\s+|\s+$/gm,"")),this.templateText=this.templateText.replace(/[ \t]*<%_/gm,"<%_").replace(/_%>[ \t]*/gm,"_%>");var e=this,n=this.parseTemplateText(),t=this.opts.delimiter,a=this.opts.openDelimiter,r=this.opts.closeDelimiter;n&&n.length&&n.forEach((function(i,o){var s;if(0===i.indexOf(a+t)&&0!==i.indexOf(a+t+t)&&(s=n[o+2])!=t+r&&s!="-"+t+r&&s!="_"+t+r)throw new Error('Could not find matching close tag for "'+i+'".');e.scanLine(i)}))},parseTemplateText:function(){for(var e,n=this.templateText,t=this.regex,a=t.exec(n),r=[];a;)0!==(e=a.index)&&(r.push(n.substring(0,e)),n=n.slice(e)),r.push(a[0]),n=n.slice(a[0].length),a=t.exec(n);return n&&r.push(n),r},_addOutput:function(e){if(this.truncate&&(e=e.replace(/^(?:\r\n|\r|\n)/,""),this.truncate=!1),!e)return e;e=(e=(e=(e=e.replace(/\\/g,"\\\\")).replace(/\n/g,"\\n")).replace(/\r/g,"\\r")).replace(/"/g,'\\"'),this.source+='    ; __append("'+e+'")\n'},scanLine:function(e){var n,t=this.opts.delimiter,a=this.opts.openDelimiter,r=this.opts.closeDelimiter;switch(n=e.split("\n").length-1,e){case a+t:case a+t+"_":this.mode=b.modes.EVAL;break;case a+t+"=":this.mode=b.modes.ESCAPED;break;case a+t+"-":this.mode=b.modes.RAW;break;case a+t+"#":this.mode=b.modes.COMMENT;break;case a+t+t:this.mode=b.modes.LITERAL,this.source+='    ; __append("'+e.replace(a+t+t,a+t)+'")\n';break;case t+t+r:this.mode=b.modes.LITERAL,this.source+='    ; __append("'+e.replace(t+t+r,t+r)+'")\n';break;case t+r:case"-"+t+r:case"_"+t+r:this.mode==b.modes.LITERAL&&this._addOutput(e),this.mode=null,this.truncate=0===e.indexOf("-")||0===e.indexOf("_");break;default:if(this.mode){switch(this.mode){case b.modes.EVAL:case b.modes.ESCAPED:case b.modes.RAW:e.lastIndexOf("//")>e.lastIndexOf("\n")&&(e+="\n")}switch(this.mode){case b.modes.EVAL:this.source+="    ; "+e+"\n";break;case b.modes.ESCAPED:this.source+="    ; __append(escapeFn("+v(e)+"))\n";break;case b.modes.RAW:this.source+="    ; __append("+v(e)+")\n";break;case b.modes.COMMENT:break;case b.modes.LITERAL:this._addOutput(e)}}else this._addOutput(e)}this.opts.compileDebug&&n&&(this.currentLine+=n,this.source+="    ; __line = "+this.currentLine+"\n")}},n.escapeXML=i.escapeXML,n.__express=n.renderFile,n.VERSION=s,n.name="ejs","undefined"!=typeof window&&(window.ejs=n)},function(e,n,t){"use strict";t.r(n);var a={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},r=(t(266),t(5)),i=Object(r.a)(a,(function(){return(0,this._self._c)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);n.default=i.exports},function(e,n,t){"use strict";t.r(n);var a={name:"CodeGroup",data:()=>({codeTabs:[],activeCodeTabIndex:-1}),watch:{activeCodeTabIndex(e){this.codeTabs.forEach(e=>{e.elm.classList.remove("theme-code-block__active")}),this.codeTabs[e].elm.classList.add("theme-code-block__active")}},mounted(){this.codeTabs=(this.$slots.default||[]).filter(e=>Boolean(e.componentOptions)).map((e,n)=>(""===e.componentOptions.propsData.active&&(this.activeCodeTabIndex=n),{title:e.componentOptions.propsData.title,elm:e.elm})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab(e){this.activeCodeTabIndex=e}}},r=(t(267),t(5)),i=Object(r.a)(a,(function(){var e=this,n=e._self._c;return n("div",{staticClass:"theme-code-group"},[n("div",{staticClass:"theme-code-group__nav"},[n("ul",{staticClass:"theme-code-group__ul"},e._l(e.codeTabs,(function(t,a){return n("li",{key:t.title,staticClass:"theme-code-group__li"},[n("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":a===e.activeCodeTabIndex},on:{click:function(n){return e.changeCodeTab(a)}}},[e._v("\n            "+e._s(t.title)+"\n          ")])])})),0)]),e._v(" "),e._t("default"),e._v(" "),e.codeTabs.length<1?n("pre",{staticClass:"pre-blank"},[e._v("// Make sure to add code blocks to your code group")]):e._e()],2)}),[],!1,null,"2f5f1757",null);n.default=i.exports},function(e,n,t){"use strict";var a=t(2),r=t(21),i=t(125),o=t(42);e.exports=function(e,n,t,s){s||(s={});var c=s.enumerable,l=void 0!==s.name?s.name:n;if(a(t)&&i(t,l,s),s.global)c?e[n]=t:o(n,t);else{try{s.unsafe?e[n]&&(c=!0):delete e[n]}catch(e){}c?e[n]=t:r.f(e,n,{value:t,enumerable:!1,configurable:!s.nonConfigurable,writable:!s.nonWritable})}return e}},function(e,n,t){"use strict";e.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(e,n,t){"use strict";var a=t(161),r=String;e.exports=function(e){if("Symbol"===a(e))throw new TypeError("Cannot convert a Symbol value to a string");return r(e)}},function(e,n,t){"use strict";var a=t(1),r=t(8),i=a.document,o=r(i)&&r(i.createElement);e.exports=function(e){return o?i.createElement(e):{}}},function(e,n,t){"use strict";var a=t(6),r=t(4);e.exports=a&&r((function(){return 42!==Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(e,n,t){"use strict";var a=t(3),r=t(4),i=t(2),o=t(10),s=t(6),c=t(137).CONFIGURABLE,l=t(138),d=t(139),u=d.enforce,h=d.get,p=String,m=Object.defineProperty,f=a("".slice),g=a("".replace),y=a([].join),v=s&&!r((function(){return 8!==m((function(){}),"length",{value:8}).length})),b=String(String).split("String"),w=e.exports=function(e,n,t){"Symbol("===f(p(n),0,7)&&(n="["+g(p(n),/^Symbol\(([^)]*)\).*$/,"$1")+"]"),t&&t.getter&&(n="get "+n),t&&t.setter&&(n="set "+n),(!o(e,"name")||c&&e.name!==n)&&(s?m(e,"name",{value:n,configurable:!0}):e.name=n),v&&t&&o(t,"arity")&&e.length!==t.arity&&m(e,"length",{value:t.arity});try{t&&o(t,"constructor")&&t.constructor?s&&m(e,"prototype",{writable:!1}):e.prototype&&(e.prototype=void 0)}catch(e){}var a=u(e);return o(a,"source")||(a.source=y(b,"string"==typeof n?n:"")),e};Function.prototype.toString=w((function(){return i(this)&&h(this).source||l(this)}),"toString")},function(e,n,t){"use strict";var a=t(70),r=t(72),i=a("keys");e.exports=function(e){return i[e]||(i[e]=r(e))}},function(e,n,t){"use strict";var a=t(3),r=t(10),i=t(32),o=t(143).indexOf,s=t(59),c=a([].push);e.exports=function(e,n){var t,a=i(e),l=0,d=[];for(t in a)!r(s,t)&&r(a,t)&&c(d,t);for(;n.length>l;)r(a,t=n[l++])&&(~o(d,t)||c(d,t));return d}},function(e,n){e.exports=function(e){var n=null==e?0:e.length;return n?e[n-1]:void 0}},function(e,n,t){e.exports=t(328)},function(e,n,t){"use strict";var a=t(30),r=t(149).left,i=t(150),o=t(69);a({target:"Array",proto:!0,forced:!t(151)&&o>79&&o<83||!i("reduce")},{reduce:function(e){var n=arguments.length;return r(this,e,n,n>1?arguments[1]:void 0)}})},function(e,n,t){"use strict";var a={}.propertyIsEnumerable,r=Object.getOwnPropertyDescriptor,i=r&&!a.call({1:2},1);n.f=i?function(e){var n=r(this,e);return!!n&&n.enumerable}:a},function(e,n,t){"use strict";var a=t(38),r=t(8),i=t(65),o=t(134),s=t(136),c=t(35),l=TypeError,d=c("toPrimitive");e.exports=function(e,n){if(!r(e)||i(e))return e;var t,c=o(e,d);if(c){if(void 0===n&&(n="default"),t=a(c,e,n),!r(t)||i(t))return t;throw new l("Can't convert object to primitive value")}return void 0===n&&(n="number"),s(e,n)}},function(e,n,t){"use strict";e.exports="undefined"!=typeof navigator&&String(navigator.userAgent)||""},function(e,n,t){"use strict";var a=t(34),r=t(63);e.exports=function(e,n){var t=e[n];return r(t)?void 0:a(t)}},function(e,n,t){"use strict";var a=String;e.exports=function(e){try{return a(e)}catch(e){return"Object"}}},function(e,n,t){"use strict";var a=t(38),r=t(2),i=t(8),o=TypeError;e.exports=function(e,n){var t,s;if("string"===n&&r(t=e.toString)&&!i(s=a(t,e)))return s;if(r(t=e.valueOf)&&!i(s=a(t,e)))return s;if("string"!==n&&r(t=e.toString)&&!i(s=a(t,e)))return s;throw new o("Can't convert object to primitive value")}},function(e,n,t){"use strict";var a=t(6),r=t(10),i=Function.prototype,o=a&&Object.getOwnPropertyDescriptor,s=r(i,"name"),c=s&&"something"===function(){}.name,l=s&&(!a||a&&o(i,"name").configurable);e.exports={EXISTS:s,PROPER:c,CONFIGURABLE:l}},function(e,n,t){"use strict";var a=t(3),r=t(2),i=t(41),o=a(Function.toString);r(i.inspectSource)||(i.inspectSource=function(e){return o(e)}),e.exports=i.inspectSource},function(e,n,t){"use strict";var a,r,i,o=t(140),s=t(1),c=t(8),l=t(18),d=t(10),u=t(41),h=t(126),p=t(59),m=s.TypeError,f=s.WeakMap;if(o||u.state){var g=u.state||(u.state=new f);g.get=g.get,g.has=g.has,g.set=g.set,a=function(e,n){if(g.has(e))throw new m("Object already initialized");return n.facade=e,g.set(e,n),n},r=function(e){return g.get(e)||{}},i=function(e){return g.has(e)}}else{var y=h("state");p[y]=!0,a=function(e,n){if(d(e,y))throw new m("Object already initialized");return n.facade=e,l(e,y,n),n},r=function(e){return d(e,y)?e[y]:{}},i=function(e){return d(e,y)}}e.exports={set:a,get:r,has:i,enforce:function(e){return i(e)?r(e):a(e,{})},getterFor:function(e){return function(n){var t;if(!c(n)||(t=r(n)).type!==e)throw new m("Incompatible receiver, "+e+" required");return t}}}},function(e,n,t){"use strict";var a=t(1),r=t(2),i=a.WeakMap;e.exports=r(i)&&/native code/.test(String(i))},function(e,n,t){"use strict";var a=t(33),r=t(3),i=t(142),o=t(147),s=t(57),c=r([].concat);e.exports=a("Reflect","ownKeys")||function(e){var n=i.f(s(e)),t=o.f;return t?c(n,t(e)):n}},function(e,n,t){"use strict";var a=t(127),r=t(121).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(e){return a(e,r)}},function(e,n,t){"use strict";var a=t(32),r=t(144),i=t(37),o=function(e){return function(n,t,o){var s=a(n),c=i(s);if(0===c)return!e&&-1;var l,d=r(o,c);if(e&&t!=t){for(;c>d;)if((l=s[d++])!=l)return!0}else for(;c>d;d++)if((e||d in s)&&s[d]===t)return e||d||0;return!e&&-1}};e.exports={includes:o(!0),indexOf:o(!1)}},function(e,n,t){"use strict";var a=t(75),r=Math.max,i=Math.min;e.exports=function(e,n){var t=a(e);return t<0?r(t+n,0):i(t,n)}},function(e,n,t){"use strict";var a=Math.ceil,r=Math.floor;e.exports=Math.trunc||function(e){var n=+e;return(n>0?r:a)(n)}},function(e,n,t){"use strict";var a=t(75),r=Math.min;e.exports=function(e){var n=a(e);return n>0?r(n,9007199254740991):0}},function(e,n,t){"use strict";n.f=Object.getOwnPropertySymbols},function(e,n,t){"use strict";var a=t(4),r=t(2),i=/#|\.prototype\./,o=function(e,n){var t=c[s(e)];return t===d||t!==l&&(r(n)?a(n):!!n)},s=o.normalize=function(e){return String(e).replace(i,".").toLowerCase()},c=o.data={},l=o.NATIVE="N",d=o.POLYFILL="P";e.exports=o},function(e,n,t){"use strict";var a=t(34),r=t(36),i=t(58),o=t(37),s=TypeError,c="Reduce of empty array with no initial value",l=function(e){return function(n,t,l,d){var u=r(n),h=i(u),p=o(u);if(a(t),0===p&&l<2)throw new s(c);var m=e?p-1:0,f=e?-1:1;if(l<2)for(;;){if(m in h){d=h[m],m+=f;break}if(m+=f,e?m<0:p<=m)throw new s(c)}for(;e?m>=0:p>m;m+=f)m in h&&(d=t(d,h[m],m,u));return d}};e.exports={left:l(!1),right:l(!0)}},function(e,n,t){"use strict";var a=t(4);e.exports=function(e,n){var t=[][e];return!!t&&a((function(){t.call(null,n||function(){return 1},1)}))}},function(e,n,t){"use strict";var a=t(1),r=t(22);e.exports="process"===r(a.process)},function(e,n,t){"use strict";var a=t(30),r=t(1),i=t(153),o=t(154),s=r.WebAssembly,c=7!==new Error("e",{cause:7}).cause,l=function(e,n){var t={};t[e]=o(e,n,c),a({global:!0,constructor:!0,arity:1,forced:c},t)},d=function(e,n){if(s&&s[e]){var t={};t[e]=o("WebAssembly."+e,n,c),a({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:c},t)}};l("Error",(function(e){return function(n){return i(e,this,arguments)}})),l("EvalError",(function(e){return function(n){return i(e,this,arguments)}})),l("RangeError",(function(e){return function(n){return i(e,this,arguments)}})),l("ReferenceError",(function(e){return function(n){return i(e,this,arguments)}})),l("SyntaxError",(function(e){return function(n){return i(e,this,arguments)}})),l("TypeError",(function(e){return function(n){return i(e,this,arguments)}})),l("URIError",(function(e){return function(n){return i(e,this,arguments)}})),d("CompileError",(function(e){return function(n){return i(e,this,arguments)}})),d("LinkError",(function(e){return function(n){return i(e,this,arguments)}})),d("RuntimeError",(function(e){return function(n){return i(e,this,arguments)}}))},function(e,n,t){"use strict";var a=t(31),r=Function.prototype,i=r.apply,o=r.call;e.exports="object"==typeof Reflect&&Reflect.apply||(a?o.bind(i):function(){return o.apply(i,arguments)})},function(e,n,t){"use strict";var a=t(33),r=t(10),i=t(18),o=t(66),s=t(76),c=t(74),l=t(158),d=t(159),u=t(160),h=t(163),p=t(164),m=t(6),f=t(71);e.exports=function(e,n,t,g){var y=g?2:1,v=e.split("."),b=v[v.length-1],w=a.apply(null,v);if(w){var k=w.prototype;if(!f&&r(k,"cause")&&delete k.cause,!t)return w;var _=a("Error"),x=n((function(e,n){var t=u(g?n:e,void 0),a=g?new w(e):new w;return void 0!==t&&i(a,"message",t),p(a,x,a.stack,2),this&&o(k,this)&&d(a,this,x),arguments.length>y&&h(a,arguments[y]),a}));if(x.prototype=k,"Error"!==b?s?s(x,_):c(x,_,{name:!0}):m&&"stackTraceLimit"in w&&(l(x,w,"stackTraceLimit"),l(x,w,"prepareStackTrace")),c(x,w),!f)try{k.name!==b&&i(k,"name",b),k.constructor=x}catch(e){}return x}}},function(e,n,t){"use strict";var a=t(3),r=t(34);e.exports=function(e,n,t){try{return a(r(Object.getOwnPropertyDescriptor(e,n)[t]))}catch(e){}}},function(e,n,t){"use strict";var a=t(157),r=String,i=TypeError;e.exports=function(e){if(a(e))return e;throw new i("Can't set "+r(e)+" as a prototype")}},function(e,n,t){"use strict";var a=t(8);e.exports=function(e){return a(e)||null===e}},function(e,n,t){"use strict";var a=t(21).f;e.exports=function(e,n,t){t in e||a(e,t,{configurable:!0,get:function(){return n[t]},set:function(e){n[t]=e}})}},function(e,n,t){"use strict";var a=t(2),r=t(8),i=t(76);e.exports=function(e,n,t){var o,s;return i&&a(o=n.constructor)&&o!==t&&r(s=o.prototype)&&s!==t.prototype&&i(e,s),e}},function(e,n,t){"use strict";var a=t(122);e.exports=function(e,n){return void 0===e?arguments.length<2?"":n:a(e)}},function(e,n,t){"use strict";var a=t(162),r=t(2),i=t(22),o=t(35)("toStringTag"),s=Object,c="Arguments"===i(function(){return arguments}());e.exports=a?i:function(e){var n,t,a;return void 0===e?"Undefined":null===e?"Null":"string"==typeof(t=function(e,n){try{return e[n]}catch(e){}}(n=s(e),o))?t:c?i(n):"Object"===(a=i(n))&&r(n.callee)?"Arguments":a}},function(e,n,t){"use strict";var a={};a[t(35)("toStringTag")]="z",e.exports="[object z]"===String(a)},function(e,n,t){"use strict";var a=t(8),r=t(18);e.exports=function(e,n){a(n)&&"cause"in n&&r(e,"cause",n.cause)}},function(e,n,t){"use strict";var a=t(18),r=t(165),i=t(166),o=Error.captureStackTrace;e.exports=function(e,n,t,s){i&&(o?o(e,n):a(e,"stack",r(t,s)))}},function(e,n,t){"use strict";var a=t(3),r=Error,i=a("".replace),o=String(new r("zxcasd").stack),s=/\n\s*at [^:]*:[^\n]*/,c=s.test(o);e.exports=function(e,n){if(c&&"string"==typeof e&&!r.prepareStackTrace)for(;n--;)e=i(e,s,"");return e}},function(e,n,t){"use strict";var a=t(4),r=t(39);e.exports=!a((function(){var e=new Error("a");return!("stack"in e)||(Object.defineProperty(e,"stack",r(1,7)),7!==e.stack)}))},function(e,n,t){"use strict";var a=t(6),r=t(168),i=TypeError,o=Object.getOwnPropertyDescriptor,s=a&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(e){return e instanceof TypeError}}();e.exports=s?function(e,n){if(r(e)&&!o(e,"length").writable)throw new i("Cannot set read only .length");return e.length=n}:function(e,n){return e.length=n}},function(e,n,t){"use strict";var a=t(22);e.exports=Array.isArray||function(e){return"Array"===a(e)}},function(e,n,t){"use strict";var a=TypeError;e.exports=function(e){if(e>9007199254740991)throw a("Maximum allowed index exceeded");return e}},function(e,n,t){var a=t(77),r=t(171);e.exports=function e(n,t,i,o,s){var c=-1,l=n.length;for(i||(i=r),s||(s=[]);++c<l;){var d=n[c];t>0&&i(d)?t>1?e(d,t-1,i,o,s):a(s,d):o||(s[s.length]=d)}return s}},function(e,n,t){var a=t(19),r=t(43),i=t(7),o=a?a.isConcatSpreadable:void 0;e.exports=function(e){return i(e)||r(e)||!!(o&&e&&e[o])}},function(e,n,t){var a=t(17),r=t(14);e.exports=function(e){return r(e)&&"[object Arguments]"==a(e)}},function(e,n,t){var a=t(19),r=Object.prototype,i=r.hasOwnProperty,o=r.toString,s=a?a.toStringTag:void 0;e.exports=function(e){var n=i.call(e,s),t=e[s];try{e[s]=void 0;var a=!0}catch(e){}var r=o.call(e);return a&&(n?e[s]=t:delete e[s]),r}},function(e,n){var t=Object.prototype.toString;e.exports=function(e){return t.call(e)}},function(e,n,t){var a=t(176),r=t(232),i=t(51),o=t(7),s=t(243);e.exports=function(e){return"function"==typeof e?e:null==e?i:"object"==typeof e?o(e)?r(e[0],e[1]):a(e):s(e)}},function(e,n,t){var a=t(177),r=t(231),i=t(94);e.exports=function(e){var n=r(e);return 1==n.length&&n[0][2]?i(n[0][0],n[0][1]):function(t){return t===e||a(t,e,n)}}},function(e,n,t){var a=t(79),r=t(83);e.exports=function(e,n,t,i){var o=t.length,s=o,c=!i;if(null==e)return!s;for(e=Object(e);o--;){var l=t[o];if(c&&l[2]?l[1]!==e[l[0]]:!(l[0]in e))return!1}for(;++o<s;){var d=(l=t[o])[0],u=e[d],h=l[1];if(c&&l[2]){if(void 0===u&&!(d in e))return!1}else{var p=new a;if(i)var m=i(u,h,d,e,n,p);if(!(void 0===m?r(h,u,3,i,p):m))return!1}}return!0}},function(e,n){e.exports=function(){this.__data__=[],this.size=0}},function(e,n,t){var a=t(24),r=Array.prototype.splice;e.exports=function(e){var n=this.__data__,t=a(n,e);return!(t<0)&&(t==n.length-1?n.pop():r.call(n,t,1),--this.size,!0)}},function(e,n,t){var a=t(24);e.exports=function(e){var n=this.__data__,t=a(n,e);return t<0?void 0:n[t][1]}},function(e,n,t){var a=t(24);e.exports=function(e){return a(this.__data__,e)>-1}},function(e,n,t){var a=t(24);e.exports=function(e,n){var t=this.__data__,r=a(t,e);return r<0?(++this.size,t.push([e,n])):t[r][1]=n,this}},function(e,n,t){var a=t(23);e.exports=function(){this.__data__=new a,this.size=0}},function(e,n){e.exports=function(e){var n=this.__data__,t=n.delete(e);return this.size=n.size,t}},function(e,n){e.exports=function(e){return this.__data__.get(e)}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n,t){var a=t(23),r=t(44),i=t(46);e.exports=function(e,n){var t=this.__data__;if(t instanceof a){var o=t.__data__;if(!r||o.length<199)return o.push([e,n]),this.size=++t.size,this;t=this.__data__=new i(o)}return t.set(e,n),this.size=t.size,this}},function(e,n,t){var a=t(81),r=t(189),i=t(45),o=t(82),s=/^\[object .+?Constructor\]$/,c=Function.prototype,l=Object.prototype,d=c.toString,u=l.hasOwnProperty,h=RegExp("^"+d.call(u).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");e.exports=function(e){return!(!i(e)||r(e))&&(a(e)?h:s).test(o(e))}},function(e,n,t){var a,r=t(190),i=(a=/[^.]+$/.exec(r&&r.keys&&r.keys.IE_PROTO||""))?"Symbol(src)_1."+a:"";e.exports=function(e){return!!i&&i in e}},function(e,n,t){var a=t(9)["__core-js_shared__"];e.exports=a},function(e,n){e.exports=function(e,n){return null==e?void 0:e[n]}},function(e,n,t){var a=t(193),r=t(23),i=t(44);e.exports=function(){this.size=0,this.__data__={hash:new a,map:new(i||r),string:new a}}},function(e,n,t){var a=t(194),r=t(195),i=t(196),o=t(197),s=t(198);function c(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}c.prototype.clear=a,c.prototype.delete=r,c.prototype.get=i,c.prototype.has=o,c.prototype.set=s,e.exports=c},function(e,n,t){var a=t(25);e.exports=function(){this.__data__=a?a(null):{},this.size=0}},function(e,n){e.exports=function(e){var n=this.has(e)&&delete this.__data__[e];return this.size-=n?1:0,n}},function(e,n,t){var a=t(25),r=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;if(a){var t=n[e];return"__lodash_hash_undefined__"===t?void 0:t}return r.call(n,e)?n[e]:void 0}},function(e,n,t){var a=t(25),r=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;return a?void 0!==n[e]:r.call(n,e)}},function(e,n,t){var a=t(25);e.exports=function(e,n){var t=this.__data__;return this.size+=this.has(e)?0:1,t[e]=a&&void 0===n?"__lodash_hash_undefined__":n,this}},function(e,n,t){var a=t(26);e.exports=function(e){var n=a(this,e).delete(e);return this.size-=n?1:0,n}},function(e,n){e.exports=function(e){var n=typeof e;return"string"==n||"number"==n||"symbol"==n||"boolean"==n?"__proto__"!==e:null===e}},function(e,n,t){var a=t(26);e.exports=function(e){return a(this,e).get(e)}},function(e,n,t){var a=t(26);e.exports=function(e){return a(this,e).has(e)}},function(e,n,t){var a=t(26);e.exports=function(e,n){var t=a(this,e),r=t.size;return t.set(e,n),this.size+=t.size==r?0:1,this}},function(e,n,t){var a=t(79),r=t(84),i=t(208),o=t(211),s=t(227),c=t(7),l=t(88),d=t(90),u="[object Object]",h=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,p,m,f){var g=c(e),y=c(n),v=g?"[object Array]":s(e),b=y?"[object Array]":s(n),w=(v="[object Arguments]"==v?u:v)==u,k=(b="[object Arguments]"==b?u:b)==u,_=v==b;if(_&&l(e)){if(!l(n))return!1;g=!0,w=!1}if(_&&!w)return f||(f=new a),g||d(e)?r(e,n,t,p,m,f):i(e,n,v,t,p,m,f);if(!(1&t)){var x=w&&h.call(e,"__wrapped__"),C=k&&h.call(n,"__wrapped__");if(x||C){var A=x?e.value():e,M=C?n.value():n;return f||(f=new a),m(A,M,t,p,f)}}return!!_&&(f||(f=new a),o(e,n,t,p,m,f))}},function(e,n){e.exports=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length;++t<a;)if(n(e[t],t,e))return!0;return!1}},function(e,n,t){var a=t(19),r=t(209),i=t(80),o=t(84),s=t(210),c=t(47),l=a?a.prototype:void 0,d=l?l.valueOf:void 0;e.exports=function(e,n,t,a,l,u,h){switch(t){case"[object DataView]":if(e.byteLength!=n.byteLength||e.byteOffset!=n.byteOffset)return!1;e=e.buffer,n=n.buffer;case"[object ArrayBuffer]":return!(e.byteLength!=n.byteLength||!u(new r(e),new r(n)));case"[object Boolean]":case"[object Date]":case"[object Number]":return i(+e,+n);case"[object Error]":return e.name==n.name&&e.message==n.message;case"[object RegExp]":case"[object String]":return e==n+"";case"[object Map]":var p=s;case"[object Set]":var m=1&a;if(p||(p=c),e.size!=n.size&&!m)return!1;var f=h.get(e);if(f)return f==n;a|=2,h.set(e,n);var g=o(p(e),p(n),a,l,u,h);return h.delete(e),g;case"[object Symbol]":if(d)return d.call(e)==d.call(n)}return!1}},function(e,n,t){var a=t(9).Uint8Array;e.exports=a},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e,a){t[++n]=[a,e]})),t}},function(e,n,t){var a=t(212),r=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,i,o,s){var c=1&t,l=a(e),d=l.length;if(d!=a(n).length&&!c)return!1;for(var u=d;u--;){var h=l[u];if(!(c?h in n:r.call(n,h)))return!1}var p=s.get(e),m=s.get(n);if(p&&m)return p==n&&m==e;var f=!0;s.set(e,n),s.set(n,e);for(var g=c;++u<d;){var y=e[h=l[u]],v=n[h];if(i)var b=c?i(v,y,h,n,e,s):i(y,v,h,e,n,s);if(!(void 0===b?y===v||o(y,v,t,i,s):b)){f=!1;break}g||(g="constructor"==h)}if(f&&!g){var w=e.constructor,k=n.constructor;w==k||!("constructor"in e)||!("constructor"in n)||"function"==typeof w&&w instanceof w&&"function"==typeof k&&k instanceof k||(f=!1)}return s.delete(e),s.delete(n),f}},function(e,n,t){var a=t(213),r=t(214),i=t(87);e.exports=function(e){return a(e,i,r)}},function(e,n,t){var a=t(77),r=t(7);e.exports=function(e,n,t){var i=n(e);return r(e)?i:a(i,t(e))}},function(e,n,t){var a=t(215),r=t(216),i=Object.prototype.propertyIsEnumerable,o=Object.getOwnPropertySymbols,s=o?function(e){return null==e?[]:(e=Object(e),a(o(e),(function(n){return i.call(e,n)})))}:r;e.exports=s},function(e,n){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length,r=0,i=[];++t<a;){var o=e[t];n(o,t,e)&&(i[r++]=o)}return i}},function(e,n){e.exports=function(){return[]}},function(e,n,t){var a=t(218),r=t(43),i=t(7),o=t(88),s=t(89),c=t(90),l=Object.prototype.hasOwnProperty;e.exports=function(e,n){var t=i(e),d=!t&&r(e),u=!t&&!d&&o(e),h=!t&&!d&&!u&&c(e),p=t||d||u||h,m=p?a(e.length,String):[],f=m.length;for(var g in e)!n&&!l.call(e,g)||p&&("length"==g||u&&("offset"==g||"parent"==g)||h&&("buffer"==g||"byteLength"==g||"byteOffset"==g)||s(g,f))||m.push(g);return m}},function(e,n){e.exports=function(e,n){for(var t=-1,a=Array(e);++t<e;)a[t]=n(t);return a}},function(e,n){e.exports=function(){return!1}},function(e,n,t){var a=t(17),r=t(48),i=t(14),o={};o["[object Float32Array]"]=o["[object Float64Array]"]=o["[object Int8Array]"]=o["[object Int16Array]"]=o["[object Int32Array]"]=o["[object Uint8Array]"]=o["[object Uint8ClampedArray]"]=o["[object Uint16Array]"]=o["[object Uint32Array]"]=!0,o["[object Arguments]"]=o["[object Array]"]=o["[object ArrayBuffer]"]=o["[object Boolean]"]=o["[object DataView]"]=o["[object Date]"]=o["[object Error]"]=o["[object Function]"]=o["[object Map]"]=o["[object Number]"]=o["[object Object]"]=o["[object RegExp]"]=o["[object Set]"]=o["[object String]"]=o["[object WeakMap]"]=!1,e.exports=function(e){return i(e)&&r(e.length)&&!!o[a(e)]}},function(e,n){e.exports=function(e){return function(n){return e(n)}}},function(e,n,t){(function(e){var a=t(78),r=n&&!n.nodeType&&n,i=r&&"object"==typeof e&&e&&!e.nodeType&&e,o=i&&i.exports===r&&a.process,s=function(){try{var e=i&&i.require&&i.require("util").types;return e||o&&o.binding&&o.binding("util")}catch(e){}}();e.exports=s}).call(this,t(60)(e))},function(e,n,t){var a=t(224),r=t(225),i=Object.prototype.hasOwnProperty;e.exports=function(e){if(!a(e))return r(e);var n=[];for(var t in Object(e))i.call(e,t)&&"constructor"!=t&&n.push(t);return n}},function(e,n){var t=Object.prototype;e.exports=function(e){var n=e&&e.constructor;return e===("function"==typeof n&&n.prototype||t)}},function(e,n,t){var a=t(226)(Object.keys,Object);e.exports=a},function(e,n){e.exports=function(e,n){return function(t){return e(n(t))}}},function(e,n,t){var a=t(228),r=t(44),i=t(229),o=t(92),s=t(230),c=t(17),l=t(82),d=l(a),u=l(r),h=l(i),p=l(o),m=l(s),f=c;(a&&"[object DataView]"!=f(new a(new ArrayBuffer(1)))||r&&"[object Map]"!=f(new r)||i&&"[object Promise]"!=f(i.resolve())||o&&"[object Set]"!=f(new o)||s&&"[object WeakMap]"!=f(new s))&&(f=function(e){var n=c(e),t="[object Object]"==n?e.constructor:void 0,a=t?l(t):"";if(a)switch(a){case d:return"[object DataView]";case u:return"[object Map]";case h:return"[object Promise]";case p:return"[object Set]";case m:return"[object WeakMap]"}return n}),e.exports=f},function(e,n,t){var a=t(11)(t(9),"DataView");e.exports=a},function(e,n,t){var a=t(11)(t(9),"Promise");e.exports=a},function(e,n,t){var a=t(11)(t(9),"WeakMap");e.exports=a},function(e,n,t){var a=t(93),r=t(87);e.exports=function(e){for(var n=r(e),t=n.length;t--;){var i=n[t],o=e[i];n[t]=[i,o,a(o)]}return n}},function(e,n,t){var a=t(83),r=t(233),i=t(240),o=t(49),s=t(93),c=t(94),l=t(27);e.exports=function(e,n){return o(e)&&s(n)?c(l(e),n):function(t){var o=r(t,e);return void 0===o&&o===n?i(t,e):a(n,o,3)}}},function(e,n,t){var a=t(95);e.exports=function(e,n,t){var r=null==e?void 0:a(e,n);return void 0===r?t:r}},function(e,n,t){var a=t(235),r=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,i=/\\(\\)?/g,o=a((function(e){var n=[];return 46===e.charCodeAt(0)&&n.push(""),e.replace(r,(function(e,t,a,r){n.push(a?r.replace(i,"$1"):t||e)})),n}));e.exports=o},function(e,n,t){var a=t(236);e.exports=function(e){var n=a(e,(function(e){return 500===t.size&&t.clear(),e})),t=n.cache;return n}},function(e,n,t){var a=t(46);function r(e,n){if("function"!=typeof e||null!=n&&"function"!=typeof n)throw new TypeError("Expected a function");var t=function(){var a=arguments,r=n?n.apply(this,a):a[0],i=t.cache;if(i.has(r))return i.get(r);var o=e.apply(this,a);return t.cache=i.set(r,o)||i,o};return t.cache=new(r.Cache||a),t}r.Cache=a,e.exports=r},function(e,n,t){var a=t(238);e.exports=function(e){return null==e?"":a(e)}},function(e,n,t){var a=t(19),r=t(239),i=t(7),o=t(50),s=a?a.prototype:void 0,c=s?s.toString:void 0;e.exports=function e(n){if("string"==typeof n)return n;if(i(n))return r(n,e)+"";if(o(n))return c?c.call(n):"";var t=n+"";return"0"==t&&1/n==-1/0?"-0":t}},function(e,n){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length,r=Array(a);++t<a;)r[t]=n(e[t],t,e);return r}},function(e,n,t){var a=t(241),r=t(242);e.exports=function(e,n){return null!=e&&r(e,n,a)}},function(e,n){e.exports=function(e,n){return null!=e&&n in Object(e)}},function(e,n,t){var a=t(96),r=t(43),i=t(7),o=t(89),s=t(48),c=t(27);e.exports=function(e,n,t){for(var l=-1,d=(n=a(n,e)).length,u=!1;++l<d;){var h=c(n[l]);if(!(u=null!=e&&t(e,h)))break;e=e[h]}return u||++l!=d?u:!!(d=null==e?0:e.length)&&s(d)&&o(h,d)&&(i(e)||r(e))}},function(e,n,t){var a=t(244),r=t(245),i=t(49),o=t(27);e.exports=function(e){return i(e)?a(o(e)):r(e)}},function(e,n){e.exports=function(e){return function(n){return null==n?void 0:n[e]}}},function(e,n,t){var a=t(95);e.exports=function(e){return function(n){return a(n,e)}}},function(e,n,t){var a=t(51),r=t(247),i=t(249);e.exports=function(e,n){return i(r(e,n,a),e+"")}},function(e,n,t){var a=t(248),r=Math.max;e.exports=function(e,n,t){return n=r(void 0===n?e.length-1:n,0),function(){for(var i=arguments,o=-1,s=r(i.length-n,0),c=Array(s);++o<s;)c[o]=i[n+o];o=-1;for(var l=Array(n+1);++o<n;)l[o]=i[o];return l[n]=t(c),a(e,this,l)}}},function(e,n){e.exports=function(e,n,t){switch(t.length){case 0:return e.call(n);case 1:return e.call(n,t[0]);case 2:return e.call(n,t[0],t[1]);case 3:return e.call(n,t[0],t[1],t[2])}return e.apply(n,t)}},function(e,n,t){var a=t(250),r=t(253)(a);e.exports=r},function(e,n,t){var a=t(251),r=t(252),i=t(51),o=r?function(e,n){return r(e,"toString",{configurable:!0,enumerable:!1,value:a(n),writable:!0})}:i;e.exports=o},function(e,n){e.exports=function(e){return function(){return e}}},function(e,n,t){var a=t(11),r=function(){try{var e=a(Object,"defineProperty");return e({},"",{}),e}catch(e){}}();e.exports=r},function(e,n){var t=Date.now;e.exports=function(e){var n=0,a=0;return function(){var r=t(),i=16-(r-a);if(a=r,i>0){if(++n>=800)return arguments[0]}else n=0;return e.apply(void 0,arguments)}}},function(e,n,t){var a=t(85),r=t(255),i=t(260),o=t(86),s=t(261),c=t(47);e.exports=function(e,n,t){var l=-1,d=r,u=e.length,h=!0,p=[],m=p;if(t)h=!1,d=i;else if(u>=200){var f=n?null:s(e);if(f)return c(f);h=!1,d=o,m=new a}else m=n?[]:p;e:for(;++l<u;){var g=e[l],y=n?n(g):g;if(g=t||0!==g?g:0,h&&y==y){for(var v=m.length;v--;)if(m[v]===y)continue e;n&&m.push(y),p.push(g)}else d(m,y,t)||(m!==p&&m.push(y),p.push(g))}return p}},function(e,n,t){var a=t(256);e.exports=function(e,n){return!!(null==e?0:e.length)&&a(e,n,0)>-1}},function(e,n,t){var a=t(257),r=t(258),i=t(259);e.exports=function(e,n,t){return n==n?i(e,n,t):a(e,r,t)}},function(e,n){e.exports=function(e,n,t,a){for(var r=e.length,i=t+(a?1:-1);a?i--:++i<r;)if(n(e[i],i,e))return i;return-1}},function(e,n){e.exports=function(e){return e!=e}},function(e,n){e.exports=function(e,n,t){for(var a=t-1,r=e.length;++a<r;)if(e[a]===n)return a;return-1}},function(e,n){e.exports=function(e,n,t){for(var a=-1,r=null==e?0:e.length;++a<r;)if(t(n,e[a]))return!0;return!1}},function(e,n,t){var a=t(92),r=t(262),i=t(47),o=a&&1/i(new a([,-0]))[1]==1/0?function(e){return new a(e)}:r;e.exports=o},function(e,n){e.exports=function(){}},function(e,n,t){var a=t(91),r=t(14);e.exports=function(e){return r(e)&&a(e)}},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";t(97)},function(e,n,t){"use strict";t(98)},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";var a=t(271),r=t(101),i=t(54),o=Object.prototype.hasOwnProperty,s={brackets:function(e){return e+"[]"},comma:"comma",indices:function(e,n){return e+"["+n+"]"},repeat:function(e){return e}},c=Array.isArray,l=Array.prototype.push,d=function(e,n){l.apply(e,c(n)?n:[n])},u=Date.prototype.toISOString,h=i.default,p={addQueryPrefix:!1,allowDots:!1,allowEmptyArrays:!1,arrayFormat:"indices",charset:"utf-8",charsetSentinel:!1,delimiter:"&",encode:!0,encodeDotInKeys:!1,encoder:r.encode,encodeValuesOnly:!1,format:h,formatter:i.formatters[h],indices:!1,serializeDate:function(e){return u.call(e)},skipNulls:!1,strictNullHandling:!1},m={},f=function e(n,t,i,o,s,l,u,h,f,g,y,v,b,w,k,_,x,C){for(var A,M=n,S=C,T=0,P=!1;void 0!==(S=S.get(m))&&!P;){var I=S.get(n);if(T+=1,void 0!==I){if(I===T)throw new RangeError("Cyclic object value");P=!0}void 0===S.get(m)&&(T=0)}if("function"==typeof g?M=g(t,M):M instanceof Date?M=b(M):"comma"===i&&c(M)&&(M=r.maybeMap(M,(function(e){return e instanceof Date?b(e):e}))),null===M){if(l)return f&&!_?f(t,p.encoder,x,"key",w):t;M=""}if("string"==typeof(A=M)||"number"==typeof A||"boolean"==typeof A||"symbol"==typeof A||"bigint"==typeof A||r.isBuffer(M))return f?[k(_?t:f(t,p.encoder,x,"key",w))+"="+k(f(M,p.encoder,x,"value",w))]:[k(t)+"="+k(String(M))];var D,L=[];if(void 0===M)return L;if("comma"===i&&c(M))_&&f&&(M=r.maybeMap(M,f)),D=[{value:M.length>0?M.join(",")||null:void 0}];else if(c(g))D=g;else{var O=Object.keys(M);D=y?O.sort(y):O}var z=h?t.replace(/\./g,"%2E"):t,R=o&&c(M)&&1===M.length?z+"[]":z;if(s&&c(M)&&0===M.length)return R+"[]";for(var E=0;E<D.length;++E){var U=D[E],j="object"==typeof U&&void 0!==U.value?U.value:M[U];if(!u||null!==j){var B=v&&h?U.replace(/\./g,"%2E"):U,G=c(M)?"function"==typeof i?i(R,B):R:R+(v?"."+B:"["+B+"]");C.set(n,T);var N=a();N.set(m,C),d(L,e(j,G,i,o,s,l,u,h,"comma"===i&&_&&c(M)?null:f,g,y,v,b,w,k,_,x,N))}}return L};e.exports=function(e,n){var t,r=e,l=function(e){if(!e)return p;if(void 0!==e.allowEmptyArrays&&"boolean"!=typeof e.allowEmptyArrays)throw new TypeError("`allowEmptyArrays` option can only be `true` or `false`, when provided");if(void 0!==e.encodeDotInKeys&&"boolean"!=typeof e.encodeDotInKeys)throw new TypeError("`encodeDotInKeys` option can only be `true` or `false`, when provided");if(null!==e.encoder&&void 0!==e.encoder&&"function"!=typeof e.encoder)throw new TypeError("Encoder has to be a function.");var n=e.charset||p.charset;if(void 0!==e.charset&&"utf-8"!==e.charset&&"iso-8859-1"!==e.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");var t=i.default;if(void 0!==e.format){if(!o.call(i.formatters,e.format))throw new TypeError("Unknown format option provided.");t=e.format}var a,r=i.formatters[t],l=p.filter;if(("function"==typeof e.filter||c(e.filter))&&(l=e.filter),a=e.arrayFormat in s?e.arrayFormat:"indices"in e?e.indices?"indices":"repeat":p.arrayFormat,"commaRoundTrip"in e&&"boolean"!=typeof e.commaRoundTrip)throw new TypeError("`commaRoundTrip` must be a boolean, or absent");var d=void 0===e.allowDots?!0===e.encodeDotInKeys||p.allowDots:!!e.allowDots;return{addQueryPrefix:"boolean"==typeof e.addQueryPrefix?e.addQueryPrefix:p.addQueryPrefix,allowDots:d,allowEmptyArrays:"boolean"==typeof e.allowEmptyArrays?!!e.allowEmptyArrays:p.allowEmptyArrays,arrayFormat:a,charset:n,charsetSentinel:"boolean"==typeof e.charsetSentinel?e.charsetSentinel:p.charsetSentinel,commaRoundTrip:e.commaRoundTrip,delimiter:void 0===e.delimiter?p.delimiter:e.delimiter,encode:"boolean"==typeof e.encode?e.encode:p.encode,encodeDotInKeys:"boolean"==typeof e.encodeDotInKeys?e.encodeDotInKeys:p.encodeDotInKeys,encoder:"function"==typeof e.encoder?e.encoder:p.encoder,encodeValuesOnly:"boolean"==typeof e.encodeValuesOnly?e.encodeValuesOnly:p.encodeValuesOnly,filter:l,format:t,formatter:r,serializeDate:"function"==typeof e.serializeDate?e.serializeDate:p.serializeDate,skipNulls:"boolean"==typeof e.skipNulls?e.skipNulls:p.skipNulls,sort:"function"==typeof e.sort?e.sort:null,strictNullHandling:"boolean"==typeof e.strictNullHandling?e.strictNullHandling:p.strictNullHandling}}(n);"function"==typeof l.filter?r=(0,l.filter)("",r):c(l.filter)&&(t=l.filter);var u=[];if("object"!=typeof r||null===r)return"";var h=s[l.arrayFormat],m="comma"===h&&l.commaRoundTrip;t||(t=Object.keys(r)),l.sort&&t.sort(l.sort);for(var g=a(),y=0;y<t.length;++y){var v=t[y];l.skipNulls&&null===r[v]||d(u,f(r[v],v,h,m,l.allowEmptyArrays,l.strictNullHandling,l.skipNulls,l.encodeDotInKeys,l.encode?l.encoder:null,l.filter,l.sort,l.allowDots,l.serializeDate,l.format,l.formatter,l.encodeValuesOnly,l.charset,g))}var b=u.join(l.delimiter),w=!0===l.addQueryPrefix?"?":"";return l.charsetSentinel&&("iso-8859-1"===l.charset?w+="utf8=%26%2310003%3B&":w+="utf8=%E2%9C%93&"),b.length>0?w+b:""}},function(e,n,t){"use strict";var a=t(15),r=t(282),i=t(287),o=t(20),s=a("%WeakMap%",!0),c=a("%Map%",!0),l=r("WeakMap.prototype.get",!0),d=r("WeakMap.prototype.set",!0),u=r("WeakMap.prototype.has",!0),h=r("Map.prototype.get",!0),p=r("Map.prototype.set",!0),m=r("Map.prototype.has",!0),f=function(e,n){for(var t,a=e;null!==(t=a.next);a=t)if(t.key===n)return a.next=t.next,t.next=e.next,e.next=t,t};e.exports=function(){var e,n,t,a={assert:function(e){if(!a.has(e))throw new o("Side channel does not contain "+i(e))},get:function(a){if(s&&a&&("object"==typeof a||"function"==typeof a)){if(e)return l(e,a)}else if(c){if(n)return h(n,a)}else if(t)return function(e,n){var t=f(e,n);return t&&t.value}(t,a)},has:function(a){if(s&&a&&("object"==typeof a||"function"==typeof a)){if(e)return u(e,a)}else if(c){if(n)return m(n,a)}else if(t)return function(e,n){return!!f(e,n)}(t,a);return!1},set:function(a,r){s&&a&&("object"==typeof a||"function"==typeof a)?(e||(e=new s),d(e,a,r)):c?(n||(n=new c),p(n,a,r)):(t||(t={key:{},next:null}),function(e,n,t){var a=f(e,n);a?a.value=t:e.next={key:n,next:e.next,value:t}}(t,a,r))}};return a}},function(e,n,t){"use strict";e.exports=Error},function(e,n,t){"use strict";e.exports=EvalError},function(e,n,t){"use strict";e.exports=RangeError},function(e,n,t){"use strict";e.exports=ReferenceError},function(e,n,t){"use strict";e.exports=URIError},function(e,n,t){"use strict";var a="undefined"!=typeof Symbol&&Symbol,r=t(278);e.exports=function(){return"function"==typeof a&&("function"==typeof Symbol&&("symbol"==typeof a("foo")&&("symbol"==typeof Symbol("bar")&&r())))}},function(e,n,t){"use strict";e.exports=function(){if("function"!=typeof Symbol||"function"!=typeof Object.getOwnPropertySymbols)return!1;if("symbol"==typeof Symbol.iterator)return!0;var e={},n=Symbol("test"),t=Object(n);if("string"==typeof n)return!1;if("[object Symbol]"!==Object.prototype.toString.call(n))return!1;if("[object Symbol]"!==Object.prototype.toString.call(t))return!1;for(n in e[n]=42,e)return!1;if("function"==typeof Object.keys&&0!==Object.keys(e).length)return!1;if("function"==typeof Object.getOwnPropertyNames&&0!==Object.getOwnPropertyNames(e).length)return!1;var a=Object.getOwnPropertySymbols(e);if(1!==a.length||a[0]!==n)return!1;if(!Object.prototype.propertyIsEnumerable.call(e,n))return!1;if("function"==typeof Object.getOwnPropertyDescriptor){var r=Object.getOwnPropertyDescriptor(e,n);if(42!==r.value||!0!==r.enumerable)return!1}return!0}},function(e,n,t){"use strict";var a={__proto__:null,foo:{}},r=Object;e.exports=function(){return{__proto__:a}.foo===a.foo&&!(a instanceof r)}},function(e,n,t){"use strict";var a="Function.prototype.bind called on incompatible ",r=Object.prototype.toString,i=Math.max,o=function(e,n){for(var t=[],a=0;a<e.length;a+=1)t[a]=e[a];for(var r=0;r<n.length;r+=1)t[r+e.length]=n[r];return t},s=function(e,n){for(var t=[],a=n||0,r=0;a<e.length;a+=1,r+=1)t[r]=e[a];return t},c=function(e,n){for(var t="",a=0;a<e.length;a+=1)t+=e[a],a+1<e.length&&(t+=n);return t};e.exports=function(e){var n=this;if("function"!=typeof n||"[object Function]"!==r.apply(n))throw new TypeError(a+n);for(var t,l=s(arguments,1),d=function(){if(this instanceof t){var a=n.apply(this,o(l,arguments));return Object(a)===a?a:this}return n.apply(e,o(l,arguments))},u=i(0,n.length-l.length),h=[],p=0;p<u;p++)h[p]="$"+p;if(t=Function("binder","return function ("+c(h,",")+"){ return binder.apply(this,arguments); }")(d),n.prototype){var m=function(){};m.prototype=n.prototype,t.prototype=new m,m.prototype=null}return t}},function(e,n,t){"use strict";var a=Function.prototype.call,r=Object.prototype.hasOwnProperty,i=t(52);e.exports=i.call(a,r)},function(e,n,t){"use strict";var a=t(15),r=t(283),i=r(a("String.prototype.indexOf"));e.exports=function(e,n){var t=a(e,!!n);return"function"==typeof t&&i(e,".prototype.")>-1?r(t):t}},function(e,n,t){"use strict";var a=t(52),r=t(15),i=t(284),o=t(20),s=r("%Function.prototype.apply%"),c=r("%Function.prototype.call%"),l=r("%Reflect.apply%",!0)||a.call(c,s),d=t(53),u=r("%Math.max%");e.exports=function(e){if("function"!=typeof e)throw new o("a function is required");var n=l(a,c,arguments);return i(n,1+u(0,e.length-(arguments.length-1)),!0)};var h=function(){return l(a,s,arguments)};d?d(e.exports,"apply",{value:h}):e.exports.apply=h},function(e,n,t){"use strict";var a=t(15),r=t(285),i=t(286)(),o=t(100),s=t(20),c=a("%Math.floor%");e.exports=function(e,n){if("function"!=typeof e)throw new s("`fn` is not a function");if("number"!=typeof n||n<0||n>4294967295||c(n)!==n)throw new s("`length` must be a positive 32-bit integer");var t=arguments.length>2&&!!arguments[2],a=!0,l=!0;if("length"in e&&o){var d=o(e,"length");d&&!d.configurable&&(a=!1),d&&!d.writable&&(l=!1)}return(a||l||!t)&&(i?r(e,"length",n,!0,!0):r(e,"length",n)),e}},function(e,n,t){"use strict";var a=t(53),r=t(99),i=t(20),o=t(100);e.exports=function(e,n,t){if(!e||"object"!=typeof e&&"function"!=typeof e)throw new i("`obj` must be an object or a function`");if("string"!=typeof n&&"symbol"!=typeof n)throw new i("`property` must be a string or a symbol`");if(arguments.length>3&&"boolean"!=typeof arguments[3]&&null!==arguments[3])throw new i("`nonEnumerable`, if provided, must be a boolean or null");if(arguments.length>4&&"boolean"!=typeof arguments[4]&&null!==arguments[4])throw new i("`nonWritable`, if provided, must be a boolean or null");if(arguments.length>5&&"boolean"!=typeof arguments[5]&&null!==arguments[5])throw new i("`nonConfigurable`, if provided, must be a boolean or null");if(arguments.length>6&&"boolean"!=typeof arguments[6])throw new i("`loose`, if provided, must be a boolean");var s=arguments.length>3?arguments[3]:null,c=arguments.length>4?arguments[4]:null,l=arguments.length>5?arguments[5]:null,d=arguments.length>6&&arguments[6],u=!!o&&o(e,n);if(a)a(e,n,{configurable:null===l&&u?u.configurable:!l,enumerable:null===s&&u?u.enumerable:!s,value:t,writable:null===c&&u?u.writable:!c});else{if(!d&&(s||c||l))throw new r("This environment does not support defining a property as non-configurable, non-writable, or non-enumerable.");e[n]=t}}},function(e,n,t){"use strict";var a=t(53),r=function(){return!!a};r.hasArrayLengthDefineBug=function(){if(!a)return null;try{return 1!==a([],"length",{value:1}).length}catch(e){return!0}},e.exports=r},function(e,n,t){var a="function"==typeof Map&&Map.prototype,r=Object.getOwnPropertyDescriptor&&a?Object.getOwnPropertyDescriptor(Map.prototype,"size"):null,i=a&&r&&"function"==typeof r.get?r.get:null,o=a&&Map.prototype.forEach,s="function"==typeof Set&&Set.prototype,c=Object.getOwnPropertyDescriptor&&s?Object.getOwnPropertyDescriptor(Set.prototype,"size"):null,l=s&&c&&"function"==typeof c.get?c.get:null,d=s&&Set.prototype.forEach,u="function"==typeof WeakMap&&WeakMap.prototype?WeakMap.prototype.has:null,h="function"==typeof WeakSet&&WeakSet.prototype?WeakSet.prototype.has:null,p="function"==typeof WeakRef&&WeakRef.prototype?WeakRef.prototype.deref:null,m=Boolean.prototype.valueOf,f=Object.prototype.toString,g=Function.prototype.toString,y=String.prototype.match,v=String.prototype.slice,b=String.prototype.replace,w=String.prototype.toUpperCase,k=String.prototype.toLowerCase,_=RegExp.prototype.test,x=Array.prototype.concat,C=Array.prototype.join,A=Array.prototype.slice,M=Math.floor,S="function"==typeof BigInt?BigInt.prototype.valueOf:null,T=Object.getOwnPropertySymbols,P="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?Symbol.prototype.toString:null,I="function"==typeof Symbol&&"object"==typeof Symbol.iterator,D="function"==typeof Symbol&&Symbol.toStringTag&&(typeof Symbol.toStringTag===I||"symbol")?Symbol.toStringTag:null,L=Object.prototype.propertyIsEnumerable,O=("function"==typeof Reflect?Reflect.getPrototypeOf:Object.getPrototypeOf)||([].__proto__===Array.prototype?function(e){return e.__proto__}:null);function z(e,n){if(e===1/0||e===-1/0||e!=e||e&&e>-1e3&&e<1e3||_.call(/e/,n))return n;var t=/[0-9](?=(?:[0-9]{3})+(?![0-9]))/g;if("number"==typeof e){var a=e<0?-M(-e):M(e);if(a!==e){var r=String(a),i=v.call(n,r.length+1);return b.call(r,t,"$&_")+"."+b.call(b.call(i,/([0-9]{3})/g,"$&_"),/_$/,"")}}return b.call(n,t,"$&_")}var R=t(288),E=R.custom,U=$(E)?E:null;function j(e,n,t){var a="double"===(t.quoteStyle||n)?'"':"'";return a+e+a}function B(e){return b.call(String(e),/"/g,"&quot;")}function G(e){return!("[object Array]"!==V(e)||D&&"object"==typeof e&&D in e)}function N(e){return!("[object RegExp]"!==V(e)||D&&"object"==typeof e&&D in e)}function $(e){if(I)return e&&"object"==typeof e&&e instanceof Symbol;if("symbol"==typeof e)return!0;if(!e||"object"!=typeof e||!P)return!1;try{return P.call(e),!0}catch(e){}return!1}e.exports=function e(n,t,a,r){var s=t||{};if(F(s,"quoteStyle")&&"single"!==s.quoteStyle&&"double"!==s.quoteStyle)throw new TypeError('option "quoteStyle" must be "single" or "double"');if(F(s,"maxStringLength")&&("number"==typeof s.maxStringLength?s.maxStringLength<0&&s.maxStringLength!==1/0:null!==s.maxStringLength))throw new TypeError('option "maxStringLength", if provided, must be a positive integer, Infinity, or `null`');var c=!F(s,"customInspect")||s.customInspect;if("boolean"!=typeof c&&"symbol"!==c)throw new TypeError("option \"customInspect\", if provided, must be `true`, `false`, or `'symbol'`");if(F(s,"indent")&&null!==s.indent&&"\t"!==s.indent&&!(parseInt(s.indent,10)===s.indent&&s.indent>0))throw new TypeError('option "indent" must be "\\t", an integer > 0, or `null`');if(F(s,"numericSeparator")&&"boolean"!=typeof s.numericSeparator)throw new TypeError('option "numericSeparator", if provided, must be `true` or `false`');var f=s.numericSeparator;if(void 0===n)return"undefined";if(null===n)return"null";if("boolean"==typeof n)return n?"true":"false";if("string"==typeof n)return function e(n,t){if(n.length>t.maxStringLength){var a=n.length-t.maxStringLength,r="... "+a+" more character"+(a>1?"s":"");return e(v.call(n,0,t.maxStringLength),t)+r}return j(b.call(b.call(n,/(['\\])/g,"\\$1"),/[\x00-\x1f]/g,W),"single",t)}(n,s);if("number"==typeof n){if(0===n)return 1/0/n>0?"0":"-0";var w=String(n);return f?z(n,w):w}if("bigint"==typeof n){var _=String(n)+"n";return f?z(n,_):_}var M=void 0===s.depth?5:s.depth;if(void 0===a&&(a=0),a>=M&&M>0&&"object"==typeof n)return G(n)?"[Array]":"[Object]";var T=function(e,n){var t;if("\t"===e.indent)t="\t";else{if(!("number"==typeof e.indent&&e.indent>0))return null;t=C.call(Array(e.indent+1)," ")}return{base:t,prev:C.call(Array(n+1),t)}}(s,a);if(void 0===r)r=[];else if(H(r,n)>=0)return"[Circular]";function E(n,t,i){if(t&&(r=A.call(r)).push(t),i){var o={depth:s.depth};return F(s,"quoteStyle")&&(o.quoteStyle=s.quoteStyle),e(n,o,a+1,r)}return e(n,s,a+1,r)}if("function"==typeof n&&!N(n)){var q=function(e){if(e.name)return e.name;var n=y.call(g.call(e),/^function\s*([\w$]+)/);if(n)return n[1];return null}(n),J=X(n,E);return"[Function"+(q?": "+q:" (anonymous)")+"]"+(J.length>0?" { "+C.call(J,", ")+" }":"")}if($(n)){var ee=I?b.call(String(n),/^(Symbol\(.*\))_[^)]*$/,"$1"):P.call(n);return"object"!=typeof n||I?ee:K(ee)}if(function(e){if(!e||"object"!=typeof e)return!1;if("undefined"!=typeof HTMLElement&&e instanceof HTMLElement)return!0;return"string"==typeof e.nodeName&&"function"==typeof e.getAttribute}(n)){for(var ne="<"+k.call(String(n.nodeName)),te=n.attributes||[],ae=0;ae<te.length;ae++)ne+=" "+te[ae].name+"="+j(B(te[ae].value),"double",s);return ne+=">",n.childNodes&&n.childNodes.length&&(ne+="..."),ne+="</"+k.call(String(n.nodeName))+">"}if(G(n)){if(0===n.length)return"[]";var re=X(n,E);return T&&!function(e){for(var n=0;n<e.length;n++)if(H(e[n],"\n")>=0)return!1;return!0}(re)?"["+Z(re,T)+"]":"[ "+C.call(re,", ")+" ]"}if(function(e){return!("[object Error]"!==V(e)||D&&"object"==typeof e&&D in e)}(n)){var ie=X(n,E);return"cause"in Error.prototype||!("cause"in n)||L.call(n,"cause")?0===ie.length?"["+String(n)+"]":"{ ["+String(n)+"] "+C.call(ie,", ")+" }":"{ ["+String(n)+"] "+C.call(x.call("[cause]: "+E(n.cause),ie),", ")+" }"}if("object"==typeof n&&c){if(U&&"function"==typeof n[U]&&R)return R(n,{depth:M-a});if("symbol"!==c&&"function"==typeof n.inspect)return n.inspect()}if(function(e){if(!i||!e||"object"!=typeof e)return!1;try{i.call(e);try{l.call(e)}catch(e){return!0}return e instanceof Map}catch(e){}return!1}(n)){var oe=[];return o&&o.call(n,(function(e,t){oe.push(E(t,n,!0)+" => "+E(e,n))})),Q("Map",i.call(n),oe,T)}if(function(e){if(!l||!e||"object"!=typeof e)return!1;try{l.call(e);try{i.call(e)}catch(e){return!0}return e instanceof Set}catch(e){}return!1}(n)){var se=[];return d&&d.call(n,(function(e){se.push(E(e,n))})),Q("Set",l.call(n),se,T)}if(function(e){if(!u||!e||"object"!=typeof e)return!1;try{u.call(e,u);try{h.call(e,h)}catch(e){return!0}return e instanceof WeakMap}catch(e){}return!1}(n))return Y("WeakMap");if(function(e){if(!h||!e||"object"!=typeof e)return!1;try{h.call(e,h);try{u.call(e,u)}catch(e){return!0}return e instanceof WeakSet}catch(e){}return!1}(n))return Y("WeakSet");if(function(e){if(!p||!e||"object"!=typeof e)return!1;try{return p.call(e),!0}catch(e){}return!1}(n))return Y("WeakRef");if(function(e){return!("[object Number]"!==V(e)||D&&"object"==typeof e&&D in e)}(n))return K(E(Number(n)));if(function(e){if(!e||"object"!=typeof e||!S)return!1;try{return S.call(e),!0}catch(e){}return!1}(n))return K(E(S.call(n)));if(function(e){return!("[object Boolean]"!==V(e)||D&&"object"==typeof e&&D in e)}(n))return K(m.call(n));if(function(e){return!("[object String]"!==V(e)||D&&"object"==typeof e&&D in e)}(n))return K(E(String(n)));if("undefined"!=typeof window&&n===window)return"{ [object Window] }";if("undefined"!=typeof globalThis&&n===globalThis||"undefined"!=typeof global&&n===global)return"{ [object globalThis] }";if(!function(e){return!("[object Date]"!==V(e)||D&&"object"==typeof e&&D in e)}(n)&&!N(n)){var ce=X(n,E),le=O?O(n)===Object.prototype:n instanceof Object||n.constructor===Object,de=n instanceof Object?"":"null prototype",ue=!le&&D&&Object(n)===n&&D in n?v.call(V(n),8,-1):de?"Object":"",he=(le||"function"!=typeof n.constructor?"":n.constructor.name?n.constructor.name+" ":"")+(ue||de?"["+C.call(x.call([],ue||[],de||[]),": ")+"] ":"");return 0===ce.length?he+"{}":T?he+"{"+Z(ce,T)+"}":he+"{ "+C.call(ce,", ")+" }"}return String(n)};var q=Object.prototype.hasOwnProperty||function(e){return e in this};function F(e,n){return q.call(e,n)}function V(e){return f.call(e)}function H(e,n){if(e.indexOf)return e.indexOf(n);for(var t=0,a=e.length;t<a;t++)if(e[t]===n)return t;return-1}function W(e){var n=e.charCodeAt(0),t={8:"b",9:"t",10:"n",12:"f",13:"r"}[n];return t?"\\"+t:"\\x"+(n<16?"0":"")+w.call(n.toString(16))}function K(e){return"Object("+e+")"}function Y(e){return e+" { ? }"}function Q(e,n,t,a){return e+" ("+n+") {"+(a?Z(t,a):C.call(t,", "))+"}"}function Z(e,n){if(0===e.length)return"";var t="\n"+n.prev+n.base;return t+C.call(e,","+t)+"\n"+n.prev}function X(e,n){var t=G(e),a=[];if(t){a.length=e.length;for(var r=0;r<e.length;r++)a[r]=F(e,r)?n(e[r],e):""}var i,o="function"==typeof T?T(e):[];if(I){i={};for(var s=0;s<o.length;s++)i["$"+o[s]]=o[s]}for(var c in e)F(e,c)&&(t&&String(Number(c))===c&&c<e.length||I&&i["$"+c]instanceof Symbol||(_.call(/[^\w$]/,c)?a.push(n(c,e)+": "+n(e[c],e)):a.push(c+": "+n(e[c],e))));if("function"==typeof T)for(var l=0;l<o.length;l++)L.call(e,o[l])&&a.push("["+n(o[l])+"]: "+n(e[o[l]],e));return a}},function(e,n){},function(e,n,t){"use strict";var a=t(101),r=Object.prototype.hasOwnProperty,i=Array.isArray,o={allowDots:!1,allowEmptyArrays:!1,allowPrototypes:!1,allowSparse:!1,arrayLimit:20,charset:"utf-8",charsetSentinel:!1,comma:!1,decodeDotInKeys:!1,decoder:a.decode,delimiter:"&",depth:5,duplicates:"combine",ignoreQueryPrefix:!1,interpretNumericEntities:!1,parameterLimit:1e3,parseArrays:!0,plainObjects:!1,strictNullHandling:!1},s=function(e){return e.replace(/&#(\d+);/g,(function(e,n){return String.fromCharCode(parseInt(n,10))}))},c=function(e,n){return e&&"string"==typeof e&&n.comma&&e.indexOf(",")>-1?e.split(","):e},l=function(e,n,t,a){if(e){var i=t.allowDots?e.replace(/\.([^.[]+)/g,"[$1]"):e,o=/(\[[^[\]]*])/g,s=t.depth>0&&/(\[[^[\]]*])/.exec(i),l=s?i.slice(0,s.index):i,d=[];if(l){if(!t.plainObjects&&r.call(Object.prototype,l)&&!t.allowPrototypes)return;d.push(l)}for(var u=0;t.depth>0&&null!==(s=o.exec(i))&&u<t.depth;){if(u+=1,!t.plainObjects&&r.call(Object.prototype,s[1].slice(1,-1))&&!t.allowPrototypes)return;d.push(s[1])}return s&&d.push("["+i.slice(s.index)+"]"),function(e,n,t,a){for(var r=a?n:c(n,t),i=e.length-1;i>=0;--i){var o,s=e[i];if("[]"===s&&t.parseArrays)o=t.allowEmptyArrays&&(""===r||t.strictNullHandling&&null===r)?[]:[].concat(r);else{o=t.plainObjects?Object.create(null):{};var l="["===s.charAt(0)&&"]"===s.charAt(s.length-1)?s.slice(1,-1):s,d=t.decodeDotInKeys?l.replace(/%2E/g,"."):l,u=parseInt(d,10);t.parseArrays||""!==d?!isNaN(u)&&s!==d&&String(u)===d&&u>=0&&t.parseArrays&&u<=t.arrayLimit?(o=[])[u]=r:"__proto__"!==d&&(o[d]=r):o={0:r}}r=o}return r}(d,n,t,a)}};e.exports=function(e,n){var t=function(e){if(!e)return o;if(void 0!==e.allowEmptyArrays&&"boolean"!=typeof e.allowEmptyArrays)throw new TypeError("`allowEmptyArrays` option can only be `true` or `false`, when provided");if(void 0!==e.decodeDotInKeys&&"boolean"!=typeof e.decodeDotInKeys)throw new TypeError("`decodeDotInKeys` option can only be `true` or `false`, when provided");if(null!==e.decoder&&void 0!==e.decoder&&"function"!=typeof e.decoder)throw new TypeError("Decoder has to be a function.");if(void 0!==e.charset&&"utf-8"!==e.charset&&"iso-8859-1"!==e.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");var n=void 0===e.charset?o.charset:e.charset,t=void 0===e.duplicates?o.duplicates:e.duplicates;if("combine"!==t&&"first"!==t&&"last"!==t)throw new TypeError("The duplicates option must be either combine, first, or last");return{allowDots:void 0===e.allowDots?!0===e.decodeDotInKeys||o.allowDots:!!e.allowDots,allowEmptyArrays:"boolean"==typeof e.allowEmptyArrays?!!e.allowEmptyArrays:o.allowEmptyArrays,allowPrototypes:"boolean"==typeof e.allowPrototypes?e.allowPrototypes:o.allowPrototypes,allowSparse:"boolean"==typeof e.allowSparse?e.allowSparse:o.allowSparse,arrayLimit:"number"==typeof e.arrayLimit?e.arrayLimit:o.arrayLimit,charset:n,charsetSentinel:"boolean"==typeof e.charsetSentinel?e.charsetSentinel:o.charsetSentinel,comma:"boolean"==typeof e.comma?e.comma:o.comma,decodeDotInKeys:"boolean"==typeof e.decodeDotInKeys?e.decodeDotInKeys:o.decodeDotInKeys,decoder:"function"==typeof e.decoder?e.decoder:o.decoder,delimiter:"string"==typeof e.delimiter||a.isRegExp(e.delimiter)?e.delimiter:o.delimiter,depth:"number"==typeof e.depth||!1===e.depth?+e.depth:o.depth,duplicates:t,ignoreQueryPrefix:!0===e.ignoreQueryPrefix,interpretNumericEntities:"boolean"==typeof e.interpretNumericEntities?e.interpretNumericEntities:o.interpretNumericEntities,parameterLimit:"number"==typeof e.parameterLimit?e.parameterLimit:o.parameterLimit,parseArrays:!1!==e.parseArrays,plainObjects:"boolean"==typeof e.plainObjects?e.plainObjects:o.plainObjects,strictNullHandling:"boolean"==typeof e.strictNullHandling?e.strictNullHandling:o.strictNullHandling}}(n);if(""===e||null==e)return t.plainObjects?Object.create(null):{};for(var d="string"==typeof e?function(e,n){var t={__proto__:null},l=n.ignoreQueryPrefix?e.replace(/^\?/,""):e;l=l.replace(/%5B/gi,"[").replace(/%5D/gi,"]");var d,u=n.parameterLimit===1/0?void 0:n.parameterLimit,h=l.split(n.delimiter,u),p=-1,m=n.charset;if(n.charsetSentinel)for(d=0;d<h.length;++d)0===h[d].indexOf("utf8=")&&("utf8=%E2%9C%93"===h[d]?m="utf-8":"utf8=%26%2310003%3B"===h[d]&&(m="iso-8859-1"),p=d,d=h.length);for(d=0;d<h.length;++d)if(d!==p){var f,g,y=h[d],v=y.indexOf("]="),b=-1===v?y.indexOf("="):v+1;-1===b?(f=n.decoder(y,o.decoder,m,"key"),g=n.strictNullHandling?null:""):(f=n.decoder(y.slice(0,b),o.decoder,m,"key"),g=a.maybeMap(c(y.slice(b+1),n),(function(e){return n.decoder(e,o.decoder,m,"value")}))),g&&n.interpretNumericEntities&&"iso-8859-1"===m&&(g=s(g)),y.indexOf("[]=")>-1&&(g=i(g)?[g]:g);var w=r.call(t,f);w&&"combine"===n.duplicates?t[f]=a.combine(t[f],g):w&&"last"!==n.duplicates||(t[f]=g)}return t}(e,t):e,u=t.plainObjects?Object.create(null):{},h=Object.keys(d),p=0;p<h.length;++p){var m=h[p],f=l(m,d[m],t,"string"==typeof e);u=a.merge(u,f,t)}return!0===t.allowSparse?u:a.compact(u)}},function(e,n,t){var a=t(12),r=t(292),i=t(293);e.exports=function(e){var n=a(e);return i(n,r(n))+1}},function(e,n){e.exports=function(e){var n=new Date(e.getTime()),t=n.getTimezoneOffset();return n.setSeconds(0,0),6e4*t+n.getTime()%6e4}},function(e,n,t){var a=t(12);e.exports=function(e){var n=a(e),t=new Date(0);return t.setFullYear(n.getFullYear(),0,1),t.setHours(0,0,0,0),t}},function(e,n,t){var a=t(294);e.exports=function(e,n){var t=a(e),r=a(n),i=t.getTime()-6e4*t.getTimezoneOffset(),o=r.getTime()-6e4*r.getTimezoneOffset();return Math.round((i-o)/864e5)}},function(e,n,t){var a=t(12);e.exports=function(e){var n=a(e);return n.setHours(0,0,0,0),n}},function(e,n,t){var a=t(12),r=t(55),i=t(297);e.exports=function(e){var n=a(e),t=r(n).getTime()-i(n).getTime();return Math.round(t/6048e5)+1}},function(e,n,t){var a=t(12);e.exports=function(e,n){var t=n&&Number(n.weekStartsOn)||0,r=a(e),i=r.getDay(),o=(i<t?7:0)+i-t;return r.setDate(r.getDate()-o),r.setHours(0,0,0,0),r}},function(e,n,t){var a=t(103),r=t(55);e.exports=function(e){var n=a(e),t=new Date(0);return t.setFullYear(n,0,4),t.setHours(0,0,0,0),r(t)}},function(e,n,t){var a=t(102);e.exports=function(e){if(a(e))return!isNaN(e);throw new TypeError(toString.call(e)+" is not an instance of Date")}},function(e,n,t){var a=t(300),r=t(301);e.exports={distanceInWords:a(),format:r()}},function(e,n){e.exports=function(){var e={lessThanXSeconds:{one:"less than a second",other:"less than {{count}} seconds"},xSeconds:{one:"1 second",other:"{{count}} seconds"},halfAMinute:"half a minute",lessThanXMinutes:{one:"less than a minute",other:"less than {{count}} minutes"},xMinutes:{one:"1 minute",other:"{{count}} minutes"},aboutXHours:{one:"about 1 hour",other:"about {{count}} hours"},xHours:{one:"1 hour",other:"{{count}} hours"},xDays:{one:"1 day",other:"{{count}} days"},aboutXMonths:{one:"about 1 month",other:"about {{count}} months"},xMonths:{one:"1 month",other:"{{count}} months"},aboutXYears:{one:"about 1 year",other:"about {{count}} years"},xYears:{one:"1 year",other:"{{count}} years"},overXYears:{one:"over 1 year",other:"over {{count}} years"},almostXYears:{one:"almost 1 year",other:"almost {{count}} years"}};return{localize:function(n,t,a){var r;return a=a||{},r="string"==typeof e[n]?e[n]:1===t?e[n].one:e[n].other.replace("{{count}}",t),a.addSuffix?a.comparison>0?"in "+r:r+" ago":r}}}},function(e,n,t){var a=t(302);e.exports=function(){var e=["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],n=["January","February","March","April","May","June","July","August","September","October","November","December"],t=["Su","Mo","Tu","We","Th","Fr","Sa"],r=["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],i=["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],o=["AM","PM"],s=["am","pm"],c=["a.m.","p.m."],l={MMM:function(n){return e[n.getMonth()]},MMMM:function(e){return n[e.getMonth()]},dd:function(e){return t[e.getDay()]},ddd:function(e){return r[e.getDay()]},dddd:function(e){return i[e.getDay()]},A:function(e){return e.getHours()/12>=1?o[1]:o[0]},a:function(e){return e.getHours()/12>=1?s[1]:s[0]},aa:function(e){return e.getHours()/12>=1?c[1]:c[0]}};return["M","D","DDD","d","Q","W"].forEach((function(e){l[e+"o"]=function(n,t){return function(e){var n=e%100;if(n>20||n<10)switch(n%10){case 1:return e+"st";case 2:return e+"nd";case 3:return e+"rd"}return e+"th"}(t[e](n))}})),{formatters:l,formattingTokensRegExp:a(l)}}},function(e,n){var t=["M","MM","Q","D","DD","DDD","DDDD","d","E","W","WW","YY","YYYY","GG","GGGG","H","HH","h","hh","m","mm","s","ss","S","SS","SSS","Z","ZZ","X","x"];e.exports=function(e){var n=[];for(var a in e)e.hasOwnProperty(a)&&n.push(a);var r=t.concat(n).sort().reverse();return new RegExp("(\\[[^\\[]*\\])|(\\\\)?("+r.join("|")+"|.)","g")}},function(e,n,t){"use strict";var a=t(0),r=t(104),i=t(304),o=t(110);function s(e){var n=new i(e),t=r(i.prototype.request,n);return a.extend(t,i.prototype,n),a.extend(t,n),t}var c=s(t(56));c.Axios=i,c.create=function(e){return s(o(c.defaults,e))},c.Cancel=t(111),c.CancelToken=t(318),c.isCancel=t(109),c.all=function(e){return Promise.all(e)},c.spread=t(319),c.isAxiosError=t(320),e.exports=c,e.exports.default=c},function(e,n,t){"use strict";var a=t(0),r=t(105),i=t(305),o=t(306),s=t(110),c=t(316),l=c.validators;function d(e){this.defaults=e,this.interceptors={request:new i,response:new i}}d.prototype.request=function(e){"string"==typeof e?(e=arguments[1]||{}).url=arguments[0]:e=e||{},(e=s(this.defaults,e)).method?e.method=e.method.toLowerCase():this.defaults.method?e.method=this.defaults.method.toLowerCase():e.method="get";var n=e.transitional;void 0!==n&&c.assertOptions(n,{silentJSONParsing:l.transitional(l.boolean,"1.0.0"),forcedJSONParsing:l.transitional(l.boolean,"1.0.0"),clarifyTimeoutError:l.transitional(l.boolean,"1.0.0")},!1);var t=[],a=!0;this.interceptors.request.forEach((function(n){"function"==typeof n.runWhen&&!1===n.runWhen(e)||(a=a&&n.synchronous,t.unshift(n.fulfilled,n.rejected))}));var r,i=[];if(this.interceptors.response.forEach((function(e){i.push(e.fulfilled,e.rejected)})),!a){var d=[o,void 0];for(Array.prototype.unshift.apply(d,t),d=d.concat(i),r=Promise.resolve(e);d.length;)r=r.then(d.shift(),d.shift());return r}for(var u=e;t.length;){var h=t.shift(),p=t.shift();try{u=h(u)}catch(e){p(e);break}}try{r=o(u)}catch(e){return Promise.reject(e)}for(;i.length;)r=r.then(i.shift(),i.shift());return r},d.prototype.getUri=function(e){return e=s(this.defaults,e),r(e.url,e.params,e.paramsSerializer).replace(/^\?/,"")},a.forEach(["delete","get","head","options"],(function(e){d.prototype[e]=function(n,t){return this.request(s(t||{},{method:e,url:n,data:(t||{}).data}))}})),a.forEach(["post","put","patch"],(function(e){d.prototype[e]=function(n,t,a){return this.request(s(a||{},{method:e,url:n,data:t}))}})),e.exports=d},function(e,n,t){"use strict";var a=t(0);function r(){this.handlers=[]}r.prototype.use=function(e,n,t){return this.handlers.push({fulfilled:e,rejected:n,synchronous:!!t&&t.synchronous,runWhen:t?t.runWhen:null}),this.handlers.length-1},r.prototype.eject=function(e){this.handlers[e]&&(this.handlers[e]=null)},r.prototype.forEach=function(e){a.forEach(this.handlers,(function(n){null!==n&&e(n)}))},e.exports=r},function(e,n,t){"use strict";var a=t(0),r=t(307),i=t(109),o=t(56);function s(e){e.cancelToken&&e.cancelToken.throwIfRequested()}e.exports=function(e){return s(e),e.headers=e.headers||{},e.data=r.call(e,e.data,e.headers,e.transformRequest),e.headers=a.merge(e.headers.common||{},e.headers[e.method]||{},e.headers),a.forEach(["delete","get","head","post","put","patch","common"],(function(n){delete e.headers[n]})),(e.adapter||o.adapter)(e).then((function(n){return s(e),n.data=r.call(e,n.data,n.headers,e.transformResponse),n}),(function(n){return i(n)||(s(e),n&&n.response&&(n.response.data=r.call(e,n.response.data,n.response.headers,e.transformResponse))),Promise.reject(n)}))}},function(e,n,t){"use strict";var a=t(0),r=t(56);e.exports=function(e,n,t){var i=this||r;return a.forEach(t,(function(t){e=t.call(i,e,n)})),e}},function(e,n,t){"use strict";var a=t(0);e.exports=function(e,n){a.forEach(e,(function(t,a){a!==n&&a.toUpperCase()===n.toUpperCase()&&(e[n]=t,delete e[a])}))}},function(e,n,t){"use strict";var a=t(108);e.exports=function(e,n,t){var r=t.config.validateStatus;t.status&&r&&!r(t.status)?n(a("Request failed with status code "+t.status,t.config,null,t.request,t)):e(t)}},function(e,n,t){"use strict";var a=t(0);e.exports=a.isStandardBrowserEnv()?{write:function(e,n,t,r,i,o){var s=[];s.push(e+"="+encodeURIComponent(n)),a.isNumber(t)&&s.push("expires="+new Date(t).toGMTString()),a.isString(r)&&s.push("path="+r),a.isString(i)&&s.push("domain="+i),!0===o&&s.push("secure"),document.cookie=s.join("; ")},read:function(e){var n=document.cookie.match(new RegExp("(^|;\\s*)("+e+")=([^;]*)"));return n?decodeURIComponent(n[3]):null},remove:function(e){this.write(e,"",Date.now()-864e5)}}:{write:function(){},read:function(){return null},remove:function(){}}},function(e,n,t){"use strict";var a=t(312),r=t(313);e.exports=function(e,n){return e&&!a(n)?r(e,n):n}},function(e,n,t){"use strict";e.exports=function(e){return/^([a-z][a-z\d\+\-\.]*:)?\/\//i.test(e)}},function(e,n,t){"use strict";e.exports=function(e,n){return n?e.replace(/\/+$/,"")+"/"+n.replace(/^\/+/,""):e}},function(e,n,t){"use strict";var a=t(0),r=["age","authorization","content-length","content-type","etag","expires","from","host","if-modified-since","if-unmodified-since","last-modified","location","max-forwards","proxy-authorization","referer","retry-after","user-agent"];e.exports=function(e){var n,t,i,o={};return e?(a.forEach(e.split("\n"),(function(e){if(i=e.indexOf(":"),n=a.trim(e.substr(0,i)).toLowerCase(),t=a.trim(e.substr(i+1)),n){if(o[n]&&r.indexOf(n)>=0)return;o[n]="set-cookie"===n?(o[n]?o[n]:[]).concat([t]):o[n]?o[n]+", "+t:t}})),o):o}},function(e,n,t){"use strict";var a=t(0);e.exports=a.isStandardBrowserEnv()?function(){var e,n=/(msie|trident)/i.test(navigator.userAgent),t=document.createElement("a");function r(e){var a=e;return n&&(t.setAttribute("href",a),a=t.href),t.setAttribute("href",a),{href:t.href,protocol:t.protocol?t.protocol.replace(/:$/,""):"",host:t.host,search:t.search?t.search.replace(/^\?/,""):"",hash:t.hash?t.hash.replace(/^#/,""):"",hostname:t.hostname,port:t.port,pathname:"/"===t.pathname.charAt(0)?t.pathname:"/"+t.pathname}}return e=r(window.location.href),function(n){var t=a.isString(n)?r(n):n;return t.protocol===e.protocol&&t.host===e.host}}():function(){return!0}},function(e,n,t){"use strict";var a=t(317),r={};["object","boolean","number","function","string","symbol"].forEach((function(e,n){r[e]=function(t){return typeof t===e||"a"+(n<1?"n ":" ")+e}}));var i={},o=a.version.split(".");function s(e,n){for(var t=n?n.split("."):o,a=e.split("."),r=0;r<3;r++){if(t[r]>a[r])return!0;if(t[r]<a[r])return!1}return!1}r.transitional=function(e,n,t){var r=n&&s(n);function o(e,n){return"[Axios v"+a.version+"] Transitional option '"+e+"'"+n+(t?". "+t:"")}return function(t,a,s){if(!1===e)throw new Error(o(a," has been removed in "+n));return r&&!i[a]&&(i[a]=!0,console.warn(o(a," has been deprecated since v"+n+" and will be removed in the near future"))),!e||e(t,a,s)}},e.exports={isOlderVersion:s,assertOptions:function(e,n,t){if("object"!=typeof e)throw new TypeError("options must be an object");for(var a=Object.keys(e),r=a.length;r-- >0;){var i=a[r],o=n[i];if(o){var s=e[i],c=void 0===s||o(s,i,e);if(!0!==c)throw new TypeError("option "+i+" must be "+c)}else if(!0!==t)throw Error("Unknown option "+i)}},validators:r}},function(e){e.exports=JSON.parse('{"name":"axios","version":"0.21.4","description":"Promise based HTTP client for the browser and node.js","main":"index.js","scripts":{"test":"grunt test","start":"node ./sandbox/server.js","build":"NODE_ENV=production grunt build","preversion":"npm test","version":"npm run build && grunt version && git add -A dist && git add CHANGELOG.md bower.json package.json","postversion":"git push && git push --tags","examples":"node ./examples/server.js","coveralls":"cat coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js","fix":"eslint --fix lib/**/*.js"},"repository":{"type":"git","url":"https://github.com/axios/axios.git"},"keywords":["xhr","http","ajax","promise","node"],"author":"Matt Zabriskie","license":"MIT","bugs":{"url":"https://github.com/axios/axios/issues"},"homepage":"https://axios-http.com","devDependencies":{"coveralls":"^3.0.0","es6-promise":"^4.2.4","grunt":"^1.3.0","grunt-banner":"^0.6.0","grunt-cli":"^1.2.0","grunt-contrib-clean":"^1.1.0","grunt-contrib-watch":"^1.0.0","grunt-eslint":"^23.0.0","grunt-karma":"^4.0.0","grunt-mocha-test":"^0.13.3","grunt-ts":"^6.0.0-beta.19","grunt-webpack":"^4.0.2","istanbul-instrumenter-loader":"^1.0.0","jasmine-core":"^2.4.1","karma":"^6.3.2","karma-chrome-launcher":"^3.1.0","karma-firefox-launcher":"^2.1.0","karma-jasmine":"^1.1.1","karma-jasmine-ajax":"^0.1.13","karma-safari-launcher":"^1.0.0","karma-sauce-launcher":"^4.3.6","karma-sinon":"^1.0.5","karma-sourcemap-loader":"^0.3.8","karma-webpack":"^4.0.2","load-grunt-tasks":"^3.5.2","minimist":"^1.2.0","mocha":"^8.2.1","sinon":"^4.5.0","terser-webpack-plugin":"^4.2.3","typescript":"^4.0.5","url-search-params":"^0.10.0","webpack":"^4.44.2","webpack-dev-server":"^3.11.0"},"browser":{"./lib/adapters/http.js":"./lib/adapters/xhr.js"},"jsdelivr":"dist/axios.min.js","unpkg":"dist/axios.min.js","typings":"./index.d.ts","dependencies":{"follow-redirects":"^1.14.0"},"bundlesize":[{"path":"./dist/axios.min.js","threshold":"5kB"}]}')},function(e,n,t){"use strict";var a=t(111);function r(e){if("function"!=typeof e)throw new TypeError("executor must be a function.");var n;this.promise=new Promise((function(e){n=e}));var t=this;e((function(e){t.reason||(t.reason=new a(e),n(t.reason))}))}r.prototype.throwIfRequested=function(){if(this.reason)throw this.reason},r.source=function(){var e;return{token:new r((function(n){e=n})),cancel:e}},e.exports=r},function(e,n,t){"use strict";e.exports=function(e){return function(n){return e.apply(null,n)}}},function(e,n,t){"use strict";e.exports=function(e){return"object"==typeof e&&!0===e.isAxiosError}},function(e,n){},function(e,n){function t(e,n){for(var t=0,a=e.length-1;a>=0;a--){var r=e[a];"."===r?e.splice(a,1):".."===r?(e.splice(a,1),t++):t&&(e.splice(a,1),t--)}if(n)for(;t--;t)e.unshift("..");return e}function a(e,n){if(e.filter)return e.filter(n);for(var t=[],a=0;a<e.length;a++)n(e[a],a,e)&&t.push(e[a]);return t}n.resolve=function(){for(var e="",n=!1,r=arguments.length-1;r>=-1&&!n;r--){var i=r>=0?arguments[r]:process.cwd();if("string"!=typeof i)throw new TypeError("Arguments to path.resolve must be strings");i&&(e=i+"/"+e,n="/"===i.charAt(0))}return(n?"/":"")+(e=t(a(e.split("/"),(function(e){return!!e})),!n).join("/"))||"."},n.normalize=function(e){var i=n.isAbsolute(e),o="/"===r(e,-1);return(e=t(a(e.split("/"),(function(e){return!!e})),!i).join("/"))||i||(e="."),e&&o&&(e+="/"),(i?"/":"")+e},n.isAbsolute=function(e){return"/"===e.charAt(0)},n.join=function(){var e=Array.prototype.slice.call(arguments,0);return n.normalize(a(e,(function(e,n){if("string"!=typeof e)throw new TypeError("Arguments to path.join must be strings");return e})).join("/"))},n.relative=function(e,t){function a(e){for(var n=0;n<e.length&&""===e[n];n++);for(var t=e.length-1;t>=0&&""===e[t];t--);return n>t?[]:e.slice(n,t-n+1)}e=n.resolve(e).substr(1),t=n.resolve(t).substr(1);for(var r=a(e.split("/")),i=a(t.split("/")),o=Math.min(r.length,i.length),s=o,c=0;c<o;c++)if(r[c]!==i[c]){s=c;break}var l=[];for(c=s;c<r.length;c++)l.push("..");return(l=l.concat(i.slice(s))).join("/")},n.sep="/",n.delimiter=":",n.dirname=function(e){if("string"!=typeof e&&(e+=""),0===e.length)return".";for(var n=e.charCodeAt(0),t=47===n,a=-1,r=!0,i=e.length-1;i>=1;--i)if(47===(n=e.charCodeAt(i))){if(!r){a=i;break}}else r=!1;return-1===a?t?"/":".":t&&1===a?"/":e.slice(0,a)},n.basename=function(e,n){var t=function(e){"string"!=typeof e&&(e+="");var n,t=0,a=-1,r=!0;for(n=e.length-1;n>=0;--n)if(47===e.charCodeAt(n)){if(!r){t=n+1;break}}else-1===a&&(r=!1,a=n+1);return-1===a?"":e.slice(t,a)}(e);return n&&t.substr(-1*n.length)===n&&(t=t.substr(0,t.length-n.length)),t},n.extname=function(e){"string"!=typeof e&&(e+="");for(var n=-1,t=0,a=-1,r=!0,i=0,o=e.length-1;o>=0;--o){var s=e.charCodeAt(o);if(47!==s)-1===a&&(r=!1,a=o+1),46===s?-1===n?n=o:1!==i&&(i=1):-1!==n&&(i=-1);else if(!r){t=o+1;break}}return-1===n||-1===a||0===i||1===i&&n===a-1&&n===t+1?"":e.slice(n,a)};var r="b"==="ab".substr(-1)?function(e,n,t){return e.substr(n,t)}:function(e,n,t){return n<0&&(n=e.length+n),e.substr(n,t)}},function(e,n,t){"use strict";var a=/[|\\{}()[\]^$+*?.]/g,r=Object.prototype.hasOwnProperty,i=function(e,n){return r.apply(e,[n])};n.escapeRegExpChars=function(e){return e?String(e).replace(a,"\\$&"):""};var o={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&#34;","'":"&#39;"},s=/[&<>'"]/g;function c(e){return o[e]||e}function l(){return Function.prototype.toString.call(this)+';\nvar _ENCODE_HTML_RULES = {\n      "&": "&amp;"\n    , "<": "&lt;"\n    , ">": "&gt;"\n    , \'"\': "&#34;"\n    , "\'": "&#39;"\n    }\n  , _MATCH_HTML = /[&<>\'"]/g;\nfunction encode_char(c) {\n  return _ENCODE_HTML_RULES[c] || c;\n};\n'}n.escapeXML=function(e){return null==e?"":String(e).replace(s,c)};try{"function"==typeof Object.defineProperty?Object.defineProperty(n.escapeXML,"toString",{value:l}):n.escapeXML.toString=l}catch(e){console.warn("Unable to set escapeXML.toString (is the Function prototype frozen?)")}n.shallowCopy=function(e,n){if(n=n||{},null!=e)for(var t in n)i(n,t)&&"__proto__"!==t&&"constructor"!==t&&(e[t]=n[t]);return e},n.shallowCopyFromList=function(e,n,t){if(t=t||[],n=n||{},null!=e)for(var a=0;a<t.length;a++){var r=t[a];if(void 0!==n[r]){if(!i(n,r))continue;if("__proto__"===r||"constructor"===r)continue;e[r]=n[r]}}return e},n.cache={_data:{},set:function(e,n){this._data[e]=n},get:function(e){return this._data[e]},remove:function(e){delete this._data[e]},reset:function(){this._data={}}},n.hyphenToCamel=function(e){return e.replace(/-[a-z]/g,(function(e){return e[1].toUpperCase()}))},n.createNullProtoObjWherePossible="function"==typeof Object.create?function(){return Object.create(null)}:{__proto__:null}instanceof Object?function(){return{}}:function(){return{__proto__:null}},n.hasOwnOnlyObject=function(e){var t=n.createNullProtoObjWherePossible();for(var a in e)i(e,a)&&(t[a]=e[a]);return t}},function(e){e.exports=JSON.parse('{"name":"ejs","description":"Embedded JavaScript templates","keywords":["template","engine","ejs"],"version":"3.1.10","author":"Matthew Eernisse <mde@fleegix.org> (http://fleegix.org)","license":"Apache-2.0","bin":{"ejs":"./bin/cli.js"},"main":"./lib/ejs.js","jsdelivr":"ejs.min.js","unpkg":"ejs.min.js","repository":{"type":"git","url":"git://github.com/mde/ejs.git"},"bugs":"https://github.com/mde/ejs/issues","homepage":"https://github.com/mde/ejs","dependencies":{"jake":"^10.8.5"},"devDependencies":{"browserify":"^16.5.1","eslint":"^6.8.0","git-directory-deploy":"^1.5.1","jsdoc":"^4.0.2","lru-cache":"^4.0.1","mocha":"^10.2.0","uglify-js":"^3.3.16"},"engines":{"node":">=0.10.0"},"scripts":{"test":"npx jake test"}}')},function(e,n,t){},function(e,n,t){"use strict";t(112)},function(e,n,t){"use strict";t(113)},function(e,n,t){"use strict";t.r(n);
/*!
 * Vue.js v2.7.16
 * (c) 2014-2023 Evan You
 * Released under the MIT License.
 */
var a=Object.freeze({}),r=Array.isArray;function i(e){return null==e}function o(e){return null!=e}function s(e){return!0===e}function c(e){return"string"==typeof e||"number"==typeof e||"symbol"==typeof e||"boolean"==typeof e}function l(e){return"function"==typeof e}function d(e){return null!==e&&"object"==typeof e}var u=Object.prototype.toString;function h(e){return"[object Object]"===u.call(e)}function p(e){return"[object RegExp]"===u.call(e)}function m(e){var n=parseFloat(String(e));return n>=0&&Math.floor(n)===n&&isFinite(e)}function f(e){return o(e)&&"function"==typeof e.then&&"function"==typeof e.catch}function g(e){return null==e?"":Array.isArray(e)||h(e)&&e.toString===u?JSON.stringify(e,y,2):String(e)}function y(e,n){return n&&n.__v_isRef?n.value:n}function v(e){var n=parseFloat(e);return isNaN(n)?e:n}function b(e,n){for(var t=Object.create(null),a=e.split(","),r=0;r<a.length;r++)t[a[r]]=!0;return n?function(e){return t[e.toLowerCase()]}:function(e){return t[e]}}b("slot,component",!0);var w=b("key,ref,slot,slot-scope,is");function k(e,n){var t=e.length;if(t){if(n===e[t-1])return void(e.length=t-1);var a=e.indexOf(n);if(a>-1)return e.splice(a,1)}}var _=Object.prototype.hasOwnProperty;function x(e,n){return _.call(e,n)}function C(e){var n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}var A=/-(\w)/g,M=C((function(e){return e.replace(A,(function(e,n){return n?n.toUpperCase():""}))})),S=C((function(e){return e.charAt(0).toUpperCase()+e.slice(1)})),T=/\B([A-Z])/g,P=C((function(e){return e.replace(T,"-$1").toLowerCase()}));var I=Function.prototype.bind?function(e,n){return e.bind(n)}:function(e,n){function t(t){var a=arguments.length;return a?a>1?e.apply(n,arguments):e.call(n,t):e.call(n)}return t._length=e.length,t};function D(e,n){n=n||0;for(var t=e.length-n,a=new Array(t);t--;)a[t]=e[t+n];return a}function L(e,n){for(var t in n)e[t]=n[t];return e}function O(e){for(var n={},t=0;t<e.length;t++)e[t]&&L(n,e[t]);return n}function z(e,n,t){}var R=function(e,n,t){return!1},E=function(e){return e};function U(e,n){if(e===n)return!0;var t=d(e),a=d(n);if(!t||!a)return!t&&!a&&String(e)===String(n);try{var r=Array.isArray(e),i=Array.isArray(n);if(r&&i)return e.length===n.length&&e.every((function(e,t){return U(e,n[t])}));if(e instanceof Date&&n instanceof Date)return e.getTime()===n.getTime();if(r||i)return!1;var o=Object.keys(e),s=Object.keys(n);return o.length===s.length&&o.every((function(t){return U(e[t],n[t])}))}catch(e){return!1}}function j(e,n){for(var t=0;t<e.length;t++)if(U(e[t],n))return t;return-1}function B(e){var n=!1;return function(){n||(n=!0,e.apply(this,arguments))}}function G(e,n){return e===n?0===e&&1/e!=1/n:e==e||n==n}var N=["component","directive","filter"],$=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],q={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:R,isReservedAttr:R,isUnknownElement:R,getTagNamespace:z,parsePlatformTagName:E,mustUseProp:R,async:!0,_lifecycleHooks:$},F=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function V(e){var n=(e+"").charCodeAt(0);return 36===n||95===n}function H(e,n,t,a){Object.defineProperty(e,n,{value:t,enumerable:!!a,writable:!0,configurable:!0})}var W=new RegExp("[^".concat(F.source,".$_\\d]"));var K="__proto__"in{},Y="undefined"!=typeof window,Q=Y&&window.navigator.userAgent.toLowerCase(),Z=Q&&/msie|trident/.test(Q),X=Q&&Q.indexOf("msie 9.0")>0,J=Q&&Q.indexOf("edge/")>0;Q&&Q.indexOf("android");var ee=Q&&/iphone|ipad|ipod|ios/.test(Q);Q&&/chrome\/\d+/.test(Q),Q&&/phantomjs/.test(Q);var ne,te=Q&&Q.match(/firefox\/(\d+)/),ae={}.watch,re=!1;if(Y)try{var ie={};Object.defineProperty(ie,"passive",{get:function(){re=!0}}),window.addEventListener("test-passive",null,ie)}catch(e){}var oe=function(){return void 0===ne&&(ne=!Y&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),ne},se=Y&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ce(e){return"function"==typeof e&&/native code/.test(e.toString())}var le,de="undefined"!=typeof Symbol&&ce(Symbol)&&"undefined"!=typeof Reflect&&ce(Reflect.ownKeys);le="undefined"!=typeof Set&&ce(Set)?Set:function(){function e(){this.set=Object.create(null)}return e.prototype.has=function(e){return!0===this.set[e]},e.prototype.add=function(e){this.set[e]=!0},e.prototype.clear=function(){this.set=Object.create(null)},e}();var ue=null;function he(e){void 0===e&&(e=null),e||ue&&ue._scope.off(),ue=e,e&&e._scope.on()}var pe=function(){function e(e,n,t,a,r,i,o,s){this.tag=e,this.data=n,this.children=t,this.text=a,this.elm=r,this.ns=void 0,this.context=i,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=n&&n.key,this.componentOptions=o,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(e.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),e}(),me=function(e){void 0===e&&(e="");var n=new pe;return n.text=e,n.isComment=!0,n};function fe(e){return new pe(void 0,void 0,void 0,String(e))}function ge(e){var n=new pe(e.tag,e.data,e.children&&e.children.slice(),e.text,e.elm,e.context,e.componentOptions,e.asyncFactory);return n.ns=e.ns,n.isStatic=e.isStatic,n.key=e.key,n.isComment=e.isComment,n.fnContext=e.fnContext,n.fnOptions=e.fnOptions,n.fnScopeId=e.fnScopeId,n.asyncMeta=e.asyncMeta,n.isCloned=!0,n}"function"==typeof SuppressedError&&SuppressedError;var ye=0,ve=[],be=function(){function e(){this._pending=!1,this.id=ye++,this.subs=[]}return e.prototype.addSub=function(e){this.subs.push(e)},e.prototype.removeSub=function(e){this.subs[this.subs.indexOf(e)]=null,this._pending||(this._pending=!0,ve.push(this))},e.prototype.depend=function(n){e.target&&e.target.addDep(this)},e.prototype.notify=function(e){var n=this.subs.filter((function(e){return e}));for(var t=0,a=n.length;t<a;t++){0,n[t].update()}},e}();be.target=null;var we=[];function ke(e){we.push(e),be.target=e}function _e(){we.pop(),be.target=we[we.length-1]}var xe=Array.prototype,Ce=Object.create(xe);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(e){var n=xe[e];H(Ce,e,(function(){for(var t=[],a=0;a<arguments.length;a++)t[a]=arguments[a];var r,i=n.apply(this,t),o=this.__ob__;switch(e){case"push":case"unshift":r=t;break;case"splice":r=t.slice(2)}return r&&o.observeArray(r),o.dep.notify(),i}))}));var Ae=Object.getOwnPropertyNames(Ce),Me={},Se=!0;function Te(e){Se=e}var Pe={notify:z,depend:z,addSub:z,removeSub:z},Ie=function(){function e(e,n,t){if(void 0===n&&(n=!1),void 0===t&&(t=!1),this.value=e,this.shallow=n,this.mock=t,this.dep=t?Pe:new be,this.vmCount=0,H(e,"__ob__",this),r(e)){if(!t)if(K)e.__proto__=Ce;else for(var a=0,i=Ae.length;a<i;a++){H(e,s=Ae[a],Ce[s])}n||this.observeArray(e)}else{var o=Object.keys(e);for(a=0;a<o.length;a++){var s;Le(e,s=o[a],Me,void 0,n,t)}}}return e.prototype.observeArray=function(e){for(var n=0,t=e.length;n<t;n++)De(e[n],!1,this.mock)},e}();function De(e,n,t){return e&&x(e,"__ob__")&&e.__ob__ instanceof Ie?e.__ob__:!Se||!t&&oe()||!r(e)&&!h(e)||!Object.isExtensible(e)||e.__v_skip||Be(e)||e instanceof pe?void 0:new Ie(e,n,t)}function Le(e,n,t,a,i,o,s){void 0===s&&(s=!1);var c=new be,l=Object.getOwnPropertyDescriptor(e,n);if(!l||!1!==l.configurable){var d=l&&l.get,u=l&&l.set;d&&!u||t!==Me&&2!==arguments.length||(t=e[n]);var h=i?t&&t.__ob__:De(t,!1,o);return Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){var n=d?d.call(e):t;return be.target&&(c.depend(),h&&(h.dep.depend(),r(n)&&Re(n))),Be(n)&&!i?n.value:n},set:function(n){var a=d?d.call(e):t;if(G(a,n)){if(u)u.call(e,n);else{if(d)return;if(!i&&Be(a)&&!Be(n))return void(a.value=n);t=n}h=i?n&&n.__ob__:De(n,!1,o),c.notify()}}}),c}}function Oe(e,n,t){if(!je(e)){var a=e.__ob__;return r(e)&&m(n)?(e.length=Math.max(e.length,n),e.splice(n,1,t),a&&!a.shallow&&a.mock&&De(t,!1,!0),t):n in e&&!(n in Object.prototype)?(e[n]=t,t):e._isVue||a&&a.vmCount?t:a?(Le(a.value,n,t,void 0,a.shallow,a.mock),a.dep.notify(),t):(e[n]=t,t)}}function ze(e,n){if(r(e)&&m(n))e.splice(n,1);else{var t=e.__ob__;e._isVue||t&&t.vmCount||je(e)||x(e,n)&&(delete e[n],t&&t.dep.notify())}}function Re(e){for(var n=void 0,t=0,a=e.length;t<a;t++)(n=e[t])&&n.__ob__&&n.__ob__.dep.depend(),r(n)&&Re(n)}function Ee(e){return Ue(e,!0),H(e,"__v_isShallow",!0),e}function Ue(e,n){if(!je(e)){De(e,n,oe());0}}function je(e){return!(!e||!e.__v_isReadonly)}function Be(e){return!(!e||!0!==e.__v_isRef)}function Ge(e,n,t){Object.defineProperty(e,t,{enumerable:!0,configurable:!0,get:function(){var e=n[t];if(Be(e))return e.value;var a=e&&e.__ob__;return a&&a.dep.depend(),e},set:function(e){var a=n[t];Be(a)&&!Be(e)?a.value=e:n[t]=e}})}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var Ne;var $e=function(){function e(e){void 0===e&&(e=!1),this.detached=e,this.active=!0,this.effects=[],this.cleanups=[],this.parent=Ne,!e&&Ne&&(this.index=(Ne.scopes||(Ne.scopes=[])).push(this)-1)}return e.prototype.run=function(e){if(this.active){var n=Ne;try{return Ne=this,e()}finally{Ne=n}}else 0},e.prototype.on=function(){Ne=this},e.prototype.off=function(){Ne=this.parent},e.prototype.stop=function(e){if(this.active){var n=void 0,t=void 0;for(n=0,t=this.effects.length;n<t;n++)this.effects[n].teardown();for(n=0,t=this.cleanups.length;n<t;n++)this.cleanups[n]();if(this.scopes)for(n=0,t=this.scopes.length;n<t;n++)this.scopes[n].stop(!0);if(!this.detached&&this.parent&&!e){var a=this.parent.scopes.pop();a&&a!==this&&(this.parent.scopes[this.index]=a,a.index=this.index)}this.parent=void 0,this.active=!1}},e}();function qe(e){var n=e._provided,t=e.$parent&&e.$parent._provided;return t===n?e._provided=Object.create(t):n}var Fe=C((function(e){var n="&"===e.charAt(0),t="~"===(e=n?e.slice(1):e).charAt(0),a="!"===(e=t?e.slice(1):e).charAt(0);return{name:e=a?e.slice(1):e,once:t,capture:a,passive:n}}));function Ve(e,n){function t(){var e=t.fns;if(!r(e))return Tn(e,null,arguments,n,"v-on handler");for(var a=e.slice(),i=0;i<a.length;i++)Tn(a[i],null,arguments,n,"v-on handler")}return t.fns=e,t}function He(e,n,t,a,r,o){var c,l,d,u;for(c in e)l=e[c],d=n[c],u=Fe(c),i(l)||(i(d)?(i(l.fns)&&(l=e[c]=Ve(l,o)),s(u.once)&&(l=e[c]=r(u.name,l,u.capture)),t(u.name,l,u.capture,u.passive,u.params)):l!==d&&(d.fns=l,e[c]=d));for(c in n)i(e[c])&&a((u=Fe(c)).name,n[c],u.capture)}function We(e,n,t){var a;e instanceof pe&&(e=e.data.hook||(e.data.hook={}));var r=e[n];function c(){t.apply(this,arguments),k(a.fns,c)}i(r)?a=Ve([c]):o(r.fns)&&s(r.merged)?(a=r).fns.push(c):a=Ve([r,c]),a.merged=!0,e[n]=a}function Ke(e,n,t,a,r){if(o(n)){if(x(n,t))return e[t]=n[t],r||delete n[t],!0;if(x(n,a))return e[t]=n[a],r||delete n[a],!0}return!1}function Ye(e){return c(e)?[fe(e)]:r(e)?function e(n,t){var a,l,d,u,h=[];for(a=0;a<n.length;a++)i(l=n[a])||"boolean"==typeof l||(d=h.length-1,u=h[d],r(l)?l.length>0&&(Qe((l=e(l,"".concat(t||"","_").concat(a)))[0])&&Qe(u)&&(h[d]=fe(u.text+l[0].text),l.shift()),h.push.apply(h,l)):c(l)?Qe(u)?h[d]=fe(u.text+l):""!==l&&h.push(fe(l)):Qe(l)&&Qe(u)?h[d]=fe(u.text+l.text):(s(n._isVList)&&o(l.tag)&&i(l.key)&&o(t)&&(l.key="__vlist".concat(t,"_").concat(a,"__")),h.push(l)));return h}(e):void 0}function Qe(e){return o(e)&&o(e.text)&&!1===e.isComment}function Ze(e,n){var t,a,i,s,c=null;if(r(e)||"string"==typeof e)for(c=new Array(e.length),t=0,a=e.length;t<a;t++)c[t]=n(e[t],t);else if("number"==typeof e)for(c=new Array(e),t=0;t<e;t++)c[t]=n(t+1,t);else if(d(e))if(de&&e[Symbol.iterator]){c=[];for(var l=e[Symbol.iterator](),u=l.next();!u.done;)c.push(n(u.value,c.length)),u=l.next()}else for(i=Object.keys(e),c=new Array(i.length),t=0,a=i.length;t<a;t++)s=i[t],c[t]=n(e[s],s,t);return o(c)||(c=[]),c._isVList=!0,c}function Xe(e,n,t,a){var r,i=this.$scopedSlots[e];i?(t=t||{},a&&(t=L(L({},a),t)),r=i(t)||(l(n)?n():n)):r=this.$slots[e]||(l(n)?n():n);var o=t&&t.slot;return o?this.$createElement("template",{slot:o},r):r}function Je(e){return Dt(this.$options,"filters",e,!0)||E}function en(e,n){return r(e)?-1===e.indexOf(n):e!==n}function nn(e,n,t,a,r){var i=q.keyCodes[n]||t;return r&&a&&!q.keyCodes[n]?en(r,a):i?en(i,e):a?P(a)!==n:void 0===e}function tn(e,n,t,a,i){if(t)if(d(t)){r(t)&&(t=O(t));var o=void 0,s=function(r){if("class"===r||"style"===r||w(r))o=e;else{var s=e.attrs&&e.attrs.type;o=a||q.mustUseProp(n,s,r)?e.domProps||(e.domProps={}):e.attrs||(e.attrs={})}var c=M(r),l=P(r);c in o||l in o||(o[r]=t[r],i&&((e.on||(e.on={}))["update:".concat(r)]=function(e){t[r]=e}))};for(var c in t)s(c)}else;return e}function an(e,n){var t=this._staticTrees||(this._staticTrees=[]),a=t[e];return a&&!n||on(a=t[e]=this.$options.staticRenderFns[e].call(this._renderProxy,this._c,this),"__static__".concat(e),!1),a}function rn(e,n,t){return on(e,"__once__".concat(n).concat(t?"_".concat(t):""),!0),e}function on(e,n,t){if(r(e))for(var a=0;a<e.length;a++)e[a]&&"string"!=typeof e[a]&&sn(e[a],"".concat(n,"_").concat(a),t);else sn(e,n,t)}function sn(e,n,t){e.isStatic=!0,e.key=n,e.isOnce=t}function cn(e,n){if(n)if(h(n)){var t=e.on=e.on?L({},e.on):{};for(var a in n){var r=t[a],i=n[a];t[a]=r?[].concat(r,i):i}}else;return e}function ln(e,n,t,a){n=n||{$stable:!t};for(var i=0;i<e.length;i++){var o=e[i];r(o)?ln(o,n,t):o&&(o.proxy&&(o.fn.proxy=!0),n[o.key]=o.fn)}return a&&(n.$key=a),n}function dn(e,n){for(var t=0;t<n.length;t+=2){var a=n[t];"string"==typeof a&&a&&(e[n[t]]=n[t+1])}return e}function un(e,n){return"string"==typeof e?n+e:e}function hn(e){e._o=rn,e._n=v,e._s=g,e._l=Ze,e._t=Xe,e._q=U,e._i=j,e._m=an,e._f=Je,e._k=nn,e._b=tn,e._v=fe,e._e=me,e._u=ln,e._g=cn,e._d=dn,e._p=un}function pn(e,n){if(!e||!e.length)return{};for(var t={},a=0,r=e.length;a<r;a++){var i=e[a],o=i.data;if(o&&o.attrs&&o.attrs.slot&&delete o.attrs.slot,i.context!==n&&i.fnContext!==n||!o||null==o.slot)(t.default||(t.default=[])).push(i);else{var s=o.slot,c=t[s]||(t[s]=[]);"template"===i.tag?c.push.apply(c,i.children||[]):c.push(i)}}for(var l in t)t[l].every(mn)&&delete t[l];return t}function mn(e){return e.isComment&&!e.asyncFactory||" "===e.text}function fn(e){return e.isComment&&e.asyncFactory}function gn(e,n,t,r){var i,o=Object.keys(t).length>0,s=n?!!n.$stable:!o,c=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(s&&r&&r!==a&&c===r.$key&&!o&&!r.$hasNormal)return r;for(var l in i={},n)n[l]&&"$"!==l[0]&&(i[l]=yn(e,t,l,n[l]))}else i={};for(var d in t)d in i||(i[d]=vn(t,d));return n&&Object.isExtensible(n)&&(n._normalized=i),H(i,"$stable",s),H(i,"$key",c),H(i,"$hasNormal",o),i}function yn(e,n,t,a){var i=function(){var n=ue;he(e);var t=arguments.length?a.apply(null,arguments):a({}),i=(t=t&&"object"==typeof t&&!r(t)?[t]:Ye(t))&&t[0];return he(n),t&&(!i||1===t.length&&i.isComment&&!fn(i))?void 0:t};return a.proxy&&Object.defineProperty(n,t,{get:i,enumerable:!0,configurable:!0}),i}function vn(e,n){return function(){return e[n]}}function bn(e){return{get attrs(){if(!e._attrsProxy){var n=e._attrsProxy={};H(n,"_v_attr_proxy",!0),wn(n,e.$attrs,a,e,"$attrs")}return e._attrsProxy},get listeners(){e._listenersProxy||wn(e._listenersProxy={},e.$listeners,a,e,"$listeners");return e._listenersProxy},get slots(){return function(e){e._slotsProxy||_n(e._slotsProxy={},e.$scopedSlots);return e._slotsProxy}(e)},emit:I(e.$emit,e),expose:function(n){n&&Object.keys(n).forEach((function(t){return Ge(e,n,t)}))}}}function wn(e,n,t,a,r){var i=!1;for(var o in n)o in e?n[o]!==t[o]&&(i=!0):(i=!0,kn(e,o,a,r));for(var o in e)o in n||(i=!0,delete e[o]);return i}function kn(e,n,t,a){Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){return t[a][n]}})}function _n(e,n){for(var t in n)e[t]=n[t];for(var t in e)t in n||delete e[t]}var xn=null;function Cn(e,n){return(e.__esModule||de&&"Module"===e[Symbol.toStringTag])&&(e=e.default),d(e)?n.extend(e):e}function An(e){if(r(e))for(var n=0;n<e.length;n++){var t=e[n];if(o(t)&&(o(t.componentOptions)||fn(t)))return t}}function Mn(e,n,t,a,u,h){return(r(t)||c(t))&&(u=a,a=t,t=void 0),s(h)&&(u=2),function(e,n,t,a,c){if(o(t)&&o(t.__ob__))return me();o(t)&&o(t.is)&&(n=t.is);if(!n)return me();0;r(a)&&l(a[0])&&((t=t||{}).scopedSlots={default:a[0]},a.length=0);2===c?a=Ye(a):1===c&&(a=function(e){for(var n=0;n<e.length;n++)if(r(e[n]))return Array.prototype.concat.apply([],e);return e}(a));var u,h;if("string"==typeof n){var p=void 0;h=e.$vnode&&e.$vnode.ns||q.getTagNamespace(n),u=q.isReservedTag(n)?new pe(q.parsePlatformTagName(n),t,a,void 0,void 0,e):t&&t.pre||!o(p=Dt(e.$options,"components",n))?new pe(n,t,a,void 0,void 0,e):kt(p,t,e,a,n)}else u=kt(n,t,e,a);return r(u)?u:o(u)?(o(h)&&function e(n,t,a){n.ns=t,"foreignObject"===n.tag&&(t=void 0,a=!0);if(o(n.children))for(var r=0,c=n.children.length;r<c;r++){var l=n.children[r];o(l.tag)&&(i(l.ns)||s(a)&&"svg"!==l.tag)&&e(l,t,a)}}(u,h),o(t)&&function(e){d(e.style)&&qn(e.style);d(e.class)&&qn(e.class)}(t),u):me()}(e,n,t,a,u)}function Sn(e,n,t){ke();try{if(n)for(var a=n;a=a.$parent;){var r=a.$options.errorCaptured;if(r)for(var i=0;i<r.length;i++)try{if(!1===r[i].call(a,e,n,t))return}catch(e){Pn(e,a,"errorCaptured hook")}}Pn(e,n,t)}finally{_e()}}function Tn(e,n,t,a,r){var i;try{(i=t?e.apply(n,t):e.call(n))&&!i._isVue&&f(i)&&!i._handled&&(i.catch((function(e){return Sn(e,a,r+" (Promise/async)")})),i._handled=!0)}catch(e){Sn(e,a,r)}return i}function Pn(e,n,t){if(q.errorHandler)try{return q.errorHandler.call(null,e,n,t)}catch(n){n!==e&&In(n,null,"config.errorHandler")}In(e,n,t)}function In(e,n,t){if(!Y||"undefined"==typeof console)throw e;console.error(e)}var Dn,Ln=!1,On=[],zn=!1;function Rn(){zn=!1;var e=On.slice(0);On.length=0;for(var n=0;n<e.length;n++)e[n]()}if("undefined"!=typeof Promise&&ce(Promise)){var En=Promise.resolve();Dn=function(){En.then(Rn),ee&&setTimeout(z)},Ln=!0}else if(Z||"undefined"==typeof MutationObserver||!ce(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Dn="undefined"!=typeof setImmediate&&ce(setImmediate)?function(){setImmediate(Rn)}:function(){setTimeout(Rn,0)};else{var Un=1,jn=new MutationObserver(Rn),Bn=document.createTextNode(String(Un));jn.observe(Bn,{characterData:!0}),Dn=function(){Un=(Un+1)%2,Bn.data=String(Un)},Ln=!0}function Gn(e,n){var t;if(On.push((function(){if(e)try{e.call(n)}catch(e){Sn(e,n,"nextTick")}else t&&t(n)})),zn||(zn=!0,Dn()),!e&&"undefined"!=typeof Promise)return new Promise((function(e){t=e}))}function Nn(e){return function(n,t){if(void 0===t&&(t=ue),t)return function(e,n,t){var a=e.$options;a[n]=St(a[n],t)}(t,e,n)}}Nn("beforeMount"),Nn("mounted"),Nn("beforeUpdate"),Nn("updated"),Nn("beforeDestroy"),Nn("destroyed"),Nn("activated"),Nn("deactivated"),Nn("serverPrefetch"),Nn("renderTracked"),Nn("renderTriggered"),Nn("errorCaptured");var $n=new le;function qn(e){return function e(n,t){var a,i,o=r(n);if(!o&&!d(n)||n.__v_skip||Object.isFrozen(n)||n instanceof pe)return;if(n.__ob__){var s=n.__ob__.dep.id;if(t.has(s))return;t.add(s)}if(o)for(a=n.length;a--;)e(n[a],t);else if(Be(n))e(n.value,t);else for(i=Object.keys(n),a=i.length;a--;)e(n[i[a]],t)}(e,$n),$n.clear(),e}var Fn,Vn=0,Hn=function(){function e(e,n,t,a,r){var i,o;i=this,void 0===(o=Ne&&!Ne._vm?Ne:e?e._scope:void 0)&&(o=Ne),o&&o.active&&o.effects.push(i),(this.vm=e)&&r&&(e._watcher=this),a?(this.deep=!!a.deep,this.user=!!a.user,this.lazy=!!a.lazy,this.sync=!!a.sync,this.before=a.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++Vn,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new le,this.newDepIds=new le,this.expression="",l(n)?this.getter=n:(this.getter=function(e){if(!W.test(e)){var n=e.split(".");return function(e){for(var t=0;t<n.length;t++){if(!e)return;e=e[n[t]]}return e}}}(n),this.getter||(this.getter=z)),this.value=this.lazy?void 0:this.get()}return e.prototype.get=function(){var e;ke(this);var n=this.vm;try{e=this.getter.call(n,n)}catch(e){if(!this.user)throw e;Sn(e,n,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&qn(e),_e(),this.cleanupDeps()}return e},e.prototype.addDep=function(e){var n=e.id;this.newDepIds.has(n)||(this.newDepIds.add(n),this.newDeps.push(e),this.depIds.has(n)||e.addSub(this))},e.prototype.cleanupDeps=function(){for(var e=this.deps.length;e--;){var n=this.deps[e];this.newDepIds.has(n.id)||n.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},e.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():pt(this)},e.prototype.run=function(){if(this.active){var e=this.get();if(e!==this.value||d(e)||this.deep){var n=this.value;if(this.value=e,this.user){var t='callback for watcher "'.concat(this.expression,'"');Tn(this.cb,this.vm,[e,n],this.vm,t)}else this.cb.call(this.vm,e,n)}}},e.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},e.prototype.depend=function(){for(var e=this.deps.length;e--;)this.deps[e].depend()},e.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&k(this.vm._scope.effects,this),this.active){for(var e=this.deps.length;e--;)this.deps[e].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},e}();function Wn(e,n){Fn.$on(e,n)}function Kn(e,n){Fn.$off(e,n)}function Yn(e,n){var t=Fn;return function a(){var r=n.apply(null,arguments);null!==r&&t.$off(e,a)}}function Qn(e,n,t){Fn=e,He(n,t||{},Wn,Kn,Yn,e),Fn=void 0}var Zn=null;function Xn(e){var n=Zn;return Zn=e,function(){Zn=n}}function Jn(e){for(;e&&(e=e.$parent);)if(e._inactive)return!0;return!1}function et(e,n){if(n){if(e._directInactive=!1,Jn(e))return}else if(e._directInactive)return;if(e._inactive||null===e._inactive){e._inactive=!1;for(var t=0;t<e.$children.length;t++)et(e.$children[t]);nt(e,"activated")}}function nt(e,n,t,a){void 0===a&&(a=!0),ke();var r=ue,i=Ne;a&&he(e);var o=e.$options[n],s="".concat(n," hook");if(o)for(var c=0,l=o.length;c<l;c++)Tn(o[c],e,t||null,e,s);e._hasHookEvent&&e.$emit("hook:"+n),a&&(he(r),i&&i.on()),_e()}var tt=[],at=[],rt={},it=!1,ot=!1,st=0;var ct=0,lt=Date.now;if(Y&&!Z){var dt=window.performance;dt&&"function"==typeof dt.now&&lt()>document.createEvent("Event").timeStamp&&(lt=function(){return dt.now()})}var ut=function(e,n){if(e.post){if(!n.post)return 1}else if(n.post)return-1;return e.id-n.id};function ht(){var e,n;for(ct=lt(),ot=!0,tt.sort(ut),st=0;st<tt.length;st++)(e=tt[st]).before&&e.before(),n=e.id,rt[n]=null,e.run();var t=at.slice(),a=tt.slice();st=tt.length=at.length=0,rt={},it=ot=!1,function(e){for(var n=0;n<e.length;n++)e[n]._inactive=!0,et(e[n],!0)}(t),function(e){var n=e.length;for(;n--;){var t=e[n],a=t.vm;a&&a._watcher===t&&a._isMounted&&!a._isDestroyed&&nt(a,"updated")}}(a),function(){for(var e=0;e<ve.length;e++){var n=ve[e];n.subs=n.subs.filter((function(e){return e})),n._pending=!1}ve.length=0}(),se&&q.devtools&&se.emit("flush")}function pt(e){var n=e.id;if(null==rt[n]&&(e!==be.target||!e.noRecurse)){if(rt[n]=!0,ot){for(var t=tt.length-1;t>st&&tt[t].id>e.id;)t--;tt.splice(t+1,0,e)}else tt.push(e);it||(it=!0,Gn(ht))}}function mt(e,n){if(e){for(var t=Object.create(null),a=de?Reflect.ownKeys(e):Object.keys(e),r=0;r<a.length;r++){var i=a[r];if("__ob__"!==i){var o=e[i].from;if(o in n._provided)t[i]=n._provided[o];else if("default"in e[i]){var s=e[i].default;t[i]=l(s)?s.call(n):s}else 0}}return t}}function ft(e,n,t,i,o){var c,l=this,d=o.options;x(i,"_uid")?(c=Object.create(i))._original=i:(c=i,i=i._original);var u=s(d._compiled),h=!u;this.data=e,this.props=n,this.children=t,this.parent=i,this.listeners=e.on||a,this.injections=mt(d.inject,i),this.slots=function(){return l.$slots||gn(i,e.scopedSlots,l.$slots=pn(t,i)),l.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return gn(i,e.scopedSlots,this.slots())}}),u&&(this.$options=d,this.$slots=this.slots(),this.$scopedSlots=gn(i,e.scopedSlots,this.$slots)),d._scopeId?this._c=function(e,n,t,a){var o=Mn(c,e,n,t,a,h);return o&&!r(o)&&(o.fnScopeId=d._scopeId,o.fnContext=i),o}:this._c=function(e,n,t,a){return Mn(c,e,n,t,a,h)}}function gt(e,n,t,a,r){var i=ge(e);return i.fnContext=t,i.fnOptions=a,n.slot&&((i.data||(i.data={})).slot=n.slot),i}function yt(e,n){for(var t in n)e[M(t)]=n[t]}function vt(e){return e.name||e.__name||e._componentTag}hn(ft.prototype);var bt={init:function(e,n){if(e.componentInstance&&!e.componentInstance._isDestroyed&&e.data.keepAlive){var t=e;bt.prepatch(t,t)}else{(e.componentInstance=function(e,n){var t={_isComponent:!0,_parentVnode:e,parent:n},a=e.data.inlineTemplate;o(a)&&(t.render=a.render,t.staticRenderFns=a.staticRenderFns);return new e.componentOptions.Ctor(t)}(e,Zn)).$mount(n?e.elm:void 0,n)}},prepatch:function(e,n){var t=n.componentOptions;!function(e,n,t,r,i){var o=r.data.scopedSlots,s=e.$scopedSlots,c=!!(o&&!o.$stable||s!==a&&!s.$stable||o&&e.$scopedSlots.$key!==o.$key||!o&&e.$scopedSlots.$key),l=!!(i||e.$options._renderChildren||c),d=e.$vnode;e.$options._parentVnode=r,e.$vnode=r,e._vnode&&(e._vnode.parent=r),e.$options._renderChildren=i;var u=r.data.attrs||a;e._attrsProxy&&wn(e._attrsProxy,u,d.data&&d.data.attrs||a,e,"$attrs")&&(l=!0),e.$attrs=u,t=t||a;var h=e.$options._parentListeners;if(e._listenersProxy&&wn(e._listenersProxy,t,h||a,e,"$listeners"),e.$listeners=e.$options._parentListeners=t,Qn(e,t,h),n&&e.$options.props){Te(!1);for(var p=e._props,m=e.$options._propKeys||[],f=0;f<m.length;f++){var g=m[f],y=e.$options.props;p[g]=Lt(g,y,n,e)}Te(!0),e.$options.propsData=n}l&&(e.$slots=pn(i,r.context),e.$forceUpdate())}(n.componentInstance=e.componentInstance,t.propsData,t.listeners,n,t.children)},insert:function(e){var n,t=e.context,a=e.componentInstance;a._isMounted||(a._isMounted=!0,nt(a,"mounted")),e.data.keepAlive&&(t._isMounted?((n=a)._inactive=!1,at.push(n)):et(a,!0))},destroy:function(e){var n=e.componentInstance;n._isDestroyed||(e.data.keepAlive?function e(n,t){if(!(t&&(n._directInactive=!0,Jn(n))||n._inactive)){n._inactive=!0;for(var a=0;a<n.$children.length;a++)e(n.$children[a]);nt(n,"deactivated")}}(n,!0):n.$destroy())}},wt=Object.keys(bt);function kt(e,n,t,c,l){if(!i(e)){var u=t.$options._base;if(d(e)&&(e=u.extend(e)),"function"==typeof e){var h;if(i(e.cid)&&void 0===(e=function(e,n){if(s(e.error)&&o(e.errorComp))return e.errorComp;if(o(e.resolved))return e.resolved;var t=xn;if(t&&o(e.owners)&&-1===e.owners.indexOf(t)&&e.owners.push(t),s(e.loading)&&o(e.loadingComp))return e.loadingComp;if(t&&!o(e.owners)){var a=e.owners=[t],r=!0,c=null,l=null;t.$on("hook:destroyed",(function(){return k(a,t)}));var u=function(e){for(var n=0,t=a.length;n<t;n++)a[n].$forceUpdate();e&&(a.length=0,null!==c&&(clearTimeout(c),c=null),null!==l&&(clearTimeout(l),l=null))},h=B((function(t){e.resolved=Cn(t,n),r?a.length=0:u(!0)})),p=B((function(n){o(e.errorComp)&&(e.error=!0,u(!0))})),m=e(h,p);return d(m)&&(f(m)?i(e.resolved)&&m.then(h,p):f(m.component)&&(m.component.then(h,p),o(m.error)&&(e.errorComp=Cn(m.error,n)),o(m.loading)&&(e.loadingComp=Cn(m.loading,n),0===m.delay?e.loading=!0:c=setTimeout((function(){c=null,i(e.resolved)&&i(e.error)&&(e.loading=!0,u(!1))}),m.delay||200)),o(m.timeout)&&(l=setTimeout((function(){l=null,i(e.resolved)&&p(null)}),m.timeout)))),r=!1,e.loading?e.loadingComp:e.resolved}}(h=e,u)))return function(e,n,t,a,r){var i=me();return i.asyncFactory=e,i.asyncMeta={data:n,context:t,children:a,tag:r},i}(h,n,t,c,l);n=n||{},Ht(e),o(n.model)&&function(e,n){var t=e.model&&e.model.prop||"value",a=e.model&&e.model.event||"input";(n.attrs||(n.attrs={}))[t]=n.model.value;var i=n.on||(n.on={}),s=i[a],c=n.model.callback;o(s)?(r(s)?-1===s.indexOf(c):s!==c)&&(i[a]=[c].concat(s)):i[a]=c}(e.options,n);var p=function(e,n,t){var a=n.options.props;if(!i(a)){var r={},s=e.attrs,c=e.props;if(o(s)||o(c))for(var l in a){var d=P(l);Ke(r,c,l,d,!0)||Ke(r,s,l,d,!1)}return r}}(n,e);if(s(e.options.functional))return function(e,n,t,i,s){var c=e.options,l={},d=c.props;if(o(d))for(var u in d)l[u]=Lt(u,d,n||a);else o(t.attrs)&&yt(l,t.attrs),o(t.props)&&yt(l,t.props);var h=new ft(t,l,s,i,e),p=c.render.call(null,h._c,h);if(p instanceof pe)return gt(p,t,h.parent,c,h);if(r(p)){for(var m=Ye(p)||[],f=new Array(m.length),g=0;g<m.length;g++)f[g]=gt(m[g],t,h.parent,c,h);return f}}(e,p,n,t,c);var m=n.on;if(n.on=n.nativeOn,s(e.options.abstract)){var g=n.slot;n={},g&&(n.slot=g)}!function(e){for(var n=e.hook||(e.hook={}),t=0;t<wt.length;t++){var a=wt[t],r=n[a],i=bt[a];r===i||r&&r._merged||(n[a]=r?_t(i,r):i)}}(n);var y=vt(e.options)||l;return new pe("vue-component-".concat(e.cid).concat(y?"-".concat(y):""),n,void 0,void 0,void 0,t,{Ctor:e,propsData:p,listeners:m,tag:l,children:c},h)}}}function _t(e,n){var t=function(t,a){e(t,a),n(t,a)};return t._merged=!0,t}var xt=z,Ct=q.optionMergeStrategies;function At(e,n,t){if(void 0===t&&(t=!0),!n)return e;for(var a,r,i,o=de?Reflect.ownKeys(n):Object.keys(n),s=0;s<o.length;s++)"__ob__"!==(a=o[s])&&(r=e[a],i=n[a],t&&x(e,a)?r!==i&&h(r)&&h(i)&&At(r,i):Oe(e,a,i));return e}function Mt(e,n,t){return t?function(){var a=l(n)?n.call(t,t):n,r=l(e)?e.call(t,t):e;return a?At(a,r):r}:n?e?function(){return At(l(n)?n.call(this,this):n,l(e)?e.call(this,this):e)}:n:e}function St(e,n){var t=n?e?e.concat(n):r(n)?n:[n]:e;return t?function(e){for(var n=[],t=0;t<e.length;t++)-1===n.indexOf(e[t])&&n.push(e[t]);return n}(t):t}function Tt(e,n,t,a){var r=Object.create(e||null);return n?L(r,n):r}Ct.data=function(e,n,t){return t?Mt(e,n,t):n&&"function"!=typeof n?e:Mt(e,n)},$.forEach((function(e){Ct[e]=St})),N.forEach((function(e){Ct[e+"s"]=Tt})),Ct.watch=function(e,n,t,a){if(e===ae&&(e=void 0),n===ae&&(n=void 0),!n)return Object.create(e||null);if(!e)return n;var i={};for(var o in L(i,e),n){var s=i[o],c=n[o];s&&!r(s)&&(s=[s]),i[o]=s?s.concat(c):r(c)?c:[c]}return i},Ct.props=Ct.methods=Ct.inject=Ct.computed=function(e,n,t,a){if(!e)return n;var r=Object.create(null);return L(r,e),n&&L(r,n),r},Ct.provide=function(e,n){return e?function(){var t=Object.create(null);return At(t,l(e)?e.call(this):e),n&&At(t,l(n)?n.call(this):n,!1),t}:n};var Pt=function(e,n){return void 0===n?e:n};function It(e,n,t){if(l(n)&&(n=n.options),function(e,n){var t=e.props;if(t){var a,i,o={};if(r(t))for(a=t.length;a--;)"string"==typeof(i=t[a])&&(o[M(i)]={type:null});else if(h(t))for(var s in t)i=t[s],o[M(s)]=h(i)?i:{type:i};else 0;e.props=o}}(n),function(e,n){var t=e.inject;if(t){var a=e.inject={};if(r(t))for(var i=0;i<t.length;i++)a[t[i]]={from:t[i]};else if(h(t))for(var o in t){var s=t[o];a[o]=h(s)?L({from:o},s):{from:s}}else 0}}(n),function(e){var n=e.directives;if(n)for(var t in n){var a=n[t];l(a)&&(n[t]={bind:a,update:a})}}(n),!n._base&&(n.extends&&(e=It(e,n.extends,t)),n.mixins))for(var a=0,i=n.mixins.length;a<i;a++)e=It(e,n.mixins[a],t);var o,s={};for(o in e)c(o);for(o in n)x(e,o)||c(o);function c(a){var r=Ct[a]||Pt;s[a]=r(e[a],n[a],t,a)}return s}function Dt(e,n,t,a){if("string"==typeof t){var r=e[n];if(x(r,t))return r[t];var i=M(t);if(x(r,i))return r[i];var o=S(i);return x(r,o)?r[o]:r[t]||r[i]||r[o]}}function Lt(e,n,t,a){var r=n[e],i=!x(t,e),o=t[e],s=Et(Boolean,r.type);if(s>-1)if(i&&!x(r,"default"))o=!1;else if(""===o||o===P(e)){var c=Et(String,r.type);(c<0||s<c)&&(o=!0)}if(void 0===o){o=function(e,n,t){if(!x(n,"default"))return;var a=n.default;0;if(e&&e.$options.propsData&&void 0===e.$options.propsData[t]&&void 0!==e._props[t])return e._props[t];return l(a)&&"Function"!==zt(n.type)?a.call(e):a}(a,r,e);var d=Se;Te(!0),De(o),Te(d)}return o}var Ot=/^\s*function (\w+)/;function zt(e){var n=e&&e.toString().match(Ot);return n?n[1]:""}function Rt(e,n){return zt(e)===zt(n)}function Et(e,n){if(!r(n))return Rt(n,e)?0:-1;for(var t=0,a=n.length;t<a;t++)if(Rt(n[t],e))return t;return-1}var Ut={enumerable:!0,configurable:!0,get:z,set:z};function jt(e,n,t){Ut.get=function(){return this[n][t]},Ut.set=function(e){this[n][t]=e},Object.defineProperty(e,t,Ut)}function Bt(e){var n=e.$options;if(n.props&&function(e,n){var t=e.$options.propsData||{},a=e._props=Ee({}),r=e.$options._propKeys=[];e.$parent&&Te(!1);var i=function(i){r.push(i);var o=Lt(i,n,t,e);Le(a,i,o,void 0,!0),i in e||jt(e,"_props",i)};for(var o in n)i(o);Te(!0)}(e,n.props),function(e){var n=e.$options,t=n.setup;if(t){var a=e._setupContext=bn(e);he(e),ke();var r=Tn(t,null,[e._props||Ee({}),a],e,"setup");if(_e(),he(),l(r))n.render=r;else if(d(r))if(e._setupState=r,r.__sfc){var i=e._setupProxy={};for(var o in r)"__sfc"!==o&&Ge(i,r,o)}else for(var o in r)V(o)||Ge(e,r,o);else 0}}(e),n.methods&&function(e,n){e.$options.props;for(var t in n)e[t]="function"!=typeof n[t]?z:I(n[t],e)}(e,n.methods),n.data)!function(e){var n=e.$options.data;h(n=e._data=l(n)?function(e,n){ke();try{return e.call(n,n)}catch(e){return Sn(e,n,"data()"),{}}finally{_e()}}(n,e):n||{})||(n={});var t=Object.keys(n),a=e.$options.props,r=(e.$options.methods,t.length);for(;r--;){var i=t[r];0,a&&x(a,i)||V(i)||jt(e,"_data",i)}var o=De(n);o&&o.vmCount++}(e);else{var t=De(e._data={});t&&t.vmCount++}n.computed&&function(e,n){var t=e._computedWatchers=Object.create(null),a=oe();for(var r in n){var i=n[r],o=l(i)?i:i.get;0,a||(t[r]=new Hn(e,o||z,z,Gt)),r in e||Nt(e,r,i)}}(e,n.computed),n.watch&&n.watch!==ae&&function(e,n){for(var t in n){var a=n[t];if(r(a))for(var i=0;i<a.length;i++)Ft(e,t,a[i]);else Ft(e,t,a)}}(e,n.watch)}var Gt={lazy:!0};function Nt(e,n,t){var a=!oe();l(t)?(Ut.get=a?$t(n):qt(t),Ut.set=z):(Ut.get=t.get?a&&!1!==t.cache?$t(n):qt(t.get):z,Ut.set=t.set||z),Object.defineProperty(e,n,Ut)}function $t(e){return function(){var n=this._computedWatchers&&this._computedWatchers[e];if(n)return n.dirty&&n.evaluate(),be.target&&n.depend(),n.value}}function qt(e){return function(){return e.call(this,this)}}function Ft(e,n,t,a){return h(t)&&(a=t,t=t.handler),"string"==typeof t&&(t=e[t]),e.$watch(n,t,a)}var Vt=0;function Ht(e){var n=e.options;if(e.super){var t=Ht(e.super);if(t!==e.superOptions){e.superOptions=t;var a=function(e){var n,t=e.options,a=e.sealedOptions;for(var r in t)t[r]!==a[r]&&(n||(n={}),n[r]=t[r]);return n}(e);a&&L(e.extendOptions,a),(n=e.options=It(t,e.extendOptions)).name&&(n.components[n.name]=e)}}return n}function Wt(e){this._init(e)}function Kt(e){e.cid=0;var n=1;e.extend=function(e){e=e||{};var t=this,a=t.cid,r=e._Ctor||(e._Ctor={});if(r[a])return r[a];var i=vt(e)||vt(t.options);var o=function(e){this._init(e)};return(o.prototype=Object.create(t.prototype)).constructor=o,o.cid=n++,o.options=It(t.options,e),o.super=t,o.options.props&&function(e){var n=e.options.props;for(var t in n)jt(e.prototype,"_props",t)}(o),o.options.computed&&function(e){var n=e.options.computed;for(var t in n)Nt(e.prototype,t,n[t])}(o),o.extend=t.extend,o.mixin=t.mixin,o.use=t.use,N.forEach((function(e){o[e]=t[e]})),i&&(o.options.components[i]=o),o.superOptions=t.options,o.extendOptions=e,o.sealedOptions=L({},o.options),r[a]=o,o}}function Yt(e){return e&&(vt(e.Ctor.options)||e.tag)}function Qt(e,n){return r(e)?e.indexOf(n)>-1:"string"==typeof e?e.split(",").indexOf(n)>-1:!!p(e)&&e.test(n)}function Zt(e,n){var t=e.cache,a=e.keys,r=e._vnode,i=e.$vnode;for(var o in t){var s=t[o];if(s){var c=s.name;c&&!n(c)&&Xt(t,o,a,r)}}i.componentOptions.children=void 0}function Xt(e,n,t,a){var r=e[n];!r||a&&r.tag===a.tag||r.componentInstance.$destroy(),e[n]=null,k(t,n)}Wt.prototype._init=function(e){var n=this;n._uid=Vt++,n._isVue=!0,n.__v_skip=!0,n._scope=new $e(!0),n._scope.parent=void 0,n._scope._vm=!0,e&&e._isComponent?function(e,n){var t=e.$options=Object.create(e.constructor.options),a=n._parentVnode;t.parent=n.parent,t._parentVnode=a;var r=a.componentOptions;t.propsData=r.propsData,t._parentListeners=r.listeners,t._renderChildren=r.children,t._componentTag=r.tag,n.render&&(t.render=n.render,t.staticRenderFns=n.staticRenderFns)}(n,e):n.$options=It(Ht(n.constructor),e||{},n),n._renderProxy=n,n._self=n,function(e){var n=e.$options,t=n.parent;if(t&&!n.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(e)}e.$parent=t,e.$root=t?t.$root:e,e.$children=[],e.$refs={},e._provided=t?t._provided:Object.create(null),e._watcher=null,e._inactive=null,e._directInactive=!1,e._isMounted=!1,e._isDestroyed=!1,e._isBeingDestroyed=!1}(n),function(e){e._events=Object.create(null),e._hasHookEvent=!1;var n=e.$options._parentListeners;n&&Qn(e,n)}(n),function(e){e._vnode=null,e._staticTrees=null;var n=e.$options,t=e.$vnode=n._parentVnode,r=t&&t.context;e.$slots=pn(n._renderChildren,r),e.$scopedSlots=t?gn(e.$parent,t.data.scopedSlots,e.$slots):a,e._c=function(n,t,a,r){return Mn(e,n,t,a,r,!1)},e.$createElement=function(n,t,a,r){return Mn(e,n,t,a,r,!0)};var i=t&&t.data;Le(e,"$attrs",i&&i.attrs||a,null,!0),Le(e,"$listeners",n._parentListeners||a,null,!0)}(n),nt(n,"beforeCreate",void 0,!1),function(e){var n=mt(e.$options.inject,e);n&&(Te(!1),Object.keys(n).forEach((function(t){Le(e,t,n[t])})),Te(!0))}(n),Bt(n),function(e){var n=e.$options.provide;if(n){var t=l(n)?n.call(e):n;if(!d(t))return;for(var a=qe(e),r=de?Reflect.ownKeys(t):Object.keys(t),i=0;i<r.length;i++){var o=r[i];Object.defineProperty(a,o,Object.getOwnPropertyDescriptor(t,o))}}}(n),nt(n,"created"),n.$options.el&&n.$mount(n.$options.el)},function(e){var n={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(e.prototype,"$data",n),Object.defineProperty(e.prototype,"$props",t),e.prototype.$set=Oe,e.prototype.$delete=ze,e.prototype.$watch=function(e,n,t){if(h(n))return Ft(this,e,n,t);(t=t||{}).user=!0;var a=new Hn(this,e,n,t);if(t.immediate){var r='callback for immediate watcher "'.concat(a.expression,'"');ke(),Tn(n,this,[a.value],this,r),_e()}return function(){a.teardown()}}}(Wt),function(e){var n=/^hook:/;e.prototype.$on=function(e,t){var a=this;if(r(e))for(var i=0,o=e.length;i<o;i++)a.$on(e[i],t);else(a._events[e]||(a._events[e]=[])).push(t),n.test(e)&&(a._hasHookEvent=!0);return a},e.prototype.$once=function(e,n){var t=this;function a(){t.$off(e,a),n.apply(t,arguments)}return a.fn=n,t.$on(e,a),t},e.prototype.$off=function(e,n){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(r(e)){for(var a=0,i=e.length;a<i;a++)t.$off(e[a],n);return t}var o,s=t._events[e];if(!s)return t;if(!n)return t._events[e]=null,t;for(var c=s.length;c--;)if((o=s[c])===n||o.fn===n){s.splice(c,1);break}return t},e.prototype.$emit=function(e){var n=this,t=n._events[e];if(t){t=t.length>1?D(t):t;for(var a=D(arguments,1),r='event handler for "'.concat(e,'"'),i=0,o=t.length;i<o;i++)Tn(t[i],n,a,n,r)}return n}}(Wt),function(e){e.prototype._update=function(e,n){var t=this,a=t.$el,r=t._vnode,i=Xn(t);t._vnode=e,t.$el=r?t.__patch__(r,e):t.__patch__(t.$el,e,n,!1),i(),a&&(a.__vue__=null),t.$el&&(t.$el.__vue__=t);for(var o=t;o&&o.$vnode&&o.$parent&&o.$vnode===o.$parent._vnode;)o.$parent.$el=o.$el,o=o.$parent},e.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},e.prototype.$destroy=function(){var e=this;if(!e._isBeingDestroyed){nt(e,"beforeDestroy"),e._isBeingDestroyed=!0;var n=e.$parent;!n||n._isBeingDestroyed||e.$options.abstract||k(n.$children,e),e._scope.stop(),e._data.__ob__&&e._data.__ob__.vmCount--,e._isDestroyed=!0,e.__patch__(e._vnode,null),nt(e,"destroyed"),e.$off(),e.$el&&(e.$el.__vue__=null),e.$vnode&&(e.$vnode.parent=null)}}}(Wt),function(e){hn(e.prototype),e.prototype.$nextTick=function(e){return Gn(e,this)},e.prototype._render=function(){var e=this,n=e.$options,t=n.render,a=n._parentVnode;a&&e._isMounted&&(e.$scopedSlots=gn(e.$parent,a.data.scopedSlots,e.$slots,e.$scopedSlots),e._slotsProxy&&_n(e._slotsProxy,e.$scopedSlots)),e.$vnode=a;var i,o=ue,s=xn;try{he(e),xn=e,i=t.call(e._renderProxy,e.$createElement)}catch(n){Sn(n,e,"render"),i=e._vnode}finally{xn=s,he(o)}return r(i)&&1===i.length&&(i=i[0]),i instanceof pe||(i=me()),i.parent=a,i}}(Wt);var Jt=[String,RegExp,Array],ea={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Jt,exclude:Jt,max:[String,Number]},methods:{cacheVNode:function(){var e=this.cache,n=this.keys,t=this.vnodeToCache,a=this.keyToCache;if(t){var r=t.tag,i=t.componentInstance,o=t.componentOptions;e[a]={name:Yt(o),tag:r,componentInstance:i},n.push(a),this.max&&n.length>parseInt(this.max)&&Xt(e,n[0],n,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var e in this.cache)Xt(this.cache,e,this.keys)},mounted:function(){var e=this;this.cacheVNode(),this.$watch("include",(function(n){Zt(e,(function(e){return Qt(n,e)}))})),this.$watch("exclude",(function(n){Zt(e,(function(e){return!Qt(n,e)}))}))},updated:function(){this.cacheVNode()},render:function(){var e=this.$slots.default,n=An(e),t=n&&n.componentOptions;if(t){var a=Yt(t),r=this.include,i=this.exclude;if(r&&(!a||!Qt(r,a))||i&&a&&Qt(i,a))return n;var o=this.cache,s=this.keys,c=null==n.key?t.Ctor.cid+(t.tag?"::".concat(t.tag):""):n.key;o[c]?(n.componentInstance=o[c].componentInstance,k(s,c),s.push(c)):(this.vnodeToCache=n,this.keyToCache=c),n.data.keepAlive=!0}return n||e&&e[0]}}};!function(e){var n={get:function(){return q}};Object.defineProperty(e,"config",n),e.util={warn:xt,extend:L,mergeOptions:It,defineReactive:Le},e.set=Oe,e.delete=ze,e.nextTick=Gn,e.observable=function(e){return De(e),e},e.options=Object.create(null),N.forEach((function(n){e.options[n+"s"]=Object.create(null)})),e.options._base=e,L(e.options.components,ea),function(e){e.use=function(e){var n=this._installedPlugins||(this._installedPlugins=[]);if(n.indexOf(e)>-1)return this;var t=D(arguments,1);return t.unshift(this),l(e.install)?e.install.apply(e,t):l(e)&&e.apply(null,t),n.push(e),this}}(e),function(e){e.mixin=function(e){return this.options=It(this.options,e),this}}(e),Kt(e),function(e){N.forEach((function(n){e[n]=function(e,t){return t?("component"===n&&h(t)&&(t.name=t.name||e,t=this.options._base.extend(t)),"directive"===n&&l(t)&&(t={bind:t,update:t}),this.options[n+"s"][e]=t,t):this.options[n+"s"][e]}}))}(e)}(Wt),Object.defineProperty(Wt.prototype,"$isServer",{get:oe}),Object.defineProperty(Wt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Wt,"FunctionalRenderContext",{value:ft}),Wt.version="2.7.16";var na=b("style,class"),ta=b("input,textarea,option,select,progress"),aa=b("contenteditable,draggable,spellcheck"),ra=b("events,caret,typing,plaintext-only"),ia=b("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),oa="http://www.w3.org/1999/xlink",sa=function(e){return":"===e.charAt(5)&&"xlink"===e.slice(0,5)},ca=function(e){return sa(e)?e.slice(6,e.length):""},la=function(e){return null==e||!1===e};function da(e){for(var n=e.data,t=e,a=e;o(a.componentInstance);)(a=a.componentInstance._vnode)&&a.data&&(n=ua(a.data,n));for(;o(t=t.parent);)t&&t.data&&(n=ua(n,t.data));return function(e,n){if(o(e)||o(n))return ha(e,pa(n));return""}(n.staticClass,n.class)}function ua(e,n){return{staticClass:ha(e.staticClass,n.staticClass),class:o(e.class)?[e.class,n.class]:n.class}}function ha(e,n){return e?n?e+" "+n:e:n||""}function pa(e){return Array.isArray(e)?function(e){for(var n,t="",a=0,r=e.length;a<r;a++)o(n=pa(e[a]))&&""!==n&&(t&&(t+=" "),t+=n);return t}(e):d(e)?function(e){var n="";for(var t in e)e[t]&&(n&&(n+=" "),n+=t);return n}(e):"string"==typeof e?e:""}var ma={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},fa=b("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),ga=b("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),ya=function(e){return fa(e)||ga(e)};var va=Object.create(null);var ba=b("text,number,password,search,email,tel,url");var wa=Object.freeze({__proto__:null,createElement:function(e,n){var t=document.createElement(e);return"select"!==e||n.data&&n.data.attrs&&void 0!==n.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(e,n){return document.createElementNS(ma[e],n)},createTextNode:function(e){return document.createTextNode(e)},createComment:function(e){return document.createComment(e)},insertBefore:function(e,n,t){e.insertBefore(n,t)},removeChild:function(e,n){e.removeChild(n)},appendChild:function(e,n){e.appendChild(n)},parentNode:function(e){return e.parentNode},nextSibling:function(e){return e.nextSibling},tagName:function(e){return e.tagName},setTextContent:function(e,n){e.textContent=n},setStyleScope:function(e,n){e.setAttribute(n,"")}}),ka={create:function(e,n){_a(n)},update:function(e,n){e.data.ref!==n.data.ref&&(_a(e,!0),_a(n))},destroy:function(e){_a(e,!0)}};function _a(e,n){var t=e.data.ref;if(o(t)){var a=e.context,i=e.componentInstance||e.elm,s=n?null:i,c=n?void 0:i;if(l(t))Tn(t,a,[s],a,"template ref function");else{var d=e.data.refInFor,u="string"==typeof t||"number"==typeof t,h=Be(t),p=a.$refs;if(u||h)if(d){var m=u?p[t]:t.value;n?r(m)&&k(m,i):r(m)?m.includes(i)||m.push(i):u?(p[t]=[i],xa(a,t,p[t])):t.value=[i]}else if(u){if(n&&p[t]!==i)return;p[t]=c,xa(a,t,s)}else if(h){if(n&&t.value!==i)return;t.value=s}else 0}}}function xa(e,n,t){var a=e._setupState;a&&x(a,n)&&(Be(a[n])?a[n].value=t:a[n]=t)}var Ca=new pe("",{},[]),Aa=["create","activate","update","remove","destroy"];function Ma(e,n){return e.key===n.key&&e.asyncFactory===n.asyncFactory&&(e.tag===n.tag&&e.isComment===n.isComment&&o(e.data)===o(n.data)&&function(e,n){if("input"!==e.tag)return!0;var t,a=o(t=e.data)&&o(t=t.attrs)&&t.type,r=o(t=n.data)&&o(t=t.attrs)&&t.type;return a===r||ba(a)&&ba(r)}(e,n)||s(e.isAsyncPlaceholder)&&i(n.asyncFactory.error))}function Sa(e,n,t){var a,r,i={};for(a=n;a<=t;++a)o(r=e[a].key)&&(i[r]=a);return i}var Ta={create:Pa,update:Pa,destroy:function(e){Pa(e,Ca)}};function Pa(e,n){(e.data.directives||n.data.directives)&&function(e,n){var t,a,r,i=e===Ca,o=n===Ca,s=Da(e.data.directives,e.context),c=Da(n.data.directives,n.context),l=[],d=[];for(t in c)a=s[t],r=c[t],a?(r.oldValue=a.value,r.oldArg=a.arg,Oa(r,"update",n,e),r.def&&r.def.componentUpdated&&d.push(r)):(Oa(r,"bind",n,e),r.def&&r.def.inserted&&l.push(r));if(l.length){var u=function(){for(var t=0;t<l.length;t++)Oa(l[t],"inserted",n,e)};i?We(n,"insert",u):u()}d.length&&We(n,"postpatch",(function(){for(var t=0;t<d.length;t++)Oa(d[t],"componentUpdated",n,e)}));if(!i)for(t in s)c[t]||Oa(s[t],"unbind",e,e,o)}(e,n)}var Ia=Object.create(null);function Da(e,n){var t,a,r=Object.create(null);if(!e)return r;for(t=0;t<e.length;t++){if((a=e[t]).modifiers||(a.modifiers=Ia),r[La(a)]=a,n._setupState&&n._setupState.__sfc){var i=a.def||Dt(n,"_setupState","v-"+a.name);a.def="function"==typeof i?{bind:i,update:i}:i}a.def=a.def||Dt(n.$options,"directives",a.name)}return r}function La(e){return e.rawName||"".concat(e.name,".").concat(Object.keys(e.modifiers||{}).join("."))}function Oa(e,n,t,a,r){var i=e.def&&e.def[n];if(i)try{i(t.elm,e,t,a,r)}catch(a){Sn(a,t.context,"directive ".concat(e.name," ").concat(n," hook"))}}var za=[ka,Ta];function Ra(e,n){var t=n.componentOptions;if(!(o(t)&&!1===t.Ctor.options.inheritAttrs||i(e.data.attrs)&&i(n.data.attrs))){var a,r,c=n.elm,l=e.data.attrs||{},d=n.data.attrs||{};for(a in(o(d.__ob__)||s(d._v_attr_proxy))&&(d=n.data.attrs=L({},d)),d)r=d[a],l[a]!==r&&Ea(c,a,r,n.data.pre);for(a in(Z||J)&&d.value!==l.value&&Ea(c,"value",d.value),l)i(d[a])&&(sa(a)?c.removeAttributeNS(oa,ca(a)):aa(a)||c.removeAttribute(a))}}function Ea(e,n,t,a){a||e.tagName.indexOf("-")>-1?Ua(e,n,t):ia(n)?la(t)?e.removeAttribute(n):(t="allowfullscreen"===n&&"EMBED"===e.tagName?"true":n,e.setAttribute(n,t)):aa(n)?e.setAttribute(n,function(e,n){return la(n)||"false"===n?"false":"contenteditable"===e&&ra(n)?n:"true"}(n,t)):sa(n)?la(t)?e.removeAttributeNS(oa,ca(n)):e.setAttributeNS(oa,n,t):Ua(e,n,t)}function Ua(e,n,t){if(la(t))e.removeAttribute(n);else{if(Z&&!X&&"TEXTAREA"===e.tagName&&"placeholder"===n&&""!==t&&!e.__ieph){var a=function(n){n.stopImmediatePropagation(),e.removeEventListener("input",a)};e.addEventListener("input",a),e.__ieph=!0}e.setAttribute(n,t)}}var ja={create:Ra,update:Ra};function Ba(e,n){var t=n.elm,a=n.data,r=e.data;if(!(i(a.staticClass)&&i(a.class)&&(i(r)||i(r.staticClass)&&i(r.class)))){var s=da(n),c=t._transitionClasses;o(c)&&(s=ha(s,pa(c))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var Ga,Na={create:Ba,update:Ba};function $a(e,n,t){var a=Ga;return function r(){var i=n.apply(null,arguments);null!==i&&Va(e,r,t,a)}}var qa=Ln&&!(te&&Number(te[1])<=53);function Fa(e,n,t,a){if(qa){var r=ct,i=n;n=i._wrapper=function(e){if(e.target===e.currentTarget||e.timeStamp>=r||e.timeStamp<=0||e.target.ownerDocument!==document)return i.apply(this,arguments)}}Ga.addEventListener(e,n,re?{capture:t,passive:a}:t)}function Va(e,n,t,a){(a||Ga).removeEventListener(e,n._wrapper||n,t)}function Ha(e,n){if(!i(e.data.on)||!i(n.data.on)){var t=n.data.on||{},a=e.data.on||{};Ga=n.elm||e.elm,function(e){if(o(e.__r)){var n=Z?"change":"input";e[n]=[].concat(e.__r,e[n]||[]),delete e.__r}o(e.__c)&&(e.change=[].concat(e.__c,e.change||[]),delete e.__c)}(t),He(t,a,Fa,Va,$a,n.context),Ga=void 0}}var Wa,Ka={create:Ha,update:Ha,destroy:function(e){return Ha(e,Ca)}};function Ya(e,n){if(!i(e.data.domProps)||!i(n.data.domProps)){var t,a,r=n.elm,c=e.data.domProps||{},l=n.data.domProps||{};for(t in(o(l.__ob__)||s(l._v_attr_proxy))&&(l=n.data.domProps=L({},l)),c)t in l||(r[t]="");for(t in l){if(a=l[t],"textContent"===t||"innerHTML"===t){if(n.children&&(n.children.length=0),a===c[t])continue;1===r.childNodes.length&&r.removeChild(r.childNodes[0])}if("value"===t&&"PROGRESS"!==r.tagName){r._value=a;var d=i(a)?"":String(a);Qa(r,d)&&(r.value=d)}else if("innerHTML"===t&&ga(r.tagName)&&i(r.innerHTML)){(Wa=Wa||document.createElement("div")).innerHTML="<svg>".concat(a,"</svg>");for(var u=Wa.firstChild;r.firstChild;)r.removeChild(r.firstChild);for(;u.firstChild;)r.appendChild(u.firstChild)}else if(a!==c[t])try{r[t]=a}catch(e){}}}}function Qa(e,n){return!e.composing&&("OPTION"===e.tagName||function(e,n){var t=!0;try{t=document.activeElement!==e}catch(e){}return t&&e.value!==n}(e,n)||function(e,n){var t=e.value,a=e._vModifiers;if(o(a)){if(a.number)return v(t)!==v(n);if(a.trim)return t.trim()!==n.trim()}return t!==n}(e,n))}var Za={create:Ya,update:Ya},Xa=C((function(e){var n={},t=/:(.+)/;return e.split(/;(?![^(]*\))/g).forEach((function(e){if(e){var a=e.split(t);a.length>1&&(n[a[0].trim()]=a[1].trim())}})),n}));function Ja(e){var n=er(e.style);return e.staticStyle?L(e.staticStyle,n):n}function er(e){return Array.isArray(e)?O(e):"string"==typeof e?Xa(e):e}var nr,tr=/^--/,ar=/\s*!important$/,rr=function(e,n,t){if(tr.test(n))e.style.setProperty(n,t);else if(ar.test(t))e.style.setProperty(P(n),t.replace(ar,""),"important");else{var a=or(n);if(Array.isArray(t))for(var r=0,i=t.length;r<i;r++)e.style[a]=t[r];else e.style[a]=t}},ir=["Webkit","Moz","ms"],or=C((function(e){if(nr=nr||document.createElement("div").style,"filter"!==(e=M(e))&&e in nr)return e;for(var n=e.charAt(0).toUpperCase()+e.slice(1),t=0;t<ir.length;t++){var a=ir[t]+n;if(a in nr)return a}}));function sr(e,n){var t=n.data,a=e.data;if(!(i(t.staticStyle)&&i(t.style)&&i(a.staticStyle)&&i(a.style))){var r,s,c=n.elm,l=a.staticStyle,d=a.normalizedStyle||a.style||{},u=l||d,h=er(n.data.style)||{};n.data.normalizedStyle=o(h.__ob__)?L({},h):h;var p=function(e,n){var t,a={};if(n)for(var r=e;r.componentInstance;)(r=r.componentInstance._vnode)&&r.data&&(t=Ja(r.data))&&L(a,t);(t=Ja(e.data))&&L(a,t);for(var i=e;i=i.parent;)i.data&&(t=Ja(i.data))&&L(a,t);return a}(n,!0);for(s in u)i(p[s])&&rr(c,s,"");for(s in p)r=p[s],rr(c,s,null==r?"":r)}}var cr={create:sr,update:sr},lr=/\s+/;function dr(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(lr).forEach((function(n){return e.classList.add(n)})):e.classList.add(n);else{var t=" ".concat(e.getAttribute("class")||""," ");t.indexOf(" "+n+" ")<0&&e.setAttribute("class",(t+n).trim())}}function ur(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(lr).forEach((function(n){return e.classList.remove(n)})):e.classList.remove(n),e.classList.length||e.removeAttribute("class");else{for(var t=" ".concat(e.getAttribute("class")||""," "),a=" "+n+" ";t.indexOf(a)>=0;)t=t.replace(a," ");(t=t.trim())?e.setAttribute("class",t):e.removeAttribute("class")}}function hr(e){if(e){if("object"==typeof e){var n={};return!1!==e.css&&L(n,pr(e.name||"v")),L(n,e),n}return"string"==typeof e?pr(e):void 0}}var pr=C((function(e){return{enterClass:"".concat(e,"-enter"),enterToClass:"".concat(e,"-enter-to"),enterActiveClass:"".concat(e,"-enter-active"),leaveClass:"".concat(e,"-leave"),leaveToClass:"".concat(e,"-leave-to"),leaveActiveClass:"".concat(e,"-leave-active")}})),mr=Y&&!X,fr="transition",gr="transitionend",yr="animation",vr="animationend";mr&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(fr="WebkitTransition",gr="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(yr="WebkitAnimation",vr="webkitAnimationEnd"));var br=Y?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(e){return e()};function wr(e){br((function(){br(e)}))}function kr(e,n){var t=e._transitionClasses||(e._transitionClasses=[]);t.indexOf(n)<0&&(t.push(n),dr(e,n))}function _r(e,n){e._transitionClasses&&k(e._transitionClasses,n),ur(e,n)}function xr(e,n,t){var a=Ar(e,n),r=a.type,i=a.timeout,o=a.propCount;if(!r)return t();var s="transition"===r?gr:vr,c=0,l=function(){e.removeEventListener(s,d),t()},d=function(n){n.target===e&&++c>=o&&l()};setTimeout((function(){c<o&&l()}),i+1),e.addEventListener(s,d)}var Cr=/\b(transform|all)(,|$)/;function Ar(e,n){var t,a=window.getComputedStyle(e),r=(a[fr+"Delay"]||"").split(", "),i=(a[fr+"Duration"]||"").split(", "),o=Mr(r,i),s=(a[yr+"Delay"]||"").split(", "),c=(a[yr+"Duration"]||"").split(", "),l=Mr(s,c),d=0,u=0;return"transition"===n?o>0&&(t="transition",d=o,u=i.length):"animation"===n?l>0&&(t="animation",d=l,u=c.length):u=(t=(d=Math.max(o,l))>0?o>l?"transition":"animation":null)?"transition"===t?i.length:c.length:0,{type:t,timeout:d,propCount:u,hasTransform:"transition"===t&&Cr.test(a[fr+"Property"])}}function Mr(e,n){for(;e.length<n.length;)e=e.concat(e);return Math.max.apply(null,n.map((function(n,t){return Sr(n)+Sr(e[t])})))}function Sr(e){return 1e3*Number(e.slice(0,-1).replace(",","."))}function Tr(e,n){var t=e.elm;o(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var a=hr(e.data.transition);if(!i(a)&&!o(t._enterCb)&&1===t.nodeType){for(var r=a.css,s=a.type,c=a.enterClass,u=a.enterToClass,h=a.enterActiveClass,p=a.appearClass,m=a.appearToClass,f=a.appearActiveClass,g=a.beforeEnter,y=a.enter,b=a.afterEnter,w=a.enterCancelled,k=a.beforeAppear,_=a.appear,x=a.afterAppear,C=a.appearCancelled,A=a.duration,M=Zn,S=Zn.$vnode;S&&S.parent;)M=S.context,S=S.parent;var T=!M._isMounted||!e.isRootInsert;if(!T||_||""===_){var P=T&&p?p:c,I=T&&f?f:h,D=T&&m?m:u,L=T&&k||g,O=T&&l(_)?_:y,z=T&&x||b,R=T&&C||w,E=v(d(A)?A.enter:A);0;var U=!1!==r&&!X,j=Dr(O),G=t._enterCb=B((function(){U&&(_r(t,D),_r(t,I)),G.cancelled?(U&&_r(t,P),R&&R(t)):z&&z(t),t._enterCb=null}));e.data.show||We(e,"insert",(function(){var n=t.parentNode,a=n&&n._pending&&n._pending[e.key];a&&a.tag===e.tag&&a.elm._leaveCb&&a.elm._leaveCb(),O&&O(t,G)})),L&&L(t),U&&(kr(t,P),kr(t,I),wr((function(){_r(t,P),G.cancelled||(kr(t,D),j||(Ir(E)?setTimeout(G,E):xr(t,s,G)))}))),e.data.show&&(n&&n(),O&&O(t,G)),U||j||G()}}}function Pr(e,n){var t=e.elm;o(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var a=hr(e.data.transition);if(i(a)||1!==t.nodeType)return n();if(!o(t._leaveCb)){var r=a.css,s=a.type,c=a.leaveClass,l=a.leaveToClass,u=a.leaveActiveClass,h=a.beforeLeave,p=a.leave,m=a.afterLeave,f=a.leaveCancelled,g=a.delayLeave,y=a.duration,b=!1!==r&&!X,w=Dr(p),k=v(d(y)?y.leave:y);0;var _=t._leaveCb=B((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[e.key]=null),b&&(_r(t,l),_r(t,u)),_.cancelled?(b&&_r(t,c),f&&f(t)):(n(),m&&m(t)),t._leaveCb=null}));g?g(x):x()}function x(){_.cancelled||(!e.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[e.key]=e),h&&h(t),b&&(kr(t,c),kr(t,u),wr((function(){_r(t,c),_.cancelled||(kr(t,l),w||(Ir(k)?setTimeout(_,k):xr(t,s,_)))}))),p&&p(t,_),b||w||_())}}function Ir(e){return"number"==typeof e&&!isNaN(e)}function Dr(e){if(i(e))return!1;var n=e.fns;return o(n)?Dr(Array.isArray(n)?n[0]:n):(e._length||e.length)>1}function Lr(e,n){!0!==n.data.show&&Tr(n)}var Or=function(e){var n,t,a={},l=e.modules,d=e.nodeOps;for(n=0;n<Aa.length;++n)for(a[Aa[n]]=[],t=0;t<l.length;++t)o(l[t][Aa[n]])&&a[Aa[n]].push(l[t][Aa[n]]);function u(e){var n=d.parentNode(e);o(n)&&d.removeChild(n,e)}function h(e,n,t,r,i,c,l){if(o(e.elm)&&o(c)&&(e=c[l]=ge(e)),e.isRootInsert=!i,!function(e,n,t,r){var i=e.data;if(o(i)){var c=o(e.componentInstance)&&i.keepAlive;if(o(i=i.hook)&&o(i=i.init)&&i(e,!1),o(e.componentInstance))return p(e,n),m(t,e.elm,r),s(c)&&function(e,n,t,r){var i,s=e;for(;s.componentInstance;)if(s=s.componentInstance._vnode,o(i=s.data)&&o(i=i.transition)){for(i=0;i<a.activate.length;++i)a.activate[i](Ca,s);n.push(s);break}m(t,e.elm,r)}(e,n,t,r),!0}}(e,n,t,r)){var u=e.data,h=e.children,g=e.tag;o(g)?(e.elm=e.ns?d.createElementNS(e.ns,g):d.createElement(g,e),v(e),f(e,h,n),o(u)&&y(e,n),m(t,e.elm,r)):s(e.isComment)?(e.elm=d.createComment(e.text),m(t,e.elm,r)):(e.elm=d.createTextNode(e.text),m(t,e.elm,r))}}function p(e,n){o(e.data.pendingInsert)&&(n.push.apply(n,e.data.pendingInsert),e.data.pendingInsert=null),e.elm=e.componentInstance.$el,g(e)?(y(e,n),v(e)):(_a(e),n.push(e))}function m(e,n,t){o(e)&&(o(t)?d.parentNode(t)===e&&d.insertBefore(e,n,t):d.appendChild(e,n))}function f(e,n,t){if(r(n)){0;for(var a=0;a<n.length;++a)h(n[a],t,e.elm,null,!0,n,a)}else c(e.text)&&d.appendChild(e.elm,d.createTextNode(String(e.text)))}function g(e){for(;e.componentInstance;)e=e.componentInstance._vnode;return o(e.tag)}function y(e,t){for(var r=0;r<a.create.length;++r)a.create[r](Ca,e);o(n=e.data.hook)&&(o(n.create)&&n.create(Ca,e),o(n.insert)&&t.push(e))}function v(e){var n;if(o(n=e.fnScopeId))d.setStyleScope(e.elm,n);else for(var t=e;t;)o(n=t.context)&&o(n=n.$options._scopeId)&&d.setStyleScope(e.elm,n),t=t.parent;o(n=Zn)&&n!==e.context&&n!==e.fnContext&&o(n=n.$options._scopeId)&&d.setStyleScope(e.elm,n)}function w(e,n,t,a,r,i){for(;a<=r;++a)h(t[a],i,e,n,!1,t,a)}function k(e){var n,t,r=e.data;if(o(r))for(o(n=r.hook)&&o(n=n.destroy)&&n(e),n=0;n<a.destroy.length;++n)a.destroy[n](e);if(o(n=e.children))for(t=0;t<e.children.length;++t)k(e.children[t])}function _(e,n,t){for(;n<=t;++n){var a=e[n];o(a)&&(o(a.tag)?(x(a),k(a)):u(a.elm))}}function x(e,n){if(o(n)||o(e.data)){var t,r=a.remove.length+1;for(o(n)?n.listeners+=r:n=function(e,n){function t(){0==--t.listeners&&u(e)}return t.listeners=n,t}(e.elm,r),o(t=e.componentInstance)&&o(t=t._vnode)&&o(t.data)&&x(t,n),t=0;t<a.remove.length;++t)a.remove[t](e,n);o(t=e.data.hook)&&o(t=t.remove)?t(e,n):n()}else u(e.elm)}function C(e,n,t,a){for(var r=t;r<a;r++){var i=n[r];if(o(i)&&Ma(e,i))return r}}function A(e,n,t,r,c,l){if(e!==n){o(n.elm)&&o(r)&&(n=r[c]=ge(n));var u=n.elm=e.elm;if(s(e.isAsyncPlaceholder))o(n.asyncFactory.resolved)?T(e.elm,n,t):n.isAsyncPlaceholder=!0;else if(s(n.isStatic)&&s(e.isStatic)&&n.key===e.key&&(s(n.isCloned)||s(n.isOnce)))n.componentInstance=e.componentInstance;else{var p,m=n.data;o(m)&&o(p=m.hook)&&o(p=p.prepatch)&&p(e,n);var f=e.children,y=n.children;if(o(m)&&g(n)){for(p=0;p<a.update.length;++p)a.update[p](e,n);o(p=m.hook)&&o(p=p.update)&&p(e,n)}i(n.text)?o(f)&&o(y)?f!==y&&function(e,n,t,a,r){var s,c,l,u=0,p=0,m=n.length-1,f=n[0],g=n[m],y=t.length-1,v=t[0],b=t[y],k=!r;for(0;u<=m&&p<=y;)i(f)?f=n[++u]:i(g)?g=n[--m]:Ma(f,v)?(A(f,v,a,t,p),f=n[++u],v=t[++p]):Ma(g,b)?(A(g,b,a,t,y),g=n[--m],b=t[--y]):Ma(f,b)?(A(f,b,a,t,y),k&&d.insertBefore(e,f.elm,d.nextSibling(g.elm)),f=n[++u],b=t[--y]):Ma(g,v)?(A(g,v,a,t,p),k&&d.insertBefore(e,g.elm,f.elm),g=n[--m],v=t[++p]):(i(s)&&(s=Sa(n,u,m)),i(c=o(v.key)?s[v.key]:C(v,n,u,m))?h(v,a,e,f.elm,!1,t,p):Ma(l=n[c],v)?(A(l,v,a,t,p),n[c]=void 0,k&&d.insertBefore(e,l.elm,f.elm)):h(v,a,e,f.elm,!1,t,p),v=t[++p]);u>m?w(e,i(t[y+1])?null:t[y+1].elm,t,p,y,a):p>y&&_(n,u,m)}(u,f,y,t,l):o(y)?(o(e.text)&&d.setTextContent(u,""),w(u,null,y,0,y.length-1,t)):o(f)?_(f,0,f.length-1):o(e.text)&&d.setTextContent(u,""):e.text!==n.text&&d.setTextContent(u,n.text),o(m)&&o(p=m.hook)&&o(p=p.postpatch)&&p(e,n)}}}function M(e,n,t){if(s(t)&&o(e.parent))e.parent.data.pendingInsert=n;else for(var a=0;a<n.length;++a)n[a].data.hook.insert(n[a])}var S=b("attrs,class,staticClass,staticStyle,key");function T(e,n,t,a){var r,i=n.tag,c=n.data,l=n.children;if(a=a||c&&c.pre,n.elm=e,s(n.isComment)&&o(n.asyncFactory))return n.isAsyncPlaceholder=!0,!0;if(o(c)&&(o(r=c.hook)&&o(r=r.init)&&r(n,!0),o(r=n.componentInstance)))return p(n,t),!0;if(o(i)){if(o(l))if(e.hasChildNodes())if(o(r=c)&&o(r=r.domProps)&&o(r=r.innerHTML)){if(r!==e.innerHTML)return!1}else{for(var d=!0,u=e.firstChild,h=0;h<l.length;h++){if(!u||!T(u,l[h],t,a)){d=!1;break}u=u.nextSibling}if(!d||u)return!1}else f(n,l,t);if(o(c)){var m=!1;for(var g in c)if(!S(g)){m=!0,y(n,t);break}!m&&c.class&&qn(c.class)}}else e.data!==n.text&&(e.data=n.text);return!0}return function(e,n,t,r){if(!i(n)){var c,l=!1,u=[];if(i(e))l=!0,h(n,u);else{var p=o(e.nodeType);if(!p&&Ma(e,n))A(e,n,u,null,null,r);else{if(p){if(1===e.nodeType&&e.hasAttribute("data-server-rendered")&&(e.removeAttribute("data-server-rendered"),t=!0),s(t)&&T(e,n,u))return M(n,u,!0),e;c=e,e=new pe(d.tagName(c).toLowerCase(),{},[],void 0,c)}var m=e.elm,f=d.parentNode(m);if(h(n,u,m._leaveCb?null:f,d.nextSibling(m)),o(n.parent))for(var y=n.parent,v=g(n);y;){for(var b=0;b<a.destroy.length;++b)a.destroy[b](y);if(y.elm=n.elm,v){for(var w=0;w<a.create.length;++w)a.create[w](Ca,y);var x=y.data.hook.insert;if(x.merged)for(var C=x.fns.slice(1),S=0;S<C.length;S++)C[S]()}else _a(y);y=y.parent}o(f)?_([e],0,0):o(e.tag)&&k(e)}}return M(n,u,l),n.elm}o(e)&&k(e)}}({nodeOps:wa,modules:[ja,Na,Ka,Za,cr,Y?{create:Lr,activate:Lr,remove:function(e,n){!0!==e.data.show?Pr(e,n):n()}}:{}].concat(za)});X&&document.addEventListener("selectionchange",(function(){var e=document.activeElement;e&&e.vmodel&&Nr(e,"input")}));var zr={inserted:function(e,n,t,a){"select"===t.tag?(a.elm&&!a.elm._vOptions?We(t,"postpatch",(function(){zr.componentUpdated(e,n,t)})):Rr(e,n,t.context),e._vOptions=[].map.call(e.options,jr)):("textarea"===t.tag||ba(e.type))&&(e._vModifiers=n.modifiers,n.modifiers.lazy||(e.addEventListener("compositionstart",Br),e.addEventListener("compositionend",Gr),e.addEventListener("change",Gr),X&&(e.vmodel=!0)))},componentUpdated:function(e,n,t){if("select"===t.tag){Rr(e,n,t.context);var a=e._vOptions,r=e._vOptions=[].map.call(e.options,jr);if(r.some((function(e,n){return!U(e,a[n])})))(e.multiple?n.value.some((function(e){return Ur(e,r)})):n.value!==n.oldValue&&Ur(n.value,r))&&Nr(e,"change")}}};function Rr(e,n,t){Er(e,n,t),(Z||J)&&setTimeout((function(){Er(e,n,t)}),0)}function Er(e,n,t){var a=n.value,r=e.multiple;if(!r||Array.isArray(a)){for(var i,o,s=0,c=e.options.length;s<c;s++)if(o=e.options[s],r)i=j(a,jr(o))>-1,o.selected!==i&&(o.selected=i);else if(U(jr(o),a))return void(e.selectedIndex!==s&&(e.selectedIndex=s));r||(e.selectedIndex=-1)}}function Ur(e,n){return n.every((function(n){return!U(n,e)}))}function jr(e){return"_value"in e?e._value:e.value}function Br(e){e.target.composing=!0}function Gr(e){e.target.composing&&(e.target.composing=!1,Nr(e.target,"input"))}function Nr(e,n){var t=document.createEvent("HTMLEvents");t.initEvent(n,!0,!0),e.dispatchEvent(t)}function $r(e){return!e.componentInstance||e.data&&e.data.transition?e:$r(e.componentInstance._vnode)}var qr={model:zr,show:{bind:function(e,n,t){var a=n.value,r=(t=$r(t)).data&&t.data.transition,i=e.__vOriginalDisplay="none"===e.style.display?"":e.style.display;a&&r?(t.data.show=!0,Tr(t,(function(){e.style.display=i}))):e.style.display=a?i:"none"},update:function(e,n,t){var a=n.value;!a!=!n.oldValue&&((t=$r(t)).data&&t.data.transition?(t.data.show=!0,a?Tr(t,(function(){e.style.display=e.__vOriginalDisplay})):Pr(t,(function(){e.style.display="none"}))):e.style.display=a?e.__vOriginalDisplay:"none")},unbind:function(e,n,t,a,r){r||(e.style.display=e.__vOriginalDisplay)}}},Fr={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function Vr(e){var n=e&&e.componentOptions;return n&&n.Ctor.options.abstract?Vr(An(n.children)):e}function Hr(e){var n={},t=e.$options;for(var a in t.propsData)n[a]=e[a];var r=t._parentListeners;for(var a in r)n[M(a)]=r[a];return n}function Wr(e,n){if(/\d-keep-alive$/.test(n.tag))return e("keep-alive",{props:n.componentOptions.propsData})}var Kr=function(e){return e.tag||fn(e)},Yr=function(e){return"show"===e.name},Qr={name:"transition",props:Fr,abstract:!0,render:function(e){var n=this,t=this.$slots.default;if(t&&(t=t.filter(Kr)).length){0;var a=this.mode;0;var r=t[0];if(function(e){for(;e=e.parent;)if(e.data.transition)return!0}(this.$vnode))return r;var i=Vr(r);if(!i)return r;if(this._leaving)return Wr(e,r);var o="__transition-".concat(this._uid,"-");i.key=null==i.key?i.isComment?o+"comment":o+i.tag:c(i.key)?0===String(i.key).indexOf(o)?i.key:o+i.key:i.key;var s=(i.data||(i.data={})).transition=Hr(this),l=this._vnode,d=Vr(l);if(i.data.directives&&i.data.directives.some(Yr)&&(i.data.show=!0),d&&d.data&&!function(e,n){return n.key===e.key&&n.tag===e.tag}(i,d)&&!fn(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var u=d.data.transition=L({},s);if("out-in"===a)return this._leaving=!0,We(u,"afterLeave",(function(){n._leaving=!1,n.$forceUpdate()})),Wr(e,r);if("in-out"===a){if(fn(i))return l;var h,p=function(){h()};We(s,"afterEnter",p),We(s,"enterCancelled",p),We(u,"delayLeave",(function(e){h=e}))}}return r}}},Zr=L({tag:String,moveClass:String},Fr);function Xr(e){e.elm._moveCb&&e.elm._moveCb(),e.elm._enterCb&&e.elm._enterCb()}function Jr(e){e.data.newPos=e.elm.getBoundingClientRect()}function ei(e){var n=e.data.pos,t=e.data.newPos,a=n.left-t.left,r=n.top-t.top;if(a||r){e.data.moved=!0;var i=e.elm.style;i.transform=i.WebkitTransform="translate(".concat(a,"px,").concat(r,"px)"),i.transitionDuration="0s"}}delete Zr.mode;var ni={Transition:Qr,TransitionGroup:{props:Zr,beforeMount:function(){var e=this,n=this._update;this._update=function(t,a){var r=Xn(e);e.__patch__(e._vnode,e.kept,!1,!0),e._vnode=e.kept,r(),n.call(e,t,a)}},render:function(e){for(var n=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),a=this.prevChildren=this.children,r=this.$slots.default||[],i=this.children=[],o=Hr(this),s=0;s<r.length;s++){if((d=r[s]).tag)if(null!=d.key&&0!==String(d.key).indexOf("__vlist"))i.push(d),t[d.key]=d,(d.data||(d.data={})).transition=o;else;}if(a){var c=[],l=[];for(s=0;s<a.length;s++){var d;(d=a[s]).data.transition=o,d.data.pos=d.elm.getBoundingClientRect(),t[d.key]?c.push(d):l.push(d)}this.kept=e(n,null,c),this.removed=l}return e(n,null,i)},updated:function(){var e=this.prevChildren,n=this.moveClass||(this.name||"v")+"-move";e.length&&this.hasMove(e[0].elm,n)&&(e.forEach(Xr),e.forEach(Jr),e.forEach(ei),this._reflow=document.body.offsetHeight,e.forEach((function(e){if(e.data.moved){var t=e.elm,a=t.style;kr(t,n),a.transform=a.WebkitTransform=a.transitionDuration="",t.addEventListener(gr,t._moveCb=function e(a){a&&a.target!==t||a&&!/transform$/.test(a.propertyName)||(t.removeEventListener(gr,e),t._moveCb=null,_r(t,n))})}})))},methods:{hasMove:function(e,n){if(!mr)return!1;if(this._hasMove)return this._hasMove;var t=e.cloneNode();e._transitionClasses&&e._transitionClasses.forEach((function(e){ur(t,e)})),dr(t,n),t.style.display="none",this.$el.appendChild(t);var a=Ar(t);return this.$el.removeChild(t),this._hasMove=a.hasTransform}}}};function ti(e,n){for(var t in n)e[t]=n[t];return e}Wt.config.mustUseProp=function(e,n,t){return"value"===t&&ta(e)&&"button"!==n||"selected"===t&&"option"===e||"checked"===t&&"input"===e||"muted"===t&&"video"===e},Wt.config.isReservedTag=ya,Wt.config.isReservedAttr=na,Wt.config.getTagNamespace=function(e){return ga(e)?"svg":"math"===e?"math":void 0},Wt.config.isUnknownElement=function(e){if(!Y)return!0;if(ya(e))return!1;if(e=e.toLowerCase(),null!=va[e])return va[e];var n=document.createElement(e);return e.indexOf("-")>-1?va[e]=n.constructor===window.HTMLUnknownElement||n.constructor===window.HTMLElement:va[e]=/HTMLUnknownElement/.test(n.toString())},L(Wt.options.directives,qr),L(Wt.options.components,ni),Wt.prototype.__patch__=Y?Or:z,Wt.prototype.$mount=function(e,n){return function(e,n,t){var a;e.$el=n,e.$options.render||(e.$options.render=me),nt(e,"beforeMount"),a=function(){e._update(e._render(),t)},new Hn(e,a,z,{before:function(){e._isMounted&&!e._isDestroyed&&nt(e,"beforeUpdate")}},!0),t=!1;var r=e._preWatchers;if(r)for(var i=0;i<r.length;i++)r[i].run();return null==e.$vnode&&(e._isMounted=!0,nt(e,"mounted")),e}(this,e=e&&Y?function(e){if("string"==typeof e){var n=document.querySelector(e);return n||document.createElement("div")}return e}(e):void 0,n)},Y&&setTimeout((function(){q.devtools&&se&&se.emit("init",Wt)}),0);var ai=/[!'()*]/g,ri=function(e){return"%"+e.charCodeAt(0).toString(16)},ii=/%2C/g,oi=function(e){return encodeURIComponent(e).replace(ai,ri).replace(ii,",")};function si(e){try{return decodeURIComponent(e)}catch(e){0}return e}var ci=function(e){return null==e||"object"==typeof e?e:String(e)};function li(e){var n={};return(e=e.trim().replace(/^(\?|#|&)/,""))?(e.split("&").forEach((function(e){var t=e.replace(/\+/g," ").split("="),a=si(t.shift()),r=t.length>0?si(t.join("=")):null;void 0===n[a]?n[a]=r:Array.isArray(n[a])?n[a].push(r):n[a]=[n[a],r]})),n):n}function di(e){var n=e?Object.keys(e).map((function(n){var t=e[n];if(void 0===t)return"";if(null===t)return oi(n);if(Array.isArray(t)){var a=[];return t.forEach((function(e){void 0!==e&&(null===e?a.push(oi(n)):a.push(oi(n)+"="+oi(e)))})),a.join("&")}return oi(n)+"="+oi(t)})).filter((function(e){return e.length>0})).join("&"):null;return n?"?"+n:""}var ui=/\/?$/;function hi(e,n,t,a){var r=a&&a.options.stringifyQuery,i=n.query||{};try{i=pi(i)}catch(e){}var o={name:n.name||e&&e.name,meta:e&&e.meta||{},path:n.path||"/",hash:n.hash||"",query:i,params:n.params||{},fullPath:gi(n,r),matched:e?fi(e):[]};return t&&(o.redirectedFrom=gi(t,r)),Object.freeze(o)}function pi(e){if(Array.isArray(e))return e.map(pi);if(e&&"object"==typeof e){var n={};for(var t in e)n[t]=pi(e[t]);return n}return e}var mi=hi(null,{path:"/"});function fi(e){for(var n=[];e;)n.unshift(e),e=e.parent;return n}function gi(e,n){var t=e.path,a=e.query;void 0===a&&(a={});var r=e.hash;return void 0===r&&(r=""),(t||"/")+(n||di)(a)+r}function yi(e,n,t){return n===mi?e===n:!!n&&(e.path&&n.path?e.path.replace(ui,"")===n.path.replace(ui,"")&&(t||e.hash===n.hash&&vi(e.query,n.query)):!(!e.name||!n.name)&&(e.name===n.name&&(t||e.hash===n.hash&&vi(e.query,n.query)&&vi(e.params,n.params))))}function vi(e,n){if(void 0===e&&(e={}),void 0===n&&(n={}),!e||!n)return e===n;var t=Object.keys(e).sort(),a=Object.keys(n).sort();return t.length===a.length&&t.every((function(t,r){var i=e[t];if(a[r]!==t)return!1;var o=n[t];return null==i||null==o?i===o:"object"==typeof i&&"object"==typeof o?vi(i,o):String(i)===String(o)}))}function bi(e){for(var n=0;n<e.matched.length;n++){var t=e.matched[n];for(var a in t.instances){var r=t.instances[a],i=t.enteredCbs[a];if(r&&i){delete t.enteredCbs[a];for(var o=0;o<i.length;o++)r._isBeingDestroyed||i[o](r)}}}}var wi={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(e,n){var t=n.props,a=n.children,r=n.parent,i=n.data;i.routerView=!0;for(var o=r.$createElement,s=t.name,c=r.$route,l=r._routerViewCache||(r._routerViewCache={}),d=0,u=!1;r&&r._routerRoot!==r;){var h=r.$vnode?r.$vnode.data:{};h.routerView&&d++,h.keepAlive&&r._directInactive&&r._inactive&&(u=!0),r=r.$parent}if(i.routerViewDepth=d,u){var p=l[s],m=p&&p.component;return m?(p.configProps&&ki(m,i,p.route,p.configProps),o(m,i,a)):o()}var f=c.matched[d],g=f&&f.components[s];if(!f||!g)return l[s]=null,o();l[s]={component:g},i.registerRouteInstance=function(e,n){var t=f.instances[s];(n&&t!==e||!n&&t===e)&&(f.instances[s]=n)},(i.hook||(i.hook={})).prepatch=function(e,n){f.instances[s]=n.componentInstance},i.hook.init=function(e){e.data.keepAlive&&e.componentInstance&&e.componentInstance!==f.instances[s]&&(f.instances[s]=e.componentInstance),bi(c)};var y=f.props&&f.props[s];return y&&(ti(l[s],{route:c,configProps:y}),ki(g,i,c,y)),o(g,i,a)}};function ki(e,n,t,a){var r=n.props=function(e,n){switch(typeof n){case"undefined":return;case"object":return n;case"function":return n(e);case"boolean":return n?e.params:void 0;default:0}}(t,a);if(r){r=n.props=ti({},r);var i=n.attrs=n.attrs||{};for(var o in r)e.props&&o in e.props||(i[o]=r[o],delete r[o])}}function _i(e,n,t){var a=e.charAt(0);if("/"===a)return e;if("?"===a||"#"===a)return n+e;var r=n.split("/");t&&r[r.length-1]||r.pop();for(var i=e.replace(/^\//,"").split("/"),o=0;o<i.length;o++){var s=i[o];".."===s?r.pop():"."!==s&&r.push(s)}return""!==r[0]&&r.unshift(""),r.join("/")}function xi(e){return e.replace(/\/(?:\s*\/)+/g,"/")}var Ci=Array.isArray||function(e){return"[object Array]"==Object.prototype.toString.call(e)},Ai=Bi,Mi=Di,Si=function(e,n){return Oi(Di(e,n),n)},Ti=Oi,Pi=ji,Ii=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function Di(e,n){for(var t,a=[],r=0,i=0,o="",s=n&&n.delimiter||"/";null!=(t=Ii.exec(e));){var c=t[0],l=t[1],d=t.index;if(o+=e.slice(i,d),i=d+c.length,l)o+=l[1];else{var u=e[i],h=t[2],p=t[3],m=t[4],f=t[5],g=t[6],y=t[7];o&&(a.push(o),o="");var v=null!=h&&null!=u&&u!==h,b="+"===g||"*"===g,w="?"===g||"*"===g,k=t[2]||s,_=m||f;a.push({name:p||r++,prefix:h||"",delimiter:k,optional:w,repeat:b,partial:v,asterisk:!!y,pattern:_?Ri(_):y?".*":"[^"+zi(k)+"]+?"})}}return i<e.length&&(o+=e.substr(i)),o&&a.push(o),a}function Li(e){return encodeURI(e).replace(/[\/?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()}))}function Oi(e,n){for(var t=new Array(e.length),a=0;a<e.length;a++)"object"==typeof e[a]&&(t[a]=new RegExp("^(?:"+e[a].pattern+")$",Ui(n)));return function(n,a){for(var r="",i=n||{},o=(a||{}).pretty?Li:encodeURIComponent,s=0;s<e.length;s++){var c=e[s];if("string"!=typeof c){var l,d=i[c.name];if(null==d){if(c.optional){c.partial&&(r+=c.prefix);continue}throw new TypeError('Expected "'+c.name+'" to be defined')}if(Ci(d)){if(!c.repeat)throw new TypeError('Expected "'+c.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(c.optional)continue;throw new TypeError('Expected "'+c.name+'" to not be empty')}for(var u=0;u<d.length;u++){if(l=o(d[u]),!t[s].test(l))throw new TypeError('Expected all "'+c.name+'" to match "'+c.pattern+'", but received `'+JSON.stringify(l)+"`");r+=(0===u?c.prefix:c.delimiter)+l}}else{if(l=c.asterisk?encodeURI(d).replace(/[?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()})):o(d),!t[s].test(l))throw new TypeError('Expected "'+c.name+'" to match "'+c.pattern+'", but received "'+l+'"');r+=c.prefix+l}}else r+=c}return r}}function zi(e){return e.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function Ri(e){return e.replace(/([=!:$\/()])/g,"\\$1")}function Ei(e,n){return e.keys=n,e}function Ui(e){return e&&e.sensitive?"":"i"}function ji(e,n,t){Ci(n)||(t=n||t,n=[]);for(var a=(t=t||{}).strict,r=!1!==t.end,i="",o=0;o<e.length;o++){var s=e[o];if("string"==typeof s)i+=zi(s);else{var c=zi(s.prefix),l="(?:"+s.pattern+")";n.push(s),s.repeat&&(l+="(?:"+c+l+")*"),i+=l=s.optional?s.partial?c+"("+l+")?":"(?:"+c+"("+l+"))?":c+"("+l+")"}}var d=zi(t.delimiter||"/"),u=i.slice(-d.length)===d;return a||(i=(u?i.slice(0,-d.length):i)+"(?:"+d+"(?=$))?"),i+=r?"$":a&&u?"":"(?="+d+"|$)",Ei(new RegExp("^"+i,Ui(t)),n)}function Bi(e,n,t){return Ci(n)||(t=n||t,n=[]),t=t||{},e instanceof RegExp?function(e,n){var t=e.source.match(/\((?!\?)/g);if(t)for(var a=0;a<t.length;a++)n.push({name:a,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return Ei(e,n)}(e,n):Ci(e)?function(e,n,t){for(var a=[],r=0;r<e.length;r++)a.push(Bi(e[r],n,t).source);return Ei(new RegExp("(?:"+a.join("|")+")",Ui(t)),n)}(e,n,t):function(e,n,t){return ji(Di(e,t),n,t)}(e,n,t)}Ai.parse=Mi,Ai.compile=Si,Ai.tokensToFunction=Ti,Ai.tokensToRegExp=Pi;var Gi=Object.create(null);function Ni(e,n,t){n=n||{};try{var a=Gi[e]||(Gi[e]=Ai.compile(e));return"string"==typeof n.pathMatch&&(n[0]=n.pathMatch),a(n,{pretty:!0})}catch(e){return""}finally{delete n[0]}}function $i(e,n,t,a){var r="string"==typeof e?{path:e}:e;if(r._normalized)return r;if(r.name){var i=(r=ti({},e)).params;return i&&"object"==typeof i&&(r.params=ti({},i)),r}if(!r.path&&r.params&&n){(r=ti({},r))._normalized=!0;var o=ti(ti({},n.params),r.params);if(n.name)r.name=n.name,r.params=o;else if(n.matched.length){var s=n.matched[n.matched.length-1].path;r.path=Ni(s,o,n.path)}else 0;return r}var c=function(e){var n="",t="",a=e.indexOf("#");a>=0&&(n=e.slice(a),e=e.slice(0,a));var r=e.indexOf("?");return r>=0&&(t=e.slice(r+1),e=e.slice(0,r)),{path:e,query:t,hash:n}}(r.path||""),l=n&&n.path||"/",d=c.path?_i(c.path,l,t||r.append):l,u=function(e,n,t){void 0===n&&(n={});var a,r=t||li;try{a=r(e||"")}catch(e){a={}}for(var i in n){var o=n[i];a[i]=Array.isArray(o)?o.map(ci):ci(o)}return a}(c.query,r.query,a&&a.options.parseQuery),h=r.hash||c.hash;return h&&"#"!==h.charAt(0)&&(h="#"+h),{_normalized:!0,path:d,query:u,hash:h}}var qi,Fi=function(){},Vi={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(e){var n=this,t=this.$router,a=this.$route,r=t.resolve(this.to,a,this.append),i=r.location,o=r.route,s=r.href,c={},l=t.options.linkActiveClass,d=t.options.linkExactActiveClass,u=null==l?"router-link-active":l,h=null==d?"router-link-exact-active":d,p=null==this.activeClass?u:this.activeClass,m=null==this.exactActiveClass?h:this.exactActiveClass,f=o.redirectedFrom?hi(null,$i(o.redirectedFrom),null,t):o;c[m]=yi(a,f,this.exactPath),c[p]=this.exact||this.exactPath?c[m]:function(e,n){return 0===e.path.replace(ui,"/").indexOf(n.path.replace(ui,"/"))&&(!n.hash||e.hash===n.hash)&&function(e,n){for(var t in n)if(!(t in e))return!1;return!0}(e.query,n.query)}(a,f);var g=c[m]?this.ariaCurrentValue:null,y=function(e){Hi(e)&&(n.replace?t.replace(i,Fi):t.push(i,Fi))},v={click:Hi};Array.isArray(this.event)?this.event.forEach((function(e){v[e]=y})):v[this.event]=y;var b={class:c},w=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:o,navigate:y,isActive:c[p],isExactActive:c[m]});if(w){if(1===w.length)return w[0];if(w.length>1||!w.length)return 0===w.length?e():e("span",{},w)}if("a"===this.tag)b.on=v,b.attrs={href:s,"aria-current":g};else{var k=function e(n){var t;if(n)for(var a=0;a<n.length;a++){if("a"===(t=n[a]).tag)return t;if(t.children&&(t=e(t.children)))return t}}(this.$slots.default);if(k){k.isStatic=!1;var _=k.data=ti({},k.data);for(var x in _.on=_.on||{},_.on){var C=_.on[x];x in v&&(_.on[x]=Array.isArray(C)?C:[C])}for(var A in v)A in _.on?_.on[A].push(v[A]):_.on[A]=y;var M=k.data.attrs=ti({},k.data.attrs);M.href=s,M["aria-current"]=g}else b.on=v}return e(this.tag,b,this.$slots.default)}};function Hi(e){if(!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey||e.defaultPrevented||void 0!==e.button&&0!==e.button)){if(e.currentTarget&&e.currentTarget.getAttribute){var n=e.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(n))return}return e.preventDefault&&e.preventDefault(),!0}}var Wi="undefined"!=typeof window;function Ki(e,n,t,a,r){var i=n||[],o=t||Object.create(null),s=a||Object.create(null);e.forEach((function(e){!function e(n,t,a,r,i,o){var s=r.path,c=r.name;0;var l=r.pathToRegexpOptions||{},d=function(e,n,t){t||(e=e.replace(/\/$/,""));if("/"===e[0])return e;if(null==n)return e;return xi(n.path+"/"+e)}(s,i,l.strict);"boolean"==typeof r.caseSensitive&&(l.sensitive=r.caseSensitive);var u={path:d,regex:Yi(d,l),components:r.components||{default:r.component},alias:r.alias?"string"==typeof r.alias?[r.alias]:r.alias:[],instances:{},enteredCbs:{},name:c,parent:i,matchAs:o,redirect:r.redirect,beforeEnter:r.beforeEnter,meta:r.meta||{},props:null==r.props?{}:r.components?r.props:{default:r.props}};r.children&&r.children.forEach((function(r){var i=o?xi(o+"/"+r.path):void 0;e(n,t,a,r,u,i)}));t[u.path]||(n.push(u.path),t[u.path]=u);if(void 0!==r.alias)for(var h=Array.isArray(r.alias)?r.alias:[r.alias],p=0;p<h.length;++p){0;var m={path:h[p],children:r.children};e(n,t,a,m,i,u.path||"/")}c&&(a[c]||(a[c]=u))}(i,o,s,e,r)}));for(var c=0,l=i.length;c<l;c++)"*"===i[c]&&(i.push(i.splice(c,1)[0]),l--,c--);return{pathList:i,pathMap:o,nameMap:s}}function Yi(e,n){return Ai(e,[],n)}function Qi(e,n){var t=Ki(e),a=t.pathList,r=t.pathMap,i=t.nameMap;function o(e,t,o){var s=$i(e,t,!1,n),l=s.name;if(l){var d=i[l];if(!d)return c(null,s);var u=d.regex.keys.filter((function(e){return!e.optional})).map((function(e){return e.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var h in t.params)!(h in s.params)&&u.indexOf(h)>-1&&(s.params[h]=t.params[h]);return s.path=Ni(d.path,s.params),c(d,s,o)}if(s.path){s.params={};for(var p=0;p<a.length;p++){var m=a[p],f=r[m];if(Zi(f.regex,s.path,s.params))return c(f,s,o)}}return c(null,s)}function s(e,t){var a=e.redirect,r="function"==typeof a?a(hi(e,t,null,n)):a;if("string"==typeof r&&(r={path:r}),!r||"object"!=typeof r)return c(null,t);var s=r,l=s.name,d=s.path,u=t.query,h=t.hash,p=t.params;if(u=s.hasOwnProperty("query")?s.query:u,h=s.hasOwnProperty("hash")?s.hash:h,p=s.hasOwnProperty("params")?s.params:p,l){i[l];return o({_normalized:!0,name:l,query:u,hash:h,params:p},void 0,t)}if(d){var m=function(e,n){return _i(e,n.parent?n.parent.path:"/",!0)}(d,e);return o({_normalized:!0,path:Ni(m,p),query:u,hash:h},void 0,t)}return c(null,t)}function c(e,t,a){return e&&e.redirect?s(e,a||t):e&&e.matchAs?function(e,n,t){var a=o({_normalized:!0,path:Ni(t,n.params)});if(a){var r=a.matched,i=r[r.length-1];return n.params=a.params,c(i,n)}return c(null,n)}(0,t,e.matchAs):hi(e,t,a,n)}return{match:o,addRoute:function(e,n){var t="object"!=typeof e?i[e]:void 0;Ki([n||e],a,r,i,t),t&&t.alias.length&&Ki(t.alias.map((function(e){return{path:e,children:[n]}})),a,r,i,t)},getRoutes:function(){return a.map((function(e){return r[e]}))},addRoutes:function(e){Ki(e,a,r,i)}}}function Zi(e,n,t){var a=n.match(e);if(!a)return!1;if(!t)return!0;for(var r=1,i=a.length;r<i;++r){var o=e.keys[r-1];o&&(t[o.name||"pathMatch"]="string"==typeof a[r]?si(a[r]):a[r])}return!0}var Xi=Wi&&window.performance&&window.performance.now?window.performance:Date;function Ji(){return Xi.now().toFixed(3)}var eo=Ji();function no(){return eo}function to(e){return eo=e}var ao=Object.create(null);function ro(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var e=window.location.protocol+"//"+window.location.host,n=window.location.href.replace(e,""),t=ti({},window.history.state);return t.key=no(),window.history.replaceState(t,"",n),window.addEventListener("popstate",so),function(){window.removeEventListener("popstate",so)}}function io(e,n,t,a){if(e.app){var r=e.options.scrollBehavior;r&&e.app.$nextTick((function(){var i=function(){var e=no();if(e)return ao[e]}(),o=r.call(e,n,t,a?i:null);o&&("function"==typeof o.then?o.then((function(e){po(e,i)})).catch((function(e){0})):po(o,i))}))}}function oo(){var e=no();e&&(ao[e]={x:window.pageXOffset,y:window.pageYOffset})}function so(e){oo(),e.state&&e.state.key&&to(e.state.key)}function co(e){return uo(e.x)||uo(e.y)}function lo(e){return{x:uo(e.x)?e.x:window.pageXOffset,y:uo(e.y)?e.y:window.pageYOffset}}function uo(e){return"number"==typeof e}var ho=/^#\d/;function po(e,n){var t,a="object"==typeof e;if(a&&"string"==typeof e.selector){var r=ho.test(e.selector)?document.getElementById(e.selector.slice(1)):document.querySelector(e.selector);if(r){var i=e.offset&&"object"==typeof e.offset?e.offset:{};n=function(e,n){var t=document.documentElement.getBoundingClientRect(),a=e.getBoundingClientRect();return{x:a.left-t.left-n.x,y:a.top-t.top-n.y}}(r,i={x:uo((t=i).x)?t.x:0,y:uo(t.y)?t.y:0})}else co(e)&&(n=lo(e))}else a&&co(e)&&(n=lo(e));n&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:n.x,top:n.y,behavior:e.behavior}):window.scrollTo(n.x,n.y))}var mo,fo=Wi&&((-1===(mo=window.navigator.userAgent).indexOf("Android 2.")&&-1===mo.indexOf("Android 4.0")||-1===mo.indexOf("Mobile Safari")||-1!==mo.indexOf("Chrome")||-1!==mo.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function go(e,n){oo();var t=window.history;try{if(n){var a=ti({},t.state);a.key=no(),t.replaceState(a,"",e)}else t.pushState({key:to(Ji())},"",e)}catch(t){window.location[n?"replace":"assign"](e)}}function yo(e){go(e,!0)}var vo={redirected:2,aborted:4,cancelled:8,duplicated:16};function bo(e,n){return ko(e,n,vo.redirected,'Redirected when going from "'+e.fullPath+'" to "'+function(e){if("string"==typeof e)return e;if("path"in e)return e.path;var n={};return _o.forEach((function(t){t in e&&(n[t]=e[t])})),JSON.stringify(n,null,2)}(n)+'" via a navigation guard.')}function wo(e,n){return ko(e,n,vo.cancelled,'Navigation cancelled from "'+e.fullPath+'" to "'+n.fullPath+'" with a new navigation.')}function ko(e,n,t,a){var r=new Error(a);return r._isRouter=!0,r.from=e,r.to=n,r.type=t,r}var _o=["params","query","hash"];function xo(e){return Object.prototype.toString.call(e).indexOf("Error")>-1}function Co(e,n){return xo(e)&&e._isRouter&&(null==n||e.type===n)}function Ao(e,n,t){var a=function(r){r>=e.length?t():e[r]?n(e[r],(function(){a(r+1)})):a(r+1)};a(0)}function Mo(e){return function(n,t,a){var r=!1,i=0,o=null;So(e,(function(e,n,t,s){if("function"==typeof e&&void 0===e.cid){r=!0,i++;var c,l=Io((function(n){var r;((r=n).__esModule||Po&&"Module"===r[Symbol.toStringTag])&&(n=n.default),e.resolved="function"==typeof n?n:qi.extend(n),t.components[s]=n,--i<=0&&a()})),d=Io((function(e){var n="Failed to resolve async component "+s+": "+e;o||(o=xo(e)?e:new Error(n),a(o))}));try{c=e(l,d)}catch(e){d(e)}if(c)if("function"==typeof c.then)c.then(l,d);else{var u=c.component;u&&"function"==typeof u.then&&u.then(l,d)}}})),r||a()}}function So(e,n){return To(e.map((function(e){return Object.keys(e.components).map((function(t){return n(e.components[t],e.instances[t],e,t)}))})))}function To(e){return Array.prototype.concat.apply([],e)}var Po="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function Io(e){var n=!1;return function(){for(var t=[],a=arguments.length;a--;)t[a]=arguments[a];if(!n)return n=!0,e.apply(this,t)}}var Do=function(e,n){this.router=e,this.base=function(e){if(!e)if(Wi){var n=document.querySelector("base");e=(e=n&&n.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else e="/";"/"!==e.charAt(0)&&(e="/"+e);return e.replace(/\/$/,"")}(n),this.current=mi,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function Lo(e,n,t,a){var r=So(e,(function(e,a,r,i){var o=function(e,n){"function"!=typeof e&&(e=qi.extend(e));return e.options[n]}(e,n);if(o)return Array.isArray(o)?o.map((function(e){return t(e,a,r,i)})):t(o,a,r,i)}));return To(a?r.reverse():r)}function Oo(e,n){if(n)return function(){return e.apply(n,arguments)}}Do.prototype.listen=function(e){this.cb=e},Do.prototype.onReady=function(e,n){this.ready?e():(this.readyCbs.push(e),n&&this.readyErrorCbs.push(n))},Do.prototype.onError=function(e){this.errorCbs.push(e)},Do.prototype.transitionTo=function(e,n,t){var a,r=this;try{a=this.router.match(e,this.current)}catch(e){throw this.errorCbs.forEach((function(n){n(e)})),e}var i=this.current;this.confirmTransition(a,(function(){r.updateRoute(a),n&&n(a),r.ensureURL(),r.router.afterHooks.forEach((function(e){e&&e(a,i)})),r.ready||(r.ready=!0,r.readyCbs.forEach((function(e){e(a)})))}),(function(e){t&&t(e),e&&!r.ready&&(Co(e,vo.redirected)&&i===mi||(r.ready=!0,r.readyErrorCbs.forEach((function(n){n(e)}))))}))},Do.prototype.confirmTransition=function(e,n,t){var a=this,r=this.current;this.pending=e;var i=function(e){!Co(e)&&xo(e)&&(a.errorCbs.length?a.errorCbs.forEach((function(n){n(e)})):console.error(e)),t&&t(e)},o=e.matched.length-1,s=r.matched.length-1;if(yi(e,r)&&o===s&&e.matched[o]===r.matched[s])return this.ensureURL(),e.hash&&io(this.router,r,e,!1),i(function(e,n){var t=ko(e,n,vo.duplicated,'Avoided redundant navigation to current location: "'+e.fullPath+'".');return t.name="NavigationDuplicated",t}(r,e));var c=function(e,n){var t,a=Math.max(e.length,n.length);for(t=0;t<a&&e[t]===n[t];t++);return{updated:n.slice(0,t),activated:n.slice(t),deactivated:e.slice(t)}}(this.current.matched,e.matched),l=c.updated,d=c.deactivated,u=c.activated,h=[].concat(function(e){return Lo(e,"beforeRouteLeave",Oo,!0)}(d),this.router.beforeHooks,function(e){return Lo(e,"beforeRouteUpdate",Oo)}(l),u.map((function(e){return e.beforeEnter})),Mo(u)),p=function(n,t){if(a.pending!==e)return i(wo(r,e));try{n(e,r,(function(n){!1===n?(a.ensureURL(!0),i(function(e,n){return ko(e,n,vo.aborted,'Navigation aborted from "'+e.fullPath+'" to "'+n.fullPath+'" via a navigation guard.')}(r,e))):xo(n)?(a.ensureURL(!0),i(n)):"string"==typeof n||"object"==typeof n&&("string"==typeof n.path||"string"==typeof n.name)?(i(bo(r,e)),"object"==typeof n&&n.replace?a.replace(n):a.push(n)):t(n)}))}catch(e){i(e)}};Ao(h,p,(function(){Ao(function(e){return Lo(e,"beforeRouteEnter",(function(e,n,t,a){return function(e,n,t){return function(a,r,i){return e(a,r,(function(e){"function"==typeof e&&(n.enteredCbs[t]||(n.enteredCbs[t]=[]),n.enteredCbs[t].push(e)),i(e)}))}}(e,t,a)}))}(u).concat(a.router.resolveHooks),p,(function(){if(a.pending!==e)return i(wo(r,e));a.pending=null,n(e),a.router.app&&a.router.app.$nextTick((function(){bi(e)}))}))}))},Do.prototype.updateRoute=function(e){this.current=e,this.cb&&this.cb(e)},Do.prototype.setupListeners=function(){},Do.prototype.teardown=function(){this.listeners.forEach((function(e){e()})),this.listeners=[],this.current=mi,this.pending=null};var zo=function(e){function n(n,t){e.call(this,n,t),this._startLocation=Ro(this.base)}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router,t=n.options.scrollBehavior,a=fo&&t;a&&this.listeners.push(ro());var r=function(){var t=e.current,r=Ro(e.base);e.current===mi&&r===e._startLocation||e.transitionTo(r,(function(e){a&&io(n,e,t,!0)}))};window.addEventListener("popstate",r),this.listeners.push((function(){window.removeEventListener("popstate",r)}))}},n.prototype.go=function(e){window.history.go(e)},n.prototype.push=function(e,n,t){var a=this,r=this.current;this.transitionTo(e,(function(e){go(xi(a.base+e.fullPath)),io(a.router,e,r,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var a=this,r=this.current;this.transitionTo(e,(function(e){yo(xi(a.base+e.fullPath)),io(a.router,e,r,!1),n&&n(e)}),t)},n.prototype.ensureURL=function(e){if(Ro(this.base)!==this.current.fullPath){var n=xi(this.base+this.current.fullPath);e?go(n):yo(n)}},n.prototype.getCurrentLocation=function(){return Ro(this.base)},n}(Do);function Ro(e){var n=window.location.pathname,t=n.toLowerCase(),a=e.toLowerCase();return!e||t!==a&&0!==t.indexOf(xi(a+"/"))||(n=n.slice(e.length)),(n||"/")+window.location.search+window.location.hash}var Eo=function(e){function n(n,t,a){e.call(this,n,t),a&&function(e){var n=Ro(e);if(!/^\/#/.test(n))return window.location.replace(xi(e+"/#"+n)),!0}(this.base)||Uo()}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router.options.scrollBehavior,t=fo&&n;t&&this.listeners.push(ro());var a=function(){var n=e.current;Uo()&&e.transitionTo(jo(),(function(a){t&&io(e.router,a,n,!0),fo||No(a.fullPath)}))},r=fo?"popstate":"hashchange";window.addEventListener(r,a),this.listeners.push((function(){window.removeEventListener(r,a)}))}},n.prototype.push=function(e,n,t){var a=this,r=this.current;this.transitionTo(e,(function(e){Go(e.fullPath),io(a.router,e,r,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var a=this,r=this.current;this.transitionTo(e,(function(e){No(e.fullPath),io(a.router,e,r,!1),n&&n(e)}),t)},n.prototype.go=function(e){window.history.go(e)},n.prototype.ensureURL=function(e){var n=this.current.fullPath;jo()!==n&&(e?Go(n):No(n))},n.prototype.getCurrentLocation=function(){return jo()},n}(Do);function Uo(){var e=jo();return"/"===e.charAt(0)||(No("/"+e),!1)}function jo(){var e=window.location.href,n=e.indexOf("#");return n<0?"":e=e.slice(n+1)}function Bo(e){var n=window.location.href,t=n.indexOf("#");return(t>=0?n.slice(0,t):n)+"#"+e}function Go(e){fo?go(Bo(e)):window.location.hash=e}function No(e){fo?yo(Bo(e)):window.location.replace(Bo(e))}var $o=function(e){function n(n,t){e.call(this,n,t),this.stack=[],this.index=-1}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.push=function(e,n,t){var a=this;this.transitionTo(e,(function(e){a.stack=a.stack.slice(0,a.index+1).concat(e),a.index++,n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var a=this;this.transitionTo(e,(function(e){a.stack=a.stack.slice(0,a.index).concat(e),n&&n(e)}),t)},n.prototype.go=function(e){var n=this,t=this.index+e;if(!(t<0||t>=this.stack.length)){var a=this.stack[t];this.confirmTransition(a,(function(){var e=n.current;n.index=t,n.updateRoute(a),n.router.afterHooks.forEach((function(n){n&&n(a,e)}))}),(function(e){Co(e,vo.duplicated)&&(n.index=t)}))}},n.prototype.getCurrentLocation=function(){var e=this.stack[this.stack.length-1];return e?e.fullPath:"/"},n.prototype.ensureURL=function(){},n}(Do),qo=function(e){void 0===e&&(e={}),this.app=null,this.apps=[],this.options=e,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Qi(e.routes||[],this);var n=e.mode||"hash";switch(this.fallback="history"===n&&!fo&&!1!==e.fallback,this.fallback&&(n="hash"),Wi||(n="abstract"),this.mode=n,n){case"history":this.history=new zo(this,e.base);break;case"hash":this.history=new Eo(this,e.base,this.fallback);break;case"abstract":this.history=new $o(this,e.base);break;default:0}},Fo={currentRoute:{configurable:!0}};qo.prototype.match=function(e,n,t){return this.matcher.match(e,n,t)},Fo.currentRoute.get=function(){return this.history&&this.history.current},qo.prototype.init=function(e){var n=this;if(this.apps.push(e),e.$once("hook:destroyed",(function(){var t=n.apps.indexOf(e);t>-1&&n.apps.splice(t,1),n.app===e&&(n.app=n.apps[0]||null),n.app||n.history.teardown()})),!this.app){this.app=e;var t=this.history;if(t instanceof zo||t instanceof Eo){var a=function(e){t.setupListeners(),function(e){var a=t.current,r=n.options.scrollBehavior;fo&&r&&"fullPath"in e&&io(n,e,a,!1)}(e)};t.transitionTo(t.getCurrentLocation(),a,a)}t.listen((function(e){n.apps.forEach((function(n){n._route=e}))}))}},qo.prototype.beforeEach=function(e){return Ho(this.beforeHooks,e)},qo.prototype.beforeResolve=function(e){return Ho(this.resolveHooks,e)},qo.prototype.afterEach=function(e){return Ho(this.afterHooks,e)},qo.prototype.onReady=function(e,n){this.history.onReady(e,n)},qo.prototype.onError=function(e){this.history.onError(e)},qo.prototype.push=function(e,n,t){var a=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){a.history.push(e,n,t)}));this.history.push(e,n,t)},qo.prototype.replace=function(e,n,t){var a=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){a.history.replace(e,n,t)}));this.history.replace(e,n,t)},qo.prototype.go=function(e){this.history.go(e)},qo.prototype.back=function(){this.go(-1)},qo.prototype.forward=function(){this.go(1)},qo.prototype.getMatchedComponents=function(e){var n=e?e.matched?e:this.resolve(e).route:this.currentRoute;return n?[].concat.apply([],n.matched.map((function(e){return Object.keys(e.components).map((function(n){return e.components[n]}))}))):[]},qo.prototype.resolve=function(e,n,t){var a=$i(e,n=n||this.history.current,t,this),r=this.match(a,n),i=r.redirectedFrom||r.fullPath;return{location:a,route:r,href:function(e,n,t){var a="hash"===t?"#"+n:n;return e?xi(e+"/"+a):a}(this.history.base,i,this.mode),normalizedTo:a,resolved:r}},qo.prototype.getRoutes=function(){return this.matcher.getRoutes()},qo.prototype.addRoute=function(e,n){this.matcher.addRoute(e,n),this.history.current!==mi&&this.history.transitionTo(this.history.getCurrentLocation())},qo.prototype.addRoutes=function(e){this.matcher.addRoutes(e),this.history.current!==mi&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(qo.prototype,Fo);var Vo=qo;function Ho(e,n){return e.push(n),function(){var t=e.indexOf(n);t>-1&&e.splice(t,1)}}qo.install=function e(n){if(!e.installed||qi!==n){e.installed=!0,qi=n;var t=function(e){return void 0!==e},a=function(e,n){var a=e.$options._parentVnode;t(a)&&t(a=a.data)&&t(a=a.registerRouteInstance)&&a(e,n)};n.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),n.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,a(this,this)},destroyed:function(){a(this)}}),Object.defineProperty(n.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(n.prototype,"$route",{get:function(){return this._routerRoot._route}}),n.component("RouterView",wi),n.component("RouterLink",Vi);var r=n.config.optionMergeStrategies;r.beforeRouteEnter=r.beforeRouteLeave=r.beforeRouteUpdate=r.created}},qo.version="3.6.5",qo.isNavigationFailure=Co,qo.NavigationFailureType=vo,qo.START_LOCATION=mi,Wi&&window.Vue&&window.Vue.use(qo);t(130);t(152),t(29);var Wo={NotFound:()=>Promise.all([t.e(0),t.e(4)]).then(t.bind(null,414)),Layout:()=>Promise.all([t.e(0),t.e(2)]).then(t.bind(null,413))},Ko={"v-0fdda314":()=>t.e(5).then(t.bind(null,415)),"v-37c5bc96":()=>t.e(6).then(t.bind(null,416)),"v-07282236":()=>t.e(8).then(t.bind(null,417)),"v-4553c3a8":()=>t.e(7).then(t.bind(null,418)),"v-bd5cd4d4":()=>t.e(9).then(t.bind(null,419)),"v-07abb6d6":()=>t.e(10).then(t.bind(null,420)),"v-577baade":()=>t.e(11).then(t.bind(null,421)),"v-abb0f8a2":()=>t.e(12).then(t.bind(null,422)),"v-d7471e16":()=>t.e(13).then(t.bind(null,423)),"v-7f8db936":()=>t.e(16).then(t.bind(null,424)),"v-9842a21a":()=>t.e(15).then(t.bind(null,425)),"v-b2fd417c":()=>t.e(14).then(t.bind(null,426)),"v-171f66aa":()=>t.e(17).then(t.bind(null,427)),"v-788071c5":()=>t.e(18).then(t.bind(null,428)),"v-25995465":()=>t.e(19).then(t.bind(null,429)),"v-662ff4fd":()=>t.e(20).then(t.bind(null,430)),"v-0b217b45":()=>t.e(23).then(t.bind(null,431)),"v-2d0923f6":()=>t.e(24).then(t.bind(null,432)),"v-065eff05":()=>t.e(21).then(t.bind(null,433)),"v-f3b213e2":()=>t.e(22).then(t.bind(null,434)),"v-4d042c96":()=>t.e(25).then(t.bind(null,435)),"v-49e4dae0":()=>t.e(26).then(t.bind(null,436)),"v-89581d5c":()=>t.e(28).then(t.bind(null,437)),"v-c73e5956":()=>t.e(27).then(t.bind(null,438)),"v-6ffd4505":()=>t.e(29).then(t.bind(null,439)),"v-b25c491a":()=>t.e(30).then(t.bind(null,440)),"v-f96426b0":()=>t.e(31).then(t.bind(null,441)),"v-5849f946":()=>t.e(32).then(t.bind(null,442)),"v-2861ec8b":()=>t.e(33).then(t.bind(null,443)),"v-04292a45":()=>t.e(34).then(t.bind(null,444)),"v-d22a70f6":()=>t.e(35).then(t.bind(null,445)),"v-e33712b6":()=>t.e(37).then(t.bind(null,446)),"v-65bd31e5":()=>t.e(36).then(t.bind(null,447)),"v-a0731602":()=>t.e(38).then(t.bind(null,448)),"v-02aef12e":()=>t.e(39).then(t.bind(null,449)),"v-891a4db6":()=>t.e(42).then(t.bind(null,450)),"v-121d90a9":()=>t.e(40).then(t.bind(null,451)),"v-b267e276":()=>t.e(45).then(t.bind(null,452)),"v-3e3c1a05":()=>t.e(41).then(t.bind(null,453)),"v-c1b2e1c6":()=>t.e(43).then(t.bind(null,454)),"v-259f44f9":()=>t.e(46).then(t.bind(null,455)),"v-26a9c736":()=>t.e(44).then(t.bind(null,456)),"v-0ee7ecc5":()=>t.e(47).then(t.bind(null,457)),"v-e114a476":()=>t.e(48).then(t.bind(null,458)),"v-2e715932":()=>t.e(49).then(t.bind(null,459)),"v-bb1ed876":()=>t.e(50).then(t.bind(null,460)),"v-f8e02736":()=>t.e(51).then(t.bind(null,461)),"v-2f8a0a7e":()=>t.e(52).then(t.bind(null,462)),"v-01216c79":()=>t.e(53).then(t.bind(null,463))};function Yo(e){const n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}const Qo=/-(\w)/g,Zo=Yo(e=>e.replace(Qo,(e,n)=>n?n.toUpperCase():"")),Xo=/\B([A-Z])/g,Jo=Yo(e=>e.replace(Xo,"-$1").toLowerCase()),es=Yo(e=>e.charAt(0).toUpperCase()+e.slice(1));function ns(e,n){if(!n)return;if(e(n))return e(n);return n.includes("-")?e(es(Zo(n))):e(es(n))||e(Jo(n))}const ts=Object.assign({},Wo,Ko),as=e=>ts[e],rs=e=>Ko[e],is=e=>Wo[e],os=e=>Wt.component(e);function ss(e){return ns(rs,e)}function cs(e){return ns(is,e)}function ls(e){return ns(as,e)}function ds(e){return ns(os,e)}function us(...e){return Promise.all(e.filter(e=>e).map(async e=>{if(!ds(e)&&ls(e)){const n=await ls(e)();Wt.component(e,n.default)}}))}function hs(e,n){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[e]=n)}var ps=t(114),ms=t.n(ps),fs={created(){if(this.siteMeta=this.$site.headTags.filter(([e])=>"meta"===e).map(([e,n])=>n),this.$ssrContext){const n=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=n)?e.map(e=>{let n="<meta";return Object.keys(e).forEach(t=>{n+=` ${t}="${e[t]}"`}),n+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=ys(this.$canonicalUrl)}var e},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const e=this.getMergedMetaTags();this.currentMetaTags=vs(e,this.currentMetaTags)},getMergedMetaTags(){const e=this.$page.frontmatter.meta||[];return ms()([{name:"description",content:this.$description}],e,this.siteMeta,bs)},updateCanonicalLink(){gs(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",ys(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){vs(null,this.currentMetaTags),gs()}};function gs(){const e=document.querySelector("link[rel='canonical']");e&&e.remove()}function ys(e=""){return e?`<link href="${e}" rel="canonical" />`:""}function vs(e,n){if(n&&[...n].filter(e=>e.parentNode===document.head).forEach(e=>document.head.removeChild(e)),e)return e.map(e=>{const n=document.createElement("meta");return Object.keys(e).forEach(t=>{n.setAttribute(t,e[t])}),document.head.appendChild(n),n})}function bs(e){for(const n of["name","property","itemprop"])if(e.hasOwnProperty(n))return e[n]+n;return JSON.stringify(e)}var ws=t(61),ks={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(ws)()((function(){this.setActiveHash()}),300),setActiveHash(){const e=[].slice.call(document.querySelectorAll(".sidebar-link")),n=[].slice.call(document.querySelectorAll(".header-anchor")).filter(n=>e.some(e=>e.hash===n.hash)),t=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),a=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),r=window.innerHeight+t;for(let e=0;e<n.length;e++){const i=n[e],o=n[e+1],s=0===e&&0===t||t>=i.parentElement.offsetTop+10&&(!o||t<o.parentElement.offsetTop-10),c=decodeURIComponent(this.$route.hash);if(s&&c!==decodeURIComponent(i.hash)){const t=i;if(r===a)for(let t=e+1;t<n.length;t++)if(c===decodeURIComponent(n[t].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(t.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},_s=t(28),xs=t.n(_s),Cs={mounted(){xs.a.configure({showSpinner:!1}),this.$router.beforeEach((e,n,t)=>{e.path===n.path||Wt.component(e.name)||xs.a.start(),t()}),this.$router.afterEach(()=>{xs.a.done(),this.isSidebarOpen=!1})}};t(264),t(265);class As{constructor(){this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}show({text:e="",duration:n=3e3}){let t=document.createElement("div");t.className="message move-in",t.innerHTML=`\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">${e}</div>\n    `,this.containerEl.appendChild(t),n>0&&setTimeout(()=>{this.close(t)},n)}close(e){e.className=e.className.replace("move-in",""),e.className+="move-out",e.addEventListener("animationend",()=>{e.remove()})}}var Ms={mounted(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy(){setTimeout(()=>{(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach(e=>{document.querySelectorAll(e).forEach(this.generateCopyButton)})},1e3)},generateCopyButton(e){if(e.classList.contains("codecopy-enabled"))return;const n=document.createElement("i");n.className="code-copy",n.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',n.title="Copy to clipboard",n.addEventListener("click",()=>{this.copyToClipboard(e.innerText)}),e.appendChild(n),e.classList.add("codecopy-enabled")},copyToClipboard(e){const n=document.createElement("textarea");n.value=e,n.setAttribute("readonly",""),n.style.position="absolute",n.style.left="-9999px",document.body.appendChild(n);const t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);n.select(),document.execCommand("copy");(new As).show({text:"复制成功",duration:1e3}),document.body.removeChild(n),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}},Ss="auto",Ts="zoom-in",Ps="zoom-out",Is="grab",Ds="move";function Ls(e,n,t){var a=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],r={passive:!1};a?e.addEventListener(n,t,r):e.removeEventListener(n,t,r)}function Os(e,n){if(e){var t=new Image;t.onload=function(){n&&n(t)},t.src=e}}function zs(e){return e.dataset.original?e.dataset.original:"A"===e.parentNode.tagName?e.parentNode.getAttribute("href"):null}function Rs(e,n,t){!function(e){var n=Es,t=Us;if(e.transition){var a=e.transition;delete e.transition,e[n]=a}if(e.transform){var r=e.transform;delete e.transform,e[t]=r}}(n);var a=e.style,r={};for(var i in n)t&&(r[i]=a[i]||""),a[i]=n[i];return r}var Es="transition",Us="transform",js="transform",Bs="transitionend";var Gs=function(){},Ns={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Gs,onClose:Gs,onGrab:Gs,onMove:Gs,onRelease:Gs,onBeforeOpen:Gs,onBeforeClose:Gs,onBeforeGrab:Gs,onBeforeRelease:Gs,onImageLoading:Gs,onImageLoaded:Gs},$s={init:function(e){var n,t;n=this,t=e,Object.getOwnPropertyNames(Object.getPrototypeOf(n)).forEach((function(e){n[e]=n[e].bind(t)}))},click:function(e){if(e.preventDefault(),Fs(e))return window.open(this.target.srcOriginal||e.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(e.currentTarget)},scroll:function(){var e=document.documentElement||document.body.parentNode||document.body,n=window.pageXOffset||e.scrollLeft,t=window.pageYOffset||e.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:n,y:t});var a=this.lastScrollPosition.x-n,r=this.lastScrollPosition.y-t,i=this.options.scrollThreshold;(Math.abs(r)>=i||Math.abs(a)>=i)&&(this.lastScrollPosition=null,this.close())},keydown:function(e){(function(e){return"Escape"===(e.key||e.code)||27===e.keyCode})(e)&&(this.released?this.close():this.release(this.close))},mousedown:function(e){if(qs(e)&&!Fs(e)){e.preventDefault();var n=e.clientX,t=e.clientY;this.pressTimer=setTimeout(function(){this.grab(n,t)}.bind(this),200)}},mousemove:function(e){this.released||this.move(e.clientX,e.clientY)},mouseup:function(e){qs(e)&&!Fs(e)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(e){e.preventDefault();var n=e.touches[0],t=n.clientX,a=n.clientY;this.pressTimer=setTimeout(function(){this.grab(t,a)}.bind(this),200)},touchmove:function(e){if(!this.released){var n=e.touches[0],t=n.clientX,a=n.clientY;this.move(t,a)}},touchend:function(e){(function(e){e.targetTouches.length})(e)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function qs(e){return 0===e.button}function Fs(e){return e.metaKey||e.ctrlKey}var Vs={init:function(e){this.el=document.createElement("div"),this.instance=e,this.parent=document.body,Rs(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(e.options),Ls(this.el,"click",e.handler.clickOverlay.bind(e))},updateStyle:function(e){Rs(this.el,{zIndex:e.zIndex,backgroundColor:e.bgColor,transition:"opacity\n        "+e.transitionDuration+"s\n        "+e.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},Hs="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},Ws=function(){function e(e,n){for(var t=0;t<n.length;t++){var a=n[t];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),Object.defineProperty(e,a.key,a)}}return function(n,t,a){return t&&e(n.prototype,t),a&&e(n,a),n}}(),Ks=Object.assign||function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)Object.prototype.hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},Ys={init:function(e,n){this.el=e,this.instance=n,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=zs(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var e=this.instance.options,n=e.zIndex,t=e.enableGrab,a=e.transitionDuration,r=e.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:n+1,cursor:t?Is:Ps,transition:js+"\n        "+a+"s\n        "+r,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=Rs(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,Rs(this.el,{transform:"none"})},grab:function(e,n,t){var a=Qs(),r=a.x-e,i=a.y-n;Rs(this.el,{cursor:Ds,transform:"translate3d(\n        "+(this.translate.x+r)+"px, "+(this.translate.y+i)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(e,n,t){var a=Qs(),r=a.x-e,i=a.y-n;Rs(this.el,{transition:js,transform:"translate3d(\n        "+(this.translate.x+r)+"px, "+(this.translate.y+i)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){Rs(this.el,this.styleClose)},restoreOpenStyle:function(){Rs(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var e=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var n=this.el.cloneNode(!1);n.setAttribute("src",this.srcOriginal),n.style.position="fixed",n.style.visibility="hidden",e.appendChild(n),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),e.removeChild(n)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var e=Qs(),n=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:e.x-n,y:e.y-t}},calculateScale:function(){var e=this.el.dataset,n=e.zoomingHeight,t=e.zoomingWidth,a=this.instance.options,r=a.customSize,i=a.scaleBase;if(!r&&n&&t)return{x:t/this.rect.width,y:n/this.rect.height};if(r&&"object"===(void 0===r?"undefined":Hs(r)))return{x:r.width/this.rect.width,y:r.height/this.rect.height};var o=this.rect.width/2,s=this.rect.height/2,c=Qs(),l={x:c.x-o,y:c.y-s},d=l.x/o,u=l.y/s,h=i+Math.min(d,u);if(r&&"string"==typeof r){var p=t||this.el.naturalWidth,m=n||this.el.naturalHeight,f=parseFloat(r)*p/(100*this.rect.width),g=parseFloat(r)*m/(100*this.rect.height);if(h>f||h>g)return{x:f,y:g}}return{x:h,y:h}}};function Qs(){var e=document.documentElement;return{x:Math.min(e.clientWidth,window.innerWidth)/2,y:Math.min(e.clientHeight,window.innerHeight)/2}}function Zs(e,n,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(a){Ls(e,a,n[a],t)}))}var Xs=function(){function e(n){!function(e,n){if(!(e instanceof n))throw new TypeError("Cannot call a class as a function")}(this,e),this.target=Object.create(Ys),this.overlay=Object.create(Vs),this.handler=Object.create($s),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=Ks({},Ns,n),this.overlay.init(this),this.handler.init(this)}return Ws(e,[{key:"listen",value:function(e){if("string"==typeof e)for(var n=document.querySelectorAll(e),t=n.length;t--;)this.listen(n[t]);else"IMG"===e.tagName&&(e.style.cursor=Ts,Ls(e,"click",this.handler.click),this.options.preloadImage&&Os(zs(e)));return this}},{key:"config",value:function(e){return e?(Ks(this.options,e),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(e){var n=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var a="string"==typeof e?document.querySelector(e):e;if("IMG"===a.tagName){if(this.options.onBeforeOpen(a),this.target.init(a,this),!this.options.preloadImage){var r=this.target.srcOriginal;null!=r&&(this.options.onImageLoading(a),Os(r,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),Ls(document,"scroll",this.handler.scroll),Ls(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&Ls(window,"resize",this.handler.resizeWindow);var i=function e(){Ls(a,Bs,e,!1),n.lock=!1,n.target.upgradeSource(),n.options.enableGrab&&Zs(document,n.handler,!0),t(a)};return Ls(a,Bs,i),this}}}},{key:"close",value:function(){var e=this,n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=Ss,this.overlay.fadeOut(),this.target.zoomOut(),Ls(document,"scroll",this.handler.scroll,!1),Ls(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&Ls(window,"resize",this.handler.resizeWindow,!1);var a=function a(){Ls(t,Bs,a,!1),e.shown=!1,e.lock=!1,e.target.downgradeSource(),e.options.enableGrab&&Zs(document,e.handler,!1),e.target.restoreCloseStyle(),e.overlay.remove(),n(t)};return Ls(t,Bs,a),this}}},{key:"grab",value:function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var r=this.target.el;this.options.onBeforeGrab(r),this.released=!1,this.target.grab(e,n,t);var i=function e(){Ls(r,Bs,e,!1),a(r)};return Ls(r,Bs,i),this}}},{key:"move",value:function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Ds,this.target.move(e,n,t);var r=this.target.el,i=function e(){Ls(r,Bs,e,!1),a(r)};return Ls(r,Bs,i),this}}},{key:"release",value:function(){var e=this,n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=Ss,this.target.restoreOpenStyle();var a=function a(){Ls(t,Bs,a,!1),e.lock=!1,e.released=!0,n(t)};return Ls(t,Bs,a),this}}}]),e}();const Js=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),ec=Number("500");class nc{constructor(){this.instance=new Xs(Js)}update(e=".theme-vdoing-content img:not(.no-zoom)"){"undefined"!=typeof window&&this.instance.listen(e)}updateDelay(e=".theme-vdoing-content img:not(.no-zoom)",n=ec){setTimeout(()=>this.update(e),n)}}var tc=[fs,ks,Cs,Ms,{watch:{"$page.path"(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted(){this.$vuepress.zooming=new nc,this.$vuepress.zooming.updateDelay()}}],ac={name:"GlobalLayout",computed:{layout(){const e=this.getLayout();return hs("layout",e),Wt.component(e)}},methods:{getLayout(){if(this.$page.path){const e=this.$page.frontmatter.layout;return e&&(this.$vuepress.getLayoutAsyncComponent(e)||this.$vuepress.getVueComponent(e))?e:"Layout"}return"NotFound"}}},rc=t(5),ic=Object(rc.a)(ac,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(e,n,t){switch(n){case"components":e[n]||(e[n]={}),Object.assign(e[n],t);break;case"mixins":e[n]||(e[n]=[]),e[n].push(...t);break;default:throw new Error("Unknown option name.")}}(ic,"mixins",tc);const oc=[{name:"v-0fdda314",path:"/pages/f27694/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-0fdda314").then(t)}},{path:"/pages/f27694/index.html",redirect:"/pages/f27694/"},{path:"/00.目录页/00.Content.html",redirect:"/pages/f27694/"},{name:"v-37c5bc96",path:"/hbm/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-37c5bc96").then(t)}},{path:"/hbm/index.html",redirect:"/hbm/"},{path:"/00.目录页/01.hbm.html",redirect:"/hbm/"},{name:"v-07282236",path:"/gpu/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-07282236").then(t)}},{path:"/gpu/index.html",redirect:"/gpu/"},{path:"/00.目录页/03.gpu.html",redirect:"/gpu/"},{name:"v-4553c3a8",path:"/compiler/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-4553c3a8").then(t)}},{path:"/compiler/index.html",redirect:"/compiler/"},{path:"/00.目录页/02.compiler.html",redirect:"/compiler/"},{name:"v-bd5cd4d4",path:"/cpu/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-bd5cd4d4").then(t)}},{path:"/cpu/index.html",redirect:"/cpu/"},{path:"/00.目录页/04.cpu.html",redirect:"/cpu/"},{name:"v-07abb6d6",path:"/llm/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-07abb6d6").then(t)}},{path:"/llm/index.html",redirect:"/llm/"},{path:"/00.目录页/05.llm.html",redirect:"/llm/"},{name:"v-577baade",path:"/unix/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-577baade").then(t)}},{path:"/unix/index.html",redirect:"/unix/"},{path:"/00.目录页/06.unix.html",redirect:"/unix/"},{name:"v-abb0f8a2",path:"/pages/24769e/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-abb0f8a2").then(t)}},{path:"/pages/24769e/index.html",redirect:"/pages/24769e/"},{path:"/01.hbm/01.HBM_Paper_List.html",redirect:"/pages/24769e/"},{name:"v-d7471e16",path:"/pages/2476af/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-d7471e16").then(t)}},{path:"/pages/2476af/index.html",redirect:"/pages/2476af/"},{path:"/01.hbm/02.hbm_dead_block_predictor.html",redirect:"/pages/2476af/"},{name:"v-7f8db936",path:"/pages/2476bf/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-7f8db936").then(t)}},{path:"/pages/2476bf/index.html",redirect:"/pages/2476bf/"},{path:"/01.hbm/05.cache_mem_compression.html",redirect:"/pages/2476bf/"},{name:"v-9842a21a",path:"/pages/24760e/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-9842a21a").then(t)}},{path:"/pages/24760e/index.html",redirect:"/pages/24760e/"},{path:"/01.hbm/04.DRAM_PCM_NVM_Cache.html",redirect:"/pages/24760e/"},{name:"v-b2fd417c",path:"/pages/24769f/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-b2fd417c").then(t)}},{path:"/pages/24769f/index.html",redirect:"/pages/24769f/"},{path:"/01.hbm/03.Dynamically_Adapting _Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.html",redirect:"/pages/24769f/"},{name:"v-171f66aa",path:"/pages/f07695/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-171f66aa").then(t)}},{path:"/pages/f07695/index.html",redirect:"/pages/f07695/"},{path:"/01.hbm/06.memory ecc.html",redirect:"/pages/f07695/"},{name:"v-788071c5",path:"/pages/f07696/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-788071c5").then(t)}},{path:"/pages/f07696/index.html",redirect:"/pages/f07696/"},{path:"/01.hbm/07.hbm-latency.html",redirect:"/pages/f07696/"},{name:"v-25995465",path:"/pages/f07698/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-25995465").then(t)}},{path:"/pages/f07698/index.html",redirect:"/pages/f07698/"},{path:"/01.hbm/08.compression.html",redirect:"/pages/f07698/"},{name:"v-662ff4fd",path:"/pages/f07699/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-662ff4fd").then(t)}},{path:"/pages/f07699/index.html",redirect:"/pages/f07699/"},{path:"/01.hbm/09.compressibility_prediction.html",redirect:"/pages/f07699/"},{name:"v-0b217b45",path:"/pages/000002/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-0b217b45").then(t)}},{path:"/pages/000002/index.html",redirect:"/pages/000002/"},{path:"/02.compiler/02.GetStartedLLVMChap5Notes.html",redirect:"/pages/000002/"},{name:"v-2d0923f6",path:"/pages/000003/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-2d0923f6").then(t)}},{path:"/pages/000003/index.html",redirect:"/pages/000003/"},{path:"/02.compiler/03.GetStartedLLVMChap6Notes.html",redirect:"/pages/000003/"},{name:"v-065eff05",path:"/pages/f07692/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-065eff05").then(t)}},{path:"/pages/f07692/index.html",redirect:"/pages/f07692/"},{path:"/01.hbm/10.software_memory_paper.html",redirect:"/pages/f07692/"},{name:"v-f3b213e2",path:"/pages/000001/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-f3b213e2").then(t)}},{path:"/pages/000001/index.html",redirect:"/pages/000001/"},{path:"/02.compiler/01.llvm_frontend.html",redirect:"/pages/000001/"},{name:"v-4d042c96",path:"/pages/000004/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-4d042c96").then(t)}},{path:"/pages/000004/index.html",redirect:"/pages/000004/"},{path:"/02.compiler/04. LearningLLVMDiary0.html",redirect:"/pages/000004/"},{name:"v-49e4dae0",path:"/pages/000005/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-49e4dae0").then(t)}},{path:"/pages/000005/index.html",redirect:"/pages/000005/"},{path:"/02.compiler/05. addInstACE.html",redirect:"/pages/000005/"},{name:"v-89581d5c",path:"/pages/000007/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-89581d5c").then(t)}},{path:"/pages/000007/index.html",redirect:"/pages/000007/"},{path:"/02.compiler/07. UnderstaningLLVMwithSourceCode.html",redirect:"/pages/000007/"},{name:"v-c73e5956",path:"/pages/000006/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-c73e5956").then(t)}},{path:"/pages/000006/index.html",redirect:"/pages/000006/"},{path:"/02.compiler/06.Value&Use.html",redirect:"/pages/000006/"},{name:"v-6ffd4505",path:"/pages/cc7034/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-6ffd4505").then(t)}},{path:"/pages/cc7034/index.html",redirect:"/pages/cc7034/"},{path:"/03.gpu/01.operand_collector.html",redirect:"/pages/cc7034/"},{name:"v-b25c491a",path:"/pages/2476ae/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-b25c491a").then(t)}},{path:"/pages/2476ae/index.html",redirect:"/pages/2476ae/"},{path:"/03.gpu/02.warp_execution.html",redirect:"/pages/2476ae/"},{name:"v-f96426b0",path:"/pages/14769f/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-f96426b0").then(t)}},{path:"/pages/14769f/index.html",redirect:"/pages/14769f/"},{path:"/03.gpu/03.Precise Exception.html",redirect:"/pages/14769f/"},{name:"v-5849f946",path:"/pages/44771e/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-5849f946").then(t)}},{path:"/pages/44771e/index.html",redirect:"/pages/44771e/"},{path:"/03.gpu/04.Unified_Memory.html",redirect:"/pages/44771e/"},{name:"v-2861ec8b",path:"/pages/44871e/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-2861ec8b").then(t)}},{path:"/pages/44871e/index.html",redirect:"/pages/44871e/"},{path:"/03.gpu/05.TensorCore.html",redirect:"/pages/44871e/"},{name:"v-04292a45",path:"/pages/45871e/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-04292a45").then(t)}},{path:"/pages/45871e/index.html",redirect:"/pages/45871e/"},{path:"/03.gpu/06.MemoryBehaviour.html",redirect:"/pages/45871e/"},{name:"v-d22a70f6",path:"/pages/45871f/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-d22a70f6").then(t)}},{path:"/pages/45871f/index.html",redirect:"/pages/45871f/"},{path:"/03.gpu/07.GPUVirtualization.html",redirect:"/pages/45871f/"},{name:"v-e33712b6",path:"/pages/458721/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-e33712b6").then(t)}},{path:"/pages/458721/index.html",redirect:"/pages/458721/"},{path:"/03.gpu/09.Simulator.html",redirect:"/pages/458721/"},{name:"v-65bd31e5",path:"/pages/458720/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-65bd31e5").then(t)}},{path:"/pages/458720/index.html",redirect:"/pages/458720/"},{path:"/03.gpu/08.LLM.html",redirect:"/pages/458720/"},{name:"v-a0731602",path:"/pages/458722/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-a0731602").then(t)}},{path:"/pages/458722/index.html",redirect:"/pages/458722/"},{path:"/03.gpu/10. Architectural Survey.html",redirect:"/pages/458722/"},{name:"v-02aef12e",path:"/pages/47871e/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-02aef12e").then(t)}},{path:"/pages/47871e/index.html",redirect:"/pages/47871e/"},{path:"/03.gpu/1234.TODO.html",redirect:"/pages/47871e/"},{name:"v-891a4db6",path:"/pages/cc7037/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-891a4db6").then(t)}},{path:"/pages/cc7037/index.html",redirect:"/pages/cc7037/"},{path:"/04.cpu/03.loadstore.html",redirect:"/pages/cc7037/"},{name:"v-121d90a9",path:"/pages/cc7035/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-121d90a9").then(t)}},{path:"/pages/cc7035/index.html",redirect:"/pages/cc7035/"},{path:"/04.cpu/01.checkpoint.html",redirect:"/pages/cc7035/"},{name:"v-b267e276",path:"/pages/dc7036/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-b267e276").then(t)}},{path:"/pages/dc7036/index.html",redirect:"/pages/dc7036/"},{path:"/05.llm/02.LLM_HW_Opt.html",redirect:"/pages/dc7036/"},{name:"v-3e3c1a05",path:"/pages/cc7036/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-3e3c1a05").then(t)}},{path:"/pages/cc7036/index.html",redirect:"/pages/cc7036/"},{path:"/04.cpu/02.topdown.html",redirect:"/pages/cc7036/"},{name:"v-c1b2e1c6",path:"/pages/f07697/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-c1b2e1c6").then(t)}},{path:"/pages/f07697/index.html",redirect:"/pages/f07697/"},{path:"/04.cpu/1234.markdown.html",redirect:"/pages/f07697/"},{name:"v-259f44f9",path:"/pages/dc7037/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-259f44f9").then(t)}},{path:"/pages/dc7037/index.html",redirect:"/pages/dc7037/"},{path:"/05.llm/03.gem5_LLAMA.html",redirect:"/pages/dc7037/"},{name:"v-26a9c736",path:"/pages/dc7035/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-26a9c736").then(t)}},{path:"/pages/dc7035/index.html",redirect:"/pages/dc7035/"},{path:"/05.llm/01.How_LLM_Works.html",redirect:"/pages/dc7035/"},{name:"v-0ee7ecc5",path:"/pages/dc7038/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-0ee7ecc5").then(t)}},{path:"/pages/dc7038/index.html",redirect:"/pages/dc7038/"},{path:"/05.llm/04.mem_usage_llm.html",redirect:"/pages/dc7038/"},{name:"v-e114a476",path:"/pages/ec7035/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-e114a476").then(t)}},{path:"/pages/ec7035/index.html",redirect:"/pages/ec7035/"},{path:"/06.unix/01.malloc.html",redirect:"/pages/ec7035/"},{name:"v-2e715932",path:"/message-board/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-2e715932").then(t)}},{path:"/message-board/index.html",redirect:"/message-board/"},{path:"/09.nine/01.留言板.html",redirect:"/message-board/"},{name:"v-bb1ed876",path:"/pages/a1ccb2/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-bb1ed876").then(t)}},{path:"/pages/a1ccb2/index.html",redirect:"/pages/a1ccb2/"},{path:"/09.nine/02.template.html",redirect:"/pages/a1ccb2/"},{name:"v-f8e02736",path:"/archives/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-f8e02736").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{name:"v-2f8a0a7e",path:"/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-2f8a0a7e").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-01216c79",path:"/pages/3deeec/",component:ic,beforeEnter:(e,n,t)=>{us("Layout","v-01216c79").then(t)}},{path:"/pages/3deeec/index.html",redirect:"/pages/3deeec/"},{path:"/pictures/addPictures.html",redirect:"/pages/3deeec/"},{path:"*",component:ic}],sc={title:"Qi Shao",description:"Computer System",base:"/qishao-notes/",headTags:[["link",{rel:"stylesheet",href:"custom.css"}],["meta",{name:"google-site-verification",content:"66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzYI"}],["meta",{name:"keywords",content:"Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin"}],["meta",{name:"theme-color",content:"#11a8cd"}],["meta",{name:"referrer",content:"no-referrer-when-downgrade"}],["script",{language:"javascript",type:"text/javascript",src:"/qishao-notes/js/pgmanor-self.js"}]],pages:[{title:"Content",frontmatter:{title:"Content",date:"2022-07-18T17:23:23.000Z",permalink:"/pages/f27694/"},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/00.Content.html",relativePath:"00.目录页/00.Content.md",key:"v-0fdda314",path:"/pages/f27694/",headersStr:null,content:" 1. GPU\n 2. CPU\n 3. HBM\n 4. Compiler\n 5. LLM\n 6. unix",normalizedContent:" 1. gpu\n 2. cpu\n 3. hbm\n 4. compiler\n 5. llm\n 6. unix",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"HBM",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"01.hbm"}},title:"HBM",date:"2022-07-20T11:05:42.000Z",permalink:"/hbm/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/01.hbm.html",relativePath:"00.目录页/01.hbm.md",key:"v-37c5bc96",path:"/hbm/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"gpu",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"03.gpu"}},title:"gpu",date:"2022-07-20T11:05:54.000Z",permalink:"/gpu/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/03.gpu.html",relativePath:"00.目录页/03.gpu.md",key:"v-07282236",path:"/gpu/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"llvm & mlir",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"02.compiler"}},title:"llvm & mlir",date:"2023-11-21T11:05:54.000Z",permalink:"/compiler/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/02.compiler.html",relativePath:"00.目录页/02.compiler.md",key:"v-4553c3a8",path:"/compiler/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"cpu",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"04.cpu"}},title:"cpu",date:"2023-11-09T15:54:15.000Z",permalink:"/cpu/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/04.cpu.html",relativePath:"00.目录页/04.cpu.md",key:"v-bd5cd4d4",path:"/cpu/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"llm",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"05.llm"}},title:"llm",date:"2024-01-02T15:54:15.000Z",permalink:"/llm/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/05.llm.html",relativePath:"00.目录页/05.llm.md",key:"v-07abb6d6",path:"/llm/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"unix",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"06.unix"}},title:"unix",date:"2024-03-03T15:54:15.000Z",permalink:"/unix/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/06.unix.html",relativePath:"00.目录页/06.unix.md",key:"v-577baade",path:"/unix/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"HBM Paper List",frontmatter:{title:"HBM Paper List",date:"2023-05-08T00:00:00.000Z",permalink:"/pages/24769e/"},regularPath:"/01.hbm/01.HBM_Paper_List.html",relativePath:"01.hbm/01.HBM_Paper_List.md",key:"v-abb0f8a2",path:"/pages/24769e/",headers:[{level:3,title:"2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache",slug:"_2-cameo-a-two-level-memory-organization-with-capacity-of-main-memory-and-flexibility-of-hardware-managed-cache",normalizedTitle:"2. cameo:a two-level memory organization with capacity of main memory and flexibility of hardware-managed cache",charIndex:1920},{level:3,title:"5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories",slug:"_5-mempod-a-clustered-architecture-for-efficient-and-scalable-migration-in-flat-address-space-multi-level-memories",normalizedTitle:"5. mempod: a clustered architecture for efficient and scalable migration in flat address space multi-level memories",charIndex:2752},{level:3,title:"6. Transparent Hardware Management of Stacked DRAM as Part of Memory",slug:"_6-transparent-hardware-management-of-stacked-dram-as-part-of-memory",normalizedTitle:"6. transparent hardware management of stacked dram as part of memory",charIndex:4917},{level:3,title:"8.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM",slug:"_8-batman-techniques-for-maximizing-system-bandwidth-of-memory-systems-with-stacked-dram",normalizedTitle:"8.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram",charIndex:7541},{level:3,title:"9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories",slug:"_9-heterogeneous-memory-architectures-a-hw-sw-approach-for-mixing-die-stacked-and-off-package-memories",normalizedTitle:"9. heterogeneous memory architectures: a hw/sw approach for mixing die-stacked and off-package memories",charIndex:8728},{level:3,title:"10.Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems",slug:"_10-challenges-in-heterogeneous-die-stacked-and-off-chip-memory-systems",normalizedTitle:"10.challenges in heterogeneous die-stacked and off-chip memory systems",charIndex:10868},{level:3,title:"11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation",slug:"_11-banshee-bandwidth-efficient-dram-caching-via-software-hardware-cooperation",normalizedTitle:"11. banshee: bandwidth-efficient dram caching via software/hardware cooperation",charIndex:874},{level:3,title:"13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache",slug:"_13-unison-cache-a-scalable-and-effective-die-stacked-dram-cache",normalizedTitle:"13. unison cache: a scalable and effective die-stacked dram cache",charIndex:1006},{level:3,title:"14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors",slug:"_14-dynamically-adapting-page-migration-policies-based-on-applications-memory-access-behaviors",normalizedTitle:"14. dynamically adapting page migration policies based on applications memory access behaviors",charIndex:1073},{level:3,title:"15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems",slug:"_15-on-the-fly-page-migration-and-address-reconciliation-for-heterogeneous-memory-systems",normalizedTitle:"15. on-the-fly page migration and address reconciliation for heterogeneous memory systems",charIndex:1169}],headersStr:"2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache 5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories 6. Transparent Hardware Management of Stacked DRAM as Part of Memory 8.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM 9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories 10.Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems 11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems",content:" 1.  Baryon: Efficient Hybrid Memory Management with Compression and Sub-Blocking\n 2.  CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache\n 3.  Hybrid2: Combining Caching and Migration in Hybrid Memory Systems\n 4.  SILC-FM: Subblocked InterLeaved Cache-Like Flat Memory Organization\n 5.  MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories\n 6.  Transparent Hardware Management of Stacked DRAM as Part of Memory\n 7.  CHAMELEON: A Dynamically Reconfigurable Heterogeneous Memory System\n 8.  BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n 9.  Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories\n 10. Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems\n 11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation\n 12. Die-Stacked DRAM: Memory, Cache, or MemCache?\n 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache\n 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors\n 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems\n 16. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems [DAC]\n 17. TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems International Conference on Computer Design (ICCD)\n\nDRAM NVM\n\n 1. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture\n 2. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures\n 3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method\n 4. Page Placement in Hybrid Memory Systems\n\n----------------------------------------\n\n\n# 2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache\n\n# MemPod:\n\nCAMEO [13] proposes a cache-like flat address space memory management scheme in an attempt to close the gap between cache and flat memory organizations. CAMEO operates similarly to THM, however it does so at the granularity of cache lines (64B). Migrations are restricted within segments with one fast line location per segment. Its bookkeeping structures are entirely stored in memory, while a “Line Location Predictor” attempts to save some bookkeeping-related accesses by predicting the location of a line.\n\nCAMEO initiates a line migration upon every access to slow memory.\n\nCAMEO can incur high migration traffic as every access could induce a migration.\n\n\n\n----------------------------------------\n\n\n# 5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories\n\nYear: 2017\n\nMemPod uses MEA counters to track page access activity and identify hot pages. They are dramatically smaller than prior tracking mechanisms while capturing activity counts and temporal recency in a way that provides more effective prediction of future page access.\n\nWhat makes MEA most useful, though, is its failure mode – when it fails to find the most-accessed pages, it does so by favoring recency over quantity. That is, a page accessed several times near the end of an interval can easily knock out a page accessed many more times early in the interval. As a result, it combines both access counting and temporal locality, at a fraction of the cost of access counting alone.\n\n\n\nThree triggers are most commonly used whenever state must be updated based on tracking information (MC scheduling, migrations, dynamic voltage and frequency scaling etc.). Interval-based (or epoch-based) triggers occur with a set frequency, while threshold-based solutions trigger whenever a predetermined criterion is met. Finally, event-based triggers react to predefined events. Both interval-based and threshold-based approaches face the same challenge of identifying the optimal interval or threshold value.\n\nMemPod achieves the best performance (lower AMMAT) with 50us intervals and 64 counters per Pod. MemPod’s lightweight operation allows for such small intervals. For comparison purposes, HMA [14] identi-fied the best epoch length to be 100ms (2000x larger) in order to support all the lengthy processes that take place during a migration event for that method.\n\nBased on these results, we use 64 MEA 2-bit counters over 50us intervals for subsequent results in this paper. Each one of the 64 MEA entries needs 21 bits for addressing the 1.1M pages per Pod and 2 bits for its counter, leading to an area cost of only 184B per Pod and 736B total. Compared to the state of the art, MemPod’s activity tracking requirement is ∼712x smaller than THM’s (512KB) and ∼12800x smaller than HMA’s (9MB).\n\n\n\n----------------------------------------\n\n\n# 6. Transparent Hardware Management of Stacked DRAM as Part of Memory\n\n# MemPod:\n\nSim, et al. proposed a technique for transparent hardware management of a hybrid memory system [17],which we will refer to as “THM”. THM does not require OS intervention while managing migrations. In order to keep bookkeeping costs manageable, THM allows migrations only within sets of pages (called segments). Each segment includes one fast memory page and a set of slow memory pages. The slow pages of each segment can only migrate to the one fast page location, and any such migration results in the eviction of the currently-residing page. THM monitors memory accesses with one “competing counter” per segment resulting in a low cost profiling solution. Finally, THM supports caching part of its structures on chip while the rest is stored in memory.\n\nTHM’s competing counters can lead to false positives, allowing a cold page to migrate to fast memory.\n\nTHM offers significantly limited flexibility by restricting migrations withing segments, however this decision reduces bookkeeping costs significantly. Competing counters in each segment are used for activity tracking, occasionally leading to false (threshold-based) migration triggering if a cold page gets accessed at the right time. Identifying migration candidates incurs very little overhead since there is exactly one fast memory location for each slow memory page that triggers migration.\n\nCite from paper\n\n\n\nSimilar to set dueling, they adopted sample region. The locations in fast memory are grouped into 32 distinct regions in an interleaving fashion, and four regions are dedicated to sampling, while other 28 regions follow the threshold decision from sampling.\n\n• Nstatic: # of memory requests serviced from fast memory with static mapping • Ndynamic: # of memory requests expected to be serviced from fast memory when swapping with a given threshold • Nswap: # of expected swaps for a given threshold.\n\n\n\nK differs depending on the relative latency of fast and slow memory. The cost of a single fast swap is about 1200 cycles, and the difference in access latency between fast and slow memory is 72 cycles.Thus, in general, the swapped-in segment needs to get at least 17 more (future) hits than the swapped-out segment for swapping to be valuable. K is computed in hardware at boot time.\n\nMe Competing counter needs a threshold to invoke swap.\n\nSample region will have different threshold. Based on different threshold, Nswap is different. Thus they choose the the best threshold with max Bexpected as candidate threshold.\n\n----------------------------------------\n\n\n# 8.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n\nInsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nAs the NM simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. We observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nWe leverage our key insight on controlling data movement and propose Bandwidth-Aware Tiered-Memory Management (BATMAN), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the NM and the FM. We define the desired access rate of the NM as the target access rate (TAR). TAR is the fraction of memory accesses serviced by the NM when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2X 2/3 4X 4/5 8X 8/9\n\nMe Bandwidth-Aware Tired-Memory Management tries to distritube memory according to HBM and DRAM bandwidth ratio. And also treat it as a threshold to refuse page migration.\n\n----------------------------------------\n\n\n# 9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories\n\n# MemPod:\n\nHMA [14] is a HW/SW mechanism that attempts to predict frequently accessed pages in memory and, at predefined intervals, migrate those pages to fast memory. HW support is required for profiling memory accesses using counters for each memory page, while the migration is handled by the OS. Due to the costly OS involvement, HMA’s intervals are kept large. Additionally, the hardware cost of its profiling counters is high. However, HMA is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages as the OS can update page tables and TLBs to reflect migrations.\n\nHMA does not require a remap table due to the OS updating the existing system’s structures. For activity tracking it uses Full Counters. The costly OS involvement and the high penalty for sorting all its counters force HMA to operate at very large intervals, weakening its adaptability to phase changes. However, HMA offers full flexibility for migrations.\n\nInterrupt and TLB shoot down assumption A fixed 5us time penalty is charged for each page fault [27] to cover the basic interrupt costs, and then another 3uspenalty is applied whenever a TLB shootdown [33] is required.\n\nCite from org paper This first-touch hot-page (FTHP) policy is effectively a generalization of both the history-based and first-touch algorithms.\n\nWe propose a dynamic feedback-directed HMA policy that can dynamically adjust the hotness threshold θ to achieve a best-of-both-worlds approach between history-based and first-touch policies.\n\n * At the start of each epoch, the size of the hot set is compared to the size of the die-stacked DRAM (N).\n * If the hot set is too small to fill the fast memory, then θ is lowered which causes more pages to be classified as hot.\n * Likewise, if the hot set is too large, θ is increased which causes fewer pages to be put in the hot set.\n * If the feedback mechanism works well, then the size of the hot set should converge to N.\n\n----------------------------------------\n\n\n# 10.Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems\n\nYear: 2012 Software OS management To prevent the mapping of pages with insufficient miss traffic, we employ a threshold θ such that any page with fewer than θ LLC misses is not considered for mapping into stacked DRAM. The application of a threshold may result in cases when the list of most frequently missed pages has only k < P items. In this case, we simply keep a random set of P − k of the existing pages from the previous epoch already in stacked DRAM to avoid consuming bandwidth to swap out the page back to the off-chip memory.\n\n----------------------------------------\n\n\n# 11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation\n\nYear: 2017 Software aovid tag look up by storing DRAM Cache presence information in the page table and tlbs.\n\nSpecifically, Banshee uses a hardwaremanaged frequency-based replacement (FBR) policy that only caches hot pages to reduce unnecessary data replacement traffic. To reduce the cost of accessing/updating frequency counters (which are stored in in-package DRAM), Banshee uses a new sampling approach to only read/write counters for a fraction of memory accesses.\n\nBackground\n\n * Using tags Alloy Cache | Unison Cache\n * Using address remapping Heterogeneous Memory Architecture (HMA) | Tagless DRAM Cache (TDC)\n\n\n\nThey reuse reverse maping mechanism.\n\nBanshee tracks each page’s access frequency with a counter, stored in the metadata. We store counters not only for the pages in the DRAM cache, but also for some pages not in cache, which are candidates to bring into the cache.\n\nInstead, an access in Banshee only updates a page’s frequency counter with a certain sample rate. For a sample rate of 10%, for example, the frequency counters are accessed/updated only once for every 10 DRAM accesses.\n\nFrequency-based replacement may lead to thrashing problem.\n\nBanshee solves this problem by only replacing a page when the candidate’s counter is greater than the victim’s counter by a certain threshold. This ensures that a page just evicted from the DRAM cache must be accessed for at least 2·threshold/sampling rate times before it can enter the cache again, thus preventing a page from entering and leaving frequently.\n\nBy default, the threshold is the product of the number of cachelines in a page and the sampling coefficient divided by two (threshold = page_size x sampling_coeff / 2). Intuitively, this means replacement can happen only if the benefit of swapping the pages outweighs the cost of the replacement operation.\n\nIf a counter saturates after being incremented, all counters in the metadata will be reduced by half using a shift operation in hardware.\n\n\n\n----------------------------------------\n\n\n# 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache\n\nThe state-of-the-art block-based design, called Alloy Cache, colocates a tag with each data block (e.g., 64B) in the stacked DRAM to provide fast access to data in a single DRAM access. However, such a design suffers from low hit rates due to poor temporal locality in the DRAM cache. In contrast, the state-of-the-art page-based design, called Footprint Cache, organizes the DRAM cache at page granularity (e.g., 4KB), but fetches only the blocks that will likely be touched within a page. In doing so, the Footprint Cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. However, multi-gigabyte stacked DRAM caches will soon be practical and needed by server applications, thereby mandating tens of MBs of tag storage even for page-based DRAM caches.\n\nWe introduce a novel stacked-DRAM cache design, Unison Cache. Similar to Alloy Cache’s approach, Unison Cache incorporates the tag metadata directly into the stacked DRAM to enable scalability to arbitrary stacked-DRAM capacities. Then, leveraging the insights from the Footprint Cache design, Unison Cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. Our evaluation using server workloads and caches of up to 8GB reveals that Unison cache improves performance by 14% compared to Alloy Cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical SRAM-based tags of around 50MB.\n\nMe Block-based high miss ratio, page-based high migration penalty when fetching useless data. If footprint meta data is adopted to collect data trace, FootCache meta table cannot scale with increasing capactiy of HBM.\n\n\n\n----------------------------------------\n\n\n# 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors\n\nLink to notes for Dynamically Adapting...\n\n\n# 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems\n\n“on-the-fly” migration performs better than epoch-based page migration techniques, since we migrate recent hot pages.\n\nInstead of relying completely on OS to perform AR for the evicted entries, as done in [Ramoset al. 2011], we propose a hardware-based AR, where the MigC hardware initiates TLB shootdown and cache flushing without explicitly stopping the user program.\n\nLike previous studies [Meswani et al. 2015; Prodromou et al. 2017; Su et al. 2015], we observe that not all applications benefit from page migration, since page migration incurs performance overheads due to extra data movement.\n\nThe model works with the principle that, to get performance benefit, one should migrate the smallest set of pages from slow to fast memory that yields in the largest increase in memory accesses to fast memory to amortize the migration overhead.\n\nMe Try to use 80% principle. In our study, we look at the 80-percentile accesses and identify the “set of top-accessed pages” that contribute to more than 80% of all memory accesses.\n\nIn our proposal, we migrate a page immediately when it receives sufficient number of memory accesses, unlike any epoch-based schemes. We allow full flexibility in page relocation like HMAHS [Meswani et al. 2015] and keep a remap table for address redirection. We keep this table small by periodically evicting entries and it is placed on-chip.\n\nThe particular access count, which can separate such top-accessed pages from other pages, is referred to as “filter count.” Note that, filter count indicates an upper bound for hotness threshold\n\nMe This paper explain the address reconciliation AR in detail.\n\nFlow of AR -First, all cache lines from these pages, which are currently residing in the cache hierarchies and tagged with OS-visible PA, must be invalidated (and dirty lines written back), since the current OS-visible PA will be replaced with the new PA. All future accesses to these pages will only have access to the new PA. -Next,corresponding page table entries (PTEs) for A and B need to be updated with new PAs. -The TLB entries in all cores using the old PA must also be invalidated (as well as any other OS structures that contain the physical page addresses).\n\n(i) flush_cache_page() (ii) change PTE, (iii) flush_tlb_page()\n\nWe use a Migration Benefit Quotient (MBQ), by calculating the difference between the total number of accesses to any page and the filter count used in classifying applications’ memory locality, as described. MBQ indicates how many of the future accesses of a page may go to fast memory if the page were migrated from slow to fast memory using the filter count as a hotness threshold.\n\nSacturation account the sum of access counts of all pages with 32,909 accesses or less (the bars corresponding to x-axis 0 to 3,290) accounts for 98% of all accesses. For our purposes, we use this access count as a saturation count (or assume that the maximum number of accesses any page can receive).\n\nFor each workload, we use the difference between the saturation count and the filter count to determine MBQ.\n\n\n\n 1. Low MBQ, difference is less than 1K. For example, in Figure 5(b) xalanc, pages with memory access count 609 or less (the bars corresponding to x-axis 0 to 60) provide 98% of the memory accesses. Hence, the difference between saturation count (609) and filter count (70) is 539. These workloads may not achieve significant increase in accesses to fast memory after page migration.\n 2. Medium MBQ, difference is in between 1K to K (e.g., Figure 5(c) omnetpp). These workloads may receive moderate benefits, depending the migration overheads.\n 3. High MBQ, difference is more than 8K (e.g., Figure 5(a) mcf). These workloads are likely to receive higher hits in faster memory as a result of page migration.\n\nMe In short\n\n * sactuartion account. The sum of access counts fo all pages with sactuartion account accounts for 98% of all accesses.\n * filter account. The “set of top-accessed pages” that contribute to more than 80% of all memory accesses. If they are close, it means that this is a uniform benchmark. If they have a large difference, this means that there is a subset of pages that has far more memory accesses.\n\n----------------------------------------\n\n# 17. TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems International Conference on Computer Design (ICCD)\n\nYear: 2019",normalizedContent:" 1.  baryon: efficient hybrid memory management with compression and sub-blocking\n 2.  cameo:a two-level memory organization with capacity of main memory and flexibility of hardware-managed cache\n 3.  hybrid2: combining caching and migration in hybrid memory systems\n 4.  silc-fm: subblocked interleaved cache-like flat memory organization\n 5.  mempod: a clustered architecture for efficient and scalable migration in flat address space multi-level memories\n 6.  transparent hardware management of stacked dram as part of memory\n 7.  chameleon: a dynamically reconfigurable heterogeneous memory system\n 8.  batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n 9.  heterogeneous memory architectures: a hw/sw approach for mixing die-stacked and off-package memories\n 10. challenges in heterogeneous die-stacked and off-chip memory systems\n 11. banshee: bandwidth-efficient dram caching via software/hardware cooperation\n 12. die-stacked dram: memory, cache, or memcache?\n 13. unison cache: a scalable and effective die-stacked dram cache\n 14. dynamically adapting page migration policies based on applications memory access behaviors\n 15. on-the-fly page migration and address reconciliation for heterogeneous memory systems\n 16. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems [dac]\n 17. tictoc: enabling bandwidth-efficient dram caching for both hits and misses in hybrid memory systems international conference on computer design (iccd)\n\ndram nvm\n\n 1. an operating system level data migration scheme in hybrid dram-nvm memory architecture\n 2. clock-dwf: a write-history-aware page replacement algorithm for hybrid pcm and dram memory architectures\n 3. apmigration: improving performance of hybrid memory performance via an adaptive page migration method\n 4. page placement in hybrid memory systems\n\n----------------------------------------\n\n\n# 2. cameo:a two-level memory organization with capacity of main memory and flexibility of hardware-managed cache\n\n# mempod:\n\ncameo [13] proposes a cache-like flat address space memory management scheme in an attempt to close the gap between cache and flat memory organizations. cameo operates similarly to thm, however it does so at the granularity of cache lines (64b). migrations are restricted within segments with one fast line location per segment. its bookkeeping structures are entirely stored in memory, while a “line location predictor” attempts to save some bookkeeping-related accesses by predicting the location of a line.\n\ncameo initiates a line migration upon every access to slow memory.\n\ncameo can incur high migration traffic as every access could induce a migration.\n\n\n\n----------------------------------------\n\n\n# 5. mempod: a clustered architecture for efficient and scalable migration in flat address space multi-level memories\n\nyear: 2017\n\nmempod uses mea counters to track page access activity and identify hot pages. they are dramatically smaller than prior tracking mechanisms while capturing activity counts and temporal recency in a way that provides more effective prediction of future page access.\n\nwhat makes mea most useful, though, is its failure mode – when it fails to find the most-accessed pages, it does so by favoring recency over quantity. that is, a page accessed several times near the end of an interval can easily knock out a page accessed many more times early in the interval. as a result, it combines both access counting and temporal locality, at a fraction of the cost of access counting alone.\n\n\n\nthree triggers are most commonly used whenever state must be updated based on tracking information (mc scheduling, migrations, dynamic voltage and frequency scaling etc.). interval-based (or epoch-based) triggers occur with a set frequency, while threshold-based solutions trigger whenever a predetermined criterion is met. finally, event-based triggers react to predefined events. both interval-based and threshold-based approaches face the same challenge of identifying the optimal interval or threshold value.\n\nmempod achieves the best performance (lower ammat) with 50us intervals and 64 counters per pod. mempod’s lightweight operation allows for such small intervals. for comparison purposes, hma [14] identi-fied the best epoch length to be 100ms (2000x larger) in order to support all the lengthy processes that take place during a migration event for that method.\n\nbased on these results, we use 64 mea 2-bit counters over 50us intervals for subsequent results in this paper. each one of the 64 mea entries needs 21 bits for addressing the 1.1m pages per pod and 2 bits for its counter, leading to an area cost of only 184b per pod and 736b total. compared to the state of the art, mempod’s activity tracking requirement is ∼712x smaller than thm’s (512kb) and ∼12800x smaller than hma’s (9mb).\n\n\n\n----------------------------------------\n\n\n# 6. transparent hardware management of stacked dram as part of memory\n\n# mempod:\n\nsim, et al. proposed a technique for transparent hardware management of a hybrid memory system [17],which we will refer to as “thm”. thm does not require os intervention while managing migrations. in order to keep bookkeeping costs manageable, thm allows migrations only within sets of pages (called segments). each segment includes one fast memory page and a set of slow memory pages. the slow pages of each segment can only migrate to the one fast page location, and any such migration results in the eviction of the currently-residing page. thm monitors memory accesses with one “competing counter” per segment resulting in a low cost profiling solution. finally, thm supports caching part of its structures on chip while the rest is stored in memory.\n\nthm’s competing counters can lead to false positives, allowing a cold page to migrate to fast memory.\n\nthm offers significantly limited flexibility by restricting migrations withing segments, however this decision reduces bookkeeping costs significantly. competing counters in each segment are used for activity tracking, occasionally leading to false (threshold-based) migration triggering if a cold page gets accessed at the right time. identifying migration candidates incurs very little overhead since there is exactly one fast memory location for each slow memory page that triggers migration.\n\ncite from paper\n\n\n\nsimilar to set dueling, they adopted sample region. the locations in fast memory are grouped into 32 distinct regions in an interleaving fashion, and four regions are dedicated to sampling, while other 28 regions follow the threshold decision from sampling.\n\n• nstatic: # of memory requests serviced from fast memory with static mapping • ndynamic: # of memory requests expected to be serviced from fast memory when swapping with a given threshold • nswap: # of expected swaps for a given threshold.\n\n\n\nk differs depending on the relative latency of fast and slow memory. the cost of a single fast swap is about 1200 cycles, and the difference in access latency between fast and slow memory is 72 cycles.thus, in general, the swapped-in segment needs to get at least 17 more (future) hits than the swapped-out segment for swapping to be valuable. k is computed in hardware at boot time.\n\nme competing counter needs a threshold to invoke swap.\n\nsample region will have different threshold. based on different threshold, nswap is different. thus they choose the the best threshold with max bexpected as candidate threshold.\n\n----------------------------------------\n\n\n# 8.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n\ninsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nas the nm simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. we observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nwe leverage our key insight on controlling data movement and propose bandwidth-aware tiered-memory management (batman), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the nm and the fm. we define the desired access rate of the nm as the target access rate (tar). tar is the fraction of memory accesses serviced by the nm when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2x 2/3 4x 4/5 8x 8/9\n\nme bandwidth-aware tired-memory management tries to distritube memory according to hbm and dram bandwidth ratio. and also treat it as a threshold to refuse page migration.\n\n----------------------------------------\n\n\n# 9. heterogeneous memory architectures: a hw/sw approach for mixing die-stacked and off-package memories\n\n# mempod:\n\nhma [14] is a hw/sw mechanism that attempts to predict frequently accessed pages in memory and, at predefined intervals, migrate those pages to fast memory. hw support is required for profiling memory accesses using counters for each memory page, while the migration is handled by the os. due to the costly os involvement, hma’s intervals are kept large. additionally, the hardware cost of its profiling counters is high. however, hma is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages as the os can update page tables and tlbs to reflect migrations.\n\nhma does not require a remap table due to the os updating the existing system’s structures. for activity tracking it uses full counters. the costly os involvement and the high penalty for sorting all its counters force hma to operate at very large intervals, weakening its adaptability to phase changes. however, hma offers full flexibility for migrations.\n\ninterrupt and tlb shoot down assumption a fixed 5us time penalty is charged for each page fault [27] to cover the basic interrupt costs, and then another 3uspenalty is applied whenever a tlb shootdown [33] is required.\n\ncite from org paper this first-touch hot-page (fthp) policy is effectively a generalization of both the history-based and first-touch algorithms.\n\nwe propose a dynamic feedback-directed hma policy that can dynamically adjust the hotness threshold θ to achieve a best-of-both-worlds approach between history-based and first-touch policies.\n\n * at the start of each epoch, the size of the hot set is compared to the size of the die-stacked dram (n).\n * if the hot set is too small to fill the fast memory, then θ is lowered which causes more pages to be classified as hot.\n * likewise, if the hot set is too large, θ is increased which causes fewer pages to be put in the hot set.\n * if the feedback mechanism works well, then the size of the hot set should converge to n.\n\n----------------------------------------\n\n\n# 10.challenges in heterogeneous die-stacked and off-chip memory systems\n\nyear: 2012 software os management to prevent the mapping of pages with insufficient miss traffic, we employ a threshold θ such that any page with fewer than θ llc misses is not considered for mapping into stacked dram. the application of a threshold may result in cases when the list of most frequently missed pages has only k < p items. in this case, we simply keep a random set of p − k of the existing pages from the previous epoch already in stacked dram to avoid consuming bandwidth to swap out the page back to the off-chip memory.\n\n----------------------------------------\n\n\n# 11. banshee: bandwidth-efficient dram caching via software/hardware cooperation\n\nyear: 2017 software aovid tag look up by storing dram cache presence information in the page table and tlbs.\n\nspecifically, banshee uses a hardwaremanaged frequency-based replacement (fbr) policy that only caches hot pages to reduce unnecessary data replacement traffic. to reduce the cost of accessing/updating frequency counters (which are stored in in-package dram), banshee uses a new sampling approach to only read/write counters for a fraction of memory accesses.\n\nbackground\n\n * using tags alloy cache | unison cache\n * using address remapping heterogeneous memory architecture (hma) | tagless dram cache (tdc)\n\n\n\nthey reuse reverse maping mechanism.\n\nbanshee tracks each page’s access frequency with a counter, stored in the metadata. we store counters not only for the pages in the dram cache, but also for some pages not in cache, which are candidates to bring into the cache.\n\ninstead, an access in banshee only updates a page’s frequency counter with a certain sample rate. for a sample rate of 10%, for example, the frequency counters are accessed/updated only once for every 10 dram accesses.\n\nfrequency-based replacement may lead to thrashing problem.\n\nbanshee solves this problem by only replacing a page when the candidate’s counter is greater than the victim’s counter by a certain threshold. this ensures that a page just evicted from the dram cache must be accessed for at least 2·threshold/sampling rate times before it can enter the cache again, thus preventing a page from entering and leaving frequently.\n\nby default, the threshold is the product of the number of cachelines in a page and the sampling coefficient divided by two (threshold = page_size x sampling_coeff / 2). intuitively, this means replacement can happen only if the benefit of swapping the pages outweighs the cost of the replacement operation.\n\nif a counter saturates after being incremented, all counters in the metadata will be reduced by half using a shift operation in hardware.\n\n\n\n----------------------------------------\n\n\n# 13. unison cache: a scalable and effective die-stacked dram cache\n\nthe state-of-the-art block-based design, called alloy cache, colocates a tag with each data block (e.g., 64b) in the stacked dram to provide fast access to data in a single dram access. however, such a design suffers from low hit rates due to poor temporal locality in the dram cache. in contrast, the state-of-the-art page-based design, called footprint cache, organizes the dram cache at page granularity (e.g., 4kb), but fetches only the blocks that will likely be touched within a page. in doing so, the footprint cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. however, multi-gigabyte stacked dram caches will soon be practical and needed by server applications, thereby mandating tens of mbs of tag storage even for page-based dram caches.\n\nwe introduce a novel stacked-dram cache design, unison cache. similar to alloy cache’s approach, unison cache incorporates the tag metadata directly into the stacked dram to enable scalability to arbitrary stacked-dram capacities. then, leveraging the insights from the footprint cache design, unison cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. our evaluation using server workloads and caches of up to 8gb reveals that unison cache improves performance by 14% compared to alloy cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical sram-based tags of around 50mb.\n\nme block-based high miss ratio, page-based high migration penalty when fetching useless data. if footprint meta data is adopted to collect data trace, footcache meta table cannot scale with increasing capactiy of hbm.\n\n\n\n----------------------------------------\n\n\n# 14. dynamically adapting page migration policies based on applications memory access behaviors\n\nlink to notes for dynamically adapting...\n\n\n# 15. on-the-fly page migration and address reconciliation for heterogeneous memory systems\n\n“on-the-fly” migration performs better than epoch-based page migration techniques, since we migrate recent hot pages.\n\ninstead of relying completely on os to perform ar for the evicted entries, as done in [ramoset al. 2011], we propose a hardware-based ar, where the migc hardware initiates tlb shootdown and cache flushing without explicitly stopping the user program.\n\nlike previous studies [meswani et al. 2015; prodromou et al. 2017; su et al. 2015], we observe that not all applications benefit from page migration, since page migration incurs performance overheads due to extra data movement.\n\nthe model works with the principle that, to get performance benefit, one should migrate the smallest set of pages from slow to fast memory that yields in the largest increase in memory accesses to fast memory to amortize the migration overhead.\n\nme try to use 80% principle. in our study, we look at the 80-percentile accesses and identify the “set of top-accessed pages” that contribute to more than 80% of all memory accesses.\n\nin our proposal, we migrate a page immediately when it receives sufficient number of memory accesses, unlike any epoch-based schemes. we allow full flexibility in page relocation like hmahs [meswani et al. 2015] and keep a remap table for address redirection. we keep this table small by periodically evicting entries and it is placed on-chip.\n\nthe particular access count, which can separate such top-accessed pages from other pages, is referred to as “filter count.” note that, filter count indicates an upper bound for hotness threshold\n\nme this paper explain the address reconciliation ar in detail.\n\nflow of ar -first, all cache lines from these pages, which are currently residing in the cache hierarchies and tagged with os-visible pa, must be invalidated (and dirty lines written back), since the current os-visible pa will be replaced with the new pa. all future accesses to these pages will only have access to the new pa. -next,corresponding page table entries (ptes) for a and b need to be updated with new pas. -the tlb entries in all cores using the old pa must also be invalidated (as well as any other os structures that contain the physical page addresses).\n\n(i) flush_cache_page() (ii) change pte, (iii) flush_tlb_page()\n\nwe use a migration benefit quotient (mbq), by calculating the difference between the total number of accesses to any page and the filter count used in classifying applications’ memory locality, as described. mbq indicates how many of the future accesses of a page may go to fast memory if the page were migrated from slow to fast memory using the filter count as a hotness threshold.\n\nsacturation account the sum of access counts of all pages with 32,909 accesses or less (the bars corresponding to x-axis 0 to 3,290) accounts for 98% of all accesses. for our purposes, we use this access count as a saturation count (or assume that the maximum number of accesses any page can receive).\n\nfor each workload, we use the difference between the saturation count and the filter count to determine mbq.\n\n\n\n 1. low mbq, difference is less than 1k. for example, in figure 5(b) xalanc, pages with memory access count 609 or less (the bars corresponding to x-axis 0 to 60) provide 98% of the memory accesses. hence, the difference between saturation count (609) and filter count (70) is 539. these workloads may not achieve significant increase in accesses to fast memory after page migration.\n 2. medium mbq, difference is in between 1k to k (e.g., figure 5(c) omnetpp). these workloads may receive moderate benefits, depending the migration overheads.\n 3. high mbq, difference is more than 8k (e.g., figure 5(a) mcf). these workloads are likely to receive higher hits in faster memory as a result of page migration.\n\nme in short\n\n * sactuartion account. the sum of access counts fo all pages with sactuartion account accounts for 98% of all accesses.\n * filter account. the “set of top-accessed pages” that contribute to more than 80% of all memory accesses. if they are close, it means that this is a uniform benchmark. if they have a large difference, this means that there is a subset of pages that has far more memory accesses.\n\n----------------------------------------\n\n# 17. tictoc: enabling bandwidth-efficient dram caching for both hits and misses in hybrid memory systems international conference on computer design (iccd)\n\nyear: 2019",charsets:{cjk:!0},lastUpdated:"2024/07/16, 20:14:06"},{title:"HBM Dead Block Predictor",frontmatter:{title:"HBM Dead Block Predictor",date:"2023-05-15T00:00:00.000Z",permalink:"/pages/2476af/"},regularPath:"/01.hbm/02.hbm_dead_block_predictor.html",relativePath:"01.hbm/02.hbm_dead_block_predictor.md",key:"v-d7471e16",path:"/pages/2476af/",headers:[{level:3,title:"1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory",slug:"_1-data-placement-in-hpc-architectures-with-heterogeneous-off-chip-memory",normalizedTitle:"1. data placement in hpc architectures with heterogeneous off-chip memory",charIndex:1},{level:3,title:"2. Die-Stacked DRAM: Memory, Cache, or MemCache?",slug:"_2-die-stacked-dram-memory-cache-or-memcache",normalizedTitle:"2. die-stacked dram: memory, cache, or memcache?",charIndex:76},{level:3,title:"4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)",slug:"_4-bumblebee-a-memcache-design-for-die-stacked-and-off-chip-heterogeneous-memory-systems-2023",normalizedTitle:"4. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems (2023)",charIndex:182},{level:3,title:"5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM",slug:"_5-batman-techniques-for-maximizing-system-bandwidth-of-memory-systems-with-stacked-dram",normalizedTitle:"5.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram",charIndex:7054},{level:3,title:"6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches",slug:"_6-bear-techniques-for-mitigating-bandwidth-bloat-in-gigascale-dram-caches",normalizedTitle:"6. bear: techniques for mitigating bandwidth bloat in gigascale dram caches",charIndex:371},{level:3,title:"7.To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches",slug:"_7-to-update-or-not-to-update-bandwidth-efficient-intelligent-replacement-policies-for-dram-caches",normalizedTitle:"7.to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches",charIndex:10703},{level:3,title:"8.ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction",slug:"_8-accord-enabling-associativity-for-gigascale-dram-caches-by-coordinating-way-install-and-way-prediction",normalizedTitle:"8.accord: enabling associativity for gigascale dram caches by coordinating way-install and way-prediction",charIndex:13307},{level:3,title:"9. A Survey of Cache Bypassing Techniques",slug:"_9-a-survey-of-cache-bypassing-techniques",normalizedTitle:"9. a survey of cache bypassing techniques",charIndex:13505},{level:3,title:"10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel",slug:"_10-the-evicted-address-filter-a-unified-mechanism-to-address-both-cache-pollution-and-thrashing-not-read-yet-intel",normalizedTitle:"10. the evicted-address filter: a unified mechanism to address both cache pollution and thrashing - not read yet intel",charIndex:745},{level:3,title:"11. Bypass and Insertion Algorithms for Exclusive Last-level Caches",slug:"_11-bypass-and-insertion-algorithms-for-exclusive-last-level-caches",normalizedTitle:"11. bypass and insertion algorithms for exclusive last-level caches",charIndex:865},{level:3,title:"12. Counter-Based Cache Replacement and Bypassing Algorithms",slug:"_12-counter-based-cache-replacement-and-bypassing-algorithms",normalizedTitle:"12. counter-based cache replacement and bypassing algorithms",charIndex:934},{level:3,title:"13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)",slug:"_13-techniques-for-bandwidth-efficient-prefetching-of-linked-data-structures-in-hybrid-prefetching-systems-lds-prefetch",normalizedTitle:"13. techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems (lds prefetch)",charIndex:996}],headersStr:"1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory 2. Die-Stacked DRAM: Memory, Cache, or MemCache? 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023) 5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM 6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches 7.To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches 8.ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction 9. A Survey of Cache Bypassing Techniques 10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel 11. Bypass and Insertion Algorithms for Exclusive Last-level Caches 12. Counter-Based Cache Replacement and Bypassing Algorithms 13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)",content:' 1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory\n 2. Die-Stacked DRAM: Memory, Cache, or MemCache?\n 3. A Survey Of Techniques for Architecting DRAM Caches\n 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)\n 5. BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n 6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches\n 7. To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches\n 8. ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction\n\n----------------------------------------\n\n 9.  A Survey of Cache Bypassing Techniques\n 10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel\n 11. Bypass and Insertion Algorithms for Exclusive Last-level Caches\n 12. Counter-Based Cache Replacement and Bypassing Algorithms\n 13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)\n\n----------------------------------------\n\n\n# 1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory\n\n * Software manage DRAM and NVM\n\n 1. First touch policy\n    Alloc all pages in DRAM\n\n 2. Static profile-based policy\n\n 3. Spill Migration\n    LRU spill policy keeps track of last access time for each page in DRAM, and in case of eviction selects one that is least recently used. Spill migration policy first allocates a page in fast memory (in our case DRAM), and later evicts it to PCM. Spill profile-based policy can either spare a page from eviction if its future traffic is high, or victimize it if it is low, regardless of its previous access count.\n\n 4. Dynamic page migration\n    \n    \n    \n    When a page is first brought to the PCM we reset its access counter, regardless of how many times it was accessed in the DRAM. At the same time we keep track of the number of accesses for every page in the DRAM, as well as the average for all the pages (nDRAMavg). When a page in PCM is accessed, we compare its access counter (naccesses) with the average number of accesses to pages in DRAM. Back migration threshold (BMT) is a value that controls the aggressiveness of migration triggering. If it is set to zero, a page is migrated as soon as it is touched in PCM, so the DRAM acts as a typical cache. In this case we expect good performance as the system tends to always move active pages to DRAM, but due to a large number of migrations, number of writes to PCM may go high. On the other hand, if BMT is set to infinity the page never gets migrated back, and then the policy is equivalent to LRU spill. In between those extremes we would like to search for values that give good performace and low number of PCM writes.\n\n----------------------------------------\n\n\n# 2. Die-Stacked DRAM: Memory, Cache, or MemCache?\n\n * Part as Memory ans Part as Cache\n * Discuss and compared with Alloy Cache, Unison Cache, Banshee Cache, HMA\n * Hot Data Sets pages in memory HBM and transient pages in cache HBM\n\nCited from org paper: In this proposal, a software procedure pre-processes the application and determines hot pages,then asks the OS to map them to the memory portion of the die-stacked DRAM. The cache portion of the die-stacked DRAM is managed by hardware, caching data allocated in the off-chip memory.\n\nTo identify hot pages, we use a static profile-based approach before the execution of an application. A software procedure, incorporated into the compiler, pre-processes the application and sorts the pages based on their access frequency. Then it picks the top pages and asks the OS to map them to the memory portion of the die-stacked DRAM.\n\nAfter detection of hot pages, their details are coded into the program binary. Whenever the program gets executed, the Loader passes the required information of hot pages to the OS. Then, the OS tries to map such hot pages to physical locations that belong to the memory portion of the die-stacked DRAM. For the OS, allocating pages in the die-stacked and off-chip memory is similar to the same operations in Non-Uniform Memory Architecture (NUMA) [50] systems.\n\nIn this paper, they raised the issue that when process switches, previous hbm space allocated to a process might left inadequate space for the following process. Other orthogonal research " Various proposals (e.g., [13, 47, 54, 62, 78]) have suggested to optimize memory management in such situations typically by gradually or periodically migrating application pages between different types of memories based on factors like programming model, application’s criticality, sharing degree, and so on".\n\nThey identify the portion of hbm memory(the hot pages that need to be allocated to HBM) for hot pages by trying to allocate the maximum number of pages into hbm memory without worsening the cacheAHF.\n\n\n\n----------------------------------------\n\n\n# 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)\n\n# Me\n\n * Hybrid Memory\n * Blk/Page Size 2KB/64KB\n   64KB page size is due to the fact that it maps all memory in dram and hbm.\n   Every request will cam PRT and BLE.\n   In multi core simulation env,it have to support multi core read and write the SRAM.\n   \n * Distinguish Spacial Locality and Temporal Locality.\n   cacheHBM (cHBM) for temporal locality\n   memoryHBM (mHBM) for spacial locality\n   \n * Page Allocation.\n   Different from previous design that allocate all memory in HBM or DRAM. It allocate page according to its neighbour pages. But it does not mention how it interact with page table. If page is deallocate or written back to disk, the PRT should also be updated.\n * The ratio between cHBM and mHBM is flexible.\n\n\n\n * If memory footprint is high, all used by OS, all the HBM will be served as flat memory.\n\nCited from org paper: Program Statistics\n\n\n\nIn each remapping set, the hotness tracker includes a hot table and five parameters: the HBM occupied ratio (Rh), a hotness threshold (T) to decide if an off-chip DRAM page should be brought in HBM for high Rh condition, the number of cHBM pages (Nc), and the number of mHBM pages in which most blocks have/have not been accessed (Na/Nn).\n\n\n\nFor SL>0 (strong spatial locality), more hot data should be brought in mHBM to better exploit the spatial locality and utilize the memory bandwidth. For SL ≤ 0 (weak spatial locality), hot data should be cached in cHBM to reduce over-fetching.\n\nThe threshold T in the hotness tracker can alleviate this issue. If Rh is high, for SL>0, only pages whose hotness value is larger than T are permitted to be migrated to mHBM and for SL ≤ 0, only blocks in a page whose hotness value is larger than T are permitted to be cached in cHBM. Me From this aspect, SL means that number of mHBM pages that most blocks have been accessed is far larger than not been accessed. This means strong spatial locality.\n\n----------------------------------------\n\n\n# 5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n\nInsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nAs the NM simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. We observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nWe leverage our key insight on controlling data movement and propose Bandwidth-Aware Tiered-Memory Management (BATMAN), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the NM and the FM. We define the desired access rate of the NM as the target access rate (TAR). TAR is the fraction of memory accesses serviced by the NM when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2X 2/3 4X 4/5 8X 8/9\n\nMe Bandwidth-Aware Tired-Memory Management tries to distritube memory according to HBM and DRAM bandwidth ratio. And also treat it as a threshold to refuse page migration.\n\nThis bandwidth division is also adopted in "Design and Implementation of Bandwidth-Aware Memory Placement and Migration Policies for Heterogeneous Memory".\n\n\n\n----------------------------------------\n\n\n# 6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches\n\nYear:2015\n\nIdeally, we want the bandwidth consumed for such secondary operations to be negligible, and have almost all the bandwidth be available for transfer of useful data from the DRAM cache to the processor. BEAR integrates three components, one each for reducing the bandwidth consumed by miss detection, miss fill, and writeback probes.\n\n 1. Miss Probe (to detect a miss, we need to look up the tag store in the DRAM cache)\n 2. Miss Fill (on a cache miss the missed line is obtained from memory and filled in the cache)\n 3. Write back Probe (on a dirty eviction from the on-chip LLC identifying if that line is present in the DRAM cache)\n 4. Writeback Update (if writeback probe gives a hit, updating the content of the line in DRAM cache)\n 5. Writeback Fill (filling the writeback data in the cache, if a writeback probe gives a miss)\n\nThey define BloatFactor, the ratio of the total bandwidth consumed by the DRAM cache to the bandwidth required for transferring only the data lines to the processor chip.\n\n 1. Bandwidth Efficient Cache Fills We propose Bandwidth Aware Bypass (BAB) to reduce the bandwidth consumed by fill operations while limiting the loss in cache hit rate to a desired level.\n    \n    \n\n 2. Bandwidth Efficient Writeback Probe DRAM Cache Presence (DCP), reduces Writeback Probe by introducing state information in the on-chip Last Level Cache (LLC) to track if the line exists in the DRAM cache. Inclusive Cache\n    \n    \n\n 3. Bandwidth Efficient Miss Probe We reduce the bandwidth consumed by Miss Probe by leveraging the property of DRAM caches to streams multiple tags on each access. We buffer the tags of recently accessed adjacent cache line\'s tags in the Neighboring Tag Cache (NTC). Neighboring Tag Cache\n    \n    \n\nComment from To Update or Not to update Along the same lines, Chou et. al [6] propose a policy that bypasses the cache with 90% probability (we call this policy 90%-Bypass). Me This comment from to update or not to update is not accurate, the bear paper mentioned that "Overall, the speed up from probabilistic bypass is negligible, and we may deem PB to be ineffective at improving performance." Then it prefers set-duleling.\n\n----------------------------------------\n\n\n# 7.To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches\n\nYear: 2019\n\nMe Previous dram cache is stateless, due to the fact that maintaining state of cache would require significant bandwidth.\n\nCite from org paper We propose a stateful replacement/bypass policy called RRIP Age-On-Bypass (RRIP-AOB), that tracks reuse state for high-reuse lines, protects such lines by bypassing other lines, and Ages the state On cache Bypass.\n\nThe DRAM cache in KNL [4, 5],for example, employs an Always-Install policy. The DRAM cache places each tag information in the unused bits in the ECC space and streams out the data and tag (contained in ECC) on each access.\n\n\n\nOur goal is to increase the hit-rate of such DRAM caches. In fact, the DRAM cache only uses about 8-10 bits from the unused 28 bits in the ECC space, so we have 18-20 bits per line available for managing the DRAM cache intelligently.\n\nTo reduce significant bandwidth to update state, we propose Efficient Tracking of Reuse (ETR). ETR makes state tracking efficient by accurately tracking the state of only one line from a region, and using the state of that line to guide the replacement decisions for other lines in that region.\n\n 1. We propose a bypass version of RRIP (RRIP-AOB) suitable for caches with limited associativity. However, we find an effective replacement policy for DRAM caches must optimize not only hit-rate but also state update cost. We introduce two properties, coresidency and eviction-locality, that can be exploited to reduce state update cost for implementing intelligent replacement.\n    \n    \n    \n    Coresidency indicates that at any given time if a line is present, then several other line belonging to that 4KB region are also present in the cache. Eviction-Locality indicates that when a line gets evicted from the cache, the replacement-state of the other coresident lines belonging to that region tend to have similar replacement state as the line being evicted. Me Just a synonym for spacial locality. This granularity is 4KB. Doubt about its authenticity.\n\n 2. We propose Efficient Tracking of Reuse (ETR), a design that performs updates for only a subset of lines and uses their state to guide the replacement decisions of other lines.\n\n\n\nMe This is similar to set dueling.\n\nThe design of ETR consists of three parts: (1) Selecting a Representative-Line in the region. (2) Keeping accurate RRPV for only the Representative-Line. (3) Using the representative’s RRPV to infer coresident lines’ RRPV to make bypass decisions.\n\n----------------------------------------\n\n\n# 8.ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction\n\nA method to optimize prediction way of dram.\n\n----------------------------------------\n\n\n# 9. A Survey of Cache Bypassing Techniques\n\n----------------------------------------\n\n\n# 10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel\n\n----------------------------------------\n\n\n# 11. Bypass and Insertion Algorithms for Exclusive Last-level Caches\n\n----------------------------------------\n\n\n# 12. Counter-Based Cache Replacement and Bypassing Algorithms\n\n----------------------------------------\n\n\n# 13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)',normalizedContent:' 1. data placement in hpc architectures with heterogeneous off-chip memory\n 2. die-stacked dram: memory, cache, or memcache?\n 3. a survey of techniques for architecting dram caches\n 4. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems (2023)\n 5. batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n 6. bear: techniques for mitigating bandwidth bloat in gigascale dram caches\n 7. to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches\n 8. accord: enabling associativity for gigascale dram caches by coordinating way-install and way-prediction\n\n----------------------------------------\n\n 9.  a survey of cache bypassing techniques\n 10. the evicted-address filter: a unified mechanism to address both cache pollution and thrashing - not read yet intel\n 11. bypass and insertion algorithms for exclusive last-level caches\n 12. counter-based cache replacement and bypassing algorithms\n 13. techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems (lds prefetch)\n\n----------------------------------------\n\n\n# 1. data placement in hpc architectures with heterogeneous off-chip memory\n\n * software manage dram and nvm\n\n 1. first touch policy\n    alloc all pages in dram\n\n 2. static profile-based policy\n\n 3. spill migration\n    lru spill policy keeps track of last access time for each page in dram, and in case of eviction selects one that is least recently used. spill migration policy first allocates a page in fast memory (in our case dram), and later evicts it to pcm. spill profile-based policy can either spare a page from eviction if its future traffic is high, or victimize it if it is low, regardless of its previous access count.\n\n 4. dynamic page migration\n    \n    \n    \n    when a page is first brought to the pcm we reset its access counter, regardless of how many times it was accessed in the dram. at the same time we keep track of the number of accesses for every page in the dram, as well as the average for all the pages (ndramavg). when a page in pcm is accessed, we compare its access counter (naccesses) with the average number of accesses to pages in dram. back migration threshold (bmt) is a value that controls the aggressiveness of migration triggering. if it is set to zero, a page is migrated as soon as it is touched in pcm, so the dram acts as a typical cache. in this case we expect good performance as the system tends to always move active pages to dram, but due to a large number of migrations, number of writes to pcm may go high. on the other hand, if bmt is set to infinity the page never gets migrated back, and then the policy is equivalent to lru spill. in between those extremes we would like to search for values that give good performace and low number of pcm writes.\n\n----------------------------------------\n\n\n# 2. die-stacked dram: memory, cache, or memcache?\n\n * part as memory ans part as cache\n * discuss and compared with alloy cache, unison cache, banshee cache, hma\n * hot data sets pages in memory hbm and transient pages in cache hbm\n\ncited from org paper: in this proposal, a software procedure pre-processes the application and determines hot pages,then asks the os to map them to the memory portion of the die-stacked dram. the cache portion of the die-stacked dram is managed by hardware, caching data allocated in the off-chip memory.\n\nto identify hot pages, we use a static profile-based approach before the execution of an application. a software procedure, incorporated into the compiler, pre-processes the application and sorts the pages based on their access frequency. then it picks the top pages and asks the os to map them to the memory portion of the die-stacked dram.\n\nafter detection of hot pages, their details are coded into the program binary. whenever the program gets executed, the loader passes the required information of hot pages to the os. then, the os tries to map such hot pages to physical locations that belong to the memory portion of the die-stacked dram. for the os, allocating pages in the die-stacked and off-chip memory is similar to the same operations in non-uniform memory architecture (numa) [50] systems.\n\nin this paper, they raised the issue that when process switches, previous hbm space allocated to a process might left inadequate space for the following process. other orthogonal research " various proposals (e.g., [13, 47, 54, 62, 78]) have suggested to optimize memory management in such situations typically by gradually or periodically migrating application pages between different types of memories based on factors like programming model, application’s criticality, sharing degree, and so on".\n\nthey identify the portion of hbm memory(the hot pages that need to be allocated to hbm) for hot pages by trying to allocate the maximum number of pages into hbm memory without worsening the cacheahf.\n\n\n\n----------------------------------------\n\n\n# 4. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems (2023)\n\n# me\n\n * hybrid memory\n * blk/page size 2kb/64kb\n   64kb page size is due to the fact that it maps all memory in dram and hbm.\n   every request will cam prt and ble.\n   in multi core simulation env,it have to support multi core read and write the sram.\n   \n * distinguish spacial locality and temporal locality.\n   cachehbm (chbm) for temporal locality\n   memoryhbm (mhbm) for spacial locality\n   \n * page allocation.\n   different from previous design that allocate all memory in hbm or dram. it allocate page according to its neighbour pages. but it does not mention how it interact with page table. if page is deallocate or written back to disk, the prt should also be updated.\n * the ratio between chbm and mhbm is flexible.\n\n\n\n * if memory footprint is high, all used by os, all the hbm will be served as flat memory.\n\ncited from org paper: program statistics\n\n\n\nin each remapping set, the hotness tracker includes a hot table and five parameters: the hbm occupied ratio (rh), a hotness threshold (t) to decide if an off-chip dram page should be brought in hbm for high rh condition, the number of chbm pages (nc), and the number of mhbm pages in which most blocks have/have not been accessed (na/nn).\n\n\n\nfor sl>0 (strong spatial locality), more hot data should be brought in mhbm to better exploit the spatial locality and utilize the memory bandwidth. for sl ≤ 0 (weak spatial locality), hot data should be cached in chbm to reduce over-fetching.\n\nthe threshold t in the hotness tracker can alleviate this issue. if rh is high, for sl>0, only pages whose hotness value is larger than t are permitted to be migrated to mhbm and for sl ≤ 0, only blocks in a page whose hotness value is larger than t are permitted to be cached in chbm. me from this aspect, sl means that number of mhbm pages that most blocks have been accessed is far larger than not been accessed. this means strong spatial locality.\n\n----------------------------------------\n\n\n# 5.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n\ninsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nas the nm simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. we observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nwe leverage our key insight on controlling data movement and propose bandwidth-aware tiered-memory management (batman), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the nm and the fm. we define the desired access rate of the nm as the target access rate (tar). tar is the fraction of memory accesses serviced by the nm when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2x 2/3 4x 4/5 8x 8/9\n\nme bandwidth-aware tired-memory management tries to distritube memory according to hbm and dram bandwidth ratio. and also treat it as a threshold to refuse page migration.\n\nthis bandwidth division is also adopted in "design and implementation of bandwidth-aware memory placement and migration policies for heterogeneous memory".\n\n\n\n----------------------------------------\n\n\n# 6. bear: techniques for mitigating bandwidth bloat in gigascale dram caches\n\nyear:2015\n\nideally, we want the bandwidth consumed for such secondary operations to be negligible, and have almost all the bandwidth be available for transfer of useful data from the dram cache to the processor. bear integrates three components, one each for reducing the bandwidth consumed by miss detection, miss fill, and writeback probes.\n\n 1. miss probe (to detect a miss, we need to look up the tag store in the dram cache)\n 2. miss fill (on a cache miss the missed line is obtained from memory and filled in the cache)\n 3. write back probe (on a dirty eviction from the on-chip llc identifying if that line is present in the dram cache)\n 4. writeback update (if writeback probe gives a hit, updating the content of the line in dram cache)\n 5. writeback fill (filling the writeback data in the cache, if a writeback probe gives a miss)\n\nthey define bloatfactor, the ratio of the total bandwidth consumed by the dram cache to the bandwidth required for transferring only the data lines to the processor chip.\n\n 1. bandwidth efficient cache fills we propose bandwidth aware bypass (bab) to reduce the bandwidth consumed by fill operations while limiting the loss in cache hit rate to a desired level.\n    \n    \n\n 2. bandwidth efficient writeback probe dram cache presence (dcp), reduces writeback probe by introducing state information in the on-chip last level cache (llc) to track if the line exists in the dram cache. inclusive cache\n    \n    \n\n 3. bandwidth efficient miss probe we reduce the bandwidth consumed by miss probe by leveraging the property of dram caches to streams multiple tags on each access. we buffer the tags of recently accessed adjacent cache line\'s tags in the neighboring tag cache (ntc). neighboring tag cache\n    \n    \n\ncomment from to update or not to update along the same lines, chou et. al [6] propose a policy that bypasses the cache with 90% probability (we call this policy 90%-bypass). me this comment from to update or not to update is not accurate, the bear paper mentioned that "overall, the speed up from probabilistic bypass is negligible, and we may deem pb to be ineffective at improving performance." then it prefers set-duleling.\n\n----------------------------------------\n\n\n# 7.to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches\n\nyear: 2019\n\nme previous dram cache is stateless, due to the fact that maintaining state of cache would require significant bandwidth.\n\ncite from org paper we propose a stateful replacement/bypass policy called rrip age-on-bypass (rrip-aob), that tracks reuse state for high-reuse lines, protects such lines by bypassing other lines, and ages the state on cache bypass.\n\nthe dram cache in knl [4, 5],for example, employs an always-install policy. the dram cache places each tag information in the unused bits in the ecc space and streams out the data and tag (contained in ecc) on each access.\n\n\n\nour goal is to increase the hit-rate of such dram caches. in fact, the dram cache only uses about 8-10 bits from the unused 28 bits in the ecc space, so we have 18-20 bits per line available for managing the dram cache intelligently.\n\nto reduce significant bandwidth to update state, we propose efficient tracking of reuse (etr). etr makes state tracking efficient by accurately tracking the state of only one line from a region, and using the state of that line to guide the replacement decisions for other lines in that region.\n\n 1. we propose a bypass version of rrip (rrip-aob) suitable for caches with limited associativity. however, we find an effective replacement policy for dram caches must optimize not only hit-rate but also state update cost. we introduce two properties, coresidency and eviction-locality, that can be exploited to reduce state update cost for implementing intelligent replacement.\n    \n    \n    \n    coresidency indicates that at any given time if a line is present, then several other line belonging to that 4kb region are also present in the cache. eviction-locality indicates that when a line gets evicted from the cache, the replacement-state of the other coresident lines belonging to that region tend to have similar replacement state as the line being evicted. me just a synonym for spacial locality. this granularity is 4kb. doubt about its authenticity.\n\n 2. we propose efficient tracking of reuse (etr), a design that performs updates for only a subset of lines and uses their state to guide the replacement decisions of other lines.\n\n\n\nme this is similar to set dueling.\n\nthe design of etr consists of three parts: (1) selecting a representative-line in the region. (2) keeping accurate rrpv for only the representative-line. (3) using the representative’s rrpv to infer coresident lines’ rrpv to make bypass decisions.\n\n----------------------------------------\n\n\n# 8.accord: enabling associativity for gigascale dram caches by coordinating way-install and way-prediction\n\na method to optimize prediction way of dram.\n\n----------------------------------------\n\n\n# 9. a survey of cache bypassing techniques\n\n----------------------------------------\n\n\n# 10. the evicted-address filter: a unified mechanism to address both cache pollution and thrashing - not read yet intel\n\n----------------------------------------\n\n\n# 11. bypass and insertion algorithms for exclusive last-level caches\n\n----------------------------------------\n\n\n# 12. counter-based cache replacement and bypassing algorithms\n\n----------------------------------------\n\n\n# 13. techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems (lds prefetch)',charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Cache Memory Compression",frontmatter:{title:"Cache Memory Compression",date:"2023-06-06T00:00:00.000Z",permalink:"/pages/2476bf/"},regularPath:"/01.hbm/05.cache_mem_compression.html",relativePath:"01.hbm/05.cache_mem_compression.md",key:"v-7f8db936",path:"/pages/2476bf/",headersStr:null,content:" 1. Compresso: Pragmatic Main Memory Compression\n 2. Translation-optimized Memory Compression for Capacity\n 3. Touche: Towards Ideal and Efficient Cache Compression By Mitigating Tag Area Overheads\n\n----------------------------------------\n\n# 1. Compresso: Pragmatic Main Memory Compression\n\nCite from the paper: We propose Compresso, with optimizations to reduce compressed data movement in a hardware compressed memory, while maintaining high compression ratio by repacking data at the right time.\n\nCompresso uses the modified BPC compression algorithm, achieving 1.85x average compression on a wide range of applications.\n\nCompresso uses the compression granularity of 64B.\n\nCompresso uses LinePack with 4 possible cache line sizes.\n\nWe compare variable-sized chunks (512B, 1KB, 2KB and 4KB) with 512B fixed-sized chunks. Compresso uses incremental allocation in 512B chunks,thereby allowing 8 page sizes (512B, 1KB, 1.5KB and so on).\n\n\n\nAdditional Data Movement:\n\n 1. split-access cachelines\n 2. changes in compressibility(overflows)\n 3. metadata access\n\n\n\nDifference in exception in LCP compression Instead, Compresso allows some number of such inflated cachelines to be stored uncompressed in the inflation room at the end of an MPA page, provided that there is space in that page (Fig. 5a). This is similar to the exception region in LCP, but is used for an entirely different reason—to reduce compression-related data movement, rather than to support a specific packing scheme.\n\n\n\nWe present the first main-memory compression architecture that is designed to run an unmodified operating system.\n\n----------------------------------------\n\n# 2. Translation-optimized Memory Compression for Capacity\n\nprior workscompress and pack/migrate data at a small - memory block-level - granularity; this introduces an additional block-level translation after the page-level virtual address translation. In general, the smaller the granularity of address translation, the higher the translation overhead.\n\nA promising solution is to only save memory from cold (i.e.,less recently accessed) pages without saving memory from hot (i.e., more recently accessed) pages (e.g., keep the hot pages uncompressed).\n\nTwo challenges:\n\n 1. after a compressed cold page becomes hot again, migrating the page to a full 4KB DRAM location still adds another level (albeit page-level, instead of block-level) of translation on top of existing virtual address translation. Solution we propose compressing page table blocks in hardware to opportunistically embed compression translations into them in a software-transparent manner to effectively prefetch compression translations during a page walk, instead of serially fetching them after the walk.\n\nFirst, CTE misses typically occur after PTE misses in TLB because CTEs, especially the page-level CTEs under an OS-inspired approach, have similar translation reach as PTEs. Second, we observe page table blocks (PTBs) are highly compressible because adjacent virtual pages often have identical status bits and the most significant bits in physical page numbers are unused. As such, to hide the latency of CTE misses, TMCC transparently compresses each PTB in hardware to free up space in the PTB to embed the CTEs of the 4KB pages (i.e., either data pages or page table pages) that the PTB points to; this enables each page walk to also prefetch the matching CTE required for fetching from DRAM either the end data or the next PTB.\n\n 2. only compressing cold data require compressing them very aggressively to achieve high overall memory savings. Solution we perform a large design space exploration across many hardware configurations and diverse workloads to derive and implement in HDL an ASIC Deflate that is specialized for memory.\n\nPrior new hardware managed translation entries as Compression Translation Entries (CTEs), as they are similar to OS page table entries (PTEs). Prior works cache CTEs in the memory controller via a dedicated CTE cache, similar to the TLBs dedicated to caching PTEs.\n\nlet hardware take on an OS-inspired approach: only save memory from cold (i.e., less recently accessed) pages without saving memory from hot (i.e., recently accessed) pages (e.g., keep the hot pages uncompressed), like OS memory compression. Solves the problem of\n\n 1. translation overheads that large and/or irregular workloads suffer from high PTE miss under hardware memory compression.\n 2. Fine-grained address translation\n\nAccesses to a compressed virtual page in ML2 incurs a page fault to wake up OS to pop a free physical page from ML1’s free list and migrate the virtual page to the page.\n\nML2 also keeps many free lists, each tracking sub-physical pages of a different size, to store any compressed virtual page in a practically ideal matching sub-physical page.\n\nML2 gracefully grows and shrinks relative to ML1 with increasing and decreasing memory usage. When everything can fit in memory uncompressed, ML2 shrinks to zero bytes in physical size so ML1 can have every physical page. Specifically, when ML2’s free list(s) get large (e.g., due to reducing memory usage), ML2 donates free physical pages from its free list(s) to ML1. OS also grows ML1 free list, when it gets small, by migrating cold virtual pages to ML2. Migrating a virtual page to ML2 shrinks one of ML2’s free lists. If a ML2 free list gets empty, ML1 gives cold victim physical pages to ML2 (i.e., track them in ML2 instead of ML1), so that ML2 can compress the virtual pages currently in the victim pages to free space in the victims to grow ML2’s free list(s).\n\n\n\nKey Idea: Based on our observations, we propose transparently compressing each PTB in hardware to free up space in the PTB to embed the CTEs of the 4KB pages (i.e., either data pages or page table pages) that the PTB points to; this enables each page walk access to also prefetch the matching CTE required either for the next page walk access (i.e., to the next PTB) or for the actual data (or instruction) access after the walk.\n\n\n\nA practical challenge is that after migrating a page (e.g., from ML1 to ML2 after the page becomes cold), the corresponding CTE embedded in the page’s PTB should be updated. However, hardware has no easy way to use the PPN of the migrating page to find/access the page’s PTB(s). TMCC addresses this challenge by lazily updating the CTE in the PTB later around when the PTB is naturally accessed by the page walker, instead of updating it at the time of migrating the page. However, this means that for the first page walker access to the PTB after migrating one of the pages that the PTB points to, the corresponding CTE is out-of-date. To ensure correctness, TMCC also accesses the correct CTE in DRAM (or in CTE cache) in parallel to verify the correctness of the DRAM access. Figure 8 compares and contrasts how TMCC serves an LLC miss that also misses in CTE cache with the baseline approach. Figure 9 provides n architectural overview of TMCC.\n\n\n\n\n\n\n\nMy Comment When os access compressed page, that page is migrated from ML2 to ML1. Hardware cannot update the PTB easily. Thus it utilizes lazily update. During the page table walk, it will buffer the piggybacked CTE into CTE buffer. And when data miss req happens, L2 extracts the PPN from the received request to lookup the CTE Buffer to obtain the CTE for MC to translate the PPN.\n\n----------------------------------------\n\n# 3. Touche: Towards Ideal and Efficient Cache Compression By Mitigating Tag Area Overheads\n\nThe first component, called the “Signature” (SIGN) engine, creates shortened signatures from the tag addresses of compressed blocks. Due to this, the SIGN engine can store multiple signatures in each tag entry. On a cache access, the physical cacheline is accessed only if there is a signature match (which has a negligible probability of false positive). The second component, called the “Tag Appended Data” (TADA) mechanism, stores the full tag addresses with data. TADA enables Touch´e to detect false positive signature matches by ensuring that the actual tag address is available for comparison. The third component, called the “Superblock Marker” (SMARK) mechanism, uses a unique marker in the tag entry to indicate the occurrence of compressed cache blocks from neighboring physical addresses in the same cacheline.\n\n\n\nOn average, 55% of the blocks can be compressed to less than 48 bytes in size. Furthermore, 17% of the lines can be compressed to be less than 16 bytes in size. Therefore, several workloads tend to have blocks with low entropy and can benefit from compression.\n\n\n\n\n\n\n\nFor instance, a cacheline cannot be marked both invalid and dirty at the same time. The tag manager uses this unused state to flag cachelines that contains compressed blocks. Thereafter, for a cacheline that stores compressed blocks, the 1st and 2nd bits of the tag address encodes its valid bit and dirty bit.\n\n\n\n\n\nThe tag manager then retrieves the 16-bit marker from the SMARK mechanism. It then informs the SIGN engine to ignore the last 2-bits (corresponding to four neighboring addresses) of the full tag address to generate a unique 9-bit signature.\n\nThis SMARK generate a random 16-bit marker and concated with signature. Since non-superblocks use 3 signature to identify blks, it should also use a tag to compare not just 0.\n\nIf this paper doest not support superblock 4 compressed blks in a super block, it can only store 3* 16B compressed block or 48B + 64B block, due to extra real tag stored in data, 43bit for each data.",normalizedContent:" 1. compresso: pragmatic main memory compression\n 2. translation-optimized memory compression for capacity\n 3. touche: towards ideal and efficient cache compression by mitigating tag area overheads\n\n----------------------------------------\n\n# 1. compresso: pragmatic main memory compression\n\ncite from the paper: we propose compresso, with optimizations to reduce compressed data movement in a hardware compressed memory, while maintaining high compression ratio by repacking data at the right time.\n\ncompresso uses the modified bpc compression algorithm, achieving 1.85x average compression on a wide range of applications.\n\ncompresso uses the compression granularity of 64b.\n\ncompresso uses linepack with 4 possible cache line sizes.\n\nwe compare variable-sized chunks (512b, 1kb, 2kb and 4kb) with 512b fixed-sized chunks. compresso uses incremental allocation in 512b chunks,thereby allowing 8 page sizes (512b, 1kb, 1.5kb and so on).\n\n\n\nadditional data movement:\n\n 1. split-access cachelines\n 2. changes in compressibility(overflows)\n 3. metadata access\n\n\n\ndifference in exception in lcp compression instead, compresso allows some number of such inflated cachelines to be stored uncompressed in the inflation room at the end of an mpa page, provided that there is space in that page (fig. 5a). this is similar to the exception region in lcp, but is used for an entirely different reason—to reduce compression-related data movement, rather than to support a specific packing scheme.\n\n\n\nwe present the first main-memory compression architecture that is designed to run an unmodified operating system.\n\n----------------------------------------\n\n# 2. translation-optimized memory compression for capacity\n\nprior workscompress and pack/migrate data at a small - memory block-level - granularity; this introduces an additional block-level translation after the page-level virtual address translation. in general, the smaller the granularity of address translation, the higher the translation overhead.\n\na promising solution is to only save memory from cold (i.e.,less recently accessed) pages without saving memory from hot (i.e., more recently accessed) pages (e.g., keep the hot pages uncompressed).\n\ntwo challenges:\n\n 1. after a compressed cold page becomes hot again, migrating the page to a full 4kb dram location still adds another level (albeit page-level, instead of block-level) of translation on top of existing virtual address translation. solution we propose compressing page table blocks in hardware to opportunistically embed compression translations into them in a software-transparent manner to effectively prefetch compression translations during a page walk, instead of serially fetching them after the walk.\n\nfirst, cte misses typically occur after pte misses in tlb because ctes, especially the page-level ctes under an os-inspired approach, have similar translation reach as ptes. second, we observe page table blocks (ptbs) are highly compressible because adjacent virtual pages often have identical status bits and the most significant bits in physical page numbers are unused. as such, to hide the latency of cte misses, tmcc transparently compresses each ptb in hardware to free up space in the ptb to embed the ctes of the 4kb pages (i.e., either data pages or page table pages) that the ptb points to; this enables each page walk to also prefetch the matching cte required for fetching from dram either the end data or the next ptb.\n\n 2. only compressing cold data require compressing them very aggressively to achieve high overall memory savings. solution we perform a large design space exploration across many hardware configurations and diverse workloads to derive and implement in hdl an asic deflate that is specialized for memory.\n\nprior new hardware managed translation entries as compression translation entries (ctes), as they are similar to os page table entries (ptes). prior works cache ctes in the memory controller via a dedicated cte cache, similar to the tlbs dedicated to caching ptes.\n\nlet hardware take on an os-inspired approach: only save memory from cold (i.e., less recently accessed) pages without saving memory from hot (i.e., recently accessed) pages (e.g., keep the hot pages uncompressed), like os memory compression. solves the problem of\n\n 1. translation overheads that large and/or irregular workloads suffer from high pte miss under hardware memory compression.\n 2. fine-grained address translation\n\naccesses to a compressed virtual page in ml2 incurs a page fault to wake up os to pop a free physical page from ml1’s free list and migrate the virtual page to the page.\n\nml2 also keeps many free lists, each tracking sub-physical pages of a different size, to store any compressed virtual page in a practically ideal matching sub-physical page.\n\nml2 gracefully grows and shrinks relative to ml1 with increasing and decreasing memory usage. when everything can fit in memory uncompressed, ml2 shrinks to zero bytes in physical size so ml1 can have every physical page. specifically, when ml2’s free list(s) get large (e.g., due to reducing memory usage), ml2 donates free physical pages from its free list(s) to ml1. os also grows ml1 free list, when it gets small, by migrating cold virtual pages to ml2. migrating a virtual page to ml2 shrinks one of ml2’s free lists. if a ml2 free list gets empty, ml1 gives cold victim physical pages to ml2 (i.e., track them in ml2 instead of ml1), so that ml2 can compress the virtual pages currently in the victim pages to free space in the victims to grow ml2’s free list(s).\n\n\n\nkey idea: based on our observations, we propose transparently compressing each ptb in hardware to free up space in the ptb to embed the ctes of the 4kb pages (i.e., either data pages or page table pages) that the ptb points to; this enables each page walk access to also prefetch the matching cte required either for the next page walk access (i.e., to the next ptb) or for the actual data (or instruction) access after the walk.\n\n\n\na practical challenge is that after migrating a page (e.g., from ml1 to ml2 after the page becomes cold), the corresponding cte embedded in the page’s ptb should be updated. however, hardware has no easy way to use the ppn of the migrating page to find/access the page’s ptb(s). tmcc addresses this challenge by lazily updating the cte in the ptb later around when the ptb is naturally accessed by the page walker, instead of updating it at the time of migrating the page. however, this means that for the first page walker access to the ptb after migrating one of the pages that the ptb points to, the corresponding cte is out-of-date. to ensure correctness, tmcc also accesses the correct cte in dram (or in cte cache) in parallel to verify the correctness of the dram access. figure 8 compares and contrasts how tmcc serves an llc miss that also misses in cte cache with the baseline approach. figure 9 provides n architectural overview of tmcc.\n\n\n\n\n\n\n\nmy comment when os access compressed page, that page is migrated from ml2 to ml1. hardware cannot update the ptb easily. thus it utilizes lazily update. during the page table walk, it will buffer the piggybacked cte into cte buffer. and when data miss req happens, l2 extracts the ppn from the received request to lookup the cte buffer to obtain the cte for mc to translate the ppn.\n\n----------------------------------------\n\n# 3. touche: towards ideal and efficient cache compression by mitigating tag area overheads\n\nthe first component, called the “signature” (sign) engine, creates shortened signatures from the tag addresses of compressed blocks. due to this, the sign engine can store multiple signatures in each tag entry. on a cache access, the physical cacheline is accessed only if there is a signature match (which has a negligible probability of false positive). the second component, called the “tag appended data” (tada) mechanism, stores the full tag addresses with data. tada enables touch´e to detect false positive signature matches by ensuring that the actual tag address is available for comparison. the third component, called the “superblock marker” (smark) mechanism, uses a unique marker in the tag entry to indicate the occurrence of compressed cache blocks from neighboring physical addresses in the same cacheline.\n\n\n\non average, 55% of the blocks can be compressed to less than 48 bytes in size. furthermore, 17% of the lines can be compressed to be less than 16 bytes in size. therefore, several workloads tend to have blocks with low entropy and can benefit from compression.\n\n\n\n\n\n\n\nfor instance, a cacheline cannot be marked both invalid and dirty at the same time. the tag manager uses this unused state to flag cachelines that contains compressed blocks. thereafter, for a cacheline that stores compressed blocks, the 1st and 2nd bits of the tag address encodes its valid bit and dirty bit.\n\n\n\n\n\nthe tag manager then retrieves the 16-bit marker from the smark mechanism. it then informs the sign engine to ignore the last 2-bits (corresponding to four neighboring addresses) of the full tag address to generate a unique 9-bit signature.\n\nthis smark generate a random 16-bit marker and concated with signature. since non-superblocks use 3 signature to identify blks, it should also use a tag to compare not just 0.\n\nif this paper doest not support superblock 4 compressed blks in a super block, it can only store 3* 16b compressed block or 48b + 64b block, due to extra real tag stored in data, 43bit for each data.",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"DRAM PCM NVM Cache",frontmatter:{title:"DRAM PCM NVM Cache",date:"2023-05-12T00:00:00.000Z",permalink:"/pages/24760e/"},regularPath:"/01.hbm/04.DRAM_PCM_NVM_Cache.html",relativePath:"01.hbm/04.DRAM_PCM_NVM_Cache.md",key:"v-9842a21a",path:"/pages/24760e/",headers:[{level:3,title:"Me",slug:"me",normalizedTitle:"me",charIndex:88}],headersStr:"Me",content:" 1. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures\n 2. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture\n 3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method\n 4. Page Placement in Hybrid Memory Systems\n\n----------------------------------------\n\n# 1. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures\n\nLRU: Even though it requires only constant time and space overhead, LRU has a critical weakness in virtual memory environments. On every memory hit, LRU needs to move a page to the most recently used (MRU) position in the list. This involves list manipulations that cannot be handled by the paging unit hardware. CLOCK: Specifically, on a hit to a page, the paging unit hardware sets the reference bit of the page to 1 when a read or a write reference for that page occurs, and sets the dirty bit to 1 when a write reference occurs. Then, pages are maintained in a circular list. In the course of the scan, for every page with reference bit 1, CLOCK clears it to zero, without removing the page from the list.\n\nThe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. Even though CLOCK does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nLRU maintains the temporal locality. Frequency of Write Reference collect statistics of reference cnter.\n\nThe shape of the curves in these figures can be modeled as a monotonic decreasing function, implying that a more recently referenced page is more likely to be written in the near future.\n\nSpecifically, we can observe ranking inversion of temporal locality, i.e., a more recently used page shows a smaller fraction of writes for some ranking ranges.\n\nIn Fig. 3, x axis is the ranking by LRU. y axis represents the number of write references of the page ranking in x-axis.\n\nIn Fig. 4, the x-axis represents the ranking of pages based on their past write counts (black plot) and read/write counts (gray plot). The y-axis represents the number of writes occurring on that ranking.\n\nThe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. Even though CLOCK does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nThis indicates that frequency based estimations are more accurate compared to temporal locality based estimations for most cases. Specifically, frequency based stimations indicate that a wide range of top ranking pages, that is, pages that have been written to frequently in the past, are likely to be written to again in the future.\n\nIn summary, write frequency is generally a better estimator than temporal locality in predicting the re-reference likelihood of write references, but the very recent past write history is also a strong indicator of future writes.\n\nMe In pic4, the axis x is also ranked by number of reference. That's why its x axis can correlates with y axis. Maybe 80% rule can also explain this.\n\n----------------------------------------\n\n# 2. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture\n\nContrary to CLOCK-DWF that places page faults issued by read requests on NVM, the proposed scheme moves all pages from disk to DRAM area. This is motivated by the fact that moving to either NVM or DRAM will result in a page write in NVM since the DRAM is always full and moving a data page to DRAM will issue an eviction to NVM. Therefore, the cost of moving to NVM or DRAM is the same in terms of writes in NVM. The newly accessed data pages have higher probability of access compared to the older data pages and moving this new page to DRAM will result in increase in DRAM hit ratio instead of NVM hit ratio.\n\nFirst,it requires an ordering scheme in order to identify data pages that are cold but will be accessed once in a long time. These data pages will reside long enough in NVM to have a high counter values and therefore will be moved to DRAM where they cannot compete with hot data pages and will return to NVM which makes their migration to DRAM without any benefits. Second, there is no difference between pages that are frequently accessed and typically reside near the head of the NVM LRU queue for the entire time and data pages which go back and forth in the queue.\n\nThe housekeeping information will be only stored for a few percentage of top positions in the NVM LRU queue. Once a data page moves to the end of this selected percentage of LRU, the corresponding counter will be reset to zero. This will handle both ordering scheme and identifying burst data accesses.\n\nFinding the data page in DRAM will result in a normal LRU housekeeping. Otherwise, the extra housekeeping information in NVM will be updated based on the request type. The read and write counters will be stored for readperc and writeperc top data pages in the NVM, respectively. [Still confused why they have readperc and writeperc]\n\nThe values of read threshold and write threshold determine how aggressive we plan to prevent the migrations with low probability of being useful.\n\nTo this end, we use two Least Recently Used (LRU) queues (one for DRAM and one for NVM) and optimize the LRU queue for NVM to prevent nonbeneficial migrations to DRAM.\n\n# 3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method\n\nComments on the previous paper: CLOCK-DWF [19]. CLOCK-DWF first proposes to load the write-request pages into DRAM. For new pages, if it is for a write request, it will be swapped into DRAM. Otherwise, it will be placed in NVRAM. For pages stored in NVRAM, if one is hit by a write request, it will be migrated from NVRAM to DRAM. At this time when DRAM is full, CLOCK-DWF will select a victim page that has the lowest number of writes or that has not been accessed for the longest period of time in DRAM to be evicted. Double LRU [20]. Double LRU recognizes the high migration cost between NVRAM and DRAM, and tries to restrict the number of page migrations by setting some threshold. It uses two separate LRU linked lists to manage pages in DRAM and NVRAM. For each page in NVRAM, it maintains a read/write request count. When a page is accessed, Double LRU checks its read/write request count, and if the count reaches a certain threshold, it will be migrated from NVRAM to DRAM; otherwise, it will remain in NVRAM. In DRAM, the page at the end of the LRU list is always selected as the victim. For new pages, Double LRU stores them directly in DRAM, assuming new pages will be accessed frequently in the near future. regardless of the read or write requests.\n\nUIMigrate consists of three parts: unified hot page identification, page migration, and self-adaptive adjustment.\n\nTo consider both the number of accesses and access time,we add the attenuation factor to quantify the hotness of each page, hoping to quickly reduce the access counts for pages that are accessed a long time ago. Thus, while updating page hotness upon each access, UIMigrate also uses an attenuation coefficient to lower the page popularity of old accesses.\n\nIf all DRAM pages should be accessed in one cycle, (acc_count.global-acc.countpage)/DRAMsize denotes the number of cycles the page that has not been accessed.\n\nthe attenuation coefficient d is closely related to the value of hotold and the number of cycles the page that has not been accessed since last time.\n\nUIMigrate sets a threshold, called new page threshold, to measure the hotness of each victim page. When the quantified hotness of a selected victim is larger than the preset threshold, it means that this victim page is too hot to be evicted, and so UIMigrate will store the new page in NVRAM. Otherwise, it will be migrated to DRAM.\n\nIn order to effectively adapt to the change of access patterns, UIMigrate adjusts migration thresholds (new page threshold, hot page threshold and cold page threshold) automatically to promote or suppress the page migrations, according to real-time migration revenue.\n\nWhen they are evicted from DRAM, UIMigrate calculates the migration revenue based on Equations (3) and (4). If the migration revenue is below zero, it means that the migration cost is greater than the benefit. In this case, UIMigrate will increase hot page threshold to prevent certain pages from getting hot in NVRAM and decrease cold page threshold to prevent some pages from becoming cold in DRAM, thus retaining more pages in NVRAM. For new pages, UIMigrate will also reduce new page threshold, so that more new pages will go to NVRAM instead of DRAM. When the calculated migration benefit is larger than the migration cost, UIMigare will reduce hot page threshold and increase cold page threshold to make migrate more pages to DRAM, and increase new page threshold to keep more new pages in DRAM.\n\n# 4. Page Placement in Hybrid Memory Systems\n\nGiven the characteristics of DRAM and PCM, RaPP seeks to (1) place performance-critical pages and frequently written pages in DRAM (2) place non-critical pages and rarely written pages in PCM (3) spread writes to PCM across many physical frames.\n\nUsing this information, RaPP dynamically ranks frames based on frequency and recency of accesses, as detailed below. Frames that rank high are called “popular”, and frames that rank low are called “unpopular”.\n\n# Algorithm\n\n 1. The descriptors in queue M − 1 represent the blocks that are most frequently used. On the first access to a block, its descriptor is placed in the tail of queue 0.\n 2. In addition, the block’s expiration time ExpirationTime is set to CurrentTime + LifeTime, where both times are measured in number of accesses and LifeT ime specifies the number of consecutive accesses that n must directed to other blocks before we expire the block.\n 3. Every time the block is accessed, its reference counter is incremented, its expiration time is reset to CurrentT ime + LifeT ime, and its descriptor is moved to the tail of its current queue.\n 4. The descriptor of a frequently used block is promoted to a higher queue (saturating at queue M − 1, of course) after a certain number of accesses to the block.\n 5. Specifically, if the descriptor is currently in queue i, it will be upgraded to queue i + 1 when its reference counter reaches 2i+1.\n 6. Conversely, MQ demotes blocks that have not been accessed recently. On each access, the descriptors at the heads of all M queues (representing the LRU block of each queue) are checked for expiration (CurrentT ime > ExpirationTime).\n\nIf a block descriptor expires, it is placed at the tail of the immediately inferior queue, and has its expiration time again set to CurrentTime + LifeTime.\n\nFirst,instead of counting all accesses, we only count an access if it occurs more than a threshold time (measured in memory cycles) after the last access to the same frame. This latter threshold is called the “filter threshold”. The MC stores the time of the last access in the descriptor for the frame. Using a 2-competitive approach, we set the filter threshold to be MigrationCost/MigrationThreshold, where MigrationCost is the uncontended number of memory cycles needed to migrate a page. (MigrationCost is roughly 1.6µs in our experiments.)\n\nSecond, we modified the demotion policy in the following ways: (a) we use time, not number of accesses, as the metric for demotion to reduce space requirements (in our experiments, we set LifeT ime to 100µs, which works well for our workloads); (b) we only demote from one queue at a time (in round-robin fashion) to reduce runtime overhead; (c) a DRAM frame that is demoted twice without any intervening accesses leaves the MQ queues and becomes a candidate to receive a popular PCM page.\n\nTo select a destination DRAM frame for a page, the MC maintains an LRU list of victim DRAM frames. The victim frames are not in any of the LRU queues (the list is initialized with all DRAM frames).\n\nTo effect a page migration to DRAM, the MC (1) migrates the page stored in the selected DRAM frame to one of the unranked PCM frames, (2) migrates the content of this latter frame to the most popular PCM frame, and finally (3) migrates the content of the most popular PCM frame to the selected DRAM frame.\n\n\n# Me\n\nWhy swap 3 time? not 2?",normalizedContent:" 1. clock-dwf: a write-history-aware page replacement algorithm for hybrid pcm and dram memory architectures\n 2. an operating system level data migration scheme in hybrid dram-nvm memory architecture\n 3. apmigration: improving performance of hybrid memory performance via an adaptive page migration method\n 4. page placement in hybrid memory systems\n\n----------------------------------------\n\n# 1. clock-dwf: a write-history-aware page replacement algorithm for hybrid pcm and dram memory architectures\n\nlru: even though it requires only constant time and space overhead, lru has a critical weakness in virtual memory environments. on every memory hit, lru needs to move a page to the most recently used (mru) position in the list. this involves list manipulations that cannot be handled by the paging unit hardware. clock: specifically, on a hit to a page, the paging unit hardware sets the reference bit of the page to 1 when a read or a write reference for that page occurs, and sets the dirty bit to 1 when a write reference occurs. then, pages are maintained in a circular list. in the course of the scan, for every page with reference bit 1, clock clears it to zero, without removing the page from the list.\n\nthe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. even though clock does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nlru maintains the temporal locality. frequency of write reference collect statistics of reference cnter.\n\nthe shape of the curves in these figures can be modeled as a monotonic decreasing function, implying that a more recently referenced page is more likely to be written in the near future.\n\nspecifically, we can observe ranking inversion of temporal locality, i.e., a more recently used page shows a smaller fraction of writes for some ranking ranges.\n\nin fig. 3, x axis is the ranking by lru. y axis represents the number of write references of the page ranking in x-axis.\n\nin fig. 4, the x-axis represents the ranking of pages based on their past write counts (black plot) and read/write counts (gray plot). the y-axis represents the number of writes occurring on that ranking.\n\nthe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. even though clock does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nthis indicates that frequency based estimations are more accurate compared to temporal locality based estimations for most cases. specifically, frequency based stimations indicate that a wide range of top ranking pages, that is, pages that have been written to frequently in the past, are likely to be written to again in the future.\n\nin summary, write frequency is generally a better estimator than temporal locality in predicting the re-reference likelihood of write references, but the very recent past write history is also a strong indicator of future writes.\n\nme in pic4, the axis x is also ranked by number of reference. that's why its x axis can correlates with y axis. maybe 80% rule can also explain this.\n\n----------------------------------------\n\n# 2. an operating system level data migration scheme in hybrid dram-nvm memory architecture\n\ncontrary to clock-dwf that places page faults issued by read requests on nvm, the proposed scheme moves all pages from disk to dram area. this is motivated by the fact that moving to either nvm or dram will result in a page write in nvm since the dram is always full and moving a data page to dram will issue an eviction to nvm. therefore, the cost of moving to nvm or dram is the same in terms of writes in nvm. the newly accessed data pages have higher probability of access compared to the older data pages and moving this new page to dram will result in increase in dram hit ratio instead of nvm hit ratio.\n\nfirst,it requires an ordering scheme in order to identify data pages that are cold but will be accessed once in a long time. these data pages will reside long enough in nvm to have a high counter values and therefore will be moved to dram where they cannot compete with hot data pages and will return to nvm which makes their migration to dram without any benefits. second, there is no difference between pages that are frequently accessed and typically reside near the head of the nvm lru queue for the entire time and data pages which go back and forth in the queue.\n\nthe housekeeping information will be only stored for a few percentage of top positions in the nvm lru queue. once a data page moves to the end of this selected percentage of lru, the corresponding counter will be reset to zero. this will handle both ordering scheme and identifying burst data accesses.\n\nfinding the data page in dram will result in a normal lru housekeeping. otherwise, the extra housekeeping information in nvm will be updated based on the request type. the read and write counters will be stored for readperc and writeperc top data pages in the nvm, respectively. [still confused why they have readperc and writeperc]\n\nthe values of read threshold and write threshold determine how aggressive we plan to prevent the migrations with low probability of being useful.\n\nto this end, we use two least recently used (lru) queues (one for dram and one for nvm) and optimize the lru queue for nvm to prevent nonbeneficial migrations to dram.\n\n# 3. apmigration: improving performance of hybrid memory performance via an adaptive page migration method\n\ncomments on the previous paper: clock-dwf [19]. clock-dwf first proposes to load the write-request pages into dram. for new pages, if it is for a write request, it will be swapped into dram. otherwise, it will be placed in nvram. for pages stored in nvram, if one is hit by a write request, it will be migrated from nvram to dram. at this time when dram is full, clock-dwf will select a victim page that has the lowest number of writes or that has not been accessed for the longest period of time in dram to be evicted. double lru [20]. double lru recognizes the high migration cost between nvram and dram, and tries to restrict the number of page migrations by setting some threshold. it uses two separate lru linked lists to manage pages in dram and nvram. for each page in nvram, it maintains a read/write request count. when a page is accessed, double lru checks its read/write request count, and if the count reaches a certain threshold, it will be migrated from nvram to dram; otherwise, it will remain in nvram. in dram, the page at the end of the lru list is always selected as the victim. for new pages, double lru stores them directly in dram, assuming new pages will be accessed frequently in the near future. regardless of the read or write requests.\n\nuimigrate consists of three parts: unified hot page identification, page migration, and self-adaptive adjustment.\n\nto consider both the number of accesses and access time,we add the attenuation factor to quantify the hotness of each page, hoping to quickly reduce the access counts for pages that are accessed a long time ago. thus, while updating page hotness upon each access, uimigrate also uses an attenuation coefficient to lower the page popularity of old accesses.\n\nif all dram pages should be accessed in one cycle, (acc_count.global-acc.countpage)/dramsize denotes the number of cycles the page that has not been accessed.\n\nthe attenuation coefficient d is closely related to the value of hotold and the number of cycles the page that has not been accessed since last time.\n\nuimigrate sets a threshold, called new page threshold, to measure the hotness of each victim page. when the quantified hotness of a selected victim is larger than the preset threshold, it means that this victim page is too hot to be evicted, and so uimigrate will store the new page in nvram. otherwise, it will be migrated to dram.\n\nin order to effectively adapt to the change of access patterns, uimigrate adjusts migration thresholds (new page threshold, hot page threshold and cold page threshold) automatically to promote or suppress the page migrations, according to real-time migration revenue.\n\nwhen they are evicted from dram, uimigrate calculates the migration revenue based on equations (3) and (4). if the migration revenue is below zero, it means that the migration cost is greater than the benefit. in this case, uimigrate will increase hot page threshold to prevent certain pages from getting hot in nvram and decrease cold page threshold to prevent some pages from becoming cold in dram, thus retaining more pages in nvram. for new pages, uimigrate will also reduce new page threshold, so that more new pages will go to nvram instead of dram. when the calculated migration benefit is larger than the migration cost, uimigare will reduce hot page threshold and increase cold page threshold to make migrate more pages to dram, and increase new page threshold to keep more new pages in dram.\n\n# 4. page placement in hybrid memory systems\n\ngiven the characteristics of dram and pcm, rapp seeks to (1) place performance-critical pages and frequently written pages in dram (2) place non-critical pages and rarely written pages in pcm (3) spread writes to pcm across many physical frames.\n\nusing this information, rapp dynamically ranks frames based on frequency and recency of accesses, as detailed below. frames that rank high are called “popular”, and frames that rank low are called “unpopular”.\n\n# algorithm\n\n 1. the descriptors in queue m − 1 represent the blocks that are most frequently used. on the first access to a block, its descriptor is placed in the tail of queue 0.\n 2. in addition, the block’s expiration time expirationtime is set to currenttime + lifetime, where both times are measured in number of accesses and lifet ime specifies the number of consecutive accesses that n must directed to other blocks before we expire the block.\n 3. every time the block is accessed, its reference counter is incremented, its expiration time is reset to currentt ime + lifet ime, and its descriptor is moved to the tail of its current queue.\n 4. the descriptor of a frequently used block is promoted to a higher queue (saturating at queue m − 1, of course) after a certain number of accesses to the block.\n 5. specifically, if the descriptor is currently in queue i, it will be upgraded to queue i + 1 when its reference counter reaches 2i+1.\n 6. conversely, mq demotes blocks that have not been accessed recently. on each access, the descriptors at the heads of all m queues (representing the lru block of each queue) are checked for expiration (currentt ime > expirationtime).\n\nif a block descriptor expires, it is placed at the tail of the immediately inferior queue, and has its expiration time again set to currenttime + lifetime.\n\nfirst,instead of counting all accesses, we only count an access if it occurs more than a threshold time (measured in memory cycles) after the last access to the same frame. this latter threshold is called the “filter threshold”. the mc stores the time of the last access in the descriptor for the frame. using a 2-competitive approach, we set the filter threshold to be migrationcost/migrationthreshold, where migrationcost is the uncontended number of memory cycles needed to migrate a page. (migrationcost is roughly 1.6µs in our experiments.)\n\nsecond, we modified the demotion policy in the following ways: (a) we use time, not number of accesses, as the metric for demotion to reduce space requirements (in our experiments, we set lifet ime to 100µs, which works well for our workloads); (b) we only demote from one queue at a time (in round-robin fashion) to reduce runtime overhead; (c) a dram frame that is demoted twice without any intervening accesses leaves the mq queues and becomes a candidate to receive a popular pcm page.\n\nto select a destination dram frame for a page, the mc maintains an lru list of victim dram frames. the victim frames are not in any of the lru queues (the list is initialized with all dram frames).\n\nto effect a page migration to dram, the mc (1) migrates the page stored in the selected dram frame to one of the unranked pcm frames, (2) migrates the content of this latter frame to the most popular pcm frame, and finally (3) migrates the content of the most popular pcm frame to the selected dram frame.\n\n\n# me\n\nwhy swap 3 time? not 2?",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Dynamically Adapting  Page Migration Policies Based on Applications Memory Access Behaviors",frontmatter:{title:"Dynamically Adapting  Page Migration Policies Based on Applications Memory Access Behaviors",date:"2023-05-11T00:00:00.000Z",permalink:"/pages/24769f/"},regularPath:"/01.hbm/03.Dynamically_Adapting%20_Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.html",relativePath:"01.hbm/03.Dynamically_Adapting _Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.md",key:"v-b2fd417c",path:"/pages/24769f/",headersStr:null,content:'Year: 2021 Mem : HBM & PCM\n\n * migration friendly\n * migration unfriendly\n\nBased on previous research "On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems" from the same author.\n\n 1. Adaptive migration polices Our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n 2. AR overheads can defeat the benefits of page migration To eliminate AR, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. AR: OS tables (translation look-aside buffers (TLBs), page tables) must also be updated since physical addresses (PAs) in such memory systems are based on the physical location of pages and a migration changes PAs: we call this process of changing PAs and updating system tables address reconciliation (AR).\n\nWe discovered that an exponential-shaped histogram indicates that very few pages receive most accesses and that those applications benefit by either placing those few pages in the faster (HBM) memory at the start of execution, or migrated to HBM on demand.\n\nMcf 3% of all pages cause 97% of memory accesses. Milc 65% of pages contribute to 82% of all accesses.\n\nIf most of all pages receive about the same number of accesses, implying that too many pages may be migrated if a fixed hotness threshold is used for migrating pages, and the migration overheads outweigh performance gains. Doubt about this statement.\n\nMigration of pages to faster memories results in performance gains if those pages continue to be heavily used, because these accesses will be satisfied by faster memories.\n\nLinux3 performs the following functions when the virtual to PA mapping of a page is changed.\n\n 1. flush_cache_page\n 2. change PTE\n 3. flush_tlb_page\n\n# Key Insights\n\nAdaptive migration polices: Previous page migration techniques relied on fixed hotness thresholds: a page is migrated from slow memories to faster memories when the number of times that page was accessed exceeds the hotness threshold. In contrast, we control page migration policies based on applications’ memory access behaviors. Our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n\nAR overheads can defeat the benefits of page migration: To eliminate AR, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. Reverse migration makes page migration invisible to the OS. However, reverse migrations can result in excessive data movement between slow and fast memories. In this work, we evaluate the effectiveness of the reverse migration technique.\n\n# Algorithm\n\nIf the count is high (too many pages have been migrated), we double the hotness threshold to reduce future migrations; likewise, if too few pages have been migrated in a twindow, we halve the hotness threshold to increase future Adaptive Migration Based on Number of Pages Migrated migrations. In our experiments, we used 4 million cycles as our twindow.6 We also limit the hotness threshold variations between 64 and 256.\n\nWe define the MBQ as the average number of accesses to pages that were recently migrated to HBM.\n\n\n\n * threshold adaption We increase the threshold if more than 240 pages have been migrated in a window and reduce the threshold if fewer than 160 pages have been migrated in a window.\n * pause and resume migration If the MBQ is less than a threshold (min_MBQ), then migrations are halted. migrations are resumed if the MBQ is greater than another threshold (max_MBQ).\n\nAdaptive Migration Based on the MBQ\n\n# Summary\n\nTwo Algorithm\n\n 1. Based on number of page migrated, if too many page is migrared, reduce migration by increase threshold.\n 2. Based on reference after migration, if too less after migration, reduce or stop.',normalizedContent:'year: 2021 mem : hbm & pcm\n\n * migration friendly\n * migration unfriendly\n\nbased on previous research "on-the-fly page migration and address reconciliation for heterogeneous memory systems" from the same author.\n\n 1. adaptive migration polices our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n 2. ar overheads can defeat the benefits of page migration to eliminate ar, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. ar: os tables (translation look-aside buffers (tlbs), page tables) must also be updated since physical addresses (pas) in such memory systems are based on the physical location of pages and a migration changes pas: we call this process of changing pas and updating system tables address reconciliation (ar).\n\nwe discovered that an exponential-shaped histogram indicates that very few pages receive most accesses and that those applications benefit by either placing those few pages in the faster (hbm) memory at the start of execution, or migrated to hbm on demand.\n\nmcf 3% of all pages cause 97% of memory accesses. milc 65% of pages contribute to 82% of all accesses.\n\nif most of all pages receive about the same number of accesses, implying that too many pages may be migrated if a fixed hotness threshold is used for migrating pages, and the migration overheads outweigh performance gains. doubt about this statement.\n\nmigration of pages to faster memories results in performance gains if those pages continue to be heavily used, because these accesses will be satisfied by faster memories.\n\nlinux3 performs the following functions when the virtual to pa mapping of a page is changed.\n\n 1. flush_cache_page\n 2. change pte\n 3. flush_tlb_page\n\n# key insights\n\nadaptive migration polices: previous page migration techniques relied on fixed hotness thresholds: a page is migrated from slow memories to faster memories when the number of times that page was accessed exceeds the hotness threshold. in contrast, we control page migration policies based on applications’ memory access behaviors. our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n\nar overheads can defeat the benefits of page migration: to eliminate ar, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. reverse migration makes page migration invisible to the os. however, reverse migrations can result in excessive data movement between slow and fast memories. in this work, we evaluate the effectiveness of the reverse migration technique.\n\n# algorithm\n\nif the count is high (too many pages have been migrated), we double the hotness threshold to reduce future migrations; likewise, if too few pages have been migrated in a twindow, we halve the hotness threshold to increase future adaptive migration based on number of pages migrated migrations. in our experiments, we used 4 million cycles as our twindow.6 we also limit the hotness threshold variations between 64 and 256.\n\nwe define the mbq as the average number of accesses to pages that were recently migrated to hbm.\n\n\n\n * threshold adaption we increase the threshold if more than 240 pages have been migrated in a window and reduce the threshold if fewer than 160 pages have been migrated in a window.\n * pause and resume migration if the mbq is less than a threshold (min_mbq), then migrations are halted. migrations are resumed if the mbq is greater than another threshold (max_mbq).\n\nadaptive migration based on the mbq\n\n# summary\n\ntwo algorithm\n\n 1. based on number of page migrated, if too many page is migrared, reduce migration by increase threshold.\n 2. based on reference after migration, if too less after migration, reduce or stop.',charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"memory-ecc",frontmatter:{title:"memory-ecc",date:"2023-05-31T17:24:24.000Z",permalink:"/pages/f07695/"},regularPath:"/01.hbm/06.memory%20ecc.html",relativePath:"01.hbm/06.memory ecc.md",key:"v-171f66aa",path:"/pages/f07695/",headersStr:null,content:"Ecc length for different length of data.\n\nhttps://perswww.kuleuven.be/~u0068190/Onderwijs/Extra_info/Hamming%20ecc.pdf\n\n\n\nhttp://www.sxlist.com/techref/method/error/hamming.htm\n\n 1. 4-bit data path requires 3 bits for ECC (8 entry table) (75% increase in size)\n 2. 8-bit data path requires 5 bits for ECC or 1 bit for parity.\n 3. 11-bit data path requires 4 bits for ECC (16 entry table)\n 4. 16-bit data path requires 6 bit for ECC or 2 bits for parity\n 5. 32-bit data path requires 7 bits for ECC or 4 bits for parity (21.8% increase in size)\n 6. 64-bit (8 byte) data path requires 8 bits for ECC and parity (12.5% increase in size)\n 7. 128-bit (16 bytes) data path requires 9 bits for ECC or 16 bits for parity (7% increase in size\n\nUse ECC bit for compression CRAM: Efficient Hardware-Based Memory Compression for Bandwidth Enhancement\n\nEnabling Technologies for Memory Compression: Metadata, Mapping, and Prediction\n\nUse ECC bit for DRAM Cache To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems",normalizedContent:"ecc length for different length of data.\n\nhttps://perswww.kuleuven.be/~u0068190/onderwijs/extra_info/hamming%20ecc.pdf\n\n\n\nhttp://www.sxlist.com/techref/method/error/hamming.htm\n\n 1. 4-bit data path requires 3 bits for ecc (8 entry table) (75% increase in size)\n 2. 8-bit data path requires 5 bits for ecc or 1 bit for parity.\n 3. 11-bit data path requires 4 bits for ecc (16 entry table)\n 4. 16-bit data path requires 6 bit for ecc or 2 bits for parity\n 5. 32-bit data path requires 7 bits for ecc or 4 bits for parity (21.8% increase in size)\n 6. 64-bit (8 byte) data path requires 8 bits for ecc and parity (12.5% increase in size)\n 7. 128-bit (16 bytes) data path requires 9 bits for ecc or 16 bits for parity (7% increase in size\n\nuse ecc bit for compression cram: efficient hardware-based memory compression for bandwidth enhancement\n\nenabling technologies for memory compression: metadata, mapping, and prediction\n\nuse ecc bit for dram cache to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches tictoc: enabling bandwidth-efficient dram caching for both hits and misses in hybrid memory systems",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"hbm-latency",frontmatter:{title:"hbm-latency",date:"2023-05-31T17:24:24.000Z",permalink:"/pages/f07696/"},regularPath:"/01.hbm/07.hbm-latency.html",relativePath:"01.hbm/07.hbm-latency.md",key:"v-788071c5",path:"/pages/f07696/",headersStr:null,content:" 1. HBM latency\n\nIn flat mode on Knight’s Landing, MCDRAM latency is around 176 ns, while a DDR4 access has a latency of 147 ns.\n\nCited from blog: Knight’s Landing: Atom with AVX-512.\n\nhttps://chipsandcheese.com/2022/12/08/knights-landing-atom-with-avx-512/\n\nWe report 154.0 ns latency for HBM and 130.4 ns for DRAM.\n\nPaper: Exploring the Performance Benefit of Hybrid Memory System on HPC Environments.\n\n 2. The latency and bandwidth comparison of HBM and DRAM Paper: [UPC Phd Thesis] Memory Bandwidth and Latency in HPC: System Requirements and Performance Impact.",normalizedContent:" 1. hbm latency\n\nin flat mode on knight’s landing, mcdram latency is around 176 ns, while a ddr4 access has a latency of 147 ns.\n\ncited from blog: knight’s landing: atom with avx-512.\n\nhttps://chipsandcheese.com/2022/12/08/knights-landing-atom-with-avx-512/\n\nwe report 154.0 ns latency for hbm and 130.4 ns for dram.\n\npaper: exploring the performance benefit of hybrid memory system on hpc environments.\n\n 2. the latency and bandwidth comparison of hbm and dram paper: [upc phd thesis] memory bandwidth and latency in hpc: system requirements and performance impact.",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"compression",frontmatter:{title:"compression",date:"2023-12-20T17:24:24.000Z",permalink:"/pages/f07698/"},regularPath:"/01.hbm/08.compression.html",relativePath:"01.hbm/08.compression.md",key:"v-25995465",path:"/pages/f07698/",headersStr:null,content:" 1. Unified Compilation for Lossless Compression and Sparse Computing\n\ncompute in compressed data && compression on sparse matrix\n\n 2.  Memory Access Granularity Aware Lossless Compression for GPUs\n\n 3.  Linearly Compressed Pages: A Low-Complexity, Low-Latency Main Memory Compression Framework\n\n 4.  FlatPack: Flexible Compaction of Compressed Memory\n\n 5.  Compresso: Pragmatic Main Memory Compression\n\n 6.  Compacted CPU/GPU Data Compression via Modified Virtual Address Translation\n\n 7.  CMH: Compression Management for Improving Capacity in the Hybrid Memory Cube\n\n 8.  Buri: Scaling Big-Memory Computing with Hardware-Based Memory Expansion\n\n 9.  Compress Objects, Not Cache Lines: An Object-Based Compressed Memory Hierarchy\n\n 10. Base-Delta-Immediate Compression:Practical Data Compression for On-Chip Caches\n\n 11. Frequent Pattern Compression: A Significance-Based Compression Scheme for L2 Caches\n\n 12. \n\n 13. BCD Deduplication: Effective Memory Compression using Partial Cache-Line Deduplication\n\n 14. Could Compression Be of General Use? Evaluating Memory Compression across Domains\n\n 15. Linearly Compressed Pages: A Low-Complexity, Low-Latency Main Memory Compression Framework\n\n 16. Compresso: Pragmatic Main Memory Compression\n\n 17. FlatPack: Flexible Compaction of Compressed Memory",normalizedContent:" 1. unified compilation for lossless compression and sparse computing\n\ncompute in compressed data && compression on sparse matrix\n\n 2.  memory access granularity aware lossless compression for gpus\n\n 3.  linearly compressed pages: a low-complexity, low-latency main memory compression framework\n\n 4.  flatpack: flexible compaction of compressed memory\n\n 5.  compresso: pragmatic main memory compression\n\n 6.  compacted cpu/gpu data compression via modified virtual address translation\n\n 7.  cmh: compression management for improving capacity in the hybrid memory cube\n\n 8.  buri: scaling big-memory computing with hardware-based memory expansion\n\n 9.  compress objects, not cache lines: an object-based compressed memory hierarchy\n\n 10. base-delta-immediate compression:practical data compression for on-chip caches\n\n 11. frequent pattern compression: a significance-based compression scheme for l2 caches\n\n 12. \n\n 13. bcd deduplication: effective memory compression using partial cache-line deduplication\n\n 14. could compression be of general use? evaluating memory compression across domains\n\n 15. linearly compressed pages: a low-complexity, low-latency main memory compression framework\n\n 16. compresso: pragmatic main memory compression\n\n 17. flatpack: flexible compaction of compressed memory",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"compressibility prediction",frontmatter:{title:"compressibility prediction",date:"2024-04-04T11:42:24.000Z",permalink:"/pages/f07699/"},regularPath:"/01.hbm/09.compressibility_prediction.html",relativePath:"01.hbm/09.compressibility_prediction.md",key:"v-662ff4fd",path:"/pages/f07699/",headers:[{level:3,title:"1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO]",slug:"_1-attach-e-towards-ideal-memory-compression-by-mitigating-metadata-bandwidth-overheads-micro",normalizedTitle:"1. attach´e: towards ideal memory compression by mitigating metadata bandwidth overheads [micro]",charIndex:347},{level:3,title:"2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA]",slug:"_2-cram-enabling-transparent-memory-compression-for-commodity-memory-systems-hpca",normalizedTitle:"2. cram enabling transparent memory-compression for commodity memory systems [hpca]",charIndex:1311},{level:3,title:"3. MBZip: Multiblock Data Compression [TACO]",slug:"_3-mbzip-multiblock-data-compression-taco",normalizedTitle:"3. mbzip: multiblock data compression [taco]",charIndex:2596},{level:3,title:"4. Compresso: Pragmatic Main Memory Compression",slug:"_4-compresso-pragmatic-main-memory-compression",normalizedTitle:"4. compresso: pragmatic main memory compression",charIndex:245},{level:3,title:"5. Enabling Technologies for Memory Compression:Metadata, Mapping, and Prediction",slug:"_5-enabling-technologies-for-memory-compression-metadata-mapping-and-prediction",normalizedTitle:"5. enabling technologies for memory compression:metadata, mapping, and prediction",charIndex:4436}],headersStr:"1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO] 2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA] 3. MBZip: Multiblock Data Compression [TACO] 4. Compresso: Pragmatic Main Memory Compression 5. Enabling Technologies for Memory Compression:Metadata, Mapping, and Prediction",content:" 1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO 2018]\n 2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA 2019]\n 3. MBZip: Multiblock Data Compression [TACO 2017]\n 4. Compresso: Pragmatic Main Memory Compression [MICRO]\n\n----------------------------------------\n\n\n# 1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO]\n\nYear: 2018\n\nAttach´e does not use the free space made available by compression.\n\nCompression Predictor (COPR), predicts if the memory block is compressed.\n\nGlobal Indication(GI): GI is composed of eight two-bit saturating counters, each of which keeps track of the compressibility of 18th the memory space. GI can be used as an accurate indicator for predicting the compressibility within a memory space if there is abundant similarity in compressibility.\n\nPage-Level Predictor (PaPR): By exploiting the similarity in the compressibility of cachelines within an OS page [12], [18], [37], PaPR provides compression predictions at the page granularity.\n\nLine-Level Predictor (LiPR): LiPR is a set-associative cache structure indexed by the page number. LiPR uses the two-bit values of PaPR to determine if the neighboring cachelines have the same compressibility.\n\n\n# 2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA]\n\nYear: 2019\n\nTransparent Memory-Compression (TMC) can provide bandwidth benefits of memory compression in an OS-transparent manner by trying to exploit only the increased bandwidth and not the extra capacity.\n\nLine Location Predictor (LLP) that can determine the location of the line with 98% accuracy and a dynamic solution that disables compression if the benefits of compression are smaller than the overheads.\n\nIf we use HBM, we dont need to care too much about the bandwidth and metadata.\n\nWe propose a history-based Line Location Predictor (LLP), that can identify the correct location of the line with a high accuracy (98%). The LLP is based on the observation that lines within a page tend to have similar compressibility.\n\nLLP contains the Last Compressibility Table (LCT), that tracks the last compression status seen for a given index. The LCT is indexed with the hash of the page address. So, for a given access, the index corresponding to the page address is used to predict the compressibility, then line location.\n\n\n\nEven though the LLP is quite small, it provides an accuracy of 98%, much higher than the hit-rate of the metadata cache.\n\nLCP stores metadata inline with block.\n\n\n\n\n\n\n# 3. MBZip: Multiblock Data Compression [TACO]\n\nA write to a location in memory may not change the existing data in that location and is thus a redundant write. Such write requests to cache have been termed as silent stores [25]/writes [21].\n\nWe find that, on average, across 21 benchmarks, 9.6% of the writes are silent. More than 15% of the writes are silent in benchmarks such as bwaves, GemsFDTD, lbm, leslie3d, mcf, mesa, sjeng, soplex, vortex2, and zeusmp.\n\nIn such a scenario, we essentially issue one read request (to read the existing data) and no write request. However, if the write request is not silent, we add the overhead of a read request to the existing write request.\n\nWe observe that there is a strong correlation to a write being silent or nonsilent both across writes made to the same address during the course of the program execution and across writes to consecutive addresses. To exploit this correlation, we propose using a 2b bimodal predictor (indexed using the page addresses) to predict whether a write request is silent. The accuracy of our predictor (4kB structure) is around 94.4%, on average.\n\n\n# 4. Compresso: Pragmatic Main Memory Compression\n\n\n\nWe associate a 2-bit saturating counter with each entry in the metadata cache (Fig. 5b). The counter is incremented when any writeback to the associated page results in a cache line overflow and is decremented upon cache line underflows (i.e., new data being more compressible).\n\nAnother 3-bit global predictor changes state based on page overflows in the system. We speculatively increase a page’s size to the maximum (4KB) when the local as well as global predictors have the higher bit set.\n\nHence, a page is stored uncompressed if it receives multiple streaming cache line overflows during a phase when the overall system is experiencing page overflows\n\n\n# 5. Enabling Technologies for Memory Compression:Metadata, Mapping, and Prediction\n\n\n\n\n\nMetadata is coexist with data. Thus they have to predict, even read.\n\nReads are problematic because the size of the block is encoded in the block itself. Therefore, the read has to either be performed in two phases (read the metadata from the 0th chip, then read data from the appropriate subset of chips) or the read has to conservatively read data from all 9 chips in parallel.\n\nThe first is PCbased, where the PC of the load instruction serves as the index into a predictor table. This assumes that a load tends to access the same type of data record, with relatively uniform compressibility.\n\nThe second is page-based, where the physical page number serves as the index into a predictor table. This assumes that the data records in a single page are of a similar type and have uniform compressibility.\n\nOn a look-up, the highest-valued saturating counter indicates the predicted size of the block. In case of a tie, we conservatively predict the larger block size.\n\nThey keep a counter for each block compression length, and try to keep counter for each block length. Then predict by voting, following majority wins low.",normalizedContent:" 1. attach´e: towards ideal memory compression by mitigating metadata bandwidth overheads [micro 2018]\n 2. cram enabling transparent memory-compression for commodity memory systems [hpca 2019]\n 3. mbzip: multiblock data compression [taco 2017]\n 4. compresso: pragmatic main memory compression [micro]\n\n----------------------------------------\n\n\n# 1. attach´e: towards ideal memory compression by mitigating metadata bandwidth overheads [micro]\n\nyear: 2018\n\nattach´e does not use the free space made available by compression.\n\ncompression predictor (copr), predicts if the memory block is compressed.\n\nglobal indication(gi): gi is composed of eight two-bit saturating counters, each of which keeps track of the compressibility of 18th the memory space. gi can be used as an accurate indicator for predicting the compressibility within a memory space if there is abundant similarity in compressibility.\n\npage-level predictor (papr): by exploiting the similarity in the compressibility of cachelines within an os page [12], [18], [37], papr provides compression predictions at the page granularity.\n\nline-level predictor (lipr): lipr is a set-associative cache structure indexed by the page number. lipr uses the two-bit values of papr to determine if the neighboring cachelines have the same compressibility.\n\n\n# 2. cram enabling transparent memory-compression for commodity memory systems [hpca]\n\nyear: 2019\n\ntransparent memory-compression (tmc) can provide bandwidth benefits of memory compression in an os-transparent manner by trying to exploit only the increased bandwidth and not the extra capacity.\n\nline location predictor (llp) that can determine the location of the line with 98% accuracy and a dynamic solution that disables compression if the benefits of compression are smaller than the overheads.\n\nif we use hbm, we dont need to care too much about the bandwidth and metadata.\n\nwe propose a history-based line location predictor (llp), that can identify the correct location of the line with a high accuracy (98%). the llp is based on the observation that lines within a page tend to have similar compressibility.\n\nllp contains the last compressibility table (lct), that tracks the last compression status seen for a given index. the lct is indexed with the hash of the page address. so, for a given access, the index corresponding to the page address is used to predict the compressibility, then line location.\n\n\n\neven though the llp is quite small, it provides an accuracy of 98%, much higher than the hit-rate of the metadata cache.\n\nlcp stores metadata inline with block.\n\n\n\n\n\n\n# 3. mbzip: multiblock data compression [taco]\n\na write to a location in memory may not change the existing data in that location and is thus a redundant write. such write requests to cache have been termed as silent stores [25]/writes [21].\n\nwe find that, on average, across 21 benchmarks, 9.6% of the writes are silent. more than 15% of the writes are silent in benchmarks such as bwaves, gemsfdtd, lbm, leslie3d, mcf, mesa, sjeng, soplex, vortex2, and zeusmp.\n\nin such a scenario, we essentially issue one read request (to read the existing data) and no write request. however, if the write request is not silent, we add the overhead of a read request to the existing write request.\n\nwe observe that there is a strong correlation to a write being silent or nonsilent both across writes made to the same address during the course of the program execution and across writes to consecutive addresses. to exploit this correlation, we propose using a 2b bimodal predictor (indexed using the page addresses) to predict whether a write request is silent. the accuracy of our predictor (4kb structure) is around 94.4%, on average.\n\n\n# 4. compresso: pragmatic main memory compression\n\n\n\nwe associate a 2-bit saturating counter with each entry in the metadata cache (fig. 5b). the counter is incremented when any writeback to the associated page results in a cache line overflow and is decremented upon cache line underflows (i.e., new data being more compressible).\n\nanother 3-bit global predictor changes state based on page overflows in the system. we speculatively increase a page’s size to the maximum (4kb) when the local as well as global predictors have the higher bit set.\n\nhence, a page is stored uncompressed if it receives multiple streaming cache line overflows during a phase when the overall system is experiencing page overflows\n\n\n# 5. enabling technologies for memory compression:metadata, mapping, and prediction\n\n\n\n\n\nmetadata is coexist with data. thus they have to predict, even read.\n\nreads are problematic because the size of the block is encoded in the block itself. therefore, the read has to either be performed in two phases (read the metadata from the 0th chip, then read data from the appropriate subset of chips) or the read has to conservatively read data from all 9 chips in parallel.\n\nthe first is pcbased, where the pc of the load instruction serves as the index into a predictor table. this assumes that a load tends to access the same type of data record, with relatively uniform compressibility.\n\nthe second is page-based, where the physical page number serves as the index into a predictor table. this assumes that the data records in a single page are of a similar type and have uniform compressibility.\n\non a look-up, the highest-valued saturating counter indicates the predicted size of the block. in case of a tie, we conservatively predict the larger block size.\n\nthey keep a counter for each block compression length, and try to keep counter for each block length. then predict by voting, following majority wins low.",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Getting Started with LLVM Core Libraries-Notes Chap5 IR",frontmatter:{title:"Getting Started with LLVM Core Libraries-Notes Chap5 IR",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000002/"},regularPath:"/02.compiler/02.GetStartedLLVMChap5Notes.html",relativePath:"02.compiler/02.GetStartedLLVMChap5Notes.md",key:"v-0b217b45",path:"/pages/000002/",headers:[{level:3,title:"Chap5. LLVM Intermediate Representation",slug:"chap5-llvm-intermediate-representation",normalizedTitle:"chap5. llvm intermediate representation",charIndex:2}],headersStr:"Chap5. LLVM Intermediate Representation",content:'# Chap5. LLVM Intermediate Representation\n\n\n\n# 1. This IR has three equivalent forms:\n\n\n• An in-memory representation (the Instruction class, among others)\n• An on-disk representation that is encoded in a space-efficient form (the bitcode files)\n• An on-disk representation in a human-readable text form (the LLVM assembly files)\n\n\n# 2. LLVM still conveys some target-specific aspects\n\n\nProgram might implicitly include target-specific headers, like bits linux header folder.\n\n# 3. commands\n\n\nclang *.c -emit-llvm -c -o *.bc\nclang *.c -emit-llvm -S -c -o *.ll\nllvm-as *.ll -o *.bc\nllvm-dis *.bc -o *.ll\n\n//extract function from IR module\nllvm-extract -func=* *.bc -o *.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 4. LLVM IR Language Syntax\n\n\nmodule -> function -> block -> instruction • SSA(Static Single Assignment) Form • Thress Address Instruction • Infinite number of registers\n\ntarget datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"\n\n// type:<size>:<abi>:<preferred>\n// pointer 64bit 64bit 64 bit\n// p:64:64:64\n\n\n1\n2\n3\n4\n5\n\n\n# 5. Introducing llvm IR in-memory model\n\n\n * Module\n   \n   * Module::iterator iterates across functions in the module\n     \n   * begin(); end();\n     \n * Function\n   \n   * isDeclaration()\n     \n   * getArgumentList() or arg_begin(), arg_end()\n     \n   * Iterate through blocks: for (Function::iterator i = function.begin(), e = function.end(); i != e; ++i)\n     \n * BasicBlock\n   \n   * encapsulate all instructions\n   * iterates thorugh begin() and end()\n   * access predecessor or list through getSinglePredecessor\n * Instruction\n   \n   * Predicates: isAssociative(), isCommutative(), isIdempotent(), or isTerminator()\n   * getOpCode()\n   * access Operands() through op_begin() and op_end()\n     \n * Most powerful Value and User Interface\n   * Function and Intruction are subclasses of both Value and User.\n   * BasicBlock is a subclass of Value\n   * Value and User and be navigate through use-def and def-use chain\n   * Value defines a result can be used by others\n   * User means that this entity use one or more Value Interface.\n * Value & User\n   * Value defines use_begin() and use_end() to iterate through all Users def-use chain\n   * ReplaceAllUsesWith(Value *)\n   * User defines op_begin() and op_end() access all of the Value Interface it uses use-def chain\n   * ReplaceUsesOfWith(Value *From, Value *To)\n\n# 6. Compile-time and Link time Optimization\n\n\nopt -O3 sum.bc -o sum-O3.bc\nopt -std-compile-opts sum.bc -o sum-stdc.bc\n\nllvm-link file1.bc file2.bc file3.bc -o=all.bc\nopt -std-link-opts all.bc -o all-stdl.bc\n\nopt sum.bc -mem2reg -instcount -o sum-tmp.bc -stats\nopt sum.bc -time-passes -domtree -instcount -o sum-tmp.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 7. Discovering which passes matter\n\n\nopt -O1 sum-O0.ll -S -o sum-O1.ll\n\nclang -Xclang -print-stats -emit-llvm -O1 sum.c -c -o sum-O1.bc\n\nopt sum-O0.ll -stats -mem2reg -o sum-O1.ll\n\n\n1\n2\n3\n4\n5\n\n\n# 8. Pass Dependencies\n\n\n// full list of passes used when you request just the mem2reg pass\nopt sum-O0.ll -debug-pass=Structure -mem2reg -S -o sum-O1.ll\n\n\n1\n2\n\n\n# 9. Pass API\n\n\n * ModulePass runOnModule()\n * FunctionPass runOnFuction()\n * BasicBlockPass runOnBasicBlock()\n\nIf Unchanged, return false. Or else, return true. 10.',normalizedContent:'# chap5. llvm intermediate representation\n\n\n\n# 1. this ir has three equivalent forms:\n\n\n• an in-memory representation (the instruction class, among others)\n• an on-disk representation that is encoded in a space-efficient form (the bitcode files)\n• an on-disk representation in a human-readable text form (the llvm assembly files)\n\n\n# 2. llvm still conveys some target-specific aspects\n\n\nprogram might implicitly include target-specific headers, like bits linux header folder.\n\n# 3. commands\n\n\nclang *.c -emit-llvm -c -o *.bc\nclang *.c -emit-llvm -s -c -o *.ll\nllvm-as *.ll -o *.bc\nllvm-dis *.bc -o *.ll\n\n//extract function from ir module\nllvm-extract -func=* *.bc -o *.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 4. llvm ir language syntax\n\n\nmodule -> function -> block -> instruction • ssa(static single assignment) form • thress address instruction • infinite number of registers\n\ntarget datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-s128"\n\n// type:<size>:<abi>:<preferred>\n// pointer 64bit 64bit 64 bit\n// p:64:64:64\n\n\n1\n2\n3\n4\n5\n\n\n# 5. introducing llvm ir in-memory model\n\n\n * module\n   \n   * module::iterator iterates across functions in the module\n     \n   * begin(); end();\n     \n * function\n   \n   * isdeclaration()\n     \n   * getargumentlist() or arg_begin(), arg_end()\n     \n   * iterate through blocks: for (function::iterator i = function.begin(), e = function.end(); i != e; ++i)\n     \n * basicblock\n   \n   * encapsulate all instructions\n   * iterates thorugh begin() and end()\n   * access predecessor or list through getsinglepredecessor\n * instruction\n   \n   * predicates: isassociative(), iscommutative(), isidempotent(), or isterminator()\n   * getopcode()\n   * access operands() through op_begin() and op_end()\n     \n * most powerful value and user interface\n   * function and intruction are subclasses of both value and user.\n   * basicblock is a subclass of value\n   * value and user and be navigate through use-def and def-use chain\n   * value defines a result can be used by others\n   * user means that this entity use one or more value interface.\n * value & user\n   * value defines use_begin() and use_end() to iterate through all users def-use chain\n   * replacealluseswith(value *)\n   * user defines op_begin() and op_end() access all of the value interface it uses use-def chain\n   * replaceusesofwith(value *from, value *to)\n\n# 6. compile-time and link time optimization\n\n\nopt -o3 sum.bc -o sum-o3.bc\nopt -std-compile-opts sum.bc -o sum-stdc.bc\n\nllvm-link file1.bc file2.bc file3.bc -o=all.bc\nopt -std-link-opts all.bc -o all-stdl.bc\n\nopt sum.bc -mem2reg -instcount -o sum-tmp.bc -stats\nopt sum.bc -time-passes -domtree -instcount -o sum-tmp.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 7. discovering which passes matter\n\n\nopt -o1 sum-o0.ll -s -o sum-o1.ll\n\nclang -xclang -print-stats -emit-llvm -o1 sum.c -c -o sum-o1.bc\n\nopt sum-o0.ll -stats -mem2reg -o sum-o1.ll\n\n\n1\n2\n3\n4\n5\n\n\n# 8. pass dependencies\n\n\n// full list of passes used when you request just the mem2reg pass\nopt sum-o0.ll -debug-pass=structure -mem2reg -s -o sum-o1.ll\n\n\n1\n2\n\n\n# 9. pass api\n\n\n * modulepass runonmodule()\n * functionpass runonfuction()\n * basicblockpass runonbasicblock()\n\nif unchanged, return false. or else, return true. 10.',charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Getting Started with LLVM Core Libraries-Notes Chap6 Backend",frontmatter:{title:"Getting Started with LLVM Core Libraries-Notes Chap6 Backend",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000003/"},regularPath:"/02.compiler/03.GetStartedLLVMChap6Notes.html",relativePath:"02.compiler/03.GetStartedLLVMChap6Notes.md",key:"v-2d0923f6",path:"/pages/000003/",headers:[{level:3,title:"Chap6. The Backend",slug:"chap6-the-backend",normalizedTitle:"chap6. the backend",charIndex:2},{level:3,title:"1. Using the backend tools",slug:"_1-using-the-backend-tools",normalizedTitle:"1. using the backend tools",charIndex:1060},{level:3,title:"2. Learning backend struture",slug:"_2-learning-backend-struture",normalizedTitle:"2. learning backend struture",charIndex:1232},{level:3,title:"3. Knowing backend libraries",slug:"_3-knowing-backend-libraries",normalizedTitle:"3. knowing backend libraries",charIndex:1627},{level:3,title:"4. Learning how to use TableGen for LLVM backends",slug:"_4-learning-how-to-use-tablegen-for-llvm-backends",normalizedTitle:"4. learning how to use tablegen for llvm backends",charIndex:2024},{level:3,title:"5. Instruction Selection Phase",slug:"_5-instruction-selection-phase",normalizedTitle:"5. instruction selection phase",charIndex:3229},{level:3,title:"6. Lowering",slug:"_6-lowering",normalizedTitle:"6. lowering",charIndex:4179},{level:3,title:"7. DAG Combine and legalization",slug:"_7-dag-combine-and-legalization",normalizedTitle:"7. dag combine and legalization",charIndex:4889},{level:3,title:"8. DAG-to-DAG instruction selection",slug:"_8-dag-to-dag-instruction-selection",normalizedTitle:"8. dag-to-dag instruction selection",charIndex:6153},{level:3,title:"9. Scheduler",slug:"_9-scheduler",normalizedTitle:"9. scheduler",charIndex:8783},{level:3,title:"10. Machine Instructions",slug:"_10-machine-instructions",normalizedTitle:"10. machine instructions",charIndex:9629},{level:3,title:"11. Register Allocation",slug:"_11-register-allocation",normalizedTitle:"11. register allocation",charIndex:10221},{level:3,title:"12. Prologue and epilogue",slug:"_12-prologue-and-epilogue",normalizedTitle:"12. prologue and epilogue",charIndex:13492},{level:3,title:"13. Frame indexs",slug:"_13-frame-indexs",normalizedTitle:"13. frame indexs",charIndex:13837},{level:3,title:"13. Understanding machine code framework",slug:"_13-understanding-machine-code-framework",normalizedTitle:"13. understanding machine code framework",charIndex:14047},{level:3,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:14509}],headersStr:"Chap6. The Backend 1. Using the backend tools 2. Learning backend struture 3. Knowing backend libraries 4. Learning how to use TableGen for LLVM backends 5. Instruction Selection Phase 6. Lowering 7. DAG Combine and legalization 8. DAG-to-DAG instruction selection 9. Scheduler 10. Machine Instructions 11. Register Allocation 12. Prologue and epilogue 13. Frame indexs 13. Understanding machine code framework Summary",content:'# Chap6. The Backend\n\n\n\nWhite box Essential Gray Block For generated code efficiency\n\n# 1. Instructon Selection\n\n\n * Convert IR to target-specific SelectionDAG(Directed Acyclic Graph)\n   * Block->DAG\n   * Instruction->Node\n   * Edge contains dataflow dependence and control dependence and glue.\n * LLVM use DAG to employ tree-based pattern-matching instruction selection.\n * IN the end of this phase, IR node are converted to target-machine(machine instructions) nodes.\n\n# 2. Pre-register Allocation(RA) scheduling,the first instruction scheduling.\n\n\n * This is to explore instruction-level parallelism\n * The instructions are converted to MachineInstr three-address representation.\n\n# 3. Reguster Allocation\n\n\n# 4. Post-register Allocation(RA) Instruction Scheduling, the second instruction scheduling\n\n * Now we have real register information, we can combine information of extra hazards and delays of real register to opmitize code.\n\n# 5. Code Emission\n\n * Convert MachineInstr to MCInst\n * Emit Assembly Code\n * Emit Binary blobs to object code format\n\n\n# 1. Using the backend tools\n\nllc *bc -o *.s\nllc *.bc -filetype=obj -o *.o\n\nllc *.bc -march=mips -filetype=obj -o *.o\n\n// how march options\nllc -version\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 2. Learning backend struture\n\n * CodeGen: Instruction selection, scheduler,register allocation\n * MC: assembly parser, disassembler\n * TableGen\n * Target/*.cpp *.h *.td\n\nNotice:\n\nIselLowering is for Selection DAG Node lowering\nIselDAGtoDAG is for instruction selection.\n\n\nTargetLowering is called first for target-specific call and ret.\nThe major instruction selection is in ISelDAGtoDAG.\n\n\n\n\n# 3. Knowing backend libraries\n\n * AsmParser.a\n * AsmPrinter.a\n * CodeGen.a\n    * majority of the target-dependent functionality of the backend, as following：\n    * specific register handling rules, instruction selection, and scheduling\n\n * Desc.a\n    * low-level MC infrastructure and is responsible for registering target-specific MC objects such as MCCodeEmitter\n\n * Info.a\n * Disassembler.a\n\n\n# 4. Learning how to use TableGen for LLVM backends\n\n * instruction formats,\n * instructions,\n * registers,\n * pattern-matching DAGs,\n * instruction selection matching order,\n * calling conventions,\n * target CPU properties (supported Instruction Set Architecture (ISA) features and processor families).\n\ninsns.td\n\n\n\nGenerate code using llvm-tblgen\n\n\n\nTarget Properties: .td\nRegisters: RegisterInfo.td\n\n\n$ cd <llvm_source>/lib/Target/X86\n$ llvm-tblgen -gen-register-info X86.td -I ../../../include\n\n\n1\n2\n\n\nInstruction format: InstrFormat.td\nInstructions: InstrInfo.td\n\n\ninclude/llvm/Target/Target.td\n\n\n1\n\n\n\n\ndag in the above picture represents selectDAG for opcodes, registers or constants during instruction selection phase.\n\n\nSparcInstrInfo.td\n\n\n1\n\n\n\n\nWe can get how the template parameters are assigned to class Instruction.\n\n * OutOperandList\n * InOperandList\n * AsmString\n * Pattern\n\ncd <llvm_sources>/lib/Target/Sparc\nllvm-tblgen -print-records Sparc.td -I ../../../include | grep XNORrr -A 10\n\n\n1\n2\n\n\nThe difference between the first and second need to be checked.\n\n * GenDAGISel.inc\n * GenInstrInfo.inc\n * GenAsmWriter.inc\n * GenCodeEmitter.inc\n * GenDisassemblerTables.inc\n * GenAsmMatcher.inc\n\n\n# 5. Instruction Selection Phase\n\nLLVM IR -> SelectionDAG(SDNode)\n\n 1. Create DAG, in which node carry IR op\n 2. Nodes go through lowering, DAG combiner, and legalization phases.\n 3. Instruction selection perform DAG-to-DAG conversion, using node pattern matching and transforms SelectionDAG node into nodes representing target instructions.\n\nMost expensive ones in backend\n\n# 5.1 SelectionDAG class\n\n * DAG for each basic block\n * SDNode for instruction or operand\n\n\n\n * The black arrows represent regular edges showing a dataflow dependence.\n * The dashed blue arrows represent non-dataflow chains that exist to enforce order between two otherwise unrelated instructions.\n * The red edge guarantees that its adjacent nodes must be glued together\n\nPlease notice:\n\n * CopyFromReg: This is for getting value out of scope.\n * CopyToReg: This node copies a value to a specific register without supplying any concrete value for other nodes to consume.\n\n\n# 6. Lowering\n\n\n\n 1. SelectionDAGBuilder in SelectionDAGIsel.cpp visits every fuction and creates SelectionDAG for each basic block\n 2. During 1), special IR such as call and ret needs TargetLowering class for the first time for info like: pass call arg and how to return.\n 3. Only a smalle subset are lowered in this way. Majority are matched and replaces at instruction selection.\n\n> For instance, in SelectionDAG from sum.bc, the X86TargetLowering::LowerReturn() method (see lib/Target/X86/X86ISelLowering.cpp) is used to lower the IR ret instruction.\n> While doing this, it generates the X86ISD::RET_FLAG node, which copies the function result to EAX a-target-specific way to handle the function return.\n\n\n# 7. DAG Combine and legalization\n\n * DAG Combine\n   * Optimization for simpler code\n   * Target Independent: lib/CodeGen/SelectionDAG/DAGCombiner.cpp\n   * Target Dependnet: lib/Target/<Target_Name>/ISelLowering.cpp setTargetDAGCombine()\n\nsetTargetDAGCombine({ISD::SDIVREM, ISD::UDIVREM, ISD::SELECT, ISD::AND,\n                       ISD::OR, ISD::ADD, ISD::SUB, ISD::AssertZext, ISD::SHL});\n\nstatic SDValue performADDCombine(SDNode *N, SelectionDAG &DAG,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 const MipsSubtarget &Subtarget) {\n  ...\n  // (add v0, (add v1, abs_lo(tjt))) => (add (add v0, v1), abs_lo(tjt))\n  SDValue Add = N->getOperand(1);\n\n  if (Add.getOpcode() != ISD::ADD)\n    return SDValue();\n\n  SDValue Lo = Add.getOperand(1);\n  ...\n  EVT ValTy = N->getValueType(0);\n  SDLoc DL(N);\n\n  SDValue Add1 = DAG.getNode(ISD::ADD, DL, ValTy, N->getOperand(0),\n                             Add.getOperand(0));\n  return DAG.getNode(ISD::ADD, DL, ValTy, Add1, Lo);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n * Legalization\n\n * Support legal types: scalar: promote, expand, soften. vec split, scalarized or widened\n * Also it can be customized\n\nPromote\nExpand(library call)\nCustom\n\n\n\n\n# 8. DAG-to-DAG instruction selection\n\nTransform target-independent nodes to target-specific nodes by using pattern matching.\n\nCopyToReg, CopyFromReg and Register nodes are untouched until Register Allocation.\n\n# 8.1 Pattern Matching\n\nlib/Target/Sparc/SparcISelDAGToDAG.cpp\n\nSelect()  in SelectionDAGISel subclass\n\n\n1\n2\n3\n\n\nSelect():\n\n * receive an SDNode parameter to be matched\n * return SDNnode value representing a phycical instruction\n\nSelection() will call TableGen generateed SelectCode method.\nTableGen also contains MatcherTable, mapping ISD and ISD to physical-instruction node.\nThis table is generated by InstrInfo.td\nThe table are contained in <build_dir>/lib/Target/Sparc/SparcGenDAGISel.inc.\n\n\nWe can add other customized matching code prior to selectCode().\n\nCurDAG->getMachineNode() will create a node with phsycial instruction SP::SPAri CurDAG->SelectNodeTo() will create an instruction node and changes all use of * result to point to the "Opcode" result.\n\nvoid SparcDAGToDAGISel::Select(SDNode *N) {\n  ...\n  case ISD::UDIV: {\n    // sdivx / udivx handle 64-bit divides.\n    if (N->getValueType(0) == MVT::i64)\n      break;\n    // FIXME: should use a custom expander to expose the SRA to the dag.\n    SDValue DivLHS = N->getOperand(0);\n    SDValue DivRHS = N->getOperand(1);\n\n    // Set the Y register to the high-part.\n    SDValue TopPart;\n    if (N->getOpcode() == ISD::SDIV) {\n      TopPart = SDValue(CurDAG->getMachineNode(SP::SRAri, dl, MVT::i32, DivLHS,\n                                   CurDAG->getTargetConstant(31, dl, MVT::i32)),\n                        0);\n    } else {\n      TopPart = CurDAG->getRegister(SP::G0, MVT::i32);\n    }\n    TopPart = CurDAG->getCopyToReg(CurDAG->getEntryNode(), dl, SP::Y, TopPart,\n                                   SDValue())\n                  .getValue(1);\n\n    // FIXME: Handle div by immediate.\n    unsigned Opcode = N->getOpcode() == ISD::SDIV ? SP::SDIVrr : SP::UDIVrr;\n    CurDAG->SelectNodeTo(N, Opcode, MVT::i32, DivLHS, DivRHS, TopPart);\n    return;\n  }\n  }\n\n  SelectCode(N);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# 8.2 Visualizing the instruction selection process\n\nLLC                         PHASE\n-view-dag-combine1-dags     Before DAG combine 1\n-view-legalize-types-dags   Before legalize type\n-view-dag-combine-lt-dags   After legalize type 2 and before DAG combine\n-view-legalize-dags         Before legalization\n-view-dag-combine2-dags     Before DAG combine 2\n-view-isel-dags             Before instruction selection\n-view-sched-dags            After instruction selection and before scheduling\n\n\n# 9. Scheduler\n\nPre-register allocation works on SelectionDAG nodes(SDNodes).\n\n\n<llvm_source>/ lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp\n\nDifferent Algorithms: llc -pre-RA-sched=\n\n# 9.1 Instruction Itineraries\n\n<llvm_source>/include/llvm/Target/TargetItinerary.td\n<llvm_source>/lib/Target/ARM/ARMScheduleA8.td\n\n\nRepresent instruction latencya and hardware pipeline information.\n\n\n\n# 9.2 Hazard Detection\n\n> The ScheduleHazardRecognizer class provides an interface for hazard recognizer implementations and the ScoreboardHazardRecognizer subclass implements the scoreboard hazard recognizer (see the file <llvm_source>/lib/CodeGen/ScoreboardHazardRecognizer.cpp), which is LLVM\'s default recognizer.\n\n# 9.3 Scheduling Units\n\nThis scheduler runs before and after register allocation, which process both SDNode instruction and MachineInstr.\n\n\n# 10. Machine Instructions\n\nThe InstrEmitter pass, which runs after scheduling, transforms SDNode format into MachineInstr format.\nMI format is sequence of instructions rather than DAG.\n\n\nMI contains significant meta-information about an instruction:\n\n\n * it stores used and defined registers.\n * it distinguishes between register and memory operands (among other types).\n * it stores the instruction type (branch, return, call, and terminator, among others)\n * it stores predicates such as whether it is commutable or not, and so on.\n\n\n\nllc -print-machineinstrs\n\nllc -print-machineinstrs=\n\n\n# 11. Register Allocation\n\n * some MI code fragments might already use physical registers even before register allocation.\n   * Machine instructions that nee specific register\n   * ABI requirement\n * Destruct SSA form of IR\n\n4 register allocation algoritm\n\n-regalloc=<pbqp/greedy/basic/fast>\n\nby default, it will be basic(linear scan).\n\n\n\n# 11.1 Register Coalescer\n\nlib/CodeGen/RegisterCoalescer.cpp\n\n\n * A machine Function Pass, joinAllIntervals will iterate a work list of copy functions.\n * joinCopy creates CoalescerPair instances from copy machine instructions and coalesces copies wasy.\n\nBefore the coalescer, the phi node elimination pass runs.\nllc -print-machine-insts=phi-node-elimination will show this.\n\nMachine Instruction will be indexed with 0B, 16B, 32B(slot indexes).\n\nlive variable analysis pass runs before coalescing, thus the code is annotated with live variable information\n\n * which points each register is defined and killed\n * This is useful for ust to know which registers interfere with one other, that is are alive at the same time and need to live in distinct physical register.(Similar to graph coloring)\n\nCoalescer will also look for register copies, try to join the interval of the source register with the interval of the destination register.\nThe above is based on live interval analysis(different from live variable analysis).\n\nllc -march=sparc -debug-only=regalloc *.bc\n\n\n1\n\n\n# 11.2 Virtual Register Rewrite\n\n * Register Allocation Pass will selects the physical registers to be used for each virtual one.\n   \n * VirtRegMap contains mapping from virt to phy register.\n * VirtRegRewriter class implemented in <llvm_source>/lib/CodeGen/VirtRegMap.cpp—uses VirtRegMap and replaces virtual register references with physical ones.\n * Spill Code is also generated.\n * reg = COPY reg are deleted.\n\n> The register allocator and the instruction scheduler are sworn enemies in any compiler.\n> The job of the register allocator is to keep live ranges as short as possible, reducing the number of edges of the interference graph and thus reducing the number of necessary registers to avoid spills. To do this, the register allocator prefers to schedule instructions in a serial fashion (putting an instruction that depends on the other right next to it) because in this way the code uses less registers.\n> The job of the scheduler is the opposite: to extract instruction-level parallelism, it needs to keep alive as much unrelated and parallel computations as possible, requiring a much larger number of registers to hold intermediary values and increasing the number of interferences among live ranges.\n\n# 11.3 Target Hooks\n\n 1. TargetRegisterInfo includes if it is reserved or not, its parent register classes, and whether it is physical or virtual\n 2. InstrInfo\n     * isLoadFromStackSlot() and isStoreToStackSlot() are used during spill code generation to discover whether the machine instruction is a memory access to a stack slot.\n     * Spiller use using the storeRegToStackSlot() and loadRegFromStackSlot() methods with target-specific memory access instructions.\n     * copyPhyReg() method will also generate target-specific register copy.\n\nThe BuildMI() method is used everywhere in the code generator to generate machine instructions.\n\n\n# 12. Prologue and epilogue\n\n 1. Prologue sets up the stack frame and callee-saved registers during the beginning of a function.\n 2. Epilogue cleans up the stack frame prior to function return. They are target-specific, defined in FrameLowering::emitPrologue() and FrameLowering::emitEpilogue() at <llvm_source>/lib/Target//FrameLowering.cpp)\n\n\n# 13. Frame indexs\n\n<llvm_source>/lib/Target//RegisterInfo.cpp contains eliminateFrameIndex().\nIt will replace each frame index to real stack offset for all machine instructions that contain stack reference.\n\n\n# 13. Understanding machine code framework\n\nconvert machine instruction into machine code instructions(MC instructions).\n\n// show MC inst\nllc *.bc -march=x86-64 -show-mc-inst -o -\n\n// show assemble encoding\necho "movq 48879(,%riz), %rax" | llvm-mc -triple=x86_64 --show-encoding\n # encoding: [0x48,0x8b,0x04,0x25,0xef,0xbe,0x00,0x00]\n\n// disassemble\necho "0x8d 0x4c 0x24 0x04" | llvm-mc --disassemble -triple=x86_64\n leal 4(%rsp), %ecx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# Summary\n\n> https://jonathan2251.github.io/lbd/_images/9.png',normalizedContent:'# chap6. the backend\n\n\n\nwhite box essential gray block for generated code efficiency\n\n# 1. instructon selection\n\n\n * convert ir to target-specific selectiondag(directed acyclic graph)\n   * block->dag\n   * instruction->node\n   * edge contains dataflow dependence and control dependence and glue.\n * llvm use dag to employ tree-based pattern-matching instruction selection.\n * in the end of this phase, ir node are converted to target-machine(machine instructions) nodes.\n\n# 2. pre-register allocation(ra) scheduling,the first instruction scheduling.\n\n\n * this is to explore instruction-level parallelism\n * the instructions are converted to machineinstr three-address representation.\n\n# 3. reguster allocation\n\n\n# 4. post-register allocation(ra) instruction scheduling, the second instruction scheduling\n\n * now we have real register information, we can combine information of extra hazards and delays of real register to opmitize code.\n\n# 5. code emission\n\n * convert machineinstr to mcinst\n * emit assembly code\n * emit binary blobs to object code format\n\n\n# 1. using the backend tools\n\nllc *bc -o *.s\nllc *.bc -filetype=obj -o *.o\n\nllc *.bc -march=mips -filetype=obj -o *.o\n\n// how march options\nllc -version\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 2. learning backend struture\n\n * codegen: instruction selection, scheduler,register allocation\n * mc: assembly parser, disassembler\n * tablegen\n * target/*.cpp *.h *.td\n\nnotice:\n\nisellowering is for selection dag node lowering\niseldagtodag is for instruction selection.\n\n\ntargetlowering is called first for target-specific call and ret.\nthe major instruction selection is in iseldagtodag.\n\n\n\n\n# 3. knowing backend libraries\n\n * asmparser.a\n * asmprinter.a\n * codegen.a\n    * majority of the target-dependent functionality of the backend, as following：\n    * specific register handling rules, instruction selection, and scheduling\n\n * desc.a\n    * low-level mc infrastructure and is responsible for registering target-specific mc objects such as mccodeemitter\n\n * info.a\n * disassembler.a\n\n\n# 4. learning how to use tablegen for llvm backends\n\n * instruction formats,\n * instructions,\n * registers,\n * pattern-matching dags,\n * instruction selection matching order,\n * calling conventions,\n * target cpu properties (supported instruction set architecture (isa) features and processor families).\n\ninsns.td\n\n\n\ngenerate code using llvm-tblgen\n\n\n\ntarget properties: .td\nregisters: registerinfo.td\n\n\n$ cd <llvm_source>/lib/target/x86\n$ llvm-tblgen -gen-register-info x86.td -i ../../../include\n\n\n1\n2\n\n\ninstruction format: instrformat.td\ninstructions: instrinfo.td\n\n\ninclude/llvm/target/target.td\n\n\n1\n\n\n\n\ndag in the above picture represents selectdag for opcodes, registers or constants during instruction selection phase.\n\n\nsparcinstrinfo.td\n\n\n1\n\n\n\n\nwe can get how the template parameters are assigned to class instruction.\n\n * outoperandlist\n * inoperandlist\n * asmstring\n * pattern\n\ncd <llvm_sources>/lib/target/sparc\nllvm-tblgen -print-records sparc.td -i ../../../include | grep xnorrr -a 10\n\n\n1\n2\n\n\nthe difference between the first and second need to be checked.\n\n * gendagisel.inc\n * geninstrinfo.inc\n * genasmwriter.inc\n * gencodeemitter.inc\n * gendisassemblertables.inc\n * genasmmatcher.inc\n\n\n# 5. instruction selection phase\n\nllvm ir -> selectiondag(sdnode)\n\n 1. create dag, in which node carry ir op\n 2. nodes go through lowering, dag combiner, and legalization phases.\n 3. instruction selection perform dag-to-dag conversion, using node pattern matching and transforms selectiondag node into nodes representing target instructions.\n\nmost expensive ones in backend\n\n# 5.1 selectiondag class\n\n * dag for each basic block\n * sdnode for instruction or operand\n\n\n\n * the black arrows represent regular edges showing a dataflow dependence.\n * the dashed blue arrows represent non-dataflow chains that exist to enforce order between two otherwise unrelated instructions.\n * the red edge guarantees that its adjacent nodes must be glued together\n\nplease notice:\n\n * copyfromreg: this is for getting value out of scope.\n * copytoreg: this node copies a value to a specific register without supplying any concrete value for other nodes to consume.\n\n\n# 6. lowering\n\n\n\n 1. selectiondagbuilder in selectiondagisel.cpp visits every fuction and creates selectiondag for each basic block\n 2. during 1), special ir such as call and ret needs targetlowering class for the first time for info like: pass call arg and how to return.\n 3. only a smalle subset are lowered in this way. majority are matched and replaces at instruction selection.\n\n> for instance, in selectiondag from sum.bc, the x86targetlowering::lowerreturn() method (see lib/target/x86/x86isellowering.cpp) is used to lower the ir ret instruction.\n> while doing this, it generates the x86isd::ret_flag node, which copies the function result to eax a-target-specific way to handle the function return.\n\n\n# 7. dag combine and legalization\n\n * dag combine\n   * optimization for simpler code\n   * target independent: lib/codegen/selectiondag/dagcombiner.cpp\n   * target dependnet: lib/target/<target_name>/isellowering.cpp settargetdagcombine()\n\nsettargetdagcombine({isd::sdivrem, isd::udivrem, isd::select, isd::and,\n                       isd::or, isd::add, isd::sub, isd::assertzext, isd::shl});\n\nstatic sdvalue performaddcombine(sdnode *n, selectiondag &dag,\n                                 targetlowering::dagcombinerinfo &dci,\n                                 const mipssubtarget &subtarget) {\n  ...\n  // (add v0, (add v1, abs_lo(tjt))) => (add (add v0, v1), abs_lo(tjt))\n  sdvalue add = n->getoperand(1);\n\n  if (add.getopcode() != isd::add)\n    return sdvalue();\n\n  sdvalue lo = add.getoperand(1);\n  ...\n  evt valty = n->getvaluetype(0);\n  sdloc dl(n);\n\n  sdvalue add1 = dag.getnode(isd::add, dl, valty, n->getoperand(0),\n                             add.getoperand(0));\n  return dag.getnode(isd::add, dl, valty, add1, lo);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n * legalization\n\n * support legal types: scalar: promote, expand, soften. vec split, scalarized or widened\n * also it can be customized\n\npromote\nexpand(library call)\ncustom\n\n\n\n\n# 8. dag-to-dag instruction selection\n\ntransform target-independent nodes to target-specific nodes by using pattern matching.\n\ncopytoreg, copyfromreg and register nodes are untouched until register allocation.\n\n# 8.1 pattern matching\n\nlib/target/sparc/sparciseldagtodag.cpp\n\nselect()  in selectiondagisel subclass\n\n\n1\n2\n3\n\n\nselect():\n\n * receive an sdnode parameter to be matched\n * return sdnnode value representing a phycical instruction\n\nselection() will call tablegen generateed selectcode method.\ntablegen also contains matchertable, mapping isd and isd to physical-instruction node.\nthis table is generated by instrinfo.td\nthe table are contained in <build_dir>/lib/target/sparc/sparcgendagisel.inc.\n\n\nwe can add other customized matching code prior to selectcode().\n\ncurdag->getmachinenode() will create a node with phsycial instruction sp::spari curdag->selectnodeto() will create an instruction node and changes all use of * result to point to the "opcode" result.\n\nvoid sparcdagtodagisel::select(sdnode *n) {\n  ...\n  case isd::udiv: {\n    // sdivx / udivx handle 64-bit divides.\n    if (n->getvaluetype(0) == mvt::i64)\n      break;\n    // fixme: should use a custom expander to expose the sra to the dag.\n    sdvalue divlhs = n->getoperand(0);\n    sdvalue divrhs = n->getoperand(1);\n\n    // set the y register to the high-part.\n    sdvalue toppart;\n    if (n->getopcode() == isd::sdiv) {\n      toppart = sdvalue(curdag->getmachinenode(sp::srari, dl, mvt::i32, divlhs,\n                                   curdag->gettargetconstant(31, dl, mvt::i32)),\n                        0);\n    } else {\n      toppart = curdag->getregister(sp::g0, mvt::i32);\n    }\n    toppart = curdag->getcopytoreg(curdag->getentrynode(), dl, sp::y, toppart,\n                                   sdvalue())\n                  .getvalue(1);\n\n    // fixme: handle div by immediate.\n    unsigned opcode = n->getopcode() == isd::sdiv ? sp::sdivrr : sp::udivrr;\n    curdag->selectnodeto(n, opcode, mvt::i32, divlhs, divrhs, toppart);\n    return;\n  }\n  }\n\n  selectcode(n);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# 8.2 visualizing the instruction selection process\n\nllc                         phase\n-view-dag-combine1-dags     before dag combine 1\n-view-legalize-types-dags   before legalize type\n-view-dag-combine-lt-dags   after legalize type 2 and before dag combine\n-view-legalize-dags         before legalization\n-view-dag-combine2-dags     before dag combine 2\n-view-isel-dags             before instruction selection\n-view-sched-dags            after instruction selection and before scheduling\n\n\n# 9. scheduler\n\npre-register allocation works on selectiondag nodes(sdnodes).\n\n\n<llvm_source>/ lib/codegen/selectiondag/scheduledagsdnodes.cpp\n\ndifferent algorithms: llc -pre-ra-sched=\n\n# 9.1 instruction itineraries\n\n<llvm_source>/include/llvm/target/targetitinerary.td\n<llvm_source>/lib/target/arm/armschedulea8.td\n\n\nrepresent instruction latencya and hardware pipeline information.\n\n\n\n# 9.2 hazard detection\n\n> the schedulehazardrecognizer class provides an interface for hazard recognizer implementations and the scoreboardhazardrecognizer subclass implements the scoreboard hazard recognizer (see the file <llvm_source>/lib/codegen/scoreboardhazardrecognizer.cpp), which is llvm\'s default recognizer.\n\n# 9.3 scheduling units\n\nthis scheduler runs before and after register allocation, which process both sdnode instruction and machineinstr.\n\n\n# 10. machine instructions\n\nthe instremitter pass, which runs after scheduling, transforms sdnode format into machineinstr format.\nmi format is sequence of instructions rather than dag.\n\n\nmi contains significant meta-information about an instruction:\n\n\n * it stores used and defined registers.\n * it distinguishes between register and memory operands (among other types).\n * it stores the instruction type (branch, return, call, and terminator, among others)\n * it stores predicates such as whether it is commutable or not, and so on.\n\n\n\nllc -print-machineinstrs\n\nllc -print-machineinstrs=\n\n\n# 11. register allocation\n\n * some mi code fragments might already use physical registers even before register allocation.\n   * machine instructions that nee specific register\n   * abi requirement\n * destruct ssa form of ir\n\n4 register allocation algoritm\n\n-regalloc=<pbqp/greedy/basic/fast>\n\nby default, it will be basic(linear scan).\n\n\n\n# 11.1 register coalescer\n\nlib/codegen/registercoalescer.cpp\n\n\n * a machine function pass, joinallintervals will iterate a work list of copy functions.\n * joincopy creates coalescerpair instances from copy machine instructions and coalesces copies wasy.\n\nbefore the coalescer, the phi node elimination pass runs.\nllc -print-machine-insts=phi-node-elimination will show this.\n\nmachine instruction will be indexed with 0b, 16b, 32b(slot indexes).\n\nlive variable analysis pass runs before coalescing, thus the code is annotated with live variable information\n\n * which points each register is defined and killed\n * this is useful for ust to know which registers interfere with one other, that is are alive at the same time and need to live in distinct physical register.(similar to graph coloring)\n\ncoalescer will also look for register copies, try to join the interval of the source register with the interval of the destination register.\nthe above is based on live interval analysis(different from live variable analysis).\n\nllc -march=sparc -debug-only=regalloc *.bc\n\n\n1\n\n\n# 11.2 virtual register rewrite\n\n * register allocation pass will selects the physical registers to be used for each virtual one.\n   \n * virtregmap contains mapping from virt to phy register.\n * virtregrewriter class implemented in <llvm_source>/lib/codegen/virtregmap.cpp—uses virtregmap and replaces virtual register references with physical ones.\n * spill code is also generated.\n * reg = copy reg are deleted.\n\n> the register allocator and the instruction scheduler are sworn enemies in any compiler.\n> the job of the register allocator is to keep live ranges as short as possible, reducing the number of edges of the interference graph and thus reducing the number of necessary registers to avoid spills. to do this, the register allocator prefers to schedule instructions in a serial fashion (putting an instruction that depends on the other right next to it) because in this way the code uses less registers.\n> the job of the scheduler is the opposite: to extract instruction-level parallelism, it needs to keep alive as much unrelated and parallel computations as possible, requiring a much larger number of registers to hold intermediary values and increasing the number of interferences among live ranges.\n\n# 11.3 target hooks\n\n 1. targetregisterinfo includes if it is reserved or not, its parent register classes, and whether it is physical or virtual\n 2. instrinfo\n     * isloadfromstackslot() and isstoretostackslot() are used during spill code generation to discover whether the machine instruction is a memory access to a stack slot.\n     * spiller use using the storeregtostackslot() and loadregfromstackslot() methods with target-specific memory access instructions.\n     * copyphyreg() method will also generate target-specific register copy.\n\nthe buildmi() method is used everywhere in the code generator to generate machine instructions.\n\n\n# 12. prologue and epilogue\n\n 1. prologue sets up the stack frame and callee-saved registers during the beginning of a function.\n 2. epilogue cleans up the stack frame prior to function return. they are target-specific, defined in framelowering::emitprologue() and framelowering::emitepilogue() at <llvm_source>/lib/target//framelowering.cpp)\n\n\n# 13. frame indexs\n\n<llvm_source>/lib/target//registerinfo.cpp contains eliminateframeindex().\nit will replace each frame index to real stack offset for all machine instructions that contain stack reference.\n\n\n# 13. understanding machine code framework\n\nconvert machine instruction into machine code instructions(mc instructions).\n\n// show mc inst\nllc *.bc -march=x86-64 -show-mc-inst -o -\n\n// show assemble encoding\necho "movq 48879(,%riz), %rax" | llvm-mc -triple=x86_64 --show-encoding\n # encoding: [0x48,0x8b,0x04,0x25,0xef,0xbe,0x00,0x00]\n\n// disassemble\necho "0x8d 0x4c 0x24 0x04" | llvm-mc --disassemble -triple=x86_64\n leal 4(%rsp), %ecx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# summary\n\n> https://jonathan2251.github.io/lbd/_images/9.png',charsets:{cjk:!0},lastUpdated:"2024/07/16, 20:14:06"},{title:"memory management",frontmatter:{title:"memory management",date:"2024-05-06T11:42:24.000Z",permalink:"/pages/f07692/"},regularPath:"/01.hbm/10.software_memory_paper.html",relativePath:"01.hbm/10.software_memory_paper.md",key:"v-065eff05",path:"/pages/f07692/",headersStr:null,content:" 1. [11 ]Hardware Memory Management for Future Mobile Hybrid Memory Systems\n 2. [4 DAC] OpenMem: Hardware/Software Cooperative Management for Mobile Memory System\n 3. [TACO] FlexPointer: Fast Address Translation Based on Range TLB and Tagged Pointers\n 4. [26] Branch Prediction Is Not a Solved Problem: Measurements, Opportunities, and Future Directions\n 5. [12] When Storage Response Time Catches Up With Overall Context Switch Overhead, What Is Next?\n 6. [225] Dynamic tracking of page miss ratio curve for memory management\n 7. [299] LVI: Hijacking Transient Execution with Load Value Injection",normalizedContent:" 1. [11 ]hardware memory management for future mobile hybrid memory systems\n 2. [4 dac] openmem: hardware/software cooperative management for mobile memory system\n 3. [taco] flexpointer: fast address translation based on range tlb and tagged pointers\n 4. [26] branch prediction is not a solved problem: measurements, opportunities, and future directions\n 5. [12] when storage response time catches up with overall context switch overhead, what is next?\n 6. [225] dynamic tracking of page miss ratio curve for memory management\n 7. [299] lvi: hijacking transient execution with load value injection",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"llvm front end",frontmatter:{title:"llvm front end",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000001/"},regularPath:"/02.compiler/01.llvm_frontend.html",relativePath:"02.compiler/01.llvm_frontend.md",key:"v-f3b213e2",path:"/pages/000001/",headers:[{level:3,title:"1. Clang parser will build an Abstract Syntax Tree(AST) and then goes on to emit LLVM IR",slug:"_1-clang-parser-will-build-an-abstract-syntax-tree-ast-and-then-goes-on-to-emit-llvm-ir",normalizedTitle:"1. clang parser will build an abstract syntax tree(ast) and then goes on to emit llvm ir",charIndex:129},{level:3,title:"2. LLVM Target Independent",slug:"_2-llvm-target-independent",normalizedTitle:"2. llvm target independent",charIndex:222},{level:3,title:"3. SelectionDAG Node",slug:"_3-selectiondag-node",normalizedTitle:"3. selectiondag node",charIndex:253},{level:3,title:"4. To emit machine instructions, LLVM will legalize the operation",slug:"_4-to-emit-machine-instructions-llvm-will-legalize-the-operation",normalizedTitle:"4. to emit machine instructions, llvm will legalize the operation",charIndex:481},{level:3,title:"5. Instruction selection from SDNode to MachineSNDode",slug:"_5-instruction-selection-from-sdnode-to-machinesndode",normalizedTitle:"5. instruction selection from sdnode to machinesndode",charIndex:725},{level:3,title:"6. Scheduling and emitting a MachineInstr",slug:"_6-scheduling-and-emitting-a-machineinstr",normalizedTitle:"6. scheduling and emitting a machineinstr",charIndex:988},{level:3,title:"7. Register Allocation",slug:"_7-register-allocation",normalizedTitle:"7. register allocation",charIndex:1245},{level:3,title:"8. From MachineInstruction to MCinst",slug:"_8-from-machineinstruction-to-mcinst",normalizedTitle:"8. from machineinstruction to mcinst",charIndex:1565},{level:3,title:"9. Build LLVM",slug:"_9-build-llvm",normalizedTitle:"9. build llvm",charIndex:1678}],headersStr:"1. Clang parser will build an Abstract Syntax Tree(AST) and then goes on to emit LLVM IR 2. LLVM Target Independent 3. SelectionDAG Node 4. To emit machine instructions, LLVM will legalize the operation 5. Instruction selection from SDNode to MachineSNDode 6. Scheduling and emitting a MachineInstr 7. Register Allocation 8. From MachineInstruction to MCinst 9. Build LLVM",content:'llvm front end demo Notes from Life of an instruction in LLVM https://blog.llvm.org/2012/11/life-of-instruction-in-llvm.html\n\n\n# 1. Clang parser will build an Abstract Syntax Tree(AST) and then goes on to emit LLVM IR\n\n\n# 2. LLVM Target Independent\n\n\n# 3. SelectionDAG Node\n\n\nSelectionDAGBuild creates SDGNode\nSelectionDAGIsel goes over all IR instructions and calls SelectionDAGBuilder::visit to Dispatch them\nWe can use -debug or -view to get log or dump image of the graph\n\n\n\n# 4. To emit machine instructions, LLVM will legalize the operation\n\n\nUse target-specific hooks to convert all operations and types into ones that the target actually supports. This is done by TargetLowering.\nSelectionDAGLegalize::LegalizeOp\n\n\n# 5. Instruction selection from SDNode to MachineSNDode\n\n\nSelectionDAGISel::Select\nSelectCode\nThis step will create MachineSDNode, a subclass of SDNode which holds the information required to construct an actual machine instruction, but still in DAG node form.\n\n\n# 6. Scheduling and emitting a MachineInstr\n\n\nTranslate SDNode into Machine Instructions with InstrEmitter::EmitMachineNode, emmit into MachineBasicBlock. Here the instruction are in linear form (MI). No DAG any more.\n-print-machineinstrs\nStill SSA form.\n\n\n# 7. Register Allocation\n\n\nFor instructions that can only support fixed registers, it is already allocated. Here the virtual registers are allocated into physical registers. This assignment is done by X86DAGToDAGISel::Select.\nAfter this, another round of optimization is conducted, TargetPassConfig::addMachinePasses.\n\n\n# 8. From MachineInstruction to MCinst\n\nJIT: AsmPrinter::EmitInstruction\nObj: ObjectStreamer::EmitInstruction\n\n\n\n# 9. Build LLVM\n\ncreate build&cd build\ncmake -S llvm -B . -DCMAKE_BUILD_TYPE=Debug -DLLVM_TARGETS_TO_BUILD="MSP430;RISCV" ../llvm\nmake -j 8\n\n\n# How to build LC3\n\ncmake -S llvm -B . -DCMAKE_BUILD_TYPE=Debug -DLLVM_EXPERIMENTAL_TARGETS_TO_BUILD="LC3" ../llvm\n',normalizedContent:'llvm front end demo notes from life of an instruction in llvm https://blog.llvm.org/2012/11/life-of-instruction-in-llvm.html\n\n\n# 1. clang parser will build an abstract syntax tree(ast) and then goes on to emit llvm ir\n\n\n# 2. llvm target independent\n\n\n# 3. selectiondag node\n\n\nselectiondagbuild creates sdgnode\nselectiondagisel goes over all ir instructions and calls selectiondagbuilder::visit to dispatch them\nwe can use -debug or -view to get log or dump image of the graph\n\n\n\n# 4. to emit machine instructions, llvm will legalize the operation\n\n\nuse target-specific hooks to convert all operations and types into ones that the target actually supports. this is done by targetlowering.\nselectiondaglegalize::legalizeop\n\n\n# 5. instruction selection from sdnode to machinesndode\n\n\nselectiondagisel::select\nselectcode\nthis step will create machinesdnode, a subclass of sdnode which holds the information required to construct an actual machine instruction, but still in dag node form.\n\n\n# 6. scheduling and emitting a machineinstr\n\n\ntranslate sdnode into machine instructions with instremitter::emitmachinenode, emmit into machinebasicblock. here the instruction are in linear form (mi). no dag any more.\n-print-machineinstrs\nstill ssa form.\n\n\n# 7. register allocation\n\n\nfor instructions that can only support fixed registers, it is already allocated. here the virtual registers are allocated into physical registers. this assignment is done by x86dagtodagisel::select.\nafter this, another round of optimization is conducted, targetpassconfig::addmachinepasses.\n\n\n# 8. from machineinstruction to mcinst\n\njit: asmprinter::emitinstruction\nobj: objectstreamer::emitinstruction\n\n\n\n# 9. build llvm\n\ncreate build&cd build\ncmake -s llvm -b . -dcmake_build_type=debug -dllvm_targets_to_build="msp430;riscv" ../llvm\nmake -j 8\n\n\n# how to build lc3\n\ncmake -s llvm -b . -dcmake_build_type=debug -dllvm_experimental_targets_to_build="lc3" ../llvm\n',charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Learning LLVM Diary 00",frontmatter:{title:"Learning LLVM Diary 00",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000004/"},regularPath:"/02.compiler/04.%20LearningLLVMDiary0.html",relativePath:"02.compiler/04. LearningLLVMDiary0.md",key:"v-4d042c96",path:"/pages/000004/",headers:[{level:3,title:"3. Knowledge Fragments",slug:"_3-knowledge-fragments",normalizedTitle:"3. knowledge fragments",charIndex:7893}],headersStr:"3. Knowledge Fragments",content:"# 1. Difference between ISelDAGToDAG and ISelLowering\n\nIn LLVM's backend, ISelLowering and ISelDAGToDAG are two important classes that play different roles in the instruction selection process. Here's the difference between them and the phases in which they are called:\n\nISelDAGToDAG\n\n * Role: ISelDAGToDAG stands for \"Instruction Selection DAG to DAG\". This class is responsible for translating the Selection DAG (Directed Acyclic Graph) into a sequence of target-specific instructions.\n\n * Phase: ISelDAGToDAG is called during the \"Instruction Selection\" phase of the compiler backend. This phase comes after the DAG Legalization phase and before Register Allocation.\n\n * Functions: In ISelDAGToDAG, you will find functions like Select, which is responsible for selecting target instructions for each node in the DAG.\n\nTypical Functions:\n\nSelect: Select target instructions for nodes in the DAG.\nSelectNode: Implement target-specific node selection.\nPreprocessISelDAG: Prepare for instruction selection.\nPostprocessISelDAG: Clean up after instruction selection.\n\n\n1\n2\n3\n4\n\n\nISelLowering\n\n * Role: ISelLowering stands for \"Instruction Selection Lowering\". This class provides target-specific information and handling for aspects of instruction selection during the Selection DAG Construction phase.\n\n * Phase: ISelLowering is called during the \"Selection DAG Construction\" phase of the compiler backend. This phase transforms the generic LLVM IR into a machine-independent representation, the Selection DAG.\n\n * Functions: In ISelLowering, you will find functions that define how LLVM IR operations are translated into target-specific instructions. This includes custom lowering of specific LLVM IR operations, defining patterns for converting LLVM IR to target nodes, and providing information about target-specific features and constraints.\n\nTypical Functions:\n\nLowerCall: Lower calls to target-specific calling conventions.\nLowerReturn: Lower return instructions to target-specific sequences.\nEmitInstrWithCustomInserter: Handle target-specific instructions with custom insertion logic.\ngetTargetNodeName: Provide human-readable names for target nodes.\n\n\n1\n2\n3\n4\n\n\nPhases in Summary\n\n * Selection DAG Construction:\n   \n   * ISelLowering is called during this phase.\n   * This phase transforms the generic LLVM IR into a machine-independent representation (the Selection DAG).\n   * Functions in ISelLowering handle how LLVM IR operations are translated into target-specific instructions.\n\n * DAG Legalization:\n   \n   * Various transformations to ensure DAG conforms to target-specific constraints.\n   * Typically, no specific user-defined classes are involved in this phase.\n\n * Instruction Selection (ISelDAGToDAG):\n   \n   * ISelDAGToDAG is called during this phase.\n   * Translates the Selection DAG into a sequence of target-specific instructions.\n   * Functions in ISelDAGToDAG handle how DAG nodes are selected and transformed into machine instructions.\n\n * Register Allocation:\n   \n   * Assigns virtual registers to physical registers.\n   * Ensures that the generated code does not use more registers than available.\n\n * Frame Lowering:\n   \n   * Manages the function's stack frame.\n   * Sets up the stack frame, manages the frame pointer, and handles stack frame operations.\n\n * Prologue and Epilogue Emission:\n   \n   * Generates the machine code for the function's entry and exit sequences.\n   * Includes setting up the stack frame, saving/restoring callee-saved registers, etc.\n\nIn summary, ISelLowering is called during the Selection DAG Construction phase to handle translation of LLVM IR to target-specific instructions. ISelDAGToDAG is called during the Instruction Selection phase to translate the Selection DAG to a sequence of target-specific instructions. These phases work together to convert the LLVM IR into machine code for the target architecture.\n\n----------------------------------------\n\n# 2. Analysis Pass and Tranform Pass\n\nAnalysis Passes in LLVM:\n\n * Purpose:\n\n * Analysis passes in LLVM are used to gather information about the program without modifying it.\n * They analyze the program's code structure, control flow, data flow, and other properties.\n * This information is used by subsequent optimization passes to make informed decisions.\n\n * Characteristics:\n   \n   * Do not modify the program.\n   * Collect information about the program.\n   * Used by other passes to guide optimizations.\n   * Typically run early in the optimization pipeline.\n\n * Major Analysis Passes:\n   \n   * DominatorTree Analysis:\n     \n     * Computes the dominator tree for a function.\n     * Helps in various optimizations such as loop optimization, control flow analysis, etc.\n   \n   * LoopInfo Analysis:\n     \n     * Provides information about loops in a function.\n     * Used by loop optimization passes for loop transformations.\n   \n   * ScalarEvolution Analysis:\n     \n     * Analyzes and characterizes scalar expressions in loops.\n     * Helps in loop transformations like loop unrolling, loop vectorization, etc.\n   \n   * MemorySSA Analysis:\n     \n     * Provides a memory SSA representation of the program.\n     * Used in optimizations related to memory access analysis, alias analysis, etc.\n   \n   * AliasAnalysis Analysis:\n     \n     * Determines the aliasing relationship between memory accesses.\n     * Helps in optimizations that depend on memory aliasing information.\n\nTransform Passes in LLVM:\n\n * Purpose: Transform passes in LLVM modify the program's IR to improve its performance or reduce its size. They apply optimizations and transformations to the code.\n\n * Characteristics:\n   \n   * Modify the program's IR.\n   * Apply optimizations and transformations.\n   * Can introduce new code or modify existing code.\n   * Typically run after analysis passes.\n\n * Major Transform Passes:\n   \n   * Instruction Combining:\n     \n     * Combines multiple instructions into simpler forms.\n     * Reduces the number of instructions and improves code readability.\n   \n   * Dead Code Elimination:\n     \n     * Removes code that is guaranteed to have no effect on program output.\n     * Improves code size and execution speed.\n   \n   * Loop Unrolling:\n     \n     * Duplicates the loop body multiple times to reduce loop overhead.\n     * Improves instruction-level parallelism.\n   \n   * Function Inlining:\n     \n     * Replaces a function call with the body of the called function.\n     * Reduces function call overhead and enables further optimizations.\n   \n   * Constant Propagation:\n     \n     * Propagates constant values through the program.\n     * Enables further optimizations by replacing variables with constants.\n   \n   * Vectorization (Loop Vectorization):\n     \n     * Converts scalar operations in loops into SIMD (Single Instruction, Multiple Data) operations.\n     * Improves performance by exploiting parallelism in hardware.\n   \n   * SROA (Scalar Replacement of Aggregates):\n     \n     * Breaks down aggregates (like structs) into individual scalar variables.\n     * Improves optimization opportunities by working on individual scalar variables.\n\nSummary:\n\n * Analysis Passes:\n   * Gather information about the program.\n   * Do not modify the program.\n   * Used by other passes for optimizations.\n   * Examples: DominatorTree, LoopInfo, ScalarEvolution, MemorySSA, AliasAnalysis.\n * Transform Passes:\n   * Modify the program's IR.\n   * Apply optimizations and transformations.\n   * Examples: Instruction Combining, Dead Code Elimination, Loop Unrolling, Function Inlining, Constant Propagation, Vectorization, SROA (Scalar Replacement of Aggregates). These are just a few examples of major analysis and transform passes in LLVM. The LLVM infrastructure provides a wide range of passes for various optimizations and analyses, allowing users to construct custom optimization pipelines tailored to their specific needs.\n\n----------------------------------------\n\n\n# 3. Knowledge Fragments\n\n 1. SROA is claimed to replace mem2reg.\n    discourse.llvm\n\n 2. Alias Analysis\n    Alias Analysis (aka Pointer Analysis) is a class of techniques which attempt to determine whether or not two pointers ever can point to the same object in memory.\n    Traditionally, alias analyses respond to a query with a Must, May, or No alias response, indicating that two pointers always point to the same object, might point to the same object, or are known to never point to the same object.\n    \n\n 3. RISCV Instruction Set Might deserves further explore.\n    Berkely RISCV Instruction Set Manual\n    \n\n 4. How LLVM Optimizes a Function\n    How LLVM Optimizes a Function\n    This blog trace through different passes in llvm IR.\n    \n\n 5. CambridgeSlides\n    Cambridge Modern Compiler Design\n    \n    * Introduction\n    * Modern intermediate representations\n    * LLVM IR and transform pipeline\n    * Modern processor architectures\n    * Dynamic dispatch and duck typing\n    * Autovectorisation\n    * Garbage collection\n    * JIT Compilation",normalizedContent:"# 1. difference between iseldagtodag and isellowering\n\nin llvm's backend, isellowering and iseldagtodag are two important classes that play different roles in the instruction selection process. here's the difference between them and the phases in which they are called:\n\niseldagtodag\n\n * role: iseldagtodag stands for \"instruction selection dag to dag\". this class is responsible for translating the selection dag (directed acyclic graph) into a sequence of target-specific instructions.\n\n * phase: iseldagtodag is called during the \"instruction selection\" phase of the compiler backend. this phase comes after the dag legalization phase and before register allocation.\n\n * functions: in iseldagtodag, you will find functions like select, which is responsible for selecting target instructions for each node in the dag.\n\ntypical functions:\n\nselect: select target instructions for nodes in the dag.\nselectnode: implement target-specific node selection.\npreprocessiseldag: prepare for instruction selection.\npostprocessiseldag: clean up after instruction selection.\n\n\n1\n2\n3\n4\n\n\nisellowering\n\n * role: isellowering stands for \"instruction selection lowering\". this class provides target-specific information and handling for aspects of instruction selection during the selection dag construction phase.\n\n * phase: isellowering is called during the \"selection dag construction\" phase of the compiler backend. this phase transforms the generic llvm ir into a machine-independent representation, the selection dag.\n\n * functions: in isellowering, you will find functions that define how llvm ir operations are translated into target-specific instructions. this includes custom lowering of specific llvm ir operations, defining patterns for converting llvm ir to target nodes, and providing information about target-specific features and constraints.\n\ntypical functions:\n\nlowercall: lower calls to target-specific calling conventions.\nlowerreturn: lower return instructions to target-specific sequences.\nemitinstrwithcustominserter: handle target-specific instructions with custom insertion logic.\ngettargetnodename: provide human-readable names for target nodes.\n\n\n1\n2\n3\n4\n\n\nphases in summary\n\n * selection dag construction:\n   \n   * isellowering is called during this phase.\n   * this phase transforms the generic llvm ir into a machine-independent representation (the selection dag).\n   * functions in isellowering handle how llvm ir operations are translated into target-specific instructions.\n\n * dag legalization:\n   \n   * various transformations to ensure dag conforms to target-specific constraints.\n   * typically, no specific user-defined classes are involved in this phase.\n\n * instruction selection (iseldagtodag):\n   \n   * iseldagtodag is called during this phase.\n   * translates the selection dag into a sequence of target-specific instructions.\n   * functions in iseldagtodag handle how dag nodes are selected and transformed into machine instructions.\n\n * register allocation:\n   \n   * assigns virtual registers to physical registers.\n   * ensures that the generated code does not use more registers than available.\n\n * frame lowering:\n   \n   * manages the function's stack frame.\n   * sets up the stack frame, manages the frame pointer, and handles stack frame operations.\n\n * prologue and epilogue emission:\n   \n   * generates the machine code for the function's entry and exit sequences.\n   * includes setting up the stack frame, saving/restoring callee-saved registers, etc.\n\nin summary, isellowering is called during the selection dag construction phase to handle translation of llvm ir to target-specific instructions. iseldagtodag is called during the instruction selection phase to translate the selection dag to a sequence of target-specific instructions. these phases work together to convert the llvm ir into machine code for the target architecture.\n\n----------------------------------------\n\n# 2. analysis pass and tranform pass\n\nanalysis passes in llvm:\n\n * purpose:\n\n * analysis passes in llvm are used to gather information about the program without modifying it.\n * they analyze the program's code structure, control flow, data flow, and other properties.\n * this information is used by subsequent optimization passes to make informed decisions.\n\n * characteristics:\n   \n   * do not modify the program.\n   * collect information about the program.\n   * used by other passes to guide optimizations.\n   * typically run early in the optimization pipeline.\n\n * major analysis passes:\n   \n   * dominatortree analysis:\n     \n     * computes the dominator tree for a function.\n     * helps in various optimizations such as loop optimization, control flow analysis, etc.\n   \n   * loopinfo analysis:\n     \n     * provides information about loops in a function.\n     * used by loop optimization passes for loop transformations.\n   \n   * scalarevolution analysis:\n     \n     * analyzes and characterizes scalar expressions in loops.\n     * helps in loop transformations like loop unrolling, loop vectorization, etc.\n   \n   * memoryssa analysis:\n     \n     * provides a memory ssa representation of the program.\n     * used in optimizations related to memory access analysis, alias analysis, etc.\n   \n   * aliasanalysis analysis:\n     \n     * determines the aliasing relationship between memory accesses.\n     * helps in optimizations that depend on memory aliasing information.\n\ntransform passes in llvm:\n\n * purpose: transform passes in llvm modify the program's ir to improve its performance or reduce its size. they apply optimizations and transformations to the code.\n\n * characteristics:\n   \n   * modify the program's ir.\n   * apply optimizations and transformations.\n   * can introduce new code or modify existing code.\n   * typically run after analysis passes.\n\n * major transform passes:\n   \n   * instruction combining:\n     \n     * combines multiple instructions into simpler forms.\n     * reduces the number of instructions and improves code readability.\n   \n   * dead code elimination:\n     \n     * removes code that is guaranteed to have no effect on program output.\n     * improves code size and execution speed.\n   \n   * loop unrolling:\n     \n     * duplicates the loop body multiple times to reduce loop overhead.\n     * improves instruction-level parallelism.\n   \n   * function inlining:\n     \n     * replaces a function call with the body of the called function.\n     * reduces function call overhead and enables further optimizations.\n   \n   * constant propagation:\n     \n     * propagates constant values through the program.\n     * enables further optimizations by replacing variables with constants.\n   \n   * vectorization (loop vectorization):\n     \n     * converts scalar operations in loops into simd (single instruction, multiple data) operations.\n     * improves performance by exploiting parallelism in hardware.\n   \n   * sroa (scalar replacement of aggregates):\n     \n     * breaks down aggregates (like structs) into individual scalar variables.\n     * improves optimization opportunities by working on individual scalar variables.\n\nsummary:\n\n * analysis passes:\n   * gather information about the program.\n   * do not modify the program.\n   * used by other passes for optimizations.\n   * examples: dominatortree, loopinfo, scalarevolution, memoryssa, aliasanalysis.\n * transform passes:\n   * modify the program's ir.\n   * apply optimizations and transformations.\n   * examples: instruction combining, dead code elimination, loop unrolling, function inlining, constant propagation, vectorization, sroa (scalar replacement of aggregates). these are just a few examples of major analysis and transform passes in llvm. the llvm infrastructure provides a wide range of passes for various optimizations and analyses, allowing users to construct custom optimization pipelines tailored to their specific needs.\n\n----------------------------------------\n\n\n# 3. knowledge fragments\n\n 1. sroa is claimed to replace mem2reg.\n    discourse.llvm\n\n 2. alias analysis\n    alias analysis (aka pointer analysis) is a class of techniques which attempt to determine whether or not two pointers ever can point to the same object in memory.\n    traditionally, alias analyses respond to a query with a must, may, or no alias response, indicating that two pointers always point to the same object, might point to the same object, or are known to never point to the same object.\n    \n\n 3. riscv instruction set might deserves further explore.\n    berkely riscv instruction set manual\n    \n\n 4. how llvm optimizes a function\n    how llvm optimizes a function\n    this blog trace through different passes in llvm ir.\n    \n\n 5. cambridgeslides\n    cambridge modern compiler design\n    \n    * introduction\n    * modern intermediate representations\n    * llvm ir and transform pipeline\n    * modern processor architectures\n    * dynamic dispatch and duck typing\n    * autovectorisation\n    * garbage collection\n    * jit compilation",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:'Add New Instruction "ACE" to LLVM',frontmatter:{title:'Add New Instruction "ACE" to LLVM',date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000005/"},regularPath:"/02.compiler/05.%20addInstACE.html",relativePath:"02.compiler/05. addInstACE.md",key:"v-49e4dae0",path:"/pages/000005/",headers:[{level:3,title:"1. Add Register Class in RISCV",slug:"_1-add-register-class-in-riscv",normalizedTitle:"1. add register class in riscv",charIndex:2}],headersStr:"1. Add Register Class in RISCV",content:'# 1. Add Register Class in RISCV\n\n// RegACE - 4-bit register for RISC-V ACE inst\nclass RISCVRegACE<bits<16> Enc, string n, list<string> alt = []> : Register<n> {\n  let HWEncoding = Enc;\n  let AltNames = alt;\n}\n\n// Define the ACE registers\nlet RegAltNameIndices = [ABIRegAltName] in {\n  //foreach Index = !range(0, 16, 1) in {\n  //  def ACE#Index : RISCVRegACE<Index, "ace_reg_"#Index, ["ace_reg_"#Index]>, DwarfRegNum<[!add(Index, 128)]>;\n  //}\n  def ACE0  : RISCVRegACE<0, "ace_reg_0", ["ace_reg_0"]>, DwarfRegNum<[128]>;\n  def ACE1  : RISCVRegACE<1, "ace_reg_1", ["ace_reg_1"]>, DwarfRegNum<[129]>;\n  def ACE2  : RISCVRegACE<2, "ace_reg_2", ["ace_reg_2"]>, DwarfRegNum<[130]>;\n  def ACE3  : RISCVRegACE<3, "ace_reg_3", ["ace_reg_3"]>, DwarfRegNum<[131]>;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } GPR:{ *:[i32] m1:[i32 i64] }:$src)\nIncluded from /home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCV.td:30:\n/home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.td:1915:1: error: In anonymous_8306: Could not infer all types in pattern!\ndef : Pat<(trunc GPR:$src), (COPY GPR:$src)>;\n^\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } GPR:{ *:[i32] m1:[i32 i64] }:$src)\nanonymous_8306: \t(COPY:{ *:[i16] m1:[i16 i32] } GPR:{ *:[i32] m1:[i32 i64] }:$src)\nIncluded from /home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCV.td:30:\n/home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.td:1915:1: error: In anonymous_8306: Could not infer all types in pattern result!\ndef : Pat<(trunc GPR:$src), (COPY GPR:$src)>;\n^\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nThe failed issues after I def ACE Register Class\n\ndef ACE : RegisterClass<"RISCV", [i16], 16, (add\n    (sequence "ACE%u", 0, 3)\n)>;\n\n\n\n1\n2\n3\n4\n',normalizedContent:'# 1. add register class in riscv\n\n// regace - 4-bit register for risc-v ace inst\nclass riscvregace<bits<16> enc, string n, list<string> alt = []> : register<n> {\n  let hwencoding = enc;\n  let altnames = alt;\n}\n\n// define the ace registers\nlet regaltnameindices = [abiregaltname] in {\n  //foreach index = !range(0, 16, 1) in {\n  //  def ace#index : riscvregace<index, "ace_reg_"#index, ["ace_reg_"#index]>, dwarfregnum<[!add(index, 128)]>;\n  //}\n  def ace0  : riscvregace<0, "ace_reg_0", ["ace_reg_0"]>, dwarfregnum<[128]>;\n  def ace1  : riscvregace<1, "ace_reg_1", ["ace_reg_1"]>, dwarfregnum<[129]>;\n  def ace2  : riscvregace<2, "ace_reg_2", ["ace_reg_2"]>, dwarfregnum<[130]>;\n  def ace3  : riscvregace<3, "ace_reg_3", ["ace_reg_3"]>, dwarfregnum<[131]>;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } gpr:{ *:[i32] m1:[i32 i64] }:$src)\nincluded from /home/qishao/project/llvm-project/llvm/lib/target/riscv/riscv.td:30:\n/home/qishao/project/llvm-project/llvm/lib/target/riscv/riscvinstrinfo.td:1915:1: error: in anonymous_8306: could not infer all types in pattern!\ndef : pat<(trunc gpr:$src), (copy gpr:$src)>;\n^\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } gpr:{ *:[i32] m1:[i32 i64] }:$src)\nanonymous_8306: \t(copy:{ *:[i16] m1:[i16 i32] } gpr:{ *:[i32] m1:[i32 i64] }:$src)\nincluded from /home/qishao/project/llvm-project/llvm/lib/target/riscv/riscv.td:30:\n/home/qishao/project/llvm-project/llvm/lib/target/riscv/riscvinstrinfo.td:1915:1: error: in anonymous_8306: could not infer all types in pattern result!\ndef : pat<(trunc gpr:$src), (copy gpr:$src)>;\n^\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nthe failed issues after i def ace register class\n\ndef ace : registerclass<"riscv", [i16], 16, (add\n    (sequence "ace%u", 0, 3)\n)>;\n\n\n\n1\n2\n3\n4\n',charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Understand llvm with its source code Part 1",frontmatter:{title:"Understand llvm with its source code Part 1",output:{html_document:{code_folding:"hide"}},date:"2024-07-05T00:00:00.000Z",permalink:"/pages/000007/"},regularPath:"/02.compiler/07.%20UnderstaningLLVMwithSourceCode.html",relativePath:"02.compiler/07. UnderstaningLLVMwithSourceCode.md",key:"v-89581d5c",path:"/pages/000007/",headers:[{level:3,title:"1. Create SelectionDAG",slug:"_1-create-selectiondag",normalizedTitle:"1. create selectiondag",charIndex:2},{level:3,title:"2. Legalization",slug:"_2-legalization",normalizedTitle:"2. legalization",charIndex:1458},{level:3,title:"3. Instruction Selection",slug:"_3-instruction-selection",normalizedTitle:"3. instruction selection",charIndex:5867},{level:3,title:"4. Instruction Scheduler",slug:"_4-instruction-scheduler",normalizedTitle:"4. instruction scheduler",charIndex:10907}],headersStr:"1. Create SelectionDAG 2. Legalization 3. Instruction Selection 4. Instruction Scheduler",content:'# 1. Create SelectionDAG\n\nSelectionDAG Builder calls visit() function to build SDNode\n\n// CodeGen/SelectionDAG/SelectionDAGBuilder.cpp\nvoid SelectionDAGBuilder::visit(unsigned Opcode, const User &I) {\n  // Note: this doesn\'t use InstVisitor, because it has to work with\n  // ConstantExpr\'s in addition to instructions.\n  switch (Opcode) {\n  default: llvm_unreachable("Unknown instruction type encountered!");\n    // Build the switch statement using the Instruction.def file.\n#define HANDLE_INST(NUM, OPCODE, CLASS) \\\n    case Instruction::OPCODE: visit##OPCODE((const CLASS&)I); break;\n#include "llvm/IR/Instruction.def"\n  }\n}\n\n// include/llvm/IR/Instruction.def\nHANDLE_BINARY_INST(20, SDiv , BinaryOperator)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nvisitSDiv(const User &I) will create SDValue as operand and SDNode for each IR\n\nvoid SelectionDAGBuilder::visitSDiv(const User &I) {\n  SDValue Op1 = getValue(I.getOperand(0));\n  SDValue Op2 = getValue(I.getOperand(1));\n\n  SDNodeFlags Flags;\n  Flags.setExact(isa<PossiblyExactOperator>(&I) &&\n                 cast<PossiblyExactOperator>(&I)->isExact());\n  setValue(&I, DAG.getNode(ISD::SDIV, getCurSDLoc(), Op1.getValueType(), Op1,\n                           Op2, Flags));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nSDNode also contains dependencies in SDValue, SDValue include following:\n\n 1. data dependency\n 2. chain dependency. For example. order load, store insturction to the same address\n 3. glue of instructions.\n\n\n# 2. Legalization\n\nLegalization will legalize SDNode operation that is unsupported target into supported Node. It includes legalization of operation and operand. As to operation, it includes 3 major operation:\n\n 1. Expansion expand one op into series of op\n 2. Promotion promote data type\n 3. Custom\n\n// CodeGen/SelectionDAG/LegalizeDAG.cpp\n\n/// Return a legal replacement for the given operation, with all legal operands.\nvoid SelectionDAGLegalize::LegalizeOp(SDNode *Node) {\n.....\n    case TargetLowering::Expand:\n      if (ExpandNode(Node))\n        return;\n      [[fallthrough]];\n    case TargetLowering::LibCall:\n      ConvertNodeToLibcall(Node);\n      return;\n    case TargetLowering::Promote:\n      PromoteNode(Node);\n      return;\n    }\n\n  switch (Node->getOpcode()) {\n  case ISD::LOAD:\n    return LegalizeLoadOps(Node);\n  case ISD::STORE:\n    return LegalizeStoreOps(Node);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nLook closely to legalize of stop operation, it considered Expand, Custom and Promote.\n\nvoid SelectionDAGLegalize::LegalizeStoreOps(SDNode *Node) {\n    SDValue Value = ST->getValue();\n    MVT VT = Value.getSimpleValueType();\n    switch (TLI.getOperationAction(ISD::STORE, VT)) {\n    case TargetLowering::Legal: {\n      // If this is an unaligned store and the target doesn\'t support it, expand it.\n      EVT MemVT = ST->getMemoryVT();\n      const DataLayout &DL = DAG.getDataLayout();\n      if (!TLI.allowsMemoryAccessForAlignment(*DAG.getContext(), DL, MemVT,\n                                              *ST->getMemOperand())) {\n        SDValue Result = TLI.expandUnalignedStore(ST, DAG);\n        ReplaceNode(SDValue(ST, 0), Result);\n      }\n      break;\n    }\n    case TargetLowering::Custom: {\n      SDValue Res = TLI.LowerOperation(SDValue(Node, 0), DAG);\n      if (Res && Res != SDValue(Node, 0))\n        ReplaceNode(SDValue(Node, 0), Res);\n      return;\n    }\n    case TargetLowering::Promote: {\n      MVT NVT = TLI.getTypeToPromoteTo(ISD::STORE, VT);\n      Value = DAG.getNode(ISD::BITCAST, dl, NVT, Value);\n      SDValue Result = DAG.getStore(Chain, dl, Value, Ptr, ST->getPointerInfo(),\n                                    ST->getOriginalAlign(), MMOFlags, AAInfo);\n      ReplaceNode(SDValue(Node, 0), Result);\n      break;\n    }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\nIt also includes LibCall now.\n\n// include/llvm/CodeGen/TargetLowering.h\nclass TargetLoweringBase {\npublic:\n  /// This enum indicates whether operations are valid for a target, and if not,\n  /// what action should be used to make them valid.\n  enum LegalizeAction : uint8_t {\n    Legal,      // The target natively supports this operation.\n    Promote,    // This operation should be executed in a larger type.\n    Expand,     // Try to expand this to other ops, otherwise use a libcall.\n    LibCall,    // Don\'t try to expand this to other ops, always use a libcall.\n    Custom      // Use the LowerOperation hook to implement custom lowering.\n  };\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nAs to legalize operand, in ./CodeGen/SelectionDAG/LegalizeTypes.h, it shows supported functions.\n\n  //===--------------------------------------------------------------------===//\n  // Integer Promotion Support: LegalizeIntegerTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Integer Expansion Support: LegalizeIntegerTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Float to Integer Conversion Support: LegalizeFloatTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Float Expansion Support: LegalizeFloatTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Scalarization Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Vector Splitting Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Vector Widening Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Vector Widening Utilities Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 3. Instruction Selection\n\nCodeGen/SelectionDAG/SelectionDAGISel.cpp\nvoid SelectionDAGISel::CodeGenAndEmitDAG() {\n  // Pre-type legalization allow creation of any node types.\n  CurDAG->NewNodesMustHaveLegalTypes = false;\n  // Run the DAG combiner in pre-legalize mode.\n  CurDAG->Combine(BeforeLegalizeTypes, AA, OptLevel);\n  // Second step, hack on the DAG until it only uses operations and types that\n  // the target supports.\n  Changed = CurDAG->LegalizeTypes();\n\n  // Only allow creation of legal node types.\n  CurDAG->NewNodesMustHaveLegalTypes = true;\n   // Run the DAG combiner in post-type-legalize mode.\n  NamedRegionTimer T("combine_lt", "DAG Combining after legalize types",\n                         GroupName, GroupDescription, TimePassesIsEnabled);\n  CurDAG->Combine(AfterLegalizeTypes, AA, OptLevel);\n\n  Changed = CurDAG->LegalizeVectors();\n  // Run the DAG combiner in post-type-legalize mode.\n  NamedRegionTimer T("combine_lv", "DAG Combining after legalize vectors",\n                         GroupName, GroupDescription, TimePassesIsEnabled);\n  CurDAG->Combine(AfterLegalizeVectorOps, AA, OptLevel);\n\n  CurDAG->Legalize();\n\n  // Run the DAG combiner in post-legalize mode.\n  NamedRegionTimer T("combine2", "DAG Combining 2", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n  CurDAG->Combine(AfterLegalizeDAG, AA, OptLevel);\n  \n  ComputeLiveOutVRegInfo();\n\n  DoInstructionSelection();\n\n  // Schedule machine code.\n  ScheduleDAGSDNodes *Scheduler = CreateScheduler();\n  {\n    NamedRegionTimer T("sched", "Instruction Scheduling", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n    Scheduler->Run(CurDAG, FuncInfo->MBB);\n  }\n\n  // Emit machine code to BB.  This can change \'BB\' to the last block being\n  // inserted into.\n  MachineBasicBlock *FirstMBB = FuncInfo->MBB, *LastMBB;\n  {\n    NamedRegionTimer T("emit", "Instruction Creation", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n\n    // FuncInfo->InsertPt is passed by reference and set to the end of the\n    // scheduled instructions.\n    LastMBB = FuncInfo->MBB = Scheduler->EmitSchedule(FuncInfo->InsertPt);\n  }\n\n  // If the block was split, make sure we update any references that are used to\n  // update PHI nodes later on.\n  if (FirstMBB != LastMBB)\n    SDB->UpdateSplitBlock(FirstMBB, LastMBB);\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n//File include/llvm/CodeGen/SelectionDAGISel.h\n// Main hook for targets to transform nodes into machine nodes.\nvirtual void Select(SDNode *N) = 0;\n\n//File CodeGen/SelectionDAG/SelectionDAGISel.cpp\nvoid SelectionDAGISel::DoInstructionSelection() {\n  LLVM_DEBUG(dbgs() << "===== Instruction selection begins: "\n                    << printMBBReference(*FuncInfo->MBB) << " \'"\n                    << FuncInfo->MBB->getName() << "\'\\n");\n  PreprocessISelDAG();\n\n  // Select target instructions for the DAG.\n  // Number all nodes with a topological order and set DAGSize.\n  DAGSize = CurDAG->AssignTopologicalOrder();\n\n  // The AllNodes list is now topological-sorted. Visit the\n  // nodes by starting at the end of the list (the root of the\n  // graph) and preceding back toward the beginning (the entry\n  // node).\n  while (ISelPosition != CurDAG->allnodes_begin()) {\n    SDNode *Node = &*--ISelPosition;\n    ...\n    \n    Select(Node);\n  }\n\n  CurDAG->setRoot(Dummy.getValue());\n  \n  LLVM_DEBUG(dbgs() << "\\n===== Instruction selection ends:\\n");\n  PostprocessISelDAG();\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nSelect(SDNode *N) will be overide by target backend:\n\n//File Target/MSP430/MSP430ISelDAGToDAG.cpp\n\n  #include "MSP430GenDAGISel.inc"\nvoid MSP430DAGToDAGISel::Select(SDNode *Node) {\n  // Few custom selection stuff.\n  switch (Node->getOpcode()) {\n  default: break;\n  case ISD::LOAD:\n    if (tryIndexedLoad(Node))\n      return;\n    // Other cases are autogenerated.\n    break;\n  case ISD::ADD:\n    if (tryIndexedBinOp(Node, Node->getOperand(0), Node->getOperand(1),\n                        MSP430::ADD8rp, MSP430::ADD16rp))\n      return;\n    else if (tryIndexedBinOp(Node, Node->getOperand(1), Node->getOperand(0),\n                             MSP430::ADD8rp, MSP430::ADD16rp))\n      return;\n  // Select the default instruction\n  SelectCode(Node);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nIn the end, it is the SelectCode function. This function will be generated in XXXGenDAGISel.inc from XXXInstrInfo.td. For example, it would be like this:\n\n  SDNode *SelectCode(SDValue N) {\n    ...\n    MVT::ValueType NVT = N.getNode()->getValueType(0);\n    switch (N.getOpcode()) {\n    case ISD::STORE: {\n      switch (NVT) {\n      default:\n        return Select_ISD_STORE(N);\n        break;\n      }\n      break;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nAfter the instruction selection, the LLVM IR will be represented in form of machineDAG.\n\n\n# 4. Instruction Scheduler\n\nIt includes 3 scheulder\n\n 1. ScheduleDAG. This is in the stage of instruction selection.\n\nScheduleDAGSDNodes.cpp\nScheduleDAGFast.cpp\nScheduleDAGRRList.cpp\nScheduleDAGVLIW.cpp\n\n\n1\n2\n3\n4\n\n\nFile ScheduleDAGRRList.cpp\n//===- ScheduleDAGRRList.cpp - Reg pressure reduction list scheduler ------===//\n// This implements bottom-up and top-down register pressure reduction list\n// schedulers, using standard algorithms.  The basic approach uses a priority\n// queue of available nodes to schedule.  One at a time, nodes are taken from\n// the priority queue (thus in priority order), checked for legality to\n// schedule, and emitted if legal.\n//\n//===----------------------------------------------------------------------===//\nburrListDAGScheduler("list-burr",\n                    "Bottom-up register reduction list scheduling",\n                    createBURRListDAGScheduler);\n\nsourceListDAGScheduler("source",\n                    "Similar to list-burr but schedules in source "\n                    "order when possible",\n                    createSourceListDAGScheduler);\n\nhybridListDAGScheduler("list-hybrid",\n                    "Bottom-up register pressure aware list scheduling "\n                    "which tries to balance latency and register pressure",\n                    createHybridListDAGScheduler);\n\nILPListDAGScheduler("list-ilp",\n                    "Bottom-up register pressure aware list scheduling "\n                    "which tries to balance ILP and register pressure",\n                    createILPListDAGScheduler);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\nIn every DAGScheduler, the flow is similar, the only difference is the construction of priority queue.\n\nScheduleDAGSDNodes *\nllvm::createHybridListDAGScheduler(SelectionDAGISel *IS,\n                                   CodeGenOptLevel OptLevel) {\n  HybridBURRPriorityQueue *PQ =\n    new HybridBURRPriorityQueue(*IS->MF, true, false, TII, TRI, TLI);\n\n  ScheduleDAGRRList *SD = new ScheduleDAGRRList(*IS->MF, true, PQ, OptLevel);\n  PQ->setScheduleDAG(SD);\n  return SD;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nfunction CodeGenAdnEmitDAG contains founction CreateScheduler, it can call target-defined scheduler and also the schedulers defined above.\n\nHere llvm utilize template, it can be seen that the template parameter is different scheduler policy, it defines them as struct.\n\nIn the "template class RegReductionPriorityQueue", SF will be instantiated as "Picker", the ick will influence "isReady" and "pop".\n\nusing BURegReductionPriorityQueue = RegReductionPriorityQueue<bu_ls_rr_sort>;\nusing SrcRegReductionPriorityQueue = RegReductionPriorityQueue<src_ls_rr_sort>;\nusing HybridBURRPriorityQueue = RegReductionPriorityQueue<hybrid_ls_rr_sort>;\nusing ILPBURRPriorityQueue = RegReductionPriorityQueue<ilp_ls_rr_sort>;\n\n// src_ls_rr_sort - Priority function for source order scheduler.\nstruct src_ls_rr_sort : public queue_sort {\n  enum {\n    IsBottomUp = true,\n    HasReadyFilter = false\n  };\n  RegReductionPQBase *SPQ;\n  src_ls_rr_sort(RegReductionPQBase *spq) : SPQ(spq) {}\n  bool operator()(SUnit* left, SUnit* right) const;\n};\n\n// hybrid_ls_rr_sort - Priority function for hybrid scheduler.\nstruct hybrid_ls_rr_sort : public queue_sort {\n  enum {\n    IsBottomUp = true,\n    HasReadyFilter = false\n  };\n  RegReductionPQBase *SPQ;\n  hybrid_ls_rr_sort(RegReductionPQBase *spq) : SPQ(spq) {}\n  bool isReady(SUnit *SU, unsigned CurCycle) const;\n  bool operator()(SUnit* left, SUnit* right) const;\n};\n\n//===----------------------------------------------------------------------===//\n//                RegReductionPriorityQueue Definition\n//===----------------------------------------------------------------------===//\n//\n// This is a SchedulingPriorityQueue that schedules using Sethi Ullman numbers\n// to reduce register pressure.\n//\ntemplate<class SF>\nclass RegReductionPriorityQueue : public RegReductionPQBase {\n  SF Picker;\npublic:\n  RegReductionPriorityQueue(MachineFunction &mf,\n                            bool tracksrp,\n                            bool srcorder,\n                            const TargetInstrInfo *tii,\n                            const TargetRegisterInfo *tri,\n                            const TargetLowering *tli)\n    : RegReductionPQBase(mf, SF::HasReadyFilter, tracksrp, srcorder,\n                         tii, tri, tli),\n      Picker(this) {}\n\n  bool isBottomUp() const override { return SF::IsBottomUp; }\n\n  bool isReady(SUnit *U) const override {\n    return Picker.HasReadyFilter && Picker.isReady(U, getCurCycle());\n  }\n\n  SUnit *pop() override {\n    if (Queue.empty()) return nullptr;\n\n    SUnit *V = popFromQueue(Queue, Picker, scheduleDAG);\n    V->NodeQueueId = 0;\n    return V;\n  }\n};\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\nIn Instruction Selection, it calls creater scheduler and Run. For simplicity, I delete all the trivial code, like reset or clear, or clear function in the code.\n\n  // Schedule machine code.\n  ScheduleDAGSDNodes *Scheduler = CreateScheduler();\n  {\n    NamedRegionTimer T("sched", "Instruction Scheduling", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n    Scheduler->Run(CurDAG, FuncInfo->MBB);\n  }\n\n// File CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp\n/// Run - perform scheduling.\nvoid ScheduleDAGSDNodes::Run(SelectionDAG *dag, MachineBasicBlock *bb) {\n   ....\n  // Invoke the target\'s selection of scheduler.\n  Schedule();\n}\n\n\n// File CodeGen/SelectionDAG/ScheduleDAGRRList.cpp\n/// Schedule - Schedule the DAG using list scheduling.\nvoid ScheduleDAGRRList::Schedule() {\n  LLVM_DEBUG(dbgs() << "********** List Scheduling " << printMBBReference(*BB)\n                    << " \'" << BB->getName() << "\' **********\\n");\n  BuildSchedGraph(nullptr);\n  Topo.MarkDirty();\n  AvailableQueue->initNodes(SUnits);\n  // Execute the actual scheduling loop.\n  ListScheduleBottomUp();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\nIn Schedule() function, it contains the following code:\n\n 1. BuildSchedGraph. This function creates SUnit Graph\n\n// File CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp\n/// BuildSchedGraph - Build the SUnit graph from the selection dag that we\n/// are input.  This SUnit graph is similar to the SelectionDAG, but\n/// excludes nodes that aren\'t interesting to scheduling, and represents\n/// glued together nodes with a single SUnit.\nvoid ScheduleDAGSDNodes::BuildSchedGraph(AAResults *AA) {\n  // Cluster certain nodes which should be scheduled together.\n  ClusterNodes();\n  // Populate the SUnits array.\n  // During scheduling, the NodeId field of SDNode is used to map SDNodes\n  // to their associated SUnits by holding SUnits table indices\n  // Multiple SDNodes might be associated to one SUnit.\n  BuildSchedUnits();\n  // Compute all the scheduling dependencies between nodes.\n  AddSchedEdges();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nBuildSchedUnits will also calculate the latency for each Sunit, using computeLatency\n\n 2. ListScheduleBottomUp()\n\n// ListScheduleBottomUp - The main loop of list scheduling for bottom-up\n// schedulers.\nvoid ScheduleDAGRRList::ListScheduleBottomUp() {\n  // Release any predecessors of the special Exit node.\n  ReleasePredecessors(&ExitSU);\n  // Add root to Available queue.\n  if (!SUnits.empty()) {\n    SUnit *RootSU = &SUnits[DAG->getRoot().getNode()->getNodeId()];\n    RootSU->isAvailable = true;\n    AvailableQueue->push(RootSU);\n  }\n\n  // While Available queue is not empty, grab the node with the highest\n  // priority. If it is not ready put it back.  Schedule the node.\n  Sequence.reserve(SUnits.size());\n  while (!AvailableQueue->empty() || !Interferences.empty()) {\n    // Pick the best node to schedule taking all constraints into\n    // consideration.\n    // Return a node that can be scheduled in this cycle. Requirements:\n    // (1) Ready: latency has been satisfied\n    // (2) No Hazards: resources are available\n    // (3) No Interferences: may unschedule to break register interferences.\n    SUnit *SU = PickNodeToScheduleBottomUp();\n    /// Move the scheduler state forward until the specified node\'s dependents are\n    /// ready and can be scheduled with no resource conflicts.\n    AdvancePastStalls(SU);\n    // ScheduleNodeBottomUp - Add the node to the schedule. Decrement the pending\n    // count of its predecessors. If a predecessor pending count is zero, add it to\n    // the Available queue.\n    ScheduleNodeBottomUp(SU);\n\n    while (AvailableQueue->empty() && !PendingQueue.empty()) {\n      AdvanceToCycle(std::max(CurCycle + 1, MinAvailableCycle));\n    }\n  }\n\n  // Reverse the order if it is bottom up.\n  std::reverse(Sequence.begin(), Sequence.end());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nAdvanceToCycle(unsigned NextCycle) would be good function to be noticed. For example, in AdvancePastStalls(), the scheduler will advance to the cycle when the chosedn SUnit is ready.',normalizedContent:'# 1. create selectiondag\n\nselectiondag builder calls visit() function to build sdnode\n\n// codegen/selectiondag/selectiondagbuilder.cpp\nvoid selectiondagbuilder::visit(unsigned opcode, const user &i) {\n  // note: this doesn\'t use instvisitor, because it has to work with\n  // constantexpr\'s in addition to instructions.\n  switch (opcode) {\n  default: llvm_unreachable("unknown instruction type encountered!");\n    // build the switch statement using the instruction.def file.\n#define handle_inst(num, opcode, class) \\\n    case instruction::opcode: visit##opcode((const class&)i); break;\n#include "llvm/ir/instruction.def"\n  }\n}\n\n// include/llvm/ir/instruction.def\nhandle_binary_inst(20, sdiv , binaryoperator)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nvisitsdiv(const user &i) will create sdvalue as operand and sdnode for each ir\n\nvoid selectiondagbuilder::visitsdiv(const user &i) {\n  sdvalue op1 = getvalue(i.getoperand(0));\n  sdvalue op2 = getvalue(i.getoperand(1));\n\n  sdnodeflags flags;\n  flags.setexact(isa<possiblyexactoperator>(&i) &&\n                 cast<possiblyexactoperator>(&i)->isexact());\n  setvalue(&i, dag.getnode(isd::sdiv, getcursdloc(), op1.getvaluetype(), op1,\n                           op2, flags));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nsdnode also contains dependencies in sdvalue, sdvalue include following:\n\n 1. data dependency\n 2. chain dependency. for example. order load, store insturction to the same address\n 3. glue of instructions.\n\n\n# 2. legalization\n\nlegalization will legalize sdnode operation that is unsupported target into supported node. it includes legalization of operation and operand. as to operation, it includes 3 major operation:\n\n 1. expansion expand one op into series of op\n 2. promotion promote data type\n 3. custom\n\n// codegen/selectiondag/legalizedag.cpp\n\n/// return a legal replacement for the given operation, with all legal operands.\nvoid selectiondaglegalize::legalizeop(sdnode *node) {\n.....\n    case targetlowering::expand:\n      if (expandnode(node))\n        return;\n      [[fallthrough]];\n    case targetlowering::libcall:\n      convertnodetolibcall(node);\n      return;\n    case targetlowering::promote:\n      promotenode(node);\n      return;\n    }\n\n  switch (node->getopcode()) {\n  case isd::load:\n    return legalizeloadops(node);\n  case isd::store:\n    return legalizestoreops(node);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nlook closely to legalize of stop operation, it considered expand, custom and promote.\n\nvoid selectiondaglegalize::legalizestoreops(sdnode *node) {\n    sdvalue value = st->getvalue();\n    mvt vt = value.getsimplevaluetype();\n    switch (tli.getoperationaction(isd::store, vt)) {\n    case targetlowering::legal: {\n      // if this is an unaligned store and the target doesn\'t support it, expand it.\n      evt memvt = st->getmemoryvt();\n      const datalayout &dl = dag.getdatalayout();\n      if (!tli.allowsmemoryaccessforalignment(*dag.getcontext(), dl, memvt,\n                                              *st->getmemoperand())) {\n        sdvalue result = tli.expandunalignedstore(st, dag);\n        replacenode(sdvalue(st, 0), result);\n      }\n      break;\n    }\n    case targetlowering::custom: {\n      sdvalue res = tli.loweroperation(sdvalue(node, 0), dag);\n      if (res && res != sdvalue(node, 0))\n        replacenode(sdvalue(node, 0), res);\n      return;\n    }\n    case targetlowering::promote: {\n      mvt nvt = tli.gettypetopromoteto(isd::store, vt);\n      value = dag.getnode(isd::bitcast, dl, nvt, value);\n      sdvalue result = dag.getstore(chain, dl, value, ptr, st->getpointerinfo(),\n                                    st->getoriginalalign(), mmoflags, aainfo);\n      replacenode(sdvalue(node, 0), result);\n      break;\n    }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\nit also includes libcall now.\n\n// include/llvm/codegen/targetlowering.h\nclass targetloweringbase {\npublic:\n  /// this enum indicates whether operations are valid for a target, and if not,\n  /// what action should be used to make them valid.\n  enum legalizeaction : uint8_t {\n    legal,      // the target natively supports this operation.\n    promote,    // this operation should be executed in a larger type.\n    expand,     // try to expand this to other ops, otherwise use a libcall.\n    libcall,    // don\'t try to expand this to other ops, always use a libcall.\n    custom      // use the loweroperation hook to implement custom lowering.\n  };\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nas to legalize operand, in ./codegen/selectiondag/legalizetypes.h, it shows supported functions.\n\n  //===--------------------------------------------------------------------===//\n  // integer promotion support: legalizeintegertypes.cpp\n  //===--------------------------------------------------------------------===//\n  // integer expansion support: legalizeintegertypes.cpp\n  //===--------------------------------------------------------------------===//\n  // float to integer conversion support: legalizefloattypes.cpp\n  //===--------------------------------------------------------------------===//\n  // float expansion support: legalizefloattypes.cpp\n  //===--------------------------------------------------------------------===//\n  // scalarization support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n  // vector splitting support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n  // vector widening support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n  // vector widening utilities support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 3. instruction selection\n\ncodegen/selectiondag/selectiondagisel.cpp\nvoid selectiondagisel::codegenandemitdag() {\n  // pre-type legalization allow creation of any node types.\n  curdag->newnodesmusthavelegaltypes = false;\n  // run the dag combiner in pre-legalize mode.\n  curdag->combine(beforelegalizetypes, aa, optlevel);\n  // second step, hack on the dag until it only uses operations and types that\n  // the target supports.\n  changed = curdag->legalizetypes();\n\n  // only allow creation of legal node types.\n  curdag->newnodesmusthavelegaltypes = true;\n   // run the dag combiner in post-type-legalize mode.\n  namedregiontimer t("combine_lt", "dag combining after legalize types",\n                         groupname, groupdescription, timepassesisenabled);\n  curdag->combine(afterlegalizetypes, aa, optlevel);\n\n  changed = curdag->legalizevectors();\n  // run the dag combiner in post-type-legalize mode.\n  namedregiontimer t("combine_lv", "dag combining after legalize vectors",\n                         groupname, groupdescription, timepassesisenabled);\n  curdag->combine(afterlegalizevectorops, aa, optlevel);\n\n  curdag->legalize();\n\n  // run the dag combiner in post-legalize mode.\n  namedregiontimer t("combine2", "dag combining 2", groupname,\n                       groupdescription, timepassesisenabled);\n  curdag->combine(afterlegalizedag, aa, optlevel);\n  \n  computeliveoutvreginfo();\n\n  doinstructionselection();\n\n  // schedule machine code.\n  scheduledagsdnodes *scheduler = createscheduler();\n  {\n    namedregiontimer t("sched", "instruction scheduling", groupname,\n                       groupdescription, timepassesisenabled);\n    scheduler->run(curdag, funcinfo->mbb);\n  }\n\n  // emit machine code to bb.  this can change \'bb\' to the last block being\n  // inserted into.\n  machinebasicblock *firstmbb = funcinfo->mbb, *lastmbb;\n  {\n    namedregiontimer t("emit", "instruction creation", groupname,\n                       groupdescription, timepassesisenabled);\n\n    // funcinfo->insertpt is passed by reference and set to the end of the\n    // scheduled instructions.\n    lastmbb = funcinfo->mbb = scheduler->emitschedule(funcinfo->insertpt);\n  }\n\n  // if the block was split, make sure we update any references that are used to\n  // update phi nodes later on.\n  if (firstmbb != lastmbb)\n    sdb->updatesplitblock(firstmbb, lastmbb);\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n//file include/llvm/codegen/selectiondagisel.h\n// main hook for targets to transform nodes into machine nodes.\nvirtual void select(sdnode *n) = 0;\n\n//file codegen/selectiondag/selectiondagisel.cpp\nvoid selectiondagisel::doinstructionselection() {\n  llvm_debug(dbgs() << "===== instruction selection begins: "\n                    << printmbbreference(*funcinfo->mbb) << " \'"\n                    << funcinfo->mbb->getname() << "\'\\n");\n  preprocessiseldag();\n\n  // select target instructions for the dag.\n  // number all nodes with a topological order and set dagsize.\n  dagsize = curdag->assigntopologicalorder();\n\n  // the allnodes list is now topological-sorted. visit the\n  // nodes by starting at the end of the list (the root of the\n  // graph) and preceding back toward the beginning (the entry\n  // node).\n  while (iselposition != curdag->allnodes_begin()) {\n    sdnode *node = &*--iselposition;\n    ...\n    \n    select(node);\n  }\n\n  curdag->setroot(dummy.getvalue());\n  \n  llvm_debug(dbgs() << "\\n===== instruction selection ends:\\n");\n  postprocessiseldag();\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nselect(sdnode *n) will be overide by target backend:\n\n//file target/msp430/msp430iseldagtodag.cpp\n\n  #include "msp430gendagisel.inc"\nvoid msp430dagtodagisel::select(sdnode *node) {\n  // few custom selection stuff.\n  switch (node->getopcode()) {\n  default: break;\n  case isd::load:\n    if (tryindexedload(node))\n      return;\n    // other cases are autogenerated.\n    break;\n  case isd::add:\n    if (tryindexedbinop(node, node->getoperand(0), node->getoperand(1),\n                        msp430::add8rp, msp430::add16rp))\n      return;\n    else if (tryindexedbinop(node, node->getoperand(1), node->getoperand(0),\n                             msp430::add8rp, msp430::add16rp))\n      return;\n  // select the default instruction\n  selectcode(node);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nin the end, it is the selectcode function. this function will be generated in xxxgendagisel.inc from xxxinstrinfo.td. for example, it would be like this:\n\n  sdnode *selectcode(sdvalue n) {\n    ...\n    mvt::valuetype nvt = n.getnode()->getvaluetype(0);\n    switch (n.getopcode()) {\n    case isd::store: {\n      switch (nvt) {\n      default:\n        return select_isd_store(n);\n        break;\n      }\n      break;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nafter the instruction selection, the llvm ir will be represented in form of machinedag.\n\n\n# 4. instruction scheduler\n\nit includes 3 scheulder\n\n 1. scheduledag. this is in the stage of instruction selection.\n\nscheduledagsdnodes.cpp\nscheduledagfast.cpp\nscheduledagrrlist.cpp\nscheduledagvliw.cpp\n\n\n1\n2\n3\n4\n\n\nfile scheduledagrrlist.cpp\n//===- scheduledagrrlist.cpp - reg pressure reduction list scheduler ------===//\n// this implements bottom-up and top-down register pressure reduction list\n// schedulers, using standard algorithms.  the basic approach uses a priority\n// queue of available nodes to schedule.  one at a time, nodes are taken from\n// the priority queue (thus in priority order), checked for legality to\n// schedule, and emitted if legal.\n//\n//===----------------------------------------------------------------------===//\nburrlistdagscheduler("list-burr",\n                    "bottom-up register reduction list scheduling",\n                    createburrlistdagscheduler);\n\nsourcelistdagscheduler("source",\n                    "similar to list-burr but schedules in source "\n                    "order when possible",\n                    createsourcelistdagscheduler);\n\nhybridlistdagscheduler("list-hybrid",\n                    "bottom-up register pressure aware list scheduling "\n                    "which tries to balance latency and register pressure",\n                    createhybridlistdagscheduler);\n\nilplistdagscheduler("list-ilp",\n                    "bottom-up register pressure aware list scheduling "\n                    "which tries to balance ilp and register pressure",\n                    createilplistdagscheduler);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\nin every dagscheduler, the flow is similar, the only difference is the construction of priority queue.\n\nscheduledagsdnodes *\nllvm::createhybridlistdagscheduler(selectiondagisel *is,\n                                   codegenoptlevel optlevel) {\n  hybridburrpriorityqueue *pq =\n    new hybridburrpriorityqueue(*is->mf, true, false, tii, tri, tli);\n\n  scheduledagrrlist *sd = new scheduledagrrlist(*is->mf, true, pq, optlevel);\n  pq->setscheduledag(sd);\n  return sd;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nfunction codegenadnemitdag contains founction createscheduler, it can call target-defined scheduler and also the schedulers defined above.\n\nhere llvm utilize template, it can be seen that the template parameter is different scheduler policy, it defines them as struct.\n\nin the "template class regreductionpriorityqueue", sf will be instantiated as "picker", the ick will influence "isready" and "pop".\n\nusing buregreductionpriorityqueue = regreductionpriorityqueue<bu_ls_rr_sort>;\nusing srcregreductionpriorityqueue = regreductionpriorityqueue<src_ls_rr_sort>;\nusing hybridburrpriorityqueue = regreductionpriorityqueue<hybrid_ls_rr_sort>;\nusing ilpburrpriorityqueue = regreductionpriorityqueue<ilp_ls_rr_sort>;\n\n// src_ls_rr_sort - priority function for source order scheduler.\nstruct src_ls_rr_sort : public queue_sort {\n  enum {\n    isbottomup = true,\n    hasreadyfilter = false\n  };\n  regreductionpqbase *spq;\n  src_ls_rr_sort(regreductionpqbase *spq) : spq(spq) {}\n  bool operator()(sunit* left, sunit* right) const;\n};\n\n// hybrid_ls_rr_sort - priority function for hybrid scheduler.\nstruct hybrid_ls_rr_sort : public queue_sort {\n  enum {\n    isbottomup = true,\n    hasreadyfilter = false\n  };\n  regreductionpqbase *spq;\n  hybrid_ls_rr_sort(regreductionpqbase *spq) : spq(spq) {}\n  bool isready(sunit *su, unsigned curcycle) const;\n  bool operator()(sunit* left, sunit* right) const;\n};\n\n//===----------------------------------------------------------------------===//\n//                regreductionpriorityqueue definition\n//===----------------------------------------------------------------------===//\n//\n// this is a schedulingpriorityqueue that schedules using sethi ullman numbers\n// to reduce register pressure.\n//\ntemplate<class sf>\nclass regreductionpriorityqueue : public regreductionpqbase {\n  sf picker;\npublic:\n  regreductionpriorityqueue(machinefunction &mf,\n                            bool tracksrp,\n                            bool srcorder,\n                            const targetinstrinfo *tii,\n                            const targetregisterinfo *tri,\n                            const targetlowering *tli)\n    : regreductionpqbase(mf, sf::hasreadyfilter, tracksrp, srcorder,\n                         tii, tri, tli),\n      picker(this) {}\n\n  bool isbottomup() const override { return sf::isbottomup; }\n\n  bool isready(sunit *u) const override {\n    return picker.hasreadyfilter && picker.isready(u, getcurcycle());\n  }\n\n  sunit *pop() override {\n    if (queue.empty()) return nullptr;\n\n    sunit *v = popfromqueue(queue, picker, scheduledag);\n    v->nodequeueid = 0;\n    return v;\n  }\n};\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\nin instruction selection, it calls creater scheduler and run. for simplicity, i delete all the trivial code, like reset or clear, or clear function in the code.\n\n  // schedule machine code.\n  scheduledagsdnodes *scheduler = createscheduler();\n  {\n    namedregiontimer t("sched", "instruction scheduling", groupname,\n                       groupdescription, timepassesisenabled);\n    scheduler->run(curdag, funcinfo->mbb);\n  }\n\n// file codegen/selectiondag/scheduledagsdnodes.cpp\n/// run - perform scheduling.\nvoid scheduledagsdnodes::run(selectiondag *dag, machinebasicblock *bb) {\n   ....\n  // invoke the target\'s selection of scheduler.\n  schedule();\n}\n\n\n// file codegen/selectiondag/scheduledagrrlist.cpp\n/// schedule - schedule the dag using list scheduling.\nvoid scheduledagrrlist::schedule() {\n  llvm_debug(dbgs() << "********** list scheduling " << printmbbreference(*bb)\n                    << " \'" << bb->getname() << "\' **********\\n");\n  buildschedgraph(nullptr);\n  topo.markdirty();\n  availablequeue->initnodes(sunits);\n  // execute the actual scheduling loop.\n  listschedulebottomup();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\nin schedule() function, it contains the following code:\n\n 1. buildschedgraph. this function creates sunit graph\n\n// file codegen/selectiondag/scheduledagsdnodes.cpp\n/// buildschedgraph - build the sunit graph from the selection dag that we\n/// are input.  this sunit graph is similar to the selectiondag, but\n/// excludes nodes that aren\'t interesting to scheduling, and represents\n/// glued together nodes with a single sunit.\nvoid scheduledagsdnodes::buildschedgraph(aaresults *aa) {\n  // cluster certain nodes which should be scheduled together.\n  clusternodes();\n  // populate the sunits array.\n  // during scheduling, the nodeid field of sdnode is used to map sdnodes\n  // to their associated sunits by holding sunits table indices\n  // multiple sdnodes might be associated to one sunit.\n  buildschedunits();\n  // compute all the scheduling dependencies between nodes.\n  addschededges();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nbuildschedunits will also calculate the latency for each sunit, using computelatency\n\n 2. listschedulebottomup()\n\n// listschedulebottomup - the main loop of list scheduling for bottom-up\n// schedulers.\nvoid scheduledagrrlist::listschedulebottomup() {\n  // release any predecessors of the special exit node.\n  releasepredecessors(&exitsu);\n  // add root to available queue.\n  if (!sunits.empty()) {\n    sunit *rootsu = &sunits[dag->getroot().getnode()->getnodeid()];\n    rootsu->isavailable = true;\n    availablequeue->push(rootsu);\n  }\n\n  // while available queue is not empty, grab the node with the highest\n  // priority. if it is not ready put it back.  schedule the node.\n  sequence.reserve(sunits.size());\n  while (!availablequeue->empty() || !interferences.empty()) {\n    // pick the best node to schedule taking all constraints into\n    // consideration.\n    // return a node that can be scheduled in this cycle. requirements:\n    // (1) ready: latency has been satisfied\n    // (2) no hazards: resources are available\n    // (3) no interferences: may unschedule to break register interferences.\n    sunit *su = picknodetoschedulebottomup();\n    /// move the scheduler state forward until the specified node\'s dependents are\n    /// ready and can be scheduled with no resource conflicts.\n    advancepaststalls(su);\n    // schedulenodebottomup - add the node to the schedule. decrement the pending\n    // count of its predecessors. if a predecessor pending count is zero, add it to\n    // the available queue.\n    schedulenodebottomup(su);\n\n    while (availablequeue->empty() && !pendingqueue.empty()) {\n      advancetocycle(std::max(curcycle + 1, minavailablecycle));\n    }\n  }\n\n  // reverse the order if it is bottom up.\n  std::reverse(sequence.begin(), sequence.end());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nadvancetocycle(unsigned nextcycle) would be good function to be noticed. for example, in advancepaststalls(), the scheduler will advance to the cycle when the chosedn sunit is ready.',charsets:{cjk:!0},lastUpdated:"2024/07/16, 20:14:06"},{title:"How does LLVM perform instruction combine",frontmatter:{title:"How does LLVM perform instruction combine",output:{html_document:{code_folding:"hide"}},date:"2024-03-17T00:00:00.000Z",permalink:"/pages/000006/"},regularPath:"/02.compiler/06.Value&Use.html",relativePath:"02.compiler/06.Value&Use.md",key:"v-c73e5956",path:"/pages/000006/",headers:[{level:3,title:"1. CodeFile",slug:"_1-codefile",normalizedTitle:"1. codefile",charIndex:2},{level:3,title:"2. Values & User",slug:"_2-values-user",normalizedTitle:"2. values &amp; user",charIndex:null},{level:3,title:"3. Q & A in Stackoverflow",slug:"_3-q-a-in-stackoverflow",normalizedTitle:"3. q &amp; a in stackoverflow",charIndex:null},{level:3,title:"Reference",slug:"reference",normalizedTitle:"reference",charIndex:7957}],headersStr:"1. CodeFile 2. Values & User 3. Q & A in Stackoverflow Reference",content:"# 1. CodeFile\n\nlib/Target/Mips/MipsISelLowering.cpp\n\nAll DAG Combine is called here.\n\nSDValue  MipsTargetLowering::PerformDAGCombine(SDNode *N, DAGCombinerInfo &DCI)\n  const {\n  SelectionDAG &DAG = DCI.DAG;\n  unsigned Opc = N->getOpcode();\n\n  switch (Opc) {\n  default: break;\n  case ISD::SDIVREM:\n  case ISD::UDIVREM:\n    return performDivRemCombine(N, DAG, DCI, Subtarget);\n  case ISD::SELECT:\n    return performSELECTCombine(N, DAG, DCI, Subtarget);\n  case MipsISD::CMovFP_F:\n  case MipsISD::CMovFP_T:\n    return performCMovFPCombine(N, DAG, DCI, Subtarget);\n  case ISD::AND:\n    return performANDCombine(N, DAG, DCI, Subtarget);\n  case ISD::OR:\n    return performORCombine(N, DAG, DCI, Subtarget);\n  case ISD::ADD:\n    return performADDCombine(N, DAG, DCI, Subtarget);\n  case ISD::SHL:\n    return performSHLCombine(N, DAG, DCI, Subtarget);\n  case ISD::SUB:\n    return performSUBCombine(N, DAG, DCI, Subtarget);\n  }\n\n  return SDValue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\nDefine of performSUBCombine\n\n * we can look into the process.\n * ISD::SUB will call performSUBCombine\n * It will combine (sub v0 (mul v1, v2)) into (msub v1, v2, v0)\n * The intresting thing is that, current SDNode Opcode is sub and if precedent node is mul, it will combine it into msub.\n\nHow to identify the precedent node?\n\n 1. SDValue Mult = ROOTNode->getOperand(1); // multi SDValue\n 2. SDValue AddOperand = ROOTNode->getOperand(0); // add SDValue\n 3. how about previous instruction?\n 4. ROOTNode->getOperand(0) will point to previous instruction\n\nstatic SDValue performSUBCombine(SDNode *N, SelectionDAG &DAG,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 const MipsSubtarget &Subtarget) {\n  // (sub v0 (mul v1, v2)) => (msub v1, v2, v0)\n  if (DCI.isBeforeLegalizeOps()) {\n    if (Subtarget.hasMips32() && !Subtarget.hasMips32r6() &&\n        !Subtarget.inMips16Mode() && N->getValueType(0) == MVT::i64)\n      return performMADD_MSUBCombine(N, DAG, Subtarget);\n\n    return SDValue();\n  }\n\n  return SDValue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nstatic SDValue performMADD_MSUBCombine(SDNode *ROOTNode, SelectionDAG &CurDAG,\n                                       const MipsSubtarget &Subtarget) {\n  SDValue Mult = ROOTNode->getOperand(0).getOpcode() == ISD::MUL\n                     ? ROOTNode->getOperand(0)\n                     : ROOTNode->getOperand(1);\n\n  SDValue AddOperand = ROOTNode->getOperand(0).getOpcode() == ISD::MUL\n                     ? ROOTNode->getOperand(1)\n                     : ROOTNode->getOperand(0);\n\n  // Transform this to a MADD only if the user of this node is the add.\n  // If there are other users of the mul, this function returns here.\n  if (!Mult.hasOneUse())\n    return SDValue();\n\n  // maddu and madd are unusual instructions in that on MIPS64 bits 63..31\n  // must be in canonical form, i.e. sign extended. For MIPS32, the operands\n  // of the multiply must have 32 or more sign bits, otherwise we cannot\n  // perform this optimization. We have to check this here as we're performing\n  // this optimization pre-legalization.\n  SDValue MultLHS = Mult->getOperand(0);\n  SDValue MultRHS = Mult->getOperand(1);\n\n  bool IsSigned = MultLHS->getOpcode() == ISD::SIGN_EXTEND &&\n                  MultRHS->getOpcode() == ISD::SIGN_EXTEND;\n  bool IsUnsigned = MultLHS->getOpcode() == ISD::ZERO_EXTEND &&\n                    MultRHS->getOpcode() == ISD::ZERO_EXTEND;\n\n  if (!IsSigned && !IsUnsigned)\n    return SDValue();\n\n  // Initialize accumulator.\n  SDLoc DL(ROOTNode);\n  SDValue TopHalf;\n  SDValue BottomHalf;\n  BottomHalf = CurDAG.getNode(ISD::EXTRACT_ELEMENT, DL, MVT::i32, AddOperand,\n                              CurDAG.getIntPtrConstant(0, DL));\n\n  TopHalf = CurDAG.getNode(ISD::EXTRACT_ELEMENT, DL, MVT::i32, AddOperand,\n                           CurDAG.getIntPtrConstant(1, DL));\n  SDValue ACCIn = CurDAG.getNode(MipsISD::MTLOHI, DL, MVT::Untyped,\n                                  BottomHalf,\n                                  TopHalf);\n\n  // Create MipsMAdd(u) / MipsMSub(u) node.\n  bool IsAdd = ROOTNode->getOpcode() == ISD::ADD;\n  unsigned Opcode = IsAdd ? (IsUnsigned ? MipsISD::MAddu : MipsISD::MAdd)\n                          : (IsUnsigned ? MipsISD::MSubu : MipsISD::MSub);\n  SDValue MAddOps[3] = {\n      CurDAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Mult->getOperand(0)),\n      CurDAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Mult->getOperand(1)), ACCIn};\n  EVT VTs[2] = {MVT::i32, MVT::i32};\n  SDValue MAdd = CurDAG.getNode(Opcode, DL, VTs, MAddOps);\n\n  SDValue ResLo = CurDAG.getNode(MipsISD::MFLO, DL, MVT::i32, MAdd);\n  SDValue ResHi = CurDAG.getNode(MipsISD::MFHI, DL, MVT::i32, MAdd);\n  SDValue Combined =\n      CurDAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64, ResLo, ResHi);\n  return Combined;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n# 2. Values & User\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 3. Q & A in Stackoverflow\n\nQ&A\nSince Instruction is derived from Value it inherits both functions users and uses. The difference is that a user of Value has the Value as one of its operands.\n\nWhen you are calling uses you get a list of all Use instances holding a reference from the Value to each of the users of the particular Value. Calling users gives you a list of User directly. The following code shows how to use users and uses.\n\nfor(auto U : V->users()){  // U is of type User*\n     if (auto I = dyn_cast<Instruction>(U)){\n        // an instruction uses V\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\nYou can see users as a shortcut because you can do the same with uses:\n\nfor(auto U : V->uses()){  // U is of type Use*\n     if (auto I = dyn_cast<Instruction>(U.getUser())){\n        // an instruction uses V\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\nCommonly it is enough to use users to get all dependencies of a Value.\n\nAll Values used by a Value are the operands. This direction of dependency is not part of a Value's use list.\n\nWe have still not presented the most powerful aspect of the LLVM IR (enabled by the SSA form): the Value and User interfaces; these allow you to easily navigate the use-def and def-use chains. In the LLVM in-memory IR, a class that inherits from Value means that it defines a result that can be used by others, whereas a subclass of User means that this entity uses one or more Value interfaces. Function and Instruction are subclasses of both Value and User, while BasicBlock is a subclass of just Value. To understand this, let's analyze these two classes in depth:\n\n• The Value class defines the use_begin() and use_end() methods to allow you to iterate through Users, offering an easy way to access its def-use chain. For every Value class, you can also access its name through the getName() method. This models the fact that any LLVM value can have a distinct identifier associated with it. For example, %add1 can identify the result of an add instruction, BB1 can identify a basic block, and myfunc can identify a function. Value also has a powerful method called replaceAllUsesWith(Value *), which navigates through all of the users of this value and replaces it with some other value. This is a good example of how the SSA form allows you to easily substitute instructions and write fast optimizations. You can view the full interface at LLVM Value Class.\n\n• The User class has the op_begin() and op_end() methods that allows you to quickly access all of the Value interfaces that it uses. Note that this represents the use-def chain. You can also use a helper method called replaceUsesOfWith(Value *From, Value *To) to replace any of its used values. You can view the full interface at LLVM User Class.\n\nFor short, use_begin() iterator points to users. and op_begin() points to operand values. but the value is the basic class of instruction. By Refer to a value, you can get the producer's instructin.\n\n\n# Reference\n\nHow to Write an LLVM Backend #4: Instruction Selection\n\nMore on the LLVM Compiler\n\nIntroduction to LLVM (II)\n\n深入浅出 LLVM之 Value 、User 、Use 源码解析",normalizedContent:"# 1. codefile\n\nlib/target/mips/mipsisellowering.cpp\n\nall dag combine is called here.\n\nsdvalue  mipstargetlowering::performdagcombine(sdnode *n, dagcombinerinfo &dci)\n  const {\n  selectiondag &dag = dci.dag;\n  unsigned opc = n->getopcode();\n\n  switch (opc) {\n  default: break;\n  case isd::sdivrem:\n  case isd::udivrem:\n    return performdivremcombine(n, dag, dci, subtarget);\n  case isd::select:\n    return performselectcombine(n, dag, dci, subtarget);\n  case mipsisd::cmovfp_f:\n  case mipsisd::cmovfp_t:\n    return performcmovfpcombine(n, dag, dci, subtarget);\n  case isd::and:\n    return performandcombine(n, dag, dci, subtarget);\n  case isd::or:\n    return performorcombine(n, dag, dci, subtarget);\n  case isd::add:\n    return performaddcombine(n, dag, dci, subtarget);\n  case isd::shl:\n    return performshlcombine(n, dag, dci, subtarget);\n  case isd::sub:\n    return performsubcombine(n, dag, dci, subtarget);\n  }\n\n  return sdvalue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\ndefine of performsubcombine\n\n * we can look into the process.\n * isd::sub will call performsubcombine\n * it will combine (sub v0 (mul v1, v2)) into (msub v1, v2, v0)\n * the intresting thing is that, current sdnode opcode is sub and if precedent node is mul, it will combine it into msub.\n\nhow to identify the precedent node?\n\n 1. sdvalue mult = rootnode->getoperand(1); // multi sdvalue\n 2. sdvalue addoperand = rootnode->getoperand(0); // add sdvalue\n 3. how about previous instruction?\n 4. rootnode->getoperand(0) will point to previous instruction\n\nstatic sdvalue performsubcombine(sdnode *n, selectiondag &dag,\n                                 targetlowering::dagcombinerinfo &dci,\n                                 const mipssubtarget &subtarget) {\n  // (sub v0 (mul v1, v2)) => (msub v1, v2, v0)\n  if (dci.isbeforelegalizeops()) {\n    if (subtarget.hasmips32() && !subtarget.hasmips32r6() &&\n        !subtarget.inmips16mode() && n->getvaluetype(0) == mvt::i64)\n      return performmadd_msubcombine(n, dag, subtarget);\n\n    return sdvalue();\n  }\n\n  return sdvalue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nstatic sdvalue performmadd_msubcombine(sdnode *rootnode, selectiondag &curdag,\n                                       const mipssubtarget &subtarget) {\n  sdvalue mult = rootnode->getoperand(0).getopcode() == isd::mul\n                     ? rootnode->getoperand(0)\n                     : rootnode->getoperand(1);\n\n  sdvalue addoperand = rootnode->getoperand(0).getopcode() == isd::mul\n                     ? rootnode->getoperand(1)\n                     : rootnode->getoperand(0);\n\n  // transform this to a madd only if the user of this node is the add.\n  // if there are other users of the mul, this function returns here.\n  if (!mult.hasoneuse())\n    return sdvalue();\n\n  // maddu and madd are unusual instructions in that on mips64 bits 63..31\n  // must be in canonical form, i.e. sign extended. for mips32, the operands\n  // of the multiply must have 32 or more sign bits, otherwise we cannot\n  // perform this optimization. we have to check this here as we're performing\n  // this optimization pre-legalization.\n  sdvalue multlhs = mult->getoperand(0);\n  sdvalue multrhs = mult->getoperand(1);\n\n  bool issigned = multlhs->getopcode() == isd::sign_extend &&\n                  multrhs->getopcode() == isd::sign_extend;\n  bool isunsigned = multlhs->getopcode() == isd::zero_extend &&\n                    multrhs->getopcode() == isd::zero_extend;\n\n  if (!issigned && !isunsigned)\n    return sdvalue();\n\n  // initialize accumulator.\n  sdloc dl(rootnode);\n  sdvalue tophalf;\n  sdvalue bottomhalf;\n  bottomhalf = curdag.getnode(isd::extract_element, dl, mvt::i32, addoperand,\n                              curdag.getintptrconstant(0, dl));\n\n  tophalf = curdag.getnode(isd::extract_element, dl, mvt::i32, addoperand,\n                           curdag.getintptrconstant(1, dl));\n  sdvalue accin = curdag.getnode(mipsisd::mtlohi, dl, mvt::untyped,\n                                  bottomhalf,\n                                  tophalf);\n\n  // create mipsmadd(u) / mipsmsub(u) node.\n  bool isadd = rootnode->getopcode() == isd::add;\n  unsigned opcode = isadd ? (isunsigned ? mipsisd::maddu : mipsisd::madd)\n                          : (isunsigned ? mipsisd::msubu : mipsisd::msub);\n  sdvalue maddops[3] = {\n      curdag.getnode(isd::truncate, dl, mvt::i32, mult->getoperand(0)),\n      curdag.getnode(isd::truncate, dl, mvt::i32, mult->getoperand(1)), accin};\n  evt vts[2] = {mvt::i32, mvt::i32};\n  sdvalue madd = curdag.getnode(opcode, dl, vts, maddops);\n\n  sdvalue reslo = curdag.getnode(mipsisd::mflo, dl, mvt::i32, madd);\n  sdvalue reshi = curdag.getnode(mipsisd::mfhi, dl, mvt::i32, madd);\n  sdvalue combined =\n      curdag.getnode(isd::build_pair, dl, mvt::i64, reslo, reshi);\n  return combined;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n# 2. values & user\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 3. q & a in stackoverflow\n\nq&a\nsince instruction is derived from value it inherits both functions users and uses. the difference is that a user of value has the value as one of its operands.\n\nwhen you are calling uses you get a list of all use instances holding a reference from the value to each of the users of the particular value. calling users gives you a list of user directly. the following code shows how to use users and uses.\n\nfor(auto u : v->users()){  // u is of type user*\n     if (auto i = dyn_cast<instruction>(u)){\n        // an instruction uses v\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\nyou can see users as a shortcut because you can do the same with uses:\n\nfor(auto u : v->uses()){  // u is of type use*\n     if (auto i = dyn_cast<instruction>(u.getuser())){\n        // an instruction uses v\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\ncommonly it is enough to use users to get all dependencies of a value.\n\nall values used by a value are the operands. this direction of dependency is not part of a value's use list.\n\nwe have still not presented the most powerful aspect of the llvm ir (enabled by the ssa form): the value and user interfaces; these allow you to easily navigate the use-def and def-use chains. in the llvm in-memory ir, a class that inherits from value means that it defines a result that can be used by others, whereas a subclass of user means that this entity uses one or more value interfaces. function and instruction are subclasses of both value and user, while basicblock is a subclass of just value. to understand this, let's analyze these two classes in depth:\n\n• the value class defines the use_begin() and use_end() methods to allow you to iterate through users, offering an easy way to access its def-use chain. for every value class, you can also access its name through the getname() method. this models the fact that any llvm value can have a distinct identifier associated with it. for example, %add1 can identify the result of an add instruction, bb1 can identify a basic block, and myfunc can identify a function. value also has a powerful method called replacealluseswith(value *), which navigates through all of the users of this value and replaces it with some other value. this is a good example of how the ssa form allows you to easily substitute instructions and write fast optimizations. you can view the full interface at llvm value class.\n\n• the user class has the op_begin() and op_end() methods that allows you to quickly access all of the value interfaces that it uses. note that this represents the use-def chain. you can also use a helper method called replaceusesofwith(value *from, value *to) to replace any of its used values. you can view the full interface at llvm user class.\n\nfor short, use_begin() iterator points to users. and op_begin() points to operand values. but the value is the basic class of instruction. by refer to a value, you can get the producer's instructin.\n\n\n# reference\n\nhow to write an llvm backend #4: instruction selection\n\nmore on the llvm compiler\n\nintroduction to llvm (ii)\n\n深入浅出 llvm之 value 、user 、use 源码解析",charsets:{cjk:!0},lastUpdated:"2024/07/16, 20:14:06"},{title:"operand-collector",frontmatter:{title:"operand-collector",date:"2022-07-18T17:25:49.000Z",permalink:"/pages/cc7034/"},regularPath:"/03.gpu/01.operand_collector.html",relativePath:"03.gpu/01.operand_collector.md",key:"v-6ffd4505",path:"/pages/cc7034/",headersStr:null,content:"Warped-Compression: Enabling Power Efficient GPUs through Register Compression\n\neach register bank entry can store up to four 32-bit register values. All thread registers in a warp are statically allocated on consecutive banks with the same entry index. Therefore, to read one operand of a warp instruction,a buffering unit called operand collector needs to access up to eight register banks with the same index within each bank. While operands from different banks may be concurrently read, operands that access the same bank lead to bank conflicts.\n\nCORF: Coalescing Operand Register File for GPUs\n\nFigure 1 shows our baseline register file organization for the Fermi generation of Nvidia GPUs. It has a register file size of 128 KB per SM split across four banks. A bank is made up of 8 sub-banks that are 128 bits wide each. All 32 registers belonging to the 32 threads in the same warp are statically allocated to consecutive sub-banks (in a single bank) with the same entry index. Thus, a full register for all the threads within a warp can be striped using one entry of one bank, allowing it to be operated on in a single cycle. Each bank can store up to 256 warp-registers.\n\nSummary They all assume that 128bit entry in each bank will supply 4 register to 4 thread in 32 thread per swap.\n\nWarped-Compression assumes that 8 bank will supply 32 registers for a warp. CORF assumes that 8 subbank in each bank will supply 32 registers for a warp.",normalizedContent:"warped-compression: enabling power efficient gpus through register compression\n\neach register bank entry can store up to four 32-bit register values. all thread registers in a warp are statically allocated on consecutive banks with the same entry index. therefore, to read one operand of a warp instruction,a buffering unit called operand collector needs to access up to eight register banks with the same index within each bank. while operands from different banks may be concurrently read, operands that access the same bank lead to bank conflicts.\n\ncorf: coalescing operand register file for gpus\n\nfigure 1 shows our baseline register file organization for the fermi generation of nvidia gpus. it has a register file size of 128 kb per sm split across four banks. a bank is made up of 8 sub-banks that are 128 bits wide each. all 32 registers belonging to the 32 threads in the same warp are statically allocated to consecutive sub-banks (in a single bank) with the same entry index. thus, a full register for all the threads within a warp can be striped using one entry of one bank, allowing it to be operated on in a single cycle. each bank can store up to 256 warp-registers.\n\nsummary they all assume that 128bit entry in each bank will supply 4 register to 4 thread in 32 thread per swap.\n\nwarped-compression assumes that 8 bank will supply 32 registers for a warp. corf assumes that 8 subbank in each bank will supply 32 registers for a warp.",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"GPU WARP Scheduler",frontmatter:{title:"GPU WARP Scheduler",date:"2023-09-20T00:00:00.000Z",permalink:"/pages/2476ae/"},regularPath:"/03.gpu/02.warp_execution.html",relativePath:"03.gpu/02.warp_execution.md",key:"v-b25c491a",path:"/pages/2476ae/",headers:[{level:3,title:"1. Thread Block Compaction for Efficient SIMT Control Flow",slug:"_1-thread-block-compaction-for-efficient-simt-control-flow",normalizedTitle:"1. thread block compaction for efficient simt control flow",charIndex:1}],headersStr:"1. Thread Block Compaction for Efficient SIMT Control Flow",content:" 1. Thread Block Compaction for Efficient SIMT Control Flow\n\n----------------------------------------\n\n\n# 1. Thread Block Compaction for Efficient SIMT Control Flow",normalizedContent:" 1. thread block compaction for efficient simt control flow\n\n----------------------------------------\n\n\n# 1. thread block compaction for efficient simt control flow",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Precision Exception",frontmatter:{title:"Precision Exception",date:"2023-11-11T00:00:00.000Z",permalink:"/pages/14769f/"},regularPath:"/03.gpu/03.Precise%20Exception.html",relativePath:"03.gpu/03.Precise Exception.md",key:"v-f96426b0",path:"/pages/14769f/",headers:[{level:3,title:"1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception [2012]",slug:"_1-supporting-virtual-memory-in-gpgpu-without-supporting-precise-exception-2012",normalizedTitle:"1. supporting virtual memory in gpgpu without supporting precise exception [2012]",charIndex:489},{level:3,title:"6. Efficient Exception Handling Support for GPUs [2017]",slug:"_6-efficient-exception-handling-support-for-gpus-2017",normalizedTitle:"6. efficient exception handling support for gpus [2017]",charIndex:2960}],headersStr:"1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception [2012] 6. Efficient Exception Handling Support for GPUs [2017]",content:" 1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception 2012\n 2. Idempotent Processor Architecture 2011\n 3. iGPU: Exception Support and Speculative Execution on GPUs 2012\n 4. Implementing Virtual Memory in a Vector Processor with Software Restart Markers 2006 Not Read\n 5. Imprecise Store Exceptions 2023 ISCA\n 6. Efficient Exception Handling Support for GPUs 2017\n 7. Simple Out of Order Core for GPGPUs\n 8. Other Papers.\n\n----------------------------------------\n\n\n# 1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception [2012]\n\n👍\n\nIntroduction: GPU is designed for grahics. Supporting precise exceptions is not needed at all and it is extremely expensive due to the high number of registers. Other Designs:\n\n 1. Software restart Remarker Implementing virtual memory in a vector processor with software restart markers.[4] 2006 Reducing Exception Management Overhead with Software Restart Markers 2008\n 2. Idempotent Idempotent processor architecture [2] 2011 igpu: Exception support and speculative execution on gpus. [3] 2012\n\na) set start_maker set start_marker indicates a place where a program can be restarted after a page fault exception handler is serviced.\n\nb) LD.pfchk An LD.pfchk instruction sets pfbit, when it generates a page fault. The pfbit registers behave like predicate registers in IA-64. Instructions that can potentially change program’s states are predicated with pfbit.\n\nc) sw_call sw_call is composed of barrier and call instructions. When a processor fetches an sw_call instruction, it enforces an execution barrier.\n\nInstructions after sw_call can be fetched/renamed, but none of the instructions will be executed. call instructions invoke page fault handler. Implementing this execution barrier is very easy, but it reduces the benefit of a fully out-of-order scheduling processor.\n\nLD.pfchk will set pfbits. Instructions that can potentially change program's state are predicated with pfbit. Similar to idempotent processors, instructions that can be safely reexecuted without changing the program’s results do not need to be predicated. If all instructions are predicated, those instructions cannot be executed until the load instruction is completed, thereby degrading performance significantly.\n\n 1. Not all load/store instruction will be set as LD.pfck. Compiler's job to distinguish Static, Malloc, Large Arrays, Stack Operations, Pointers, and so on\n 2. Only those instructions that can safely reexecuted can be predicated.\n\n/* original C-code */\nfor (int ii=0; ii<N; ii++)\na[ii] = b[ii]*2;\n/* new code */\nfor (int ii=0; ii<N; ii++) {\nif (!(ii%kk)) {\n// kk = page size%(size of(a[0]))\npfchk(&(a[0])+ii*kk));\npfchk(&(b[0])+ii*kk));\n}\na[ii] = b[ii]*2;\n}\nvoid pfchk(int addr) {\n/* use intrinsics to insert assembly code */\nset start_marker;\nLD.pfchk(addr);\n(pfbit) sw_call(start_marker);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n----------------------------------------\n\n\n# 6. Efficient Exception Handling Support for GPUs [2017]\n\n👍 👍 👍 👍\n\nThis paper summerize [1] [2] [3] [4] and discuss why altough GPU solves the problem of dependency, it still meets the problem of RAW Hazard on replay\n\n\n\nIn short, since R4 has been read by C, D can issue and might overwrite R4 before C is done. Thus if we resume from C, C might read the value of new R4, which means hazard.\n\nIt propose three method to solve this:\n\n 1. Warp Disable\n\n\n\n 2. Replay Queue\n\n\n\n 3. Operand Log\n\n\n\nThis is a good paper, that deserves reading throughly. 👏",normalizedContent:" 1. supporting virtual memory in gpgpu without supporting precise exception 2012\n 2. idempotent processor architecture 2011\n 3. igpu: exception support and speculative execution on gpus 2012\n 4. implementing virtual memory in a vector processor with software restart markers 2006 not read\n 5. imprecise store exceptions 2023 isca\n 6. efficient exception handling support for gpus 2017\n 7. simple out of order core for gpgpus\n 8. other papers.\n\n----------------------------------------\n\n\n# 1. supporting virtual memory in gpgpu without supporting precise exception [2012]\n\n👍\n\nintroduction: gpu is designed for grahics. supporting precise exceptions is not needed at all and it is extremely expensive due to the high number of registers. other designs:\n\n 1. software restart remarker implementing virtual memory in a vector processor with software restart markers.[4] 2006 reducing exception management overhead with software restart markers 2008\n 2. idempotent idempotent processor architecture [2] 2011 igpu: exception support and speculative execution on gpus. [3] 2012\n\na) set start_maker set start_marker indicates a place where a program can be restarted after a page fault exception handler is serviced.\n\nb) ld.pfchk an ld.pfchk instruction sets pfbit, when it generates a page fault. the pfbit registers behave like predicate registers in ia-64. instructions that can potentially change program’s states are predicated with pfbit.\n\nc) sw_call sw_call is composed of barrier and call instructions. when a processor fetches an sw_call instruction, it enforces an execution barrier.\n\ninstructions after sw_call can be fetched/renamed, but none of the instructions will be executed. call instructions invoke page fault handler. implementing this execution barrier is very easy, but it reduces the benefit of a fully out-of-order scheduling processor.\n\nld.pfchk will set pfbits. instructions that can potentially change program's state are predicated with pfbit. similar to idempotent processors, instructions that can be safely reexecuted without changing the program’s results do not need to be predicated. if all instructions are predicated, those instructions cannot be executed until the load instruction is completed, thereby degrading performance significantly.\n\n 1. not all load/store instruction will be set as ld.pfck. compiler's job to distinguish static, malloc, large arrays, stack operations, pointers, and so on\n 2. only those instructions that can safely reexecuted can be predicated.\n\n/* original c-code */\nfor (int ii=0; ii<n; ii++)\na[ii] = b[ii]*2;\n/* new code */\nfor (int ii=0; ii<n; ii++) {\nif (!(ii%kk)) {\n// kk = page size%(size of(a[0]))\npfchk(&(a[0])+ii*kk));\npfchk(&(b[0])+ii*kk));\n}\na[ii] = b[ii]*2;\n}\nvoid pfchk(int addr) {\n/* use intrinsics to insert assembly code */\nset start_marker;\nld.pfchk(addr);\n(pfbit) sw_call(start_marker);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n----------------------------------------\n\n\n# 6. efficient exception handling support for gpus [2017]\n\n👍 👍 👍 👍\n\nthis paper summerize [1] [2] [3] [4] and discuss why altough gpu solves the problem of dependency, it still meets the problem of raw hazard on replay\n\n\n\nin short, since r4 has been read by c, d can issue and might overwrite r4 before c is done. thus if we resume from c, c might read the value of new r4, which means hazard.\n\nit propose three method to solve this:\n\n 1. warp disable\n\n\n\n 2. replay queue\n\n\n\n 3. operand log\n\n\n\nthis is a good paper, that deserves reading throughly. 👏",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Unified Memory Paper List",frontmatter:{title:"Unified Memory Paper List",date:"2023-11-11T00:00:00.000Z",permalink:"/pages/44771e/"},regularPath:"/03.gpu/04.Unified_Memory.html",relativePath:"03.gpu/04.Unified_Memory.md",key:"v-5849f946",path:"/pages/44771e/",headers:[{level:3,title:"1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory",slug:"_1-holistic-performance-analysis-and-optimization-of-unified-virtual-holistic-performance-analysis-and-optimization-of-unified-virtual-memory",normalizedTitle:"1. holistic performance analysis and optimization of unified virtual holistic performance analysis and optimization of unified virtual memory",charIndex:3797},{level:3,title:"3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions",slug:"_3-oversubscribing-gpu-unified-virtual-memory-implications-and-suggestions",normalizedTitle:"3. oversubscribing gpu unified virtual memory: implications and suggestions",charIndex:4084},{level:3,title:"4. Performance Evaluation of Advanced Features in CUDA Unified Memory",slug:"_4-performance-evaluation-of-advanced-features-in-cuda-unified-memory",normalizedTitle:"4. performance evaluation of advanced features in cuda unified memory",charIndex:6364},{level:3,title:"5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory",slug:"_5-interplay-between-hardware-prefetcher-and-page-eviction-policy-in-cpu-gpu-unified-virtual-memory",normalizedTitle:"5. interplay between hardware prefetcher and page eviction policy in cpu-gpu unified virtual memory",charIndex:9887},{level:3,title:"7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads 2020",slug:"_7-batch-aware-unified-memory-management-in-gpus-for-irregular-workloads-2020",normalizedTitle:"7. batch-aware unified memory management in gpus for irregular workloads 2020",charIndex:11620},{level:3,title:"10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019",slug:"_10-machine-learning-guided-optimal-use-of-gpu-unified-memory-2019",normalizedTitle:"10. machine learning guided optimal use of gpu unified memory 2019",charIndex:18629},{level:3,title:"14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory [2024] 👍👍👍👍",slug:"_14-fine-grain-quantitative-analysis-of-demand-paging-in-unified-virtual-memory-2024",normalizedTitle:"14. fine-grain quantitative analysis of demand paging in unified virtual memory [2024] 👍👍👍👍",charIndex:21192}],headersStr:"1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory 3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions 4. Performance Evaluation of Advanced Features in CUDA Unified Memory 5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory 7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads 2020 10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019 14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory [2024] 👍👍👍👍",content:' 1.  Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory\n 2.  In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing\n 3.  Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions\n 4.  Performance Evaluation of Advanced Features in CUDA Unified Memory\n 5.  Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory\n 6.  Unified Memory: GPGPU-Sim/UVM Smart Integration\n 7.  Batch-Aware Unified Memory Management in GPUs for Irregular Workloads\n 8.  An Intelligent Framework for Oversubscription Management in CPU-GPU Unified Memory\n 9.  Architectural Support for Address Translation on GPUs Designing Memory Management Units for CPU/GPUs with Unified Address Spaces\n 10. Machine Learning Guided Optimal Use of GPU Unified Memory\n 11. Towards High Performance Paged Memory for GPUs\n 12. [Virtualization] Virtual Thread: Maximizing Thread-Level Parallelism beyond GPU Scheduling Limit.\n 13. [Virtualization] A Survey of GPU Multitasking Methods Supported by Hardware Architecture\n 14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory\n\nPlan to read\n\n 1. Early-Adaptor: An Adaptive Framework for Proactive UVM Memory Management\n 2. Liberator: A Data Reuse Framework for Out-of-Memory Graph Computing on GPUs\n 3. [HPCA] Enabling Large Dynamic Neural Network Training with Learning-based Memory Management\n 4. GPUswap: Enabling Oversubscription of GPU Memory through Transparent Swapping\n\n----------------------------------------\n\nUnified Memory History copied from Evolution of Nvidia GPU from microarchitectures Pascal to Ampere\n\nCUDA 4 introduced UVA (Unified Virtual Addressing) to provide a single virtual memory address space for both CPU and GPU memory and enable pointers to be accessed from GPU code no matter where in the system they reside. UVA enables Zero-Copy memory, a pinned CPU memory accessible by GPU code directly, over PCIe, without the need for memory copy. This provides some of the convince of Unified Memory, but at the cost of worse performance, because GPU always accesses it with PCIe’s low bandwidth and high latency.[1]\n\nLater, CUDA 6 introduced Unified Memory, which creates a pool of managed memory that programs running on the CPU and GPU can access without explicit data movement. However, only when CPU and GPU processes are not running together because of the limitation of the Kepler and Maxwell GPU microarchitecture. Also, the Unified Memory address space was limited to the size of the GPU memory.[1, 3]\n\nCUDA 8 and Pascal microarchitectures improve Unified Memory functionality by adding 49-bit virtual addressing and page faulting capability. The larger 49-bit virtual addresses are sufficient to enable GPUs to access the entire system memory plus the memory of all GPUs in the system. Because of the memory page faulting functionality, the CUDA system software does not need to synchronize all managed memory allocations to the GPU before each kernel lunch. Instead, when a thread running on GPU faults on non-resident memory access(demanding page), it stalls until the page can be migrated and the page table updated. Alternatively, the page may be mapped for remote access over PCIe or NVLink interconnects.[1, 3, 6]\n\nThese new features of Unified Memory enable oversubscription of memory, which means that application running on a GPU can use data sets larger than ten their device memory.[1] While the Unified Memory model makes GPU programming more convenient, it comes at a cost; handling page faults and page migrations can be expensive. CUDA 8 addresses this issue with features like prefetch and memory advice.\n\n----------------------------------------\n\n\n# 1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory\n\nSame author with In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing\n\n----------------------------------------\n\n\n# 3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions\n\nUVM supports memory oversubscription, giving GPU programs the ability to use a larger amount of memory than the physical memory, without worrying about the problem of memory shortage.\n\nAdvanced optimization techniques, mainly prefetching and memory usage hints [1], can be used to fine-tune the performance of UVM applications, mitigating the overheads caused by UVM.\n\n\n\n2）Prefetching and Hints Prefetching and UVM hints are the major approaches provided by CUDA, with the hope that page faults and memory thrashing could be prevented by fine-tuning the behavior of UVM at runtime.\n\nBy calling cudaMemPrefetchAsync (PF), a memory block could be prefetched to GPU. UVM hints provide informed decisions on page handling by indicating the access patterns of data.\n\nChanging UVM hints is done by invoking cudaMemAdvise with one of the following policies：\n\n• cudaMemAdviseSetAccessedBy (AB) implies that the device keeps a direct mapping in its page table. When the data is migrated, the mapping is re-established.\n• cudaMemAdviseSetPreferredLocation (PL) pins the data and prevents the page to be migrated, which is useful when the page is mainly accessed on one side.\n• cudaMemAdviseSetReadMostly (RM) indicates the data region is read-intensive. It creates a read-only copy of the page on the faulting side, allowing on current access on both sides.\n\n\nOnly one policy (AB, PL, or RM) could be specified for each memory block, but each policy can be used along with prefetching.\n\nSuggestions: To ensure performance under all oversubscription conditions, programmer needs to choose the UVM hints dynamically based on the application’s memory usage and available GPU memory. As a prerequisite, the size of the FALL pages needs to be estimated or measured by experiment. Before kernel launch, the program should first check the size of available GPU memory (e.g. via the cudaMemGetInfo API). If no oversubscription will happen, or the available memory is larger than the size of FALL pages, the programmer could set hints based on the conclusions provided by related researches [24]. Otherwise, based on our findings, applying the hint AB is a preferable choice.\n\n----------------------------------------\n\n\n# 4. Performance Evaluation of Advanced Features in CUDA Unified Memory\n\nCUDA has introduced new features for optimizing the data migration on UM, i.e., memory advises and prefetch. Instead of solely relying on page faults, the memory advises feature allows the programmer to provide data access pattern for each memory object so that the runtime can optimize migration decisions. The prefetch proactively triggers asynchronous data migration to GPU before the data is accessed, which reduces page faults and, consequently, the overhead in handling page faults.\n\n-Using memory advises improves application performance in oversubscription execution on the Intel platform and in-memory executions on the IBM platform.\n\n-UM prefetch provides a significant performance improvement on the Intel-Volta/Pascal-PCI-E based systems while it does not show a performance improvement on the Power9-Volta-NVLink based system\n\nUM was first introduced in CUDA 6.0 [21]. Only until the recent Nvidia Pascal microarchitecture that has hardware support for page faults.\n\n\n\n• cudaMemAdviseSetAccessedBy establishes a direct mapping of data to a specified device. Figure 2c illustrates an example of a physical page on GPU being remotely access from the host. When cudaMemAdviseSetPreferredLocation is applied, CUDA runtime tries to build a direct mapping to the page to avoid data migration so that the destination can access data remotely. Differently from cudaMemAdviseSetPreferredLocation, this cudaMemAdviseSetAccessedBy does not try to pin pages on a specific device; instead, its main effect is to establish mapping on the remote device. This advice takes effect on the creation of the memory pages. The mapping will be re-established after the pages are migrated.\n\n• cudaMemAdviseSetPreferredLocation sets the preferred physical location of pages. This advice pins a page and prevents it from migrating to other memories. Figure 2b illustrates a page preferred on the host side, and GPU uses remote mapping to access the page. This advice established a direct (remote) mapping to the memory page. When accessing the page remotely, data is fetched through the remote memory instead of generating a page fault. If the underlying hardware does not support the remote mapping, the page will be migrated as in the standard UM. cudaMemAdviseSetPreferredLocation is useful for applications with little data sharing between CPU and GPU, i.e., part of the application is executed completely on the GPU, and the rest of the application executes on the host. Data that is being used mostly by the GPU can be pinned to the GPU with the advice, avoiding memory thrashing.\n\n• cudaMemAdviseSetReadMostly implies a read-intensive data region. In the basic UM, accessing a page on a remote side triggers page migration. However, with cudaMemAdviseSetReadMostly, a read-only duplicate of the page will be created on the faulting side, which prevents page faults and data migration in the future. Figure 2a illustrates an example, where the second access (step 5) has no page fault and is local access. This mechanism, however, results in a high overhead if there is any update to this memory region because all copies of the corresponding page will be invalidated to preserve consistency between different copies. Thus, this advice is often used in read-only data structures, such as lookup tables and application parameters.\n\nIn general, we found both memory advises and prefetch to be simple and effective.\n\n----------------------------------------\n\n\n# 5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory\n\nCons in traditional GPU: Complicated asynchronous user-directed constructs to overlap data migration and kernel execution are used to address this issue. The second challenge is memory over-subscription. When the working set of the GPU kernel cannot fit in the device memory, the programmers have to painstakingly redefine the data structures and tile the data to transfer back and forth in chunks.\n\nThis flow is inspired by -> 11. Towards High Performance Paged Memory for GPU.\n\n1 Scheduled threads generate global memory accesses.\n\n2 Each SM has its own load/store unit. Every load/store unit has its own TLB. Load/store unit performs a TLB look up to find whether the translation for the issued memory access is cached in TLB or not. A TLB miss is relayed to the GMMU.\n\n3 The GMMU walks through the page table looking for a PTE corresponding to the requested page with valid flag set. A far-fault occurs if there is no PTE for the requested page or the valid flag is not set. Then the far-fault is registered in the Far-fault Miss Status Handling Registers (MSHRs).\n\n4 The page is scheduled for transfer over CPU-GPU PCI-e interconnect.\n\n5 A 4KB page is allocated on demand and data is migrated from host to device memory.\n\n6 The MSHRs are consulted to notify the corresponding load/store unit and the memory access is replayed. A new PTE entry is added to the page table with valid\n\n\n\nThis paper introduces random, sequential and tree-based Neighborhood prefetcher in detail.\n\nAnd come up with pre-eviction for tree-based Neighborhood, different from LRU eviction used in Nvidia.\n\n\n\n----------------------------------------\n\n\n# 7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads 2020\n\nPropose:\n\n(1) increases the batch size (i.e., the number of page faults handled together), thereby amortizing the GPU runtime fault handling time, and reduces the number of batches by supporting CPU-like thread block context switching\n\nThread Oversubscription (TO), a CPU-like thread block context switching technique, to effectively amortize the GPU runtime fault handling time by increasing the batch size (i.e., the number of page faults handled together).\n\n(2) takes page eviction off the critical path with no hardware changes by overlapping evictions with CPU-to-GPU page migrations. Unobtrusive Eviction (UE) to take GPU page evictions off the critical path with no hardware changes based on the idea of overlapping page evictions with CPU-to-GPU page migrations.\n\nPrior work reports that page fault handling latency ranges from 20µs to 50µs [53]. We find that these numbers are conservative and can be worse depending on the applications and systems. Unfortunately, this page fault latency, which is in the order of microseconds, cannot be easily hidden even with ample thread-level parallelism (TLP) in GPUs, especially when GPU memory is oversubscribed.\n\n\n\nThe GPU runtime processes a group of GPU page faults together, rather than processing each individual one, in order to amortize the overhead of multiple round-trip latencies over the PCIe bus and to avoid invoking multiple interrupt service routines (ISRs) in the operating system (OS). To efficiently process an excessive number of page faults, the GPU runtime performs a series of operations such as preprocessing all the page faults and inserting page prefetching requests, which takes a significant amount of time (in the range of tens to hundreds of microseconds). Once all the operations (e.g., CPU page table walks for all the page faults, page allocation and eviction scheduling, etc.) are finished, page migrations between the CPU and the GPU begin.\n\nThis page fault handling is expensive because (1) it requires long latency communications between the CPU and GPU over the PCIe bus, and (2) the GPU runtime performs a very expensive fault handling service routine.\n\nTo amortize the overhead, the GPU runtime processes a group of page faults together, which we refer to as batch processing.\n\nWhen a page fault exception is raised by the GPU memory management unit (MMU), the GPU runtime begins to handle the exception, shown in 1.\n\n\n\nFrom this, we conclude that page evictions and new page allocations are serialized in modern GPUs to prevent the new pages from overwriting the evicted pages. Note that an eviction is required on every page fault once the pages resident in the GPU’s memory are at capacity.\n\n\n\nThis preprocessing includes sorting the page faults in ascending order of page addresses (to accelerate the page table walks) and the analysis of page addresses to insert page prefetching requests.1 We refer to the time taken by the GPU runtime to perform a collection of operations to handle many page faults together as GPU runtime fault handling time.\n\nhttps://github.com/acsl-technion/gaia_nvidia/blob/e23e4d926f576c2c4169664b6add89e1368ee849/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c#L787\n\n// Fault cache preprocessing for fault coalescing\n//\n// This function generates an ordered view of the given fault_cache in which faults are sorted by VA space, fault\n// address (aligned to 4K) and access type "intrusiveness" (atomic - write - read - prefetch). In order to minimize\n// the number of instance_ptr to VA space translations we perform a first sort by instance_ptr.\n//\n// This function returns NV_WARN_MORE_PROCESSING_REQUIRED if a fault buffer flush occurred during instance_ptr\n// translation and executed successfully, or the error code if it failed. NV_OK otherwise.\n//\n// Current scheme:\n// 1) sort by instance_ptr\n// 2) translate all instance_ptrs to VA spaces\n// 3) sort by va_space, fault address (GPU already reports 4K-aligned address) and access type\nstatic NV_STATUS preprocess_fault_batch(uvm_gpu_t *gpu, uvm_fault_service_batch_context_t *batch_context)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nThe batch processing time is measured to be in the range of 223µs to 553µs with a median of 313µs, of which, GPU runtime fault handling accounts for an average of 46.69% of the time (measured to be in the range of 50µs to 430µs with a median of 140µs).\n\n1）Thread Oversubscription\n\nWe enable thread oversubscription from the beginning of the execution by allocating one additional thread block to each SM ( 1 ). The thread block additionally allocated to each SM is inactive at first. It is important to note that the number of active thread blocks does not exceed that of the baseline, which is determined by the physical resource constraints. Once all of the warps in an active thread block are stalled due to page faults, the thread oversubscription mechanism context switches the active (but stalled) thread block with an inactive thread block ( 2 ). The thread oversubscription mechanism can be detrimental if it causes premature evictions. To prevent this, the GPU runtime monitors the premature eviction rates by periodically estimating the running average of the lifetime of pages by tracking when each page is allocated and evicted. We use the running average as an indicator of premature evictions. If the running average is decreased by a certain threshold, the thread oversubscription mechanism does not allow any more context switching by decrementing (and limiting) the number of concurrently runnable thread blocks ( 3 ).6 Otherwise, thread oversubscription allocates one additional thread block to each SM in an incremental manner.\n\n\n\n 2. Unobstrusive Eviction\n\nWhen a page fault interrupt is raised by the GPU MMU, the top-half interrupt service routine (ISR) responds. It checks whether the number of GPU resident pages is at capacity via the GPU memory status tracker. If so, it sends a preemptive eviction request to the GPU. The rest of the fault handling (e.g., preprocessing of the page faults, CPU-side page table walks) is performed by the bottom-half ISR.\n\n\n\n\n\nWhen the GPU runtime begins a batch’s processing, it checks the GPU memory status. If it is at capacity, it initiates a single page eviction ( 1 ). Once page X is evicted from the GPU’s memory, both CPU and GPU page tables are updated ( 2 ). Unlike the baseline case (Figure 4), page A can be migrated to the GPU memory without any delay ( 3 ). At the same time, page Y can be evicted using bidirectional transfers. Since the data transfer speed from the GPU to CPU memory is faster than the other way around [29], eviction is completely unobtrusive and migrations to the GPU can occur without any delay.\n\nIn short, thread oversubscription increase the batch size by switching in in-active thread block. and unobstrusive eviction avoid the serialization of swap pages between host and device.\n\n----------------------------------------\n\n\n# 10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019\n\nTo enable better performance of UM, CUDA allows developers to give the UM driver additional advice on managing a given GPU memory range via an API function named cudaMemAdvise(const void *, size_t, enum cudaMemoryAdvise, int). The first two parameters of this function accept a pointer to a memory range with a specified size. The memory range should be allocated via cudaMallocManaged or declared via __managed__variables. The third parameter sets the advice for the memory range. The last parameter indicates the associated device’s id, which can indicate either a CPU or GPU device. The details and differences of these four kinds of advice are presented as follows:\n\n• Default: This represents the default on-demand page migration to accessing processor, using the first-touch policy.\n\n• cudaMemAdviseSetReadMostly: This advice is used for the data which is mostly going to be read from and only occasionally written to. The UM driver may create read-only copies of the data in a processor’s memory when that processor accesses it. If this region encounters any write requests, then only the write occurred page will be valid and other copies will be invalid.\n\n• cudaMemAdviseSetPreferredLocation: Once a target device is specified, this device memory can be set as the preferred location for the allocated data. The host memory can also be specified as the preferred location. Setting the preferred location does not cause data to migrate to that location immediately. The policy only guides what will happen when a fault occurs on the specified memory region: if data is already in the preferred location, the faulting processor will try to directly establish a mapping to the region without causing page migration. Otherwise, the data will be migrated to the processor accessing it if the data is not in the preferred location or if a direct mapping cannot be established.\n\n• cudaMemAdviseSetAccessedBy: This advice implies that the data will be accessed by a specified CPU or GPU device. It has no impact on the data location and will not cause data migration. It only causes the data to be always mapped in the specified processor’s page tables, when applicable. The mapping will be accordingly updated if the data is migrated somehow. This advice is useful to indicate that avoiding faults is important for some data, especially when the data is accessed by a GPU within a system containing multiple GPUs with peer-to-peer access enabled.\n\n----------------------------------------\n\n\n# 14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory [2024] 👍👍👍👍\n\nSame author: In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing[2021]\n\n\n\nThe UVM host driver on the host is open source with dependencies on the proprietary nvidia driver/resource manager and the host OS for memory management. This driver is a runtime fault servicing engine and the memory manager for managed memory allocations.\n\n\n 1. the fault is generated and handled by the hardware thread’s corresponding µTLB. The thread may continue executing instructions not blocked by a memory dependency. The fault propagates to the GPU memory management unit (GMMU), which writes the corresponding fault information into the GPU Fault Buffer and sends a hardware interrupt to the host. The fault buffer acts as a circular array, configured and managed by the UVM driver.\n    \n 2. The nvidia-uvm driver fetches the fault information, caches it on the host, and services the faults through\n 3. page processing: page table update and TLB shootdown on the host and GPU page table update\n 4. page migration: involves page migration.\n\nThe GPU exposes two functionalities to the host via the GPU command push-buffer—host-to GPU memory copy and fault replay.\nAs part of the fault servicing process, the driver instructs the GPU to copy pages into its memory, generally using high-performance hardware “copy engines.”\nOnce the GPU’s page tables are updated and the data is successfully migrated, the driver issues a fault replay, which clears the waiting status of µTLB, causing them to “replay” the prior miss.\n\nFault Handling:\n\nFirst, the GPU sends an interrupt over the interconnect to alert the host UVM driver of a page fault. The interrupt wakes up a worker thread to begin fault servicing if none is awake.\nSecond, the host retrieves the complete fault information from the GPU Fault Buffer.\nThe default fault retrieval policy reads faults until the batch size limit (i.e., 256 faults) is reached or no faults remain in the buffer.\n\n\nThese VABlocks serve as logical boundaries; the driver processes all batch faults within a single VABlock together, and each VABlock within a batch requires a distinct processing step.\n\n\n\n\n\nNotes:\n\n 1. when prefetching is not enabled, Service Faults is the major part of delay in CPU-GPU system. In this case, even NVlink does not matter. The reason is that unmapping and tlb-shut down in multi-cpu costs a lot.\n    \n 2. Pretching reduce the overhead by reduction of page fault and also increase the efficiency of NVlink.\n    \n 3. Oversubscription worse the case by finding empty space failed first and then evictim block to GPU. This worsen the performance.\n    \n\n1 & 2 explained:\n\n(1) unmapping host-side data takes place on the fault path and incurs significant overhead\n(2) certain hostside parallelizations of an application using UVM can exaggerate these unmapping costs.\nThe host OS performs this operation, and the costs likely stem from issues with virtual mappings across CPU cores, flushing dirty pages from caches and TLBs, NUMA, and other memory-adjacent issues.\nAdditionally, these operations do not take place in bulk due to the logical separation of VABlocks within UVM.\nThis is an area that deserves particular scrutiny as HMM also performs host page unmapping on the fault path using host OS mechanisms, implying a similar cost could be applied to all devices when using HMM [15, 26].\n\ncompared to cpu-gpu case, GPU-GPU on-demand page migration is faster due to the actual page table updates offloaded to the source GPU.\nFault servicing includes operations such as page unmapping and TLB shootdown on the source device.\nGPU page table updates and TLB shootdown are hardware based and relatively much faster.\n\n\n 3. Explained Process: (1) fail allocation\n    (2) evict a VABlock and migrate the data back to the host\n    (3) restart the block migration process, including host unmapping, data transfer, GPU mapping, page population, a process by which pages are filled with zero values before data is migrated to them.\n\nInterestingly, oversubscription diminishes the benefits of NVLink2. Oversubscription, as it is currently implemented, always evicts pages back to the host memory. This causes the CPU-GPU PCIe interconnect to become active for data eviction.\n\n👉 In short, in CPU-GPU system, service faults are major issue due to tlb shutdown and page table update. This even diminish the power of NVLink. Memory Oversubscription worsen the situation by failing to allocate memory in GPU, find eviction and eviction to CPU, adding these operation worsen the performance.\n👉 GPU-GPU does not have the service faults problem since page table update and tlb shutdown are handled by faster gpu hardware.\n👉 Besides, prefetching helps to improve performance a lot by reducing fault and better bandwitdh efficiency.\n',normalizedContent:' 1.  holistic performance analysis and optimization of unified virtual holistic performance analysis and optimization of unified virtual memory\n 2.  in-depth analyses of unified virtual memory system for gpu accelerated computing\n 3.  oversubscribing gpu unified virtual memory: implications and suggestions\n 4.  performance evaluation of advanced features in cuda unified memory\n 5.  interplay between hardware prefetcher and page eviction policy in cpu-gpu unified virtual memory\n 6.  unified memory: gpgpu-sim/uvm smart integration\n 7.  batch-aware unified memory management in gpus for irregular workloads\n 8.  an intelligent framework for oversubscription management in cpu-gpu unified memory\n 9.  architectural support for address translation on gpus designing memory management units for cpu/gpus with unified address spaces\n 10. machine learning guided optimal use of gpu unified memory\n 11. towards high performance paged memory for gpus\n 12. [virtualization] virtual thread: maximizing thread-level parallelism beyond gpu scheduling limit.\n 13. [virtualization] a survey of gpu multitasking methods supported by hardware architecture\n 14. fine-grain quantitative analysis of demand paging in unified virtual memory\n\nplan to read\n\n 1. early-adaptor: an adaptive framework for proactive uvm memory management\n 2. liberator: a data reuse framework for out-of-memory graph computing on gpus\n 3. [hpca] enabling large dynamic neural network training with learning-based memory management\n 4. gpuswap: enabling oversubscription of gpu memory through transparent swapping\n\n----------------------------------------\n\nunified memory history copied from evolution of nvidia gpu from microarchitectures pascal to ampere\n\ncuda 4 introduced uva (unified virtual addressing) to provide a single virtual memory address space for both cpu and gpu memory and enable pointers to be accessed from gpu code no matter where in the system they reside. uva enables zero-copy memory, a pinned cpu memory accessible by gpu code directly, over pcie, without the need for memory copy. this provides some of the convince of unified memory, but at the cost of worse performance, because gpu always accesses it with pcie’s low bandwidth and high latency.[1]\n\nlater, cuda 6 introduced unified memory, which creates a pool of managed memory that programs running on the cpu and gpu can access without explicit data movement. however, only when cpu and gpu processes are not running together because of the limitation of the kepler and maxwell gpu microarchitecture. also, the unified memory address space was limited to the size of the gpu memory.[1, 3]\n\ncuda 8 and pascal microarchitectures improve unified memory functionality by adding 49-bit virtual addressing and page faulting capability. the larger 49-bit virtual addresses are sufficient to enable gpus to access the entire system memory plus the memory of all gpus in the system. because of the memory page faulting functionality, the cuda system software does not need to synchronize all managed memory allocations to the gpu before each kernel lunch. instead, when a thread running on gpu faults on non-resident memory access(demanding page), it stalls until the page can be migrated and the page table updated. alternatively, the page may be mapped for remote access over pcie or nvlink interconnects.[1, 3, 6]\n\nthese new features of unified memory enable oversubscription of memory, which means that application running on a gpu can use data sets larger than ten their device memory.[1] while the unified memory model makes gpu programming more convenient, it comes at a cost; handling page faults and page migrations can be expensive. cuda 8 addresses this issue with features like prefetch and memory advice.\n\n----------------------------------------\n\n\n# 1. holistic performance analysis and optimization of unified virtual holistic performance analysis and optimization of unified virtual memory\n\nsame author with in-depth analyses of unified virtual memory system for gpu accelerated computing\n\n----------------------------------------\n\n\n# 3. oversubscribing gpu unified virtual memory: implications and suggestions\n\nuvm supports memory oversubscription, giving gpu programs the ability to use a larger amount of memory than the physical memory, without worrying about the problem of memory shortage.\n\nadvanced optimization techniques, mainly prefetching and memory usage hints [1], can be used to fine-tune the performance of uvm applications, mitigating the overheads caused by uvm.\n\n\n\n2）prefetching and hints prefetching and uvm hints are the major approaches provided by cuda, with the hope that page faults and memory thrashing could be prevented by fine-tuning the behavior of uvm at runtime.\n\nby calling cudamemprefetchasync (pf), a memory block could be prefetched to gpu. uvm hints provide informed decisions on page handling by indicating the access patterns of data.\n\nchanging uvm hints is done by invoking cudamemadvise with one of the following policies：\n\n• cudamemadvisesetaccessedby (ab) implies that the device keeps a direct mapping in its page table. when the data is migrated, the mapping is re-established.\n• cudamemadvisesetpreferredlocation (pl) pins the data and prevents the page to be migrated, which is useful when the page is mainly accessed on one side.\n• cudamemadvisesetreadmostly (rm) indicates the data region is read-intensive. it creates a read-only copy of the page on the faulting side, allowing on current access on both sides.\n\n\nonly one policy (ab, pl, or rm) could be specified for each memory block, but each policy can be used along with prefetching.\n\nsuggestions: to ensure performance under all oversubscription conditions, programmer needs to choose the uvm hints dynamically based on the application’s memory usage and available gpu memory. as a prerequisite, the size of the fall pages needs to be estimated or measured by experiment. before kernel launch, the program should first check the size of available gpu memory (e.g. via the cudamemgetinfo api). if no oversubscription will happen, or the available memory is larger than the size of fall pages, the programmer could set hints based on the conclusions provided by related researches [24]. otherwise, based on our findings, applying the hint ab is a preferable choice.\n\n----------------------------------------\n\n\n# 4. performance evaluation of advanced features in cuda unified memory\n\ncuda has introduced new features for optimizing the data migration on um, i.e., memory advises and prefetch. instead of solely relying on page faults, the memory advises feature allows the programmer to provide data access pattern for each memory object so that the runtime can optimize migration decisions. the prefetch proactively triggers asynchronous data migration to gpu before the data is accessed, which reduces page faults and, consequently, the overhead in handling page faults.\n\n-using memory advises improves application performance in oversubscription execution on the intel platform and in-memory executions on the ibm platform.\n\n-um prefetch provides a significant performance improvement on the intel-volta/pascal-pci-e based systems while it does not show a performance improvement on the power9-volta-nvlink based system\n\num was first introduced in cuda 6.0 [21]. only until the recent nvidia pascal microarchitecture that has hardware support for page faults.\n\n\n\n• cudamemadvisesetaccessedby establishes a direct mapping of data to a specified device. figure 2c illustrates an example of a physical page on gpu being remotely access from the host. when cudamemadvisesetpreferredlocation is applied, cuda runtime tries to build a direct mapping to the page to avoid data migration so that the destination can access data remotely. differently from cudamemadvisesetpreferredlocation, this cudamemadvisesetaccessedby does not try to pin pages on a specific device; instead, its main effect is to establish mapping on the remote device. this advice takes effect on the creation of the memory pages. the mapping will be re-established after the pages are migrated.\n\n• cudamemadvisesetpreferredlocation sets the preferred physical location of pages. this advice pins a page and prevents it from migrating to other memories. figure 2b illustrates a page preferred on the host side, and gpu uses remote mapping to access the page. this advice established a direct (remote) mapping to the memory page. when accessing the page remotely, data is fetched through the remote memory instead of generating a page fault. if the underlying hardware does not support the remote mapping, the page will be migrated as in the standard um. cudamemadvisesetpreferredlocation is useful for applications with little data sharing between cpu and gpu, i.e., part of the application is executed completely on the gpu, and the rest of the application executes on the host. data that is being used mostly by the gpu can be pinned to the gpu with the advice, avoiding memory thrashing.\n\n• cudamemadvisesetreadmostly implies a read-intensive data region. in the basic um, accessing a page on a remote side triggers page migration. however, with cudamemadvisesetreadmostly, a read-only duplicate of the page will be created on the faulting side, which prevents page faults and data migration in the future. figure 2a illustrates an example, where the second access (step 5) has no page fault and is local access. this mechanism, however, results in a high overhead if there is any update to this memory region because all copies of the corresponding page will be invalidated to preserve consistency between different copies. thus, this advice is often used in read-only data structures, such as lookup tables and application parameters.\n\nin general, we found both memory advises and prefetch to be simple and effective.\n\n----------------------------------------\n\n\n# 5. interplay between hardware prefetcher and page eviction policy in cpu-gpu unified virtual memory\n\ncons in traditional gpu: complicated asynchronous user-directed constructs to overlap data migration and kernel execution are used to address this issue. the second challenge is memory over-subscription. when the working set of the gpu kernel cannot fit in the device memory, the programmers have to painstakingly redefine the data structures and tile the data to transfer back and forth in chunks.\n\nthis flow is inspired by -> 11. towards high performance paged memory for gpu.\n\n1 scheduled threads generate global memory accesses.\n\n2 each sm has its own load/store unit. every load/store unit has its own tlb. load/store unit performs a tlb look up to find whether the translation for the issued memory access is cached in tlb or not. a tlb miss is relayed to the gmmu.\n\n3 the gmmu walks through the page table looking for a pte corresponding to the requested page with valid flag set. a far-fault occurs if there is no pte for the requested page or the valid flag is not set. then the far-fault is registered in the far-fault miss status handling registers (mshrs).\n\n4 the page is scheduled for transfer over cpu-gpu pci-e interconnect.\n\n5 a 4kb page is allocated on demand and data is migrated from host to device memory.\n\n6 the mshrs are consulted to notify the corresponding load/store unit and the memory access is replayed. a new pte entry is added to the page table with valid\n\n\n\nthis paper introduces random, sequential and tree-based neighborhood prefetcher in detail.\n\nand come up with pre-eviction for tree-based neighborhood, different from lru eviction used in nvidia.\n\n\n\n----------------------------------------\n\n\n# 7. batch-aware unified memory management in gpus for irregular workloads 2020\n\npropose:\n\n(1) increases the batch size (i.e., the number of page faults handled together), thereby amortizing the gpu runtime fault handling time, and reduces the number of batches by supporting cpu-like thread block context switching\n\nthread oversubscription (to), a cpu-like thread block context switching technique, to effectively amortize the gpu runtime fault handling time by increasing the batch size (i.e., the number of page faults handled together).\n\n(2) takes page eviction off the critical path with no hardware changes by overlapping evictions with cpu-to-gpu page migrations. unobtrusive eviction (ue) to take gpu page evictions off the critical path with no hardware changes based on the idea of overlapping page evictions with cpu-to-gpu page migrations.\n\nprior work reports that page fault handling latency ranges from 20µs to 50µs [53]. we find that these numbers are conservative and can be worse depending on the applications and systems. unfortunately, this page fault latency, which is in the order of microseconds, cannot be easily hidden even with ample thread-level parallelism (tlp) in gpus, especially when gpu memory is oversubscribed.\n\n\n\nthe gpu runtime processes a group of gpu page faults together, rather than processing each individual one, in order to amortize the overhead of multiple round-trip latencies over the pcie bus and to avoid invoking multiple interrupt service routines (isrs) in the operating system (os). to efficiently process an excessive number of page faults, the gpu runtime performs a series of operations such as preprocessing all the page faults and inserting page prefetching requests, which takes a significant amount of time (in the range of tens to hundreds of microseconds). once all the operations (e.g., cpu page table walks for all the page faults, page allocation and eviction scheduling, etc.) are finished, page migrations between the cpu and the gpu begin.\n\nthis page fault handling is expensive because (1) it requires long latency communications between the cpu and gpu over the pcie bus, and (2) the gpu runtime performs a very expensive fault handling service routine.\n\nto amortize the overhead, the gpu runtime processes a group of page faults together, which we refer to as batch processing.\n\nwhen a page fault exception is raised by the gpu memory management unit (mmu), the gpu runtime begins to handle the exception, shown in 1.\n\n\n\nfrom this, we conclude that page evictions and new page allocations are serialized in modern gpus to prevent the new pages from overwriting the evicted pages. note that an eviction is required on every page fault once the pages resident in the gpu’s memory are at capacity.\n\n\n\nthis preprocessing includes sorting the page faults in ascending order of page addresses (to accelerate the page table walks) and the analysis of page addresses to insert page prefetching requests.1 we refer to the time taken by the gpu runtime to perform a collection of operations to handle many page faults together as gpu runtime fault handling time.\n\nhttps://github.com/acsl-technion/gaia_nvidia/blob/e23e4d926f576c2c4169664b6add89e1368ee849/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c#l787\n\n// fault cache preprocessing for fault coalescing\n//\n// this function generates an ordered view of the given fault_cache in which faults are sorted by va space, fault\n// address (aligned to 4k) and access type "intrusiveness" (atomic - write - read - prefetch). in order to minimize\n// the number of instance_ptr to va space translations we perform a first sort by instance_ptr.\n//\n// this function returns nv_warn_more_processing_required if a fault buffer flush occurred during instance_ptr\n// translation and executed successfully, or the error code if it failed. nv_ok otherwise.\n//\n// current scheme:\n// 1) sort by instance_ptr\n// 2) translate all instance_ptrs to va spaces\n// 3) sort by va_space, fault address (gpu already reports 4k-aligned address) and access type\nstatic nv_status preprocess_fault_batch(uvm_gpu_t *gpu, uvm_fault_service_batch_context_t *batch_context)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nthe batch processing time is measured to be in the range of 223µs to 553µs with a median of 313µs, of which, gpu runtime fault handling accounts for an average of 46.69% of the time (measured to be in the range of 50µs to 430µs with a median of 140µs).\n\n1）thread oversubscription\n\nwe enable thread oversubscription from the beginning of the execution by allocating one additional thread block to each sm ( 1 ). the thread block additionally allocated to each sm is inactive at first. it is important to note that the number of active thread blocks does not exceed that of the baseline, which is determined by the physical resource constraints. once all of the warps in an active thread block are stalled due to page faults, the thread oversubscription mechanism context switches the active (but stalled) thread block with an inactive thread block ( 2 ). the thread oversubscription mechanism can be detrimental if it causes premature evictions. to prevent this, the gpu runtime monitors the premature eviction rates by periodically estimating the running average of the lifetime of pages by tracking when each page is allocated and evicted. we use the running average as an indicator of premature evictions. if the running average is decreased by a certain threshold, the thread oversubscription mechanism does not allow any more context switching by decrementing (and limiting) the number of concurrently runnable thread blocks ( 3 ).6 otherwise, thread oversubscription allocates one additional thread block to each sm in an incremental manner.\n\n\n\n 2. unobstrusive eviction\n\nwhen a page fault interrupt is raised by the gpu mmu, the top-half interrupt service routine (isr) responds. it checks whether the number of gpu resident pages is at capacity via the gpu memory status tracker. if so, it sends a preemptive eviction request to the gpu. the rest of the fault handling (e.g., preprocessing of the page faults, cpu-side page table walks) is performed by the bottom-half isr.\n\n\n\n\n\nwhen the gpu runtime begins a batch’s processing, it checks the gpu memory status. if it is at capacity, it initiates a single page eviction ( 1 ). once page x is evicted from the gpu’s memory, both cpu and gpu page tables are updated ( 2 ). unlike the baseline case (figure 4), page a can be migrated to the gpu memory without any delay ( 3 ). at the same time, page y can be evicted using bidirectional transfers. since the data transfer speed from the gpu to cpu memory is faster than the other way around [29], eviction is completely unobtrusive and migrations to the gpu can occur without any delay.\n\nin short, thread oversubscription increase the batch size by switching in in-active thread block. and unobstrusive eviction avoid the serialization of swap pages between host and device.\n\n----------------------------------------\n\n\n# 10. machine learning guided optimal use of gpu unified memory 2019\n\nto enable better performance of um, cuda allows developers to give the um driver additional advice on managing a given gpu memory range via an api function named cudamemadvise(const void *, size_t, enum cudamemoryadvise, int). the first two parameters of this function accept a pointer to a memory range with a specified size. the memory range should be allocated via cudamallocmanaged or declared via __managed__variables. the third parameter sets the advice for the memory range. the last parameter indicates the associated device’s id, which can indicate either a cpu or gpu device. the details and differences of these four kinds of advice are presented as follows:\n\n• default: this represents the default on-demand page migration to accessing processor, using the first-touch policy.\n\n• cudamemadvisesetreadmostly: this advice is used for the data which is mostly going to be read from and only occasionally written to. the um driver may create read-only copies of the data in a processor’s memory when that processor accesses it. if this region encounters any write requests, then only the write occurred page will be valid and other copies will be invalid.\n\n• cudamemadvisesetpreferredlocation: once a target device is specified, this device memory can be set as the preferred location for the allocated data. the host memory can also be specified as the preferred location. setting the preferred location does not cause data to migrate to that location immediately. the policy only guides what will happen when a fault occurs on the specified memory region: if data is already in the preferred location, the faulting processor will try to directly establish a mapping to the region without causing page migration. otherwise, the data will be migrated to the processor accessing it if the data is not in the preferred location or if a direct mapping cannot be established.\n\n• cudamemadvisesetaccessedby: this advice implies that the data will be accessed by a specified cpu or gpu device. it has no impact on the data location and will not cause data migration. it only causes the data to be always mapped in the specified processor’s page tables, when applicable. the mapping will be accordingly updated if the data is migrated somehow. this advice is useful to indicate that avoiding faults is important for some data, especially when the data is accessed by a gpu within a system containing multiple gpus with peer-to-peer access enabled.\n\n----------------------------------------\n\n\n# 14. fine-grain quantitative analysis of demand paging in unified virtual memory [2024] 👍👍👍👍\n\nsame author: in-depth analyses of unified virtual memory system for gpu accelerated computing[2021]\n\n\n\nthe uvm host driver on the host is open source with dependencies on the proprietary nvidia driver/resource manager and the host os for memory management. this driver is a runtime fault servicing engine and the memory manager for managed memory allocations.\n\n\n 1. the fault is generated and handled by the hardware thread’s corresponding µtlb. the thread may continue executing instructions not blocked by a memory dependency. the fault propagates to the gpu memory management unit (gmmu), which writes the corresponding fault information into the gpu fault buffer and sends a hardware interrupt to the host. the fault buffer acts as a circular array, configured and managed by the uvm driver.\n    \n 2. the nvidia-uvm driver fetches the fault information, caches it on the host, and services the faults through\n 3. page processing: page table update and tlb shootdown on the host and gpu page table update\n 4. page migration: involves page migration.\n\nthe gpu exposes two functionalities to the host via the gpu command push-buffer—host-to gpu memory copy and fault replay.\nas part of the fault servicing process, the driver instructs the gpu to copy pages into its memory, generally using high-performance hardware “copy engines.”\nonce the gpu’s page tables are updated and the data is successfully migrated, the driver issues a fault replay, which clears the waiting status of µtlb, causing them to “replay” the prior miss.\n\nfault handling:\n\nfirst, the gpu sends an interrupt over the interconnect to alert the host uvm driver of a page fault. the interrupt wakes up a worker thread to begin fault servicing if none is awake.\nsecond, the host retrieves the complete fault information from the gpu fault buffer.\nthe default fault retrieval policy reads faults until the batch size limit (i.e., 256 faults) is reached or no faults remain in the buffer.\n\n\nthese vablocks serve as logical boundaries; the driver processes all batch faults within a single vablock together, and each vablock within a batch requires a distinct processing step.\n\n\n\n\n\nnotes:\n\n 1. when prefetching is not enabled, service faults is the major part of delay in cpu-gpu system. in this case, even nvlink does not matter. the reason is that unmapping and tlb-shut down in multi-cpu costs a lot.\n    \n 2. pretching reduce the overhead by reduction of page fault and also increase the efficiency of nvlink.\n    \n 3. oversubscription worse the case by finding empty space failed first and then evictim block to gpu. this worsen the performance.\n    \n\n1 & 2 explained:\n\n(1) unmapping host-side data takes place on the fault path and incurs significant overhead\n(2) certain hostside parallelizations of an application using uvm can exaggerate these unmapping costs.\nthe host os performs this operation, and the costs likely stem from issues with virtual mappings across cpu cores, flushing dirty pages from caches and tlbs, numa, and other memory-adjacent issues.\nadditionally, these operations do not take place in bulk due to the logical separation of vablocks within uvm.\nthis is an area that deserves particular scrutiny as hmm also performs host page unmapping on the fault path using host os mechanisms, implying a similar cost could be applied to all devices when using hmm [15, 26].\n\ncompared to cpu-gpu case, gpu-gpu on-demand page migration is faster due to the actual page table updates offloaded to the source gpu.\nfault servicing includes operations such as page unmapping and tlb shootdown on the source device.\ngpu page table updates and tlb shootdown are hardware based and relatively much faster.\n\n\n 3. explained process: (1) fail allocation\n    (2) evict a vablock and migrate the data back to the host\n    (3) restart the block migration process, including host unmapping, data transfer, gpu mapping, page population, a process by which pages are filled with zero values before data is migrated to them.\n\ninterestingly, oversubscription diminishes the benefits of nvlink2. oversubscription, as it is currently implemented, always evicts pages back to the host memory. this causes the cpu-gpu pcie interconnect to become active for data eviction.\n\n👉 in short, in cpu-gpu system, service faults are major issue due to tlb shutdown and page table update. this even diminish the power of nvlink. memory oversubscription worsen the situation by failing to allocate memory in gpu, find eviction and eviction to cpu, adding these operation worsen the performance.\n👉 gpu-gpu does not have the service faults problem since page table update and tlb shutdown are handled by faster gpu hardware.\n👉 besides, prefetching helps to improve performance a lot by reducing fault and better bandwitdh efficiency.\n',charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"TensorCore Paper List",frontmatter:{title:"TensorCore Paper List",date:"2023-11-17T00:00:00.000Z",permalink:"/pages/44871e/"},regularPath:"/03.gpu/05.TensorCore.html",relativePath:"03.gpu/05.TensorCore.md",key:"v-2861ec8b",path:"/pages/44871e/",headers:[{level:3,title:"1. Modeling Deep Learning Accelerator Enabled GPUs",slug:"_1-modeling-deep-learning-accelerator-enabled-gpus",normalizedTitle:"1. modeling deep learning accelerator enabled gpus",charIndex:1}],headersStr:"1. Modeling Deep Learning Accelerator Enabled GPUs",content:" 1. Modeling Deep Learning Accelerator Enabled GPUs 2019\n 2. Dissecting Tensor Cores via Microbenchmarks: Latency, Throughput and Numeric Behaviors 2023\n 3. Demystifying Tensor Cores to Optimize Half-Precision Matrix Multiply 2020\n 4. CS 380 - GPU and GPGPU ProgrammingLecture 26: Programming Tensor Cores\n 5. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmark 2018\n\n----------------------------------------\n\n\n# 1. Modeling Deep Learning Accelerator Enabled GPUs",normalizedContent:" 1. modeling deep learning accelerator enabled gpus 2019\n 2. dissecting tensor cores via microbenchmarks: latency, throughput and numeric behaviors 2023\n 3. demystifying tensor cores to optimize half-precision matrix multiply 2020\n 4. cs 380 - gpu and gpgpu programminglecture 26: programming tensor cores\n 5. dissecting the nvidia volta gpu architecture via microbenchmark 2018\n\n----------------------------------------\n\n\n# 1. modeling deep learning accelerator enabled gpus",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Memory Behaviour Paper List",frontmatter:{title:"Memory Behaviour Paper List",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/45871e/"},regularPath:"/03.gpu/06.MemoryBehaviour.html",relativePath:"03.gpu/06.MemoryBehaviour.md",key:"v-04292a45",path:"/pages/45871e/",headers:[{level:3,title:"1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior",slug:"_1-a-comparative-analysis-of-microarchitecture-effects-on-cpu-and-gpu-memory-system-behavior",normalizedTitle:"1. a comparative analysis of microarchitecture effects on cpu and gpu memory system behavior",charIndex:1}],headersStr:"1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior",content:" 1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior 2014\n 2. \n\n----------------------------------------\n\n\n# 1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior\n\nCPU cores must extract very wide ILP in order to expose MLP to the memory hierarchy, and this MLP can be limited to lower levels of the memory hierarchy due to L1 cache locality. On the other hand, GPU cores and caches aim to mitigate MLP limitations, allowing the programmer to focus their efforts on leveraging the available MLP.\n\n\n\nRodinia is categorized into the above features.\n\n * Pipleline\n * Iterative\n * Head Reads(I/c)\n\nA fairly common factor in compute and memory op count differences between system configurations is due to register handling. For x86 CPU applications, the small architected register set (16) can cause register spilling to the stack and recomputation of previously computed values.\n\nIn contrast, GPU cores have some flexibility in register use due to their core multithreading. By running fewer GPU threads per core and late binding register specifiers to physical registers, there is more flexibility for each thread to access more registers, which can avoid spilling and recomputation.\n\nMemory Access Pattern\n\nCPU cores use a small set of deep per-thread instruction windows, and high-frequency pipelines and caches to expose parallel memory accesses. In contrast, GPU cores expose parallel memory accesses by executing 100s–1000s more threads at lower frequencies, and threads are grouped for smaller perthread instruction windows and memory request coalescing.\n\nwhile CPU cores rely heavily on L1 caches to capture locality, GPU cores capture most locality with coalescing and lessen the L1 cache responsibilities by providing scratch memory.\n\nBeyond the L1 caches, the memory systems tend to capture very similar locality. Further, we see that different core threading and cache filtering result in extreme differences in instantaneous memory access rates; CPU caches tend to filter accesses down to regular intervals, while GPU cores tend to issue bursts of accesses.\n\n\n\nScratch memory: GPU cores provide scratch memory, which can function as local storage for groups of threads to expand the space of local storage with register-like accessibility. In CUDA benchmarks that use the GPU scratch memory, kernels are typically organized into three stages: (1) read a small portion of data from global memory into the scratch memory, (2) compute on the data in scratch memory, and (3) write results back to global memory.\n\nSince GPU request coalescing behaves similarly to CPU single-instruction, multiple-data (SIMD) vectorization, vectorization reduces the total number of memory accesses by 1.32–1.69× (1.44× geometric mean), and that most of the eliminated accesses are to heap data.\n\nOverall, GPU scratch memory and request coalescing reduce the number of global memory accesses by 18–100× compared to CPU applications (27× in the geometric mean).\n\nCompared to CPU cores, this reduction alleviates pressure on caches, which in turn allows GPU cores to operate at lower frequencies while still serving data to threads at rates comparable to or greater than CPU cores.\n\nSpatial Locality\n\nCPU threads have extremely high spatial locality, typically striding through all elements in a heap cache line in subsequent algorithm loop iterations. These access patterns, which also include accesses to stack/local memory that is persistent over many loop iterations, result in high L1 cache hit rates that even exceed those expected by simple strided read memory access.\n\nFor GPU, small number of remaining spatially local accesses is likely due to separate thread groups accessing the same data rather than thread groups being unable to fully coalesce accesses.\n\nTemporal Locality\n\nFor CPU, this leaves the L2 caches mostly responsible for capturing temporally local accesses to data shared across cores rather than temporally or spatially local accesses to data previously evicted from the L1 caches due to limited capacity.\n\nFor GPU, This indicates that instead of competing for L1 capacity, GPU threads from separate cores are generating most of the temporally local accesses to single cache lines, similar to the CPU L2.\n\nBased on the above observations, we find that CPU and GPU L1 caches have very different importance, though their filtering roles are similar.\n\nIn the aggregate for data-parallel workloads, CPU L1 caches have many responsibilities; they must be designed to capture both the spatial locality for heap data accesses and the temporal locality of stack accesses. Fortunately for data-parallel workloads, these responsibilities rarely conflict given sufficient L1 capacity, so CPU L1s are quite effective and important for capturing locality.\n\nFor GPU applications, register and scratch memory can shift local variable accesses away from the caches, which eliminates the L1 responsibility for capturing temporally local stack requests. Further, GPU coalescing greatly reduces the importance of spatial locality across separate heap accesses, so the L1 caches are mostly responsible for capturing the small number of temporally local accesses from separate GPU threads on the same core(this is different from cpu), diminishing the overall responsibility of the GPU L1s compared to CPU L1s.\n\nBandwidth Demands\n\n\n\nThe key takeaway here is that GPU burst access behavior results from the way that GPUs group and launch threads. Specifically, at the beginning of a kernel, all capable thread block contexts begin executing at roughly the same time, so this can cause very large bursts of independent accesses.\n\nFollowing this initial burst, smaller but still significant access bursts occur each time a new thread block begins executing or when thread groups pass synchronization events.\n\nBy contrast, CPU cache access filtering tends to modulate the core’s ability to issue nearly as many parallel accesses to off-chip memory.\n\nLatency Sensitivity and Bandwidth Sensitivity\n\n\n\nThis Figue show CPU is sensitive to latency, but gpu to bandwidth.\n\nInteresting thoughts*\n\n-cache shared by CPU and GPU -interconnect and off-chip memory scheduling\n\nThese should take different characteristic of CPU and GPU into consideration.",normalizedContent:" 1. a comparative analysis of microarchitecture effects on cpu and gpu memory system behavior 2014\n 2. \n\n----------------------------------------\n\n\n# 1. a comparative analysis of microarchitecture effects on cpu and gpu memory system behavior\n\ncpu cores must extract very wide ilp in order to expose mlp to the memory hierarchy, and this mlp can be limited to lower levels of the memory hierarchy due to l1 cache locality. on the other hand, gpu cores and caches aim to mitigate mlp limitations, allowing the programmer to focus their efforts on leveraging the available mlp.\n\n\n\nrodinia is categorized into the above features.\n\n * pipleline\n * iterative\n * head reads(i/c)\n\na fairly common factor in compute and memory op count differences between system configurations is due to register handling. for x86 cpu applications, the small architected register set (16) can cause register spilling to the stack and recomputation of previously computed values.\n\nin contrast, gpu cores have some flexibility in register use due to their core multithreading. by running fewer gpu threads per core and late binding register specifiers to physical registers, there is more flexibility for each thread to access more registers, which can avoid spilling and recomputation.\n\nmemory access pattern\n\ncpu cores use a small set of deep per-thread instruction windows, and high-frequency pipelines and caches to expose parallel memory accesses. in contrast, gpu cores expose parallel memory accesses by executing 100s–1000s more threads at lower frequencies, and threads are grouped for smaller perthread instruction windows and memory request coalescing.\n\nwhile cpu cores rely heavily on l1 caches to capture locality, gpu cores capture most locality with coalescing and lessen the l1 cache responsibilities by providing scratch memory.\n\nbeyond the l1 caches, the memory systems tend to capture very similar locality. further, we see that different core threading and cache filtering result in extreme differences in instantaneous memory access rates; cpu caches tend to filter accesses down to regular intervals, while gpu cores tend to issue bursts of accesses.\n\n\n\nscratch memory: gpu cores provide scratch memory, which can function as local storage for groups of threads to expand the space of local storage with register-like accessibility. in cuda benchmarks that use the gpu scratch memory, kernels are typically organized into three stages: (1) read a small portion of data from global memory into the scratch memory, (2) compute on the data in scratch memory, and (3) write results back to global memory.\n\nsince gpu request coalescing behaves similarly to cpu single-instruction, multiple-data (simd) vectorization, vectorization reduces the total number of memory accesses by 1.32–1.69× (1.44× geometric mean), and that most of the eliminated accesses are to heap data.\n\noverall, gpu scratch memory and request coalescing reduce the number of global memory accesses by 18–100× compared to cpu applications (27× in the geometric mean).\n\ncompared to cpu cores, this reduction alleviates pressure on caches, which in turn allows gpu cores to operate at lower frequencies while still serving data to threads at rates comparable to or greater than cpu cores.\n\nspatial locality\n\ncpu threads have extremely high spatial locality, typically striding through all elements in a heap cache line in subsequent algorithm loop iterations. these access patterns, which also include accesses to stack/local memory that is persistent over many loop iterations, result in high l1 cache hit rates that even exceed those expected by simple strided read memory access.\n\nfor gpu, small number of remaining spatially local accesses is likely due to separate thread groups accessing the same data rather than thread groups being unable to fully coalesce accesses.\n\ntemporal locality\n\nfor cpu, this leaves the l2 caches mostly responsible for capturing temporally local accesses to data shared across cores rather than temporally or spatially local accesses to data previously evicted from the l1 caches due to limited capacity.\n\nfor gpu, this indicates that instead of competing for l1 capacity, gpu threads from separate cores are generating most of the temporally local accesses to single cache lines, similar to the cpu l2.\n\nbased on the above observations, we find that cpu and gpu l1 caches have very different importance, though their filtering roles are similar.\n\nin the aggregate for data-parallel workloads, cpu l1 caches have many responsibilities; they must be designed to capture both the spatial locality for heap data accesses and the temporal locality of stack accesses. fortunately for data-parallel workloads, these responsibilities rarely conflict given sufficient l1 capacity, so cpu l1s are quite effective and important for capturing locality.\n\nfor gpu applications, register and scratch memory can shift local variable accesses away from the caches, which eliminates the l1 responsibility for capturing temporally local stack requests. further, gpu coalescing greatly reduces the importance of spatial locality across separate heap accesses, so the l1 caches are mostly responsible for capturing the small number of temporally local accesses from separate gpu threads on the same core(this is different from cpu), diminishing the overall responsibility of the gpu l1s compared to cpu l1s.\n\nbandwidth demands\n\n\n\nthe key takeaway here is that gpu burst access behavior results from the way that gpus group and launch threads. specifically, at the beginning of a kernel, all capable thread block contexts begin executing at roughly the same time, so this can cause very large bursts of independent accesses.\n\nfollowing this initial burst, smaller but still significant access bursts occur each time a new thread block begins executing or when thread groups pass synchronization events.\n\nby contrast, cpu cache access filtering tends to modulate the core’s ability to issue nearly as many parallel accesses to off-chip memory.\n\nlatency sensitivity and bandwidth sensitivity\n\n\n\nthis figue show cpu is sensitive to latency, but gpu to bandwidth.\n\ninteresting thoughts*\n\n-cache shared by cpu and gpu -interconnect and off-chip memory scheduling\n\nthese should take different characteristic of cpu and gpu into consideration.",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"GPU Virtualization Paper List",frontmatter:{title:"GPU Virtualization Paper List",date:"2023-12-18T00:00:00.000Z",permalink:"/pages/45871f/"},regularPath:"/03.gpu/07.GPUVirtualization.html",relativePath:"03.gpu/07.GPUVirtualization.md",key:"v-d22a70f6",path:"/pages/45871f/",headers:[{level:3,title:"1.TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments",slug:"_1-timegraph-gpu-scheduling-for-real-time-multi-tasking-environments",normalizedTitle:"1.timegraph: gpu scheduling for real-time multi-tasking environments",charIndex:500},{level:2,title:"Done.",slug:"done",normalizedTitle:"done.",charIndex:573},{level:3,title:"2.Hardware Compute Partitioning on NVIDIA GPUs",slug:"_2-hardware-compute-partitioning-on-nvidia-gpus",normalizedTitle:"2.hardware compute partitioning on nvidia gpus",charIndex:583},{level:3,title:"3.GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation]",slug:"_3-gpuvm-why-not-virtualizing-gpus-at-the-hypervisor-year-2014-citation",normalizedTitle:"3.gpuvm: why not virtualizing gpus at the hypervisor [year 2014 citation]",charIndex:683},{level:3,title:"4.Implementing Open-Source CUDA Runtime",slug:"_4-implementing-open-source-cuda-runtime",normalizedTitle:"4.implementing open-source cuda runtime",charIndex:884},{level:3,title:"5.Gdev: First-Class GPU Resource Management in the Operating System",slug:"_5-gdev-first-class-gpu-resource-management-in-the-operating-system",normalizedTitle:"5.gdev: first-class gpu resource management in the operating system",charIndex:977}],headersStr:"1.TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments Done. 2.Hardware Compute Partitioning on NVIDIA GPUs 3.GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation] 4.Implementing Open-Source CUDA Runtime 5.Gdev: First-Class GPU Resource Management in the Operating System",content:" 1. TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments [Citation 372]\n 2. Hardware Compute Partitioning on NVIDIA GPUs [Year 2022 Citation 3]\n 3. GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation 142]\n 4. Implementing Open-Source CUDA Runtime [Year 2013 Citation 13]\n 5. Gdev: First-Class GPU Resource Management in the Operating System [Year 2012 Citation 272]\n\n1.3.4.5 are written by the same author: Shinpei Kato.\n\n----------------------------------------\n\n\n# 1.TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments\n\n\n# Done.\n\n\n# 2.Hardware Compute Partitioning on NVIDIA GPUs\n\nDone.\n\n----------------------------------------\n\n\n# 3.GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation]\n\nhttps://cseweb.ucsd.edu/~yiying/cse291j-winter20/reading/GPU-Virtualization.pdf\n\n----------------------------------------\n\n\n# 4.Implementing Open-Source CUDA Runtime\n\nDone.\n\n----------------------------------------\n\n\n# 5.Gdev: First-Class GPU Resource Management in the Operating System\n\nDone.",normalizedContent:" 1. timegraph: gpu scheduling for real-time multi-tasking environments [citation 372]\n 2. hardware compute partitioning on nvidia gpus [year 2022 citation 3]\n 3. gpuvm: why not virtualizing gpus at the hypervisor [year 2014 citation 142]\n 4. implementing open-source cuda runtime [year 2013 citation 13]\n 5. gdev: first-class gpu resource management in the operating system [year 2012 citation 272]\n\n1.3.4.5 are written by the same author: shinpei kato.\n\n----------------------------------------\n\n\n# 1.timegraph: gpu scheduling for real-time multi-tasking environments\n\n\n# done.\n\n\n# 2.hardware compute partitioning on nvidia gpus\n\ndone.\n\n----------------------------------------\n\n\n# 3.gpuvm: why not virtualizing gpus at the hypervisor [year 2014 citation]\n\nhttps://cseweb.ucsd.edu/~yiying/cse291j-winter20/reading/gpu-virtualization.pdf\n\n----------------------------------------\n\n\n# 4.implementing open-source cuda runtime\n\ndone.\n\n----------------------------------------\n\n\n# 5.gdev: first-class gpu resource management in the operating system\n\ndone.",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"GPU Simulator",frontmatter:{title:"GPU Simulator",date:"2024-01-01T00:00:00.000Z",permalink:"/pages/458721/"},regularPath:"/03.gpu/09.Simulator.html",relativePath:"03.gpu/09.Simulator.md",key:"v-e33712b6",path:"/pages/458721/",headers:[{level:3,title:"1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis",slug:"_1-delta-gpu-performance-model-for-deep-learning-applications-with-in-depth-memory-system-traffic-analysis",normalizedTitle:"1.delta: gpu performance model for deep learning applications with in-depth memory system traffic analysis",charIndex:263},{level:3,title:"2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]",slug:"_2-lost-in-abstraction-pitfalls-of-analyzing-gpus-at-the-intermediate-language-level-hpca",normalizedTitle:"2.lost in abstraction: pitfalls of analyzing gpus at the intermediate language level [hpca]",charIndex:401}],headersStr:"1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis 2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]",content:" 1. DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis [Citation 39]\n 2. Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]\n\n----------------------------------------\n\n\n# 1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis\n\nStill reading in process.\n\n\n# 2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]\n\nDone. Gem5 GPU Introduction.\n\nReference Materials\n\nAMD_gem5_APU_simulator_isca_2018_gem5_wiki.pdf",normalizedContent:" 1. delta: gpu performance model for deep learning applications with in-depth memory system traffic analysis [citation 39]\n 2. lost in abstraction: pitfalls of analyzing gpus at the intermediate language level [hpca]\n\n----------------------------------------\n\n\n# 1.delta: gpu performance model for deep learning applications with in-depth memory system traffic analysis\n\nstill reading in process.\n\n\n# 2.lost in abstraction: pitfalls of analyzing gpus at the intermediate language level [hpca]\n\ndone. gem5 gpu introduction.\n\nreference materials\n\namd_gem5_apu_simulator_isca_2018_gem5_wiki.pdf",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Large Language Model Paper List",frontmatter:{title:"Large Language Model Paper List",date:"2023-12-19T00:00:00.000Z",permalink:"/pages/458720/"},regularPath:"/03.gpu/08.LLM.html",relativePath:"03.gpu/08.LLM.md",key:"v-65bd31e5",path:"/pages/458720/",headers:[{level:3,title:"1. Efficient Memory Management for Large Language Model Serving with PagedAttention",slug:"_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention",normalizedTitle:"1. efficient memory management for large language model serving with pagedattention",charIndex:1},{level:3,title:"2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory",slug:"_2-llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory",normalizedTitle:"2. llm in a flash: efficient large language model inference with limited memory",charIndex:93},{level:3,title:"1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",slug:"_1-a-survey-on-hallucination-in-large-language-models-principles-taxonomy-challenges-and-open-questions",normalizedTitle:"1. a survey on hallucination in large language models: principles, taxonomy, challenges, and open questions",charIndex:4386}],headersStr:"1. Efficient Memory Management for Large Language Model Serving with PagedAttention 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory 1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",content:" 1. Efficient Memory Management for Large Language Model Serving with PagedAttention [2023]\n 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory [Apple 2023]\n\n----------------------------------------\n\n\n# 1. Efficient Memory Management for Large Language Model Serving with PagedAttention\n\nDisscussed the GEMM in prompt and GEMV in auto regression. In GEMV, LLM is memory bound. There is lot of fragment in KVCache. It also quantize the memory necessity for parameter in KV Cache. They came up the method similar to paging in OS to manage KV in KV cache, reducing the fragment.\n\n\n# 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory\n\nUpproject matrix and downprojection matrix: https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/ Related paper: Parameter-Efficient Transfer Learning for NLP This introduce low-rank.\n\nsliding window.\n\n 1. high sparsity in FeedForward Layers, more than 90% Selectively only load parameters from memory either no-zero input or predicted have non-zero output\n\n 2. Minimize data transfer and maximize flash memory throughout Window sliding: Load parameters for only the past few tokens, reusing activations from recently computed tokens. This sliding window approach reduces the number of IO requests to load weights. Row-column bundling: We store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. This increases throughput by reading larger chunks.\n\n 3. Predict FFN sparsity and avoid loading zeroed-out parameter to minimize the number of weights to be transferred from flash memory to DRAM.\n\n 4. Static memory preallocation\n\nAlso a model to predict the tradeoff between loading less data and reading larger chunks\n\nLoad only 2% of FFN layer from flash\n\n 1. Larger chunk Although throughput growth is not linear (larger chunks take longer to transfer), the latency for the initial byte becomes a smaller fraction of the total request time, resulting in more efficient data reading.\n\n 2. Load From Flash\n\n2.1 inherent sparsity found in Feed-Forward Network (FFN) model\n\nSelective Persistence Strategy Retain the embeddings and matrices within the attention mechanism of the transformer constant.Attentions weights 1/3 of the model size.For the Feed-Forward Network (FFN) portions, only the non-sparse segments are dynamically loaded into DRAM as needed.\n\nAnticipating ReLU Sparsity\nRelu activation can induce 90% sparsity. Optimize preceding layer, up project by low-rank predictor to identify the zeroed elements post-ReLU.\nIn contrast to their work, our predictor needs only the output of the current layer’s attention module and not the previous layer’s FFN module.\n\nNeuron Data Management via Sliding Window Technique Our approach focuses on managing neuron data by employing a Sliding Window Technique. This methodology entails maintaining neuron data only for a recent subset of input tokens in the memory.\nThe key aspect of this technique is the selective loading of neuron data that differs between the current input token and its immediate predecessors.\nFrees up memory resources previously allocated to neuron data from older tokens that are no longer within the sliding window\n\nLet sagg(k) denote the cumulative use of neuron data across a sequence of k input tokens. This reduction in data loading is counterbalanced by the memory cost associated with storing sagg(k). In determining the size of the sliding window, the aim is to maximize it within the constraints imposed by the available memory capacity.\n\n2.2 Improve Transfer Throughput with Increased Chunk Sizes\n\nBundling Columns and Rows for upward and downward projection\n\nBundling Based on Co-activation fetch neuron with its cloest friend. But there is WARM-GUY problem.\n\n2.3 Optimized Data Management in DRAM\n\nWhen a substantial portion (approximately 25%) of the Feed-Forward Networks (FFNs) in DRAM needs to be rewritten.\n\nWhen introducing data for new neurons, reallocating the matrix and appending new matrices can lead to significant overhead due to the need for rewriting existing neurons data in DRAM. This involves the preallocation of all necessary memory and the establishment of a corresponding data structure for efficient management.\n\n----------------------------------------\n\nLLM Principles\n\n\n# 1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\n\n",normalizedContent:" 1. efficient memory management for large language model serving with pagedattention [2023]\n 2. llm in a flash: efficient large language model inference with limited memory [apple 2023]\n\n----------------------------------------\n\n\n# 1. efficient memory management for large language model serving with pagedattention\n\ndisscussed the gemm in prompt and gemv in auto regression. in gemv, llm is memory bound. there is lot of fragment in kvcache. it also quantize the memory necessity for parameter in kv cache. they came up the method similar to paging in os to manage kv in kv cache, reducing the fragment.\n\n\n# 2. llm in a flash: efficient large language model inference with limited memory\n\nupproject matrix and downprojection matrix: https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/ related paper: parameter-efficient transfer learning for nlp this introduce low-rank.\n\nsliding window.\n\n 1. high sparsity in feedforward layers, more than 90% selectively only load parameters from memory either no-zero input or predicted have non-zero output\n\n 2. minimize data transfer and maximize flash memory throughout window sliding: load parameters for only the past few tokens, reusing activations from recently computed tokens. this sliding window approach reduces the number of io requests to load weights. row-column bundling: we store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. this increases throughput by reading larger chunks.\n\n 3. predict ffn sparsity and avoid loading zeroed-out parameter to minimize the number of weights to be transferred from flash memory to dram.\n\n 4. static memory preallocation\n\nalso a model to predict the tradeoff between loading less data and reading larger chunks\n\nload only 2% of ffn layer from flash\n\n 1. larger chunk although throughput growth is not linear (larger chunks take longer to transfer), the latency for the initial byte becomes a smaller fraction of the total request time, resulting in more efficient data reading.\n\n 2. load from flash\n\n2.1 inherent sparsity found in feed-forward network (ffn) model\n\nselective persistence strategy retain the embeddings and matrices within the attention mechanism of the transformer constant.attentions weights 1/3 of the model size.for the feed-forward network (ffn) portions, only the non-sparse segments are dynamically loaded into dram as needed.\n\nanticipating relu sparsity\nrelu activation can induce 90% sparsity. optimize preceding layer, up project by low-rank predictor to identify the zeroed elements post-relu.\nin contrast to their work, our predictor needs only the output of the current layer’s attention module and not the previous layer’s ffn module.\n\nneuron data management via sliding window technique our approach focuses on managing neuron data by employing a sliding window technique. this methodology entails maintaining neuron data only for a recent subset of input tokens in the memory.\nthe key aspect of this technique is the selective loading of neuron data that differs between the current input token and its immediate predecessors.\nfrees up memory resources previously allocated to neuron data from older tokens that are no longer within the sliding window\n\nlet sagg(k) denote the cumulative use of neuron data across a sequence of k input tokens. this reduction in data loading is counterbalanced by the memory cost associated with storing sagg(k). in determining the size of the sliding window, the aim is to maximize it within the constraints imposed by the available memory capacity.\n\n2.2 improve transfer throughput with increased chunk sizes\n\nbundling columns and rows for upward and downward projection\n\nbundling based on co-activation fetch neuron with its cloest friend. but there is warm-guy problem.\n\n2.3 optimized data management in dram\n\nwhen a substantial portion (approximately 25%) of the feed-forward networks (ffns) in dram needs to be rewritten.\n\nwhen introducing data for new neurons, reallocating the matrix and appending new matrices can lead to significant overhead due to the need for rewriting existing neurons data in dram. this involves the preallocation of all necessary memory and the establishment of a corresponding data structure for efficient management.\n\n----------------------------------------\n\nllm principles\n\n\n# 1. a survey on hallucination in large language models: principles, taxonomy, challenges, and open questions\n\n",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Architectural Survey",frontmatter:{title:"Architectural Survey",date:"2024-03-30T00:00:00.000Z",permalink:"/pages/458722/"},regularPath:"/03.gpu/10.%20Architectural%20Survey.html",relativePath:"03.gpu/10. Architectural Survey.md",key:"v-a0731602",path:"/pages/458722/",headers:[{level:2,title:"Control flow divergence",slug:"control-flow-divergence",normalizedTitle:"control flow divergence",charIndex:643},{level:3,title:"1. Regrouping Divergent warps",slug:"_1-regrouping-divergent-warps",normalizedTitle:"1. regrouping divergent warps",charIndex:1633},{level:3,title:"2.  Large Warp/CTA compaction",slug:"_2-large-warp-cta-compaction",normalizedTitle:"2.  large warp/cta compaction",charIndex:null},{level:3,title:"3. Multi-path execution",slug:"_3-multi-path-execution",normalizedTitle:"3. multi-path execution",charIndex:2727},{level:3,title:"4. MIMD-like architecture",slug:"_4-mimd-like-architecture",normalizedTitle:"4. mimd-like architecture",charIndex:2815},{level:3,title:"5. Dynamic kernels/threads",slug:"_5-dynamic-kernels-threads",normalizedTitle:"5. dynamic kernels/threads",charIndex:3221},{level:2,title:"Efficient utilization of memory bandwidth",slug:"efficient-utilization-of-memory-bandwidth",normalizedTitle:"efficient utilization of memory bandwidth",charIndex:5054},{level:3,title:"1. Alleviating cache thrashing, and resource contention",slug:"_1-alleviating-cache-thrashing-and-resource-contention",normalizedTitle:"1. alleviating cache thrashing, and resource contention",charIndex:5100},{level:3,title:"2. High-bandwidth many-thread-aware memory hierarchy",slug:"_2-high-bandwidth-many-thread-aware-memory-hierarchy",normalizedTitle:"2. high-bandwidth many-thread-aware memory hierarchy",charIndex:11365},{level:2,title:"Increasing parallelism and improving execution pipelining",slug:"increasing-parallelism-and-improving-execution-pipelining",normalizedTitle:"increasing parallelism and improving execution pipelining",charIndex:14420},{level:3,title:"1. Reducing resource fragmentation and increasing parallelism",slug:"_1-reducing-resource-fragmentation-and-increasing-parallelism",normalizedTitle:"1. reducing resource fragmentation and increasing parallelism",charIndex:15371},{level:3,title:"2. GPU multitasking",slug:"_2-gpu-multitasking",normalizedTitle:"2. gpu multitasking",charIndex:15701},{level:3,title:"3. Exploiting scalar and value similarity opportunities",slug:"_3-exploiting-scalar-and-value-similarity-opportunities",normalizedTitle:"3. exploiting scalar and value similarity opportunities",charIndex:16480},{level:3,title:"4. Improving execution pipelining",slug:"_4-improving-execution-pipelining",normalizedTitle:"4. improving execution pipelining",charIndex:17379},{level:2,title:"Enhancing GPGPU programmability",slug:"enhancing-gpgpu-programmability",normalizedTitle:"enhancing gpgpu programmability",charIndex:18458},{level:3,title:"1. Coherence and consistency model",slug:"_1-coherence-and-consistency-model",normalizedTitle:"1. coherence and consistency model",charIndex:18496},{level:3,title:"2. Transactional memory",slug:"_2-transactional-memory",normalizedTitle:"2. transactional memory",charIndex:18795},{level:3,title:"3. Deterministic GPU",slug:"_3-deterministic-gpu",normalizedTitle:"3. deterministic gpu",charIndex:19057},{level:3,title:"4. Memory management",slug:"_4-memory-management",normalizedTitle:"4. memory management",charIndex:19082},{level:2,title:"CPU–GPU heterogeneous architecture",slug:"cpu-gpu-heterogeneous-architecture",normalizedTitle:"cpu–gpu heterogeneous architecture",charIndex:19645},{level:3,title:"1. Impacts of CPU–GPU integration",slug:"_1-impacts-of-cpu-gpu-integration",normalizedTitle:"1. impacts of cpu–gpu integration",charIndex:19684},{level:3,title:"2. CPU–GPU programmability",slug:"_2-cpu-gpu-programmability",normalizedTitle:"2. cpu–gpu programmability",charIndex:20168},{level:3,title:"3. Exploiting heterogeneity",slug:"_3-exploiting-heterogeneity",normalizedTitle:"3. exploiting heterogeneity",charIndex:20369},{level:3,title:"4. Shared resources management",slug:"_4-shared-resources-management",normalizedTitle:"4. shared resources management",charIndex:20696}],headersStr:"Control flow divergence 1. Regrouping Divergent warps 2.  Large Warp/CTA compaction 3. Multi-path execution 4. MIMD-like architecture 5. Dynamic kernels/threads Efficient utilization of memory bandwidth 1. Alleviating cache thrashing, and resource contention 2. High-bandwidth many-thread-aware memory hierarchy Increasing parallelism and improving execution pipelining 1. Reducing resource fragmentation and increasing parallelism 2. GPU multitasking 3. Exploiting scalar and value similarity opportunities 4. Improving execution pipelining Enhancing GPGPU programmability 1. Coherence and consistency model 2. Transactional memory 3. Deterministic GPU 4. Memory management CPU–GPU heterogeneous architecture 1. Impacts of CPU–GPU integration 2. CPU–GPU programmability 3. Exploiting heterogeneity 4. Shared resources management",content:' 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity\n\n----------------------------------------\n\n\n# 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity\n\nFour major improvement\n\n * mitigating the impact of control flow divergence\n * alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main memory\n * increasing the available parallelism and concurrency\n * improving pipeline execution and exploiting scalarization opportunities.\n\n\n\n\n# Control flow divergence\n\n 1. First, GPUs employ PDOM stack-based mechanism that serializes the execution of divergent paths. This serialization of divergent paths reduces the available thread level parallelism (i.e., the number of active warps at a time) which limits the ability of GPUs to hide long memory instruction latency.\n 2. Control divergence limits the number of active threads in the running warps. As a result, SIMD execution units are not efficiently utilized when a diverged warp is executed.\n 3. Control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. Memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.\n 4. Irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per CTAs) to some GPU cores are larger than others.\n\n\n\n\n# 1. Regrouping Divergent warps\n\n\nInstead, DWF dynamically re-forms divergent warps into new non-divergent warps on the fly.\nMoreover, DWF does not reconverge diverged warp at IPDOM in order to amortize coalesced memory address of converged warps.\n\n\n\n\n\n\n# 2. Large Warp/CTA compaction\n\n * Thread Block Compaction (TBC)\n   Allows a group of warps, that belong to the same thread block, to share the same PDOM stack.\n   However, TBC stalls all warps within a CTA on any potentially divergent branch until all warps reach the branch point.\n   \n   \n\nThe major difference between 1) and TBC is that 1) can only merge threads in a warp when they are ready in a queue. Thus it miss some potentials.\n\nTBC replace per-warp convergence stack with in-threadblock stack.\n\n * CAPRI\n   CAPRI dynamically identifies the compaction effectiveness of a branch and only stalls threads that are predicted to benefit from compaction.\n   \n\n * SLP proposed SIMD lane permutation (SLP) as an optimization to expand the applicability of compaction in case of conventional compaction technique is ineffective.\n   \n   \n\n\n# 3. Multi-path execution\n\n\n * DPS\n   Dual-path Stack\n   \n * Multi-path Execution\n   \n\n\n# 4. MIMD-like architecture\n\n\nRogers et al. [194] observed that regular applications perform better with a wider warp size, whereas divergent applications achieve better performance with a smaller warp size.\nVWS groups sets of these smaller warps together by ganging their execution in the warp scheduler and thus amortizing the energy consumed by fetch, decode, and warp scheduling across more threads.\n\n\n# 5. Dynamic kernels/threads\n\n\nRelated Paper: Characterization and Analysis of Dynamic Parallelism in Unstructured GPU Applications. [108]\nDynamic Thread Block Launch: A Lightweight Execution Mechanism to Support Irregular Applications on GPUs. [85]\nBy wang jing NVIDIA\n\n👍 👍 👍 These two paper has very thorough explanation of how kernels are launched, kernel parameters are gained and how thread create subkernel.\n\n\n\n\n\nCUDA enables dynamic parallsim, creating subkernels from each thread.\n\n> Copied from "Characterization"\n> \n> When a child kernel is launched, the parameter buffer pointer of the kernel is retrieved through the device runtime API cudaGetParameterBuffer.\n> \n> Then the argument values are stored in the parameter buffer and the kernel is launched by calling cudaLaunchDevice.\n> \n> After that, the device runtime manager appends the child kernels to an execution queue and dispatches the kernel to SMXs according to a certain scheduling policy.\n> \n> The CDP kernel launching overhead comprises of kernel parameter parsing, calling cudaGetParameterBuffer and cudaLaunchDevice, as well as the process that device runtime manager setups,enqueues, manages and dispatches the child kernels.\n> \n> however, the huge kernel launching overhead could negate the performance benefit of DFP. The overhead is due to the large number of launched kernels, the associated memory footprint and the low number of running warps per core.The CPU launches GPU kernels by dispatching kernel launching commands. Kernel parameters are passed from CPU to GPU at the kernel launching time and stored in the GPU global.\n> \n> Wang et al. [236] proposed new mechanism, called Dynamic Thread Block Launch (DTBL), that employs light-weight thread block rather than heavy-weight device kernel for DFP.\n\n----------------------------------------\n\n\n# Efficient utilization of memory bandwidth\n\n\n# 1. Alleviating cache thrashing, and resource contention\n\n# 1. Two-level warp scheduling\n\n * TLRR\n   \n\nThey proposed two-level round-robin warp scheduling (TL-RR), in which the warps are split into fetch groups.\nTL-RR executes only one fetch group at a time and it schedules warps from the same fetch group in a round-robin fashion.\nWhen the running warps reach a long latency operation, then the next fetch group is prioritized.\nThey try to alleviate the issue of threads in all warps arrive the same memory latency instruction at the same time.\n\n\n\n\n\n\n * OWL OWL augments the TL-RR with CTA-awareness, such that warps are split into groups of CTAs basis rather than warps basis, resulting in increased intra-CTA locality.\n   OWL gives a group of CTAs higher priority when their data exist at the L1 cache such that they get the opportunity to reuse it, therefore improving L1 hit rates and alleviating cache contention.\n   \n\n# 2. Coarse-grained CTA throttling\n\n\n\n\n\nDYNCTA\nNeither More Nor Less: Optimizing Thread-level Parallelism for GPGPUs\n👍 👍 Illustrated CTA and WARP mapping.\n\n * Always executing the maximum possible number of CTAs on a GPU core (i.e., increasing TLP to the maximum) does not always lead to better performance.\n * To alleviate resource contention, they proposed dynamic CTA scheduling mechanism (DYNCTA), which aims to allocate the optimal number of CTAs per GPU core that alleviate memory contention according to an application characteristics.\n * DYNCTA dynamically adjusts over sampling periods the number of active CTAs per GPU core that reduces the memory latency without sacrificing the available TLP.\n\nLCS\nIn contrast to DYNCTA that monitors the workload behavior for the entire kernel execution, LCS leverages GTO scheduler to find the optimal number of thread blocks at the early beginning of kernel execution.\n\n# 3. Fine-grained warp throttling\n\ndue to the massive multithreading and the limited capacity of L1 cache, divergent GPGPU applications cause severe cache contention.\n\n\n * CCWS\n   uses a victim tag array, called lost locality detector, to detect warps that have lost locality due to thrashing. These warps are prioritized till they exploit their locality while other warps are descheduled.\n   \n * DAWS\n   DAWS is a divergence-based cache footprint predictor to calculate the amount of locality in loops required by each warp.\n   DAWS uses these predictions to prioritize a group of warps such that the cache footprint of these warps do not exceed the capacity of the L1 cache.\n   \n\n# 4. Throttling and cache bypassing\n\nprevious CTA or warp throttling techniques leave memory bandwidth and other chip resources (L2 cache, interconnection and execution units) significantly underutilized.\n\n * PCAL\n   At the beginning of kernel execution, PCAL executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources. Thus, PCAL reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.\n * CCA\n   CCA improves DAWS by allowing extra inactive warps and some streaming memory instructions from the active warps to bypass the L1 cache and utilize on-chip resources.\n\n# 5. Critical warp awareness\n\nsome warps may be assigned more workload and exhibit longer latency compared to other warps within the same Thread Block. Hence, fast warps are idle at a synchronization barrier or at the end of kernel execution until the critical (i.e., the slowest) warp finishes execution. Thus, the overall execution time is dominated by the performance of these critical warps.\n\nCAWA dynamically identifies critical warps and coordinates warp scheduling and cache prioritization to accelerate the critical warp execution.\n\n * Workload Imbalance In a GPGPU kernel function, tasks are not always uniformly distributed to each thread/warp, and thereby some threads/warps have heavier workloads than others. Intuitively, the threads/warps with heavier workloads require longer time to process their tasks. Consequently, warps with heavier workloads often become the slowest running/critical warps.\n * Diverging Branch Behavior At runtime, warps can undergo different con trol paths leading to different number of dynamic instructions across different warps. This problem could be worsened if threads in a warp also take diverging control paths, i.e., the branch divergence problem, leading to a larger instruction execution gap between warps.\n * Contention in the Memory Subsystem Jog et al. ob served that the memory subsystem has a significant impact on GPGPU applications [15, 16]. . Jia et al. also pointed out that interference in the Ll data cache as well as in the interconnection between the Ll data caches and the L2 cache are the major factors that limit GPU performance.More than 60% of the cache blocks that could be reused by the slower-running, critical warps are evicted before the re-references by the critical warps.\n * Latency Introduced by the Warp Scheduler Because of the particular warp execution order determined by the scheduler, when a warp becomes ready for execution, it can experience up to N cycles of scheduling delay, where N represents the number of warps.\n\n# 6. Cache management and bypassing\n\nGCache To detect thrashing, they equip L2 cache tag array with extra bits (victim bits) to provide L1 cache with some information about the hot lines that have been evicted before. An adaptive cache replacement policy is used by L1 cache to protect these hot lines.\n\n# 7. Ordering buffers\n\nThe idea of MRPB is two-fold.\nFirst, a FIFO requests buffer is used to reorder memory references so that requests from the same warp are grouped and sent to the cache together in a more cache-friendly order. This results in drastically reducing cache contention and improving use of the limited per-thread cache capacity.\nSecond, MRPB allows memory request that encounters associativity stall to bypass L1 cache.\n\n\n\n\n\n\n# 8. Resource tuning\n\nEqualizer, a dynamic runtime system that tunes number of thread blocks, core and memory frequency to match the requirements of the running kernel, leading to efficient execution and energy saving.\n\n\n# 2. High-bandwidth many-thread-aware memory hierarchy\n\n# 1. Mitigating off-chip bandwidth bottleneck\n\nLAMAR Emerging irregular workloads benefit from fine-grain (FG) memory access by avoiding unnecessary data transfers, that may be happened under CG policy,\n\nthey proposed a locality-aware memory hierarchy (LAMAR) that adaptively tunes the memory access granularity for the running kernel.\n\nLAMAR employs CG accesses for kernels with high temporal and spatial locality, while applying FG accesses for irregular divergent workloads in attempt to reduce memory over-fetching.\n\nCABA Vijaykumar et al. [231] proposed, Core-Assisted Bottleneck Acceleration (CABA) framework, that exploits the underutilized computational resources to perform useful work and alleviate different bottlenecks in GPU execution.\n\nFor instance, to alleviate memory bandwidth bottleneck, CABA dynamically creates assist warps that execute with the original warps side by side on the same GPU core.\n\nAssist warps opportunistically use idle computational units to perform data decompression for the incoming compressed cache blocks and compression for the outgoing cache blocks, leading to less transferring data from memory and mitigating memory bandwidth problem.\n\nApproximation An approximation technique in which the GPU drops some portion of load requests which miss in the cache after approximating their values.\n\n# 2. Memory divergence normalization\n\nOrchestrated Scheduling and Prefetching for GPGPUs\n\nthey proposed prefetch-aware warp scheduling, that coordinates simple data prefetcher and warp scheduling in an intelligent manner such that the scheduling of two consecutive warps are separated in time, and thus prefetching becomes more effective.\n\n\n\n# 3. Interconnection network\n\n# 4. Main memory scheduling\n\ninterconnection network which is between cores and memory controllers can destroy memory access row-buffer locality.\n\nTo reserve row locality and reduce complexity circuit design of FR-FCFS DRAM controller, they employ an interconnection arbitration scheme to prioritize memory requests accessing the same row first.\n\n# 5. Heterogeneous memory management\n\nAgarwal et al. [5] showed that applying traditional Linux page placements policies, which have been used for CPUonly NUMA systems and aim to minimize the memory request latency, may not be effective in CPU–GPU NUMA systems. This is due to the fact that GPU performance is more sensitive to memory bandwidth.\n\nBandwidth-aware placement that maximizes GPU performance by balancing page placement across the memories based on the aggregate memory bandwidth available in a system.\n\n# 6. CPU–GPU memory transfer overhead\n\nfine-grained CPU–GPU synchronization enabled by a hardware-managed full-empty bits to track when regions of data have been transferred.\n\nThus, the GPU is able to start execution once the required block of data is available.\n\nSoftware level APIs are proposed to allow programmer to launch kernel earlier and overlap data transfer with execution.\n\n----------------------------------------\n\n\n# Increasing parallelism and improving execution pipelining\n\nSome applications have a low number of active thread blocks due to the small input size or the unavailability of some required resources in SM (e.g. registers or shared memory), thus they fail to efficiently utilize the execution units. This results in inefficient utilization of execution unit and hinders the GPU ability to hide long memory latency.\n\n\nPrevious works proposed new techniques in order to reduce resource fragmentation and run the maximum number of warps per core.\n\n\nFurther, other approaches proposed running multiple applications on the same GPU to exploit these underutilized resources and increase overall throughput.\n\n\nAnother way to improve execution efficiency and increase parallelism is to exploit scalar opportunities and value similarity between the running warps such that scalar instructions can be executed con currently along with other SIMT instructions.\n\n\n\n# 1. Reducing resource fragmentation and increasing parallelism\n\n👍 👍 👍\n\nUnifyingthey proposed a unified local memory which integrates the register file, L1 cache, and scratchpad memory into one large on-chip storage. Then, the hardware can dynamically partition the on-chip storage according to each application’s needs.\n\n\n\n\n\n\n# 2. GPU multitasking\n\nBetter Utilization and Virtualization\n\n\n * multiple applica tions execute simultaneously on different cores within the same GPU substrate.\n   \n * mixed concurrent kernels execution, in which two applications execute concurrently on the same core. especially, mixture of memory-intensive and compute-intensive workloads\n   \n\nDefault strategy may cause high-priority application suffering from a long latency to execute. a task preemption strategy is required to improve GPU ultitasking\n\n * context switching and draining\n * To further reduce preemption latency, Park et al. [178] intro duced core flushing which drops an execution of a thread block without context saving and re-executes the dropped thread block from the beginning when it is relaunched.\n\n\n# 3. Exploiting scalar and value similarity opportunities\n\nmany GPGPU workloads have scalar instructions in which computation is identical across mul tiple threads within the same warp instruction (i.e., operands are identical for all the threads in a warp).\n\nmodern GPU mi croarchitecture, like AMD’s GCN [10], leverages these scalar op portunities by statically detecting scalar instructions and executing them on a separate scalar unit attached with each GPU core.\n\nA vector is defined as an affine, when the vector contains a consecutive strided values, i.e., the vector values can be represented as V(i) = b + i ∗ s, where b is the base, s is the stride and i is the thread index.\n\n👍 👍 👍 Microarchitectural mechanisms to exploit value structure in SIMT architectures\n\nThis Paper has detailed explanation of microarchitecture in gpu execution core, how ALU and register files operates.\n\n\n\n\n\n\n# 4. Improving execution pipelining\n\nmany GPGPU applications do not have enough active threads that are ready to issue instructions and hide short read-after-write (RAW) dependencies caused by deep execution pipeline stages.\n\n * a low-power forwarded network that can considerably improve the performance of many compute-intensive GPGPU ap plications.\n\n\n\n * improve GPU performance by splitting the existing 32-bit datapath into two 16-bit datapath slices. As a result, the GPU instruction throughput can be increased by issuing dual 16-bit instructions from two different warps in parallel using the sliced 32-bit datapath.\n\n * a pre-execution approach for improving GPU latency hiding and performance by employing run-ahead out of-order execution [158].\n\n\n\n\n\nwhen a warp stalls for a long-latency operation such as off-chip memory accesses, it continues to fetch and pre-execute successive instructions that are not on the long latency dependence chain resulting in hiding processing delay of operations and performance improvement.\n\n----------------------------------------\n\n\n# Enhancing GPGPU programmability\n\n\n\n\n# 1. Coherence and consistency model\n\nCurrent GPUs lack hardware cache coherence and require dis abling of private L1 caches or employing software-based bulk coherence decisions (i.e., flush/invalidate all private L1 caches at synchronization points) if an application needs coherent memory view.\n\n\n# 2. Transactional memory\n\nKILO TM does not rely on cache coherence nor global atomic operations.\n\nInstead, it detects conflicts via a fine-grain value-based approach that supports thousands of concurrent transactions and requires negligible storage overhead.\n\n\n# 3. Deterministic GPU\n\n\n# 4. Memory management\n\nKim et al. [109] proposed GPUdmm, a high-performance dynamic memory management for GPU architecture. GPUdmm enables dynamic memory management for discrete GPU environ ments by using GPU memory as a cache of CPU memory with on demand CPU–GPU data transfers.\n\n\n\nPichai et al. [183] 👍 👍 👍 augmenting CCWS and TBC with TLB-awareness and a few simple adjustments can recover most of this lost performance and move address translation overheads into a range considered acceptable in the CPU world.\n\n----------------------------------------\n\n\n# CPU–GPU heterogeneous architecture\n\n\n# 1. Impacts of CPU–GPU integration\n\nremaining CPU code tends to have lower instruction-level parallelism (ILP), more complex load/store operations to prefetch and more difficult branch rediction.\n\n\nFurther, the serial code will not benefit significantly from SIMD instructions or increasing the number of CPU cores, owing to the limited availability of thread level parallelism (TLP) and data-level parallelism (DLP) that will be already captured and exploited by the GPU instead.\n\n\n# 2. CPU–GPU programmability\n\nHeterogeneous System Coherence for Integrated CPU-GPU Systems\n\nthey replace the fine-grained 64B-block-level directory with a coarse-grained 1KB-region-level directory.\n\n\n# 3. Exploiting heterogeneity\n\nCOMPASS uses idle GPU core resources to act as data prefetchers for CPU execution and success fully improve the memory performance of single-thread applications.\nWoo and Lee [247] proposed to collaboratively utilize CPU resources to act as programmable data prefetchers for GPGPU applications.\n\n\n# 4. Shared resources management\n\nTwo kinds of approaches have been explored to mitigate inter ference:\n\n * application-aware resource management\n * throttling based management.\n\n\n\nSMS decouples memory controller into three stages.\n\n\n * The first stage of SMS groups requests based on row buffer locality.\n * At the second stage, SMS ensures fairness between CPU and GPU memory requests by applying CPU-biased shortest job first scheduling policy or GPU-biased round robin scheduling policy. A dynamically configurable parameter is used to select between the two policies based on the system’s needs.\n * The last stage consists of simple per-bank FIFO queue to issue low-level memory commands.\n\nTAP: A TLP-Aware Cache Management Policy for a CPU-GPU Heterogeneous\n\n\n\n * A core-sampling technique, which applies a different cache management policy to each GPU core and regularly collects statistics on the performance\n   of these cores to see how these polices affect GPU applications.\n   \n * GPU cores typically access caches much more frequently than CPU cores.\n   \n * enforces a similar cache lifetime to both CPU and GPGPU appli cations and prevent GPGPU application to monopolize the shared cache.\n\nOne (CM-CPU) for boosting CPU performance in the presence of GPU interference.\nThe other (CM-BAL) for improving both CPU and GPU performance in a balanced manner and thus overall system performance.\n\n\n\n\npropose GPU concurrency management that dynamically throttles/boosts TLP (i.e., number of active warps) of GPU cores in order to minimize shared resources interference between CPU and GPU.',normalizedContent:' 1. a survey of architectural approaches for improving gpgpu performance, programmability and heterogeneity\n\n----------------------------------------\n\n\n# 1. a survey of architectural approaches for improving gpgpu performance, programmability and heterogeneity\n\nfour major improvement\n\n * mitigating the impact of control flow divergence\n * alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main memory\n * increasing the available parallelism and concurrency\n * improving pipeline execution and exploiting scalarization opportunities.\n\n\n\n\n# control flow divergence\n\n 1. first, gpus employ pdom stack-based mechanism that serializes the execution of divergent paths. this serialization of divergent paths reduces the available thread level parallelism (i.e., the number of active warps at a time) which limits the ability of gpus to hide long memory instruction latency.\n 2. control divergence limits the number of active threads in the running warps. as a result, simd execution units are not efficiently utilized when a diverged warp is executed.\n 3. control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.\n 4. irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per ctas) to some gpu cores are larger than others.\n\n\n\n\n# 1. regrouping divergent warps\n\n\ninstead, dwf dynamically re-forms divergent warps into new non-divergent warps on the fly.\nmoreover, dwf does not reconverge diverged warp at ipdom in order to amortize coalesced memory address of converged warps.\n\n\n\n\n\n\n# 2. large warp/cta compaction\n\n * thread block compaction (tbc)\n   allows a group of warps, that belong to the same thread block, to share the same pdom stack.\n   however, tbc stalls all warps within a cta on any potentially divergent branch until all warps reach the branch point.\n   \n   \n\nthe major difference between 1) and tbc is that 1) can only merge threads in a warp when they are ready in a queue. thus it miss some potentials.\n\ntbc replace per-warp convergence stack with in-threadblock stack.\n\n * capri\n   capri dynamically identifies the compaction effectiveness of a branch and only stalls threads that are predicted to benefit from compaction.\n   \n\n * slp proposed simd lane permutation (slp) as an optimization to expand the applicability of compaction in case of conventional compaction technique is ineffective.\n   \n   \n\n\n# 3. multi-path execution\n\n\n * dps\n   dual-path stack\n   \n * multi-path execution\n   \n\n\n# 4. mimd-like architecture\n\n\nrogers et al. [194] observed that regular applications perform better with a wider warp size, whereas divergent applications achieve better performance with a smaller warp size.\nvws groups sets of these smaller warps together by ganging their execution in the warp scheduler and thus amortizing the energy consumed by fetch, decode, and warp scheduling across more threads.\n\n\n# 5. dynamic kernels/threads\n\n\nrelated paper: characterization and analysis of dynamic parallelism in unstructured gpu applications. [108]\ndynamic thread block launch: a lightweight execution mechanism to support irregular applications on gpus. [85]\nby wang jing nvidia\n\n👍 👍 👍 these two paper has very thorough explanation of how kernels are launched, kernel parameters are gained and how thread create subkernel.\n\n\n\n\n\ncuda enables dynamic parallsim, creating subkernels from each thread.\n\n> copied from "characterization"\n> \n> when a child kernel is launched, the parameter buffer pointer of the kernel is retrieved through the device runtime api cudagetparameterbuffer.\n> \n> then the argument values are stored in the parameter buffer and the kernel is launched by calling cudalaunchdevice.\n> \n> after that, the device runtime manager appends the child kernels to an execution queue and dispatches the kernel to smxs according to a certain scheduling policy.\n> \n> the cdp kernel launching overhead comprises of kernel parameter parsing, calling cudagetparameterbuffer and cudalaunchdevice, as well as the process that device runtime manager setups,enqueues, manages and dispatches the child kernels.\n> \n> however, the huge kernel launching overhead could negate the performance benefit of dfp. the overhead is due to the large number of launched kernels, the associated memory footprint and the low number of running warps per core.the cpu launches gpu kernels by dispatching kernel launching commands. kernel parameters are passed from cpu to gpu at the kernel launching time and stored in the gpu global.\n> \n> wang et al. [236] proposed new mechanism, called dynamic thread block launch (dtbl), that employs light-weight thread block rather than heavy-weight device kernel for dfp.\n\n----------------------------------------\n\n\n# efficient utilization of memory bandwidth\n\n\n# 1. alleviating cache thrashing, and resource contention\n\n# 1. two-level warp scheduling\n\n * tlrr\n   \n\nthey proposed two-level round-robin warp scheduling (tl-rr), in which the warps are split into fetch groups.\ntl-rr executes only one fetch group at a time and it schedules warps from the same fetch group in a round-robin fashion.\nwhen the running warps reach a long latency operation, then the next fetch group is prioritized.\nthey try to alleviate the issue of threads in all warps arrive the same memory latency instruction at the same time.\n\n\n\n\n\n\n * owl owl augments the tl-rr with cta-awareness, such that warps are split into groups of ctas basis rather than warps basis, resulting in increased intra-cta locality.\n   owl gives a group of ctas higher priority when their data exist at the l1 cache such that they get the opportunity to reuse it, therefore improving l1 hit rates and alleviating cache contention.\n   \n\n# 2. coarse-grained cta throttling\n\n\n\n\n\ndyncta\nneither more nor less: optimizing thread-level parallelism for gpgpus\n👍 👍 illustrated cta and warp mapping.\n\n * always executing the maximum possible number of ctas on a gpu core (i.e., increasing tlp to the maximum) does not always lead to better performance.\n * to alleviate resource contention, they proposed dynamic cta scheduling mechanism (dyncta), which aims to allocate the optimal number of ctas per gpu core that alleviate memory contention according to an application characteristics.\n * dyncta dynamically adjusts over sampling periods the number of active ctas per gpu core that reduces the memory latency without sacrificing the available tlp.\n\nlcs\nin contrast to dyncta that monitors the workload behavior for the entire kernel execution, lcs leverages gto scheduler to find the optimal number of thread blocks at the early beginning of kernel execution.\n\n# 3. fine-grained warp throttling\n\ndue to the massive multithreading and the limited capacity of l1 cache, divergent gpgpu applications cause severe cache contention.\n\n\n * ccws\n   uses a victim tag array, called lost locality detector, to detect warps that have lost locality due to thrashing. these warps are prioritized till they exploit their locality while other warps are descheduled.\n   \n * daws\n   daws is a divergence-based cache footprint predictor to calculate the amount of locality in loops required by each warp.\n   daws uses these predictions to prioritize a group of warps such that the cache footprint of these warps do not exceed the capacity of the l1 cache.\n   \n\n# 4. throttling and cache bypassing\n\nprevious cta or warp throttling techniques leave memory bandwidth and other chip resources (l2 cache, interconnection and execution units) significantly underutilized.\n\n * pcal\n   at the beginning of kernel execution, pcal executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources. thus, pcal reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.\n * cca\n   cca improves daws by allowing extra inactive warps and some streaming memory instructions from the active warps to bypass the l1 cache and utilize on-chip resources.\n\n# 5. critical warp awareness\n\nsome warps may be assigned more workload and exhibit longer latency compared to other warps within the same thread block. hence, fast warps are idle at a synchronization barrier or at the end of kernel execution until the critical (i.e., the slowest) warp finishes execution. thus, the overall execution time is dominated by the performance of these critical warps.\n\ncawa dynamically identifies critical warps and coordinates warp scheduling and cache prioritization to accelerate the critical warp execution.\n\n * workload imbalance in a gpgpu kernel function, tasks are not always uniformly distributed to each thread/warp, and thereby some threads/warps have heavier workloads than others. intuitively, the threads/warps with heavier workloads require longer time to process their tasks. consequently, warps with heavier workloads often become the slowest running/critical warps.\n * diverging branch behavior at runtime, warps can undergo different con trol paths leading to different number of dynamic instructions across different warps. this problem could be worsened if threads in a warp also take diverging control paths, i.e., the branch divergence problem, leading to a larger instruction execution gap between warps.\n * contention in the memory subsystem jog et al. ob served that the memory subsystem has a significant impact on gpgpu applications [15, 16]. . jia et al. also pointed out that interference in the ll data cache as well as in the interconnection between the ll data caches and the l2 cache are the major factors that limit gpu performance.more than 60% of the cache blocks that could be reused by the slower-running, critical warps are evicted before the re-references by the critical warps.\n * latency introduced by the warp scheduler because of the particular warp execution order determined by the scheduler, when a warp becomes ready for execution, it can experience up to n cycles of scheduling delay, where n represents the number of warps.\n\n# 6. cache management and bypassing\n\ngcache to detect thrashing, they equip l2 cache tag array with extra bits (victim bits) to provide l1 cache with some information about the hot lines that have been evicted before. an adaptive cache replacement policy is used by l1 cache to protect these hot lines.\n\n# 7. ordering buffers\n\nthe idea of mrpb is two-fold.\nfirst, a fifo requests buffer is used to reorder memory references so that requests from the same warp are grouped and sent to the cache together in a more cache-friendly order. this results in drastically reducing cache contention and improving use of the limited per-thread cache capacity.\nsecond, mrpb allows memory request that encounters associativity stall to bypass l1 cache.\n\n\n\n\n\n\n# 8. resource tuning\n\nequalizer, a dynamic runtime system that tunes number of thread blocks, core and memory frequency to match the requirements of the running kernel, leading to efficient execution and energy saving.\n\n\n# 2. high-bandwidth many-thread-aware memory hierarchy\n\n# 1. mitigating off-chip bandwidth bottleneck\n\nlamar emerging irregular workloads benefit from fine-grain (fg) memory access by avoiding unnecessary data transfers, that may be happened under cg policy,\n\nthey proposed a locality-aware memory hierarchy (lamar) that adaptively tunes the memory access granularity for the running kernel.\n\nlamar employs cg accesses for kernels with high temporal and spatial locality, while applying fg accesses for irregular divergent workloads in attempt to reduce memory over-fetching.\n\ncaba vijaykumar et al. [231] proposed, core-assisted bottleneck acceleration (caba) framework, that exploits the underutilized computational resources to perform useful work and alleviate different bottlenecks in gpu execution.\n\nfor instance, to alleviate memory bandwidth bottleneck, caba dynamically creates assist warps that execute with the original warps side by side on the same gpu core.\n\nassist warps opportunistically use idle computational units to perform data decompression for the incoming compressed cache blocks and compression for the outgoing cache blocks, leading to less transferring data from memory and mitigating memory bandwidth problem.\n\napproximation an approximation technique in which the gpu drops some portion of load requests which miss in the cache after approximating their values.\n\n# 2. memory divergence normalization\n\norchestrated scheduling and prefetching for gpgpus\n\nthey proposed prefetch-aware warp scheduling, that coordinates simple data prefetcher and warp scheduling in an intelligent manner such that the scheduling of two consecutive warps are separated in time, and thus prefetching becomes more effective.\n\n\n\n# 3. interconnection network\n\n# 4. main memory scheduling\n\ninterconnection network which is between cores and memory controllers can destroy memory access row-buffer locality.\n\nto reserve row locality and reduce complexity circuit design of fr-fcfs dram controller, they employ an interconnection arbitration scheme to prioritize memory requests accessing the same row first.\n\n# 5. heterogeneous memory management\n\nagarwal et al. [5] showed that applying traditional linux page placements policies, which have been used for cpuonly numa systems and aim to minimize the memory request latency, may not be effective in cpu–gpu numa systems. this is due to the fact that gpu performance is more sensitive to memory bandwidth.\n\nbandwidth-aware placement that maximizes gpu performance by balancing page placement across the memories based on the aggregate memory bandwidth available in a system.\n\n# 6. cpu–gpu memory transfer overhead\n\nfine-grained cpu–gpu synchronization enabled by a hardware-managed full-empty bits to track when regions of data have been transferred.\n\nthus, the gpu is able to start execution once the required block of data is available.\n\nsoftware level apis are proposed to allow programmer to launch kernel earlier and overlap data transfer with execution.\n\n----------------------------------------\n\n\n# increasing parallelism and improving execution pipelining\n\nsome applications have a low number of active thread blocks due to the small input size or the unavailability of some required resources in sm (e.g. registers or shared memory), thus they fail to efficiently utilize the execution units. this results in inefficient utilization of execution unit and hinders the gpu ability to hide long memory latency.\n\n\nprevious works proposed new techniques in order to reduce resource fragmentation and run the maximum number of warps per core.\n\n\nfurther, other approaches proposed running multiple applications on the same gpu to exploit these underutilized resources and increase overall throughput.\n\n\nanother way to improve execution efficiency and increase parallelism is to exploit scalar opportunities and value similarity between the running warps such that scalar instructions can be executed con currently along with other simt instructions.\n\n\n\n# 1. reducing resource fragmentation and increasing parallelism\n\n👍 👍 👍\n\nunifyingthey proposed a unified local memory which integrates the register file, l1 cache, and scratchpad memory into one large on-chip storage. then, the hardware can dynamically partition the on-chip storage according to each application’s needs.\n\n\n\n\n\n\n# 2. gpu multitasking\n\nbetter utilization and virtualization\n\n\n * multiple applica tions execute simultaneously on different cores within the same gpu substrate.\n   \n * mixed concurrent kernels execution, in which two applications execute concurrently on the same core. especially, mixture of memory-intensive and compute-intensive workloads\n   \n\ndefault strategy may cause high-priority application suffering from a long latency to execute. a task preemption strategy is required to improve gpu ultitasking\n\n * context switching and draining\n * to further reduce preemption latency, park et al. [178] intro duced core flushing which drops an execution of a thread block without context saving and re-executes the dropped thread block from the beginning when it is relaunched.\n\n\n# 3. exploiting scalar and value similarity opportunities\n\nmany gpgpu workloads have scalar instructions in which computation is identical across mul tiple threads within the same warp instruction (i.e., operands are identical for all the threads in a warp).\n\nmodern gpu mi croarchitecture, like amd’s gcn [10], leverages these scalar op portunities by statically detecting scalar instructions and executing them on a separate scalar unit attached with each gpu core.\n\na vector is defined as an affine, when the vector contains a consecutive strided values, i.e., the vector values can be represented as v(i) = b + i ∗ s, where b is the base, s is the stride and i is the thread index.\n\n👍 👍 👍 microarchitectural mechanisms to exploit value structure in simt architectures\n\nthis paper has detailed explanation of microarchitecture in gpu execution core, how alu and register files operates.\n\n\n\n\n\n\n# 4. improving execution pipelining\n\nmany gpgpu applications do not have enough active threads that are ready to issue instructions and hide short read-after-write (raw) dependencies caused by deep execution pipeline stages.\n\n * a low-power forwarded network that can considerably improve the performance of many compute-intensive gpgpu ap plications.\n\n\n\n * improve gpu performance by splitting the existing 32-bit datapath into two 16-bit datapath slices. as a result, the gpu instruction throughput can be increased by issuing dual 16-bit instructions from two different warps in parallel using the sliced 32-bit datapath.\n\n * a pre-execution approach for improving gpu latency hiding and performance by employing run-ahead out of-order execution [158].\n\n\n\n\n\nwhen a warp stalls for a long-latency operation such as off-chip memory accesses, it continues to fetch and pre-execute successive instructions that are not on the long latency dependence chain resulting in hiding processing delay of operations and performance improvement.\n\n----------------------------------------\n\n\n# enhancing gpgpu programmability\n\n\n\n\n# 1. coherence and consistency model\n\ncurrent gpus lack hardware cache coherence and require dis abling of private l1 caches or employing software-based bulk coherence decisions (i.e., flush/invalidate all private l1 caches at synchronization points) if an application needs coherent memory view.\n\n\n# 2. transactional memory\n\nkilo tm does not rely on cache coherence nor global atomic operations.\n\ninstead, it detects conflicts via a fine-grain value-based approach that supports thousands of concurrent transactions and requires negligible storage overhead.\n\n\n# 3. deterministic gpu\n\n\n# 4. memory management\n\nkim et al. [109] proposed gpudmm, a high-performance dynamic memory management for gpu architecture. gpudmm enables dynamic memory management for discrete gpu environ ments by using gpu memory as a cache of cpu memory with on demand cpu–gpu data transfers.\n\n\n\npichai et al. [183] 👍 👍 👍 augmenting ccws and tbc with tlb-awareness and a few simple adjustments can recover most of this lost performance and move address translation overheads into a range considered acceptable in the cpu world.\n\n----------------------------------------\n\n\n# cpu–gpu heterogeneous architecture\n\n\n# 1. impacts of cpu–gpu integration\n\nremaining cpu code tends to have lower instruction-level parallelism (ilp), more complex load/store operations to prefetch and more difficult branch rediction.\n\n\nfurther, the serial code will not benefit significantly from simd instructions or increasing the number of cpu cores, owing to the limited availability of thread level parallelism (tlp) and data-level parallelism (dlp) that will be already captured and exploited by the gpu instead.\n\n\n# 2. cpu–gpu programmability\n\nheterogeneous system coherence for integrated cpu-gpu systems\n\nthey replace the fine-grained 64b-block-level directory with a coarse-grained 1kb-region-level directory.\n\n\n# 3. exploiting heterogeneity\n\ncompass uses idle gpu core resources to act as data prefetchers for cpu execution and success fully improve the memory performance of single-thread applications.\nwoo and lee [247] proposed to collaboratively utilize cpu resources to act as programmable data prefetchers for gpgpu applications.\n\n\n# 4. shared resources management\n\ntwo kinds of approaches have been explored to mitigate inter ference:\n\n * application-aware resource management\n * throttling based management.\n\n\n\nsms decouples memory controller into three stages.\n\n\n * the first stage of sms groups requests based on row buffer locality.\n * at the second stage, sms ensures fairness between cpu and gpu memory requests by applying cpu-biased shortest job first scheduling policy or gpu-biased round robin scheduling policy. a dynamically configurable parameter is used to select between the two policies based on the system’s needs.\n * the last stage consists of simple per-bank fifo queue to issue low-level memory commands.\n\ntap: a tlp-aware cache management policy for a cpu-gpu heterogeneous\n\n\n\n * a core-sampling technique, which applies a different cache management policy to each gpu core and regularly collects statistics on the performance\n   of these cores to see how these polices affect gpu applications.\n   \n * gpu cores typically access caches much more frequently than cpu cores.\n   \n * enforces a similar cache lifetime to both cpu and gpgpu appli cations and prevent gpgpu application to monopolize the shared cache.\n\none (cm-cpu) for boosting cpu performance in the presence of gpu interference.\nthe other (cm-bal) for improving both cpu and gpu performance in a balanced manner and thus overall system performance.\n\n\n\n\npropose gpu concurrency management that dynamically throttles/boosts tlp (i.e., number of active warps) of gpu cores in order to minimize shared resources interference between cpu and gpu.',charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"TO READ",frontmatter:{title:"TO READ",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/47871e/"},regularPath:"/03.gpu/1234.TODO.html",relativePath:"03.gpu/1234.TODO.md",key:"v-02aef12e",path:"/pages/47871e/",headersStr:null,content:" 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity",normalizedContent:" 1. a survey of architectural approaches for improving gpgpu performance, programmability and heterogeneity",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"load store unit",frontmatter:{title:"load store unit",date:"2024-07-15T15:32:49.000Z",permalink:"/pages/cc7037/"},regularPath:"/04.cpu/03.loadstore.html",relativePath:"04.cpu/03.loadstore.md",key:"v-891a4db6",path:"/pages/cc7037/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"checkpoint",frontmatter:{title:"checkpoint",date:"2023-11-09T15:32:49.000Z",permalink:"/pages/cc7035/"},regularPath:"/04.cpu/01.checkpoint.html",relativePath:"04.cpu/01.checkpoint.md",key:"v-121d90a9",path:"/pages/cc7035/",headersStr:null,content:"An Analysis of a Resource Efficient Checkpoint Architecture [Intel]\n\nThe main part that I like is the discussion\n\n(1) Using map table checkpoints. Map table checkpoints are created periodically either at every branch or every few cycles [Leibholz and Razdan 1997; Yeager 1996]. On a misprediction, the checkpoint corresponding to the mispredicted branch is restored. The number of checkpoints limits the number of unresolved branches allowed in the instruction window.\n\n(2) Using the retirement map table (RMAP). In this scheme, a retirement map table [Hinton et al. 2001] is used in addition to the frontend map table. Each ROB entry also has the rename map for its corresponding instruction. Once a misprediction is resolved, the mispredicted branch is allowed to reach the head of the ROB at which time the retirement map table will have the correct map table corresponding to the mispredicted branch. At this point, the retirement map table is copied to the frontend map table, after which renaming can start. Since all instructions prior to the mispredicted branch must be retired before renaming can start, this scheme can lead to significant delays if long latency operations prior to the mispredicted branch stall retirement.\n\n(3) Using the retirement map table and the ROB (RMAP+WALK). This scheme is an optimization on the scheme above. Instead of waiting for the mispredicted branch to reach the head of the ROB, we start with the current retirement map table and pro-actively walk from the head of the ROB toward the mispredicted branch, incorporating the rename information of each ROB entry. This allows renaming of correct path instructions to commence without waiting for all instructions prior to the mispredicted branch to retire. (4) Using the frontend map table and a history buffer (HBMAP+WALK). In this scheme, a history buffer is used to store overwritten maps of each instruction. On a branch misprediction, we start with the current frontend map table. We pro-actively walk from the current tail of the ROB (i.e., the most recently allocated instruction) toward the mispredicted branch, incorporating the overwritten maps of each instruction. Depending on whether the mispredicted branch is closer to the ROB head or ROB tail, RMAP + WALK, or HBMAP + WALK will perform better.\n\nIn short:\n\n1. checkpoint generated at the moment of decoding branch instruction. Recover at detection of missprediction.\n\n2. use retire map table(RMAP). wait the commit of missprediction instruction, then rewrite the RAT(remap alias table) with RMAP\n\n3. RMAP + WALK. Restart from the moment that miss prediction is detected, copy the RMAP into RAT and then modify RMAP with ROB remapping, until we get to the miss predicted intruction.\n\n4. HBMAP + WALK. Start from frontend RAT(FRAT), use history buffer to recover the overwritten register relation.\n\nRMAP+WALK utilize commited RMAP, thus it walks from the head of ROB (oldest) to the branch instruction. HBMAP+WALK utilize frontend RAT, thus it walks from the end of ROB (youngest) to the branch instruction.",normalizedContent:"an analysis of a resource efficient checkpoint architecture [intel]\n\nthe main part that i like is the discussion\n\n(1) using map table checkpoints. map table checkpoints are created periodically either at every branch or every few cycles [leibholz and razdan 1997; yeager 1996]. on a misprediction, the checkpoint corresponding to the mispredicted branch is restored. the number of checkpoints limits the number of unresolved branches allowed in the instruction window.\n\n(2) using the retirement map table (rmap). in this scheme, a retirement map table [hinton et al. 2001] is used in addition to the frontend map table. each rob entry also has the rename map for its corresponding instruction. once a misprediction is resolved, the mispredicted branch is allowed to reach the head of the rob at which time the retirement map table will have the correct map table corresponding to the mispredicted branch. at this point, the retirement map table is copied to the frontend map table, after which renaming can start. since all instructions prior to the mispredicted branch must be retired before renaming can start, this scheme can lead to significant delays if long latency operations prior to the mispredicted branch stall retirement.\n\n(3) using the retirement map table and the rob (rmap+walk). this scheme is an optimization on the scheme above. instead of waiting for the mispredicted branch to reach the head of the rob, we start with the current retirement map table and pro-actively walk from the head of the rob toward the mispredicted branch, incorporating the rename information of each rob entry. this allows renaming of correct path instructions to commence without waiting for all instructions prior to the mispredicted branch to retire. (4) using the frontend map table and a history buffer (hbmap+walk). in this scheme, a history buffer is used to store overwritten maps of each instruction. on a branch misprediction, we start with the current frontend map table. we pro-actively walk from the current tail of the rob (i.e., the most recently allocated instruction) toward the mispredicted branch, incorporating the overwritten maps of each instruction. depending on whether the mispredicted branch is closer to the rob head or rob tail, rmap + walk, or hbmap + walk will perform better.\n\nin short:\n\n1. checkpoint generated at the moment of decoding branch instruction. recover at detection of missprediction.\n\n2. use retire map table(rmap). wait the commit of missprediction instruction, then rewrite the rat(remap alias table) with rmap\n\n3. rmap + walk. restart from the moment that miss prediction is detected, copy the rmap into rat and then modify rmap with rob remapping, until we get to the miss predicted intruction.\n\n4. hbmap + walk. start from frontend rat(frat), use history buffer to recover the overwritten register relation.\n\nrmap+walk utilize commited rmap, thus it walks from the head of rob (oldest) to the branch instruction. hbmap+walk utilize frontend rat, thus it walks from the end of rob (youngest) to the branch instruction.",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"LLM Hardware Optimization",frontmatter:{title:"LLM Hardware Optimization",date:"2024-01-02T23:32:49.000Z",permalink:"/pages/dc7036/"},regularPath:"/05.llm/02.LLM_HW_Opt.html",relativePath:"05.llm/02.LLM_HW_Opt.md",key:"v-b267e276",path:"/pages/dc7036/",headers:[{level:3,title:"1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]",slug:"_1-hat-hardware-aware-transformers-for-efficient-natural-language-processing-mit-247",normalizedTitle:"1. hat: hardware-aware transformers for efficient natural language processing [mit 247]",charIndex:1},{level:3,title:"4. Making Transformer inference faster on GPUs[Blog]",slug:"_4-making-transformer-inference-faster-on-gpus-blog",normalizedTitle:"4. making transformer inference faster on gpus[blog]",charIndex:798}],headersStr:"1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247] 4. Making Transformer inference faster on GPUs[Blog]",content:" 1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]\n 2. TurboTransformers: An Efficient GPU Serving System For Transformer Models [82]\n 3. Improving the Efficiency of Transformers for Resource-Constrained Devices [8]\n 4. Bag of Tricks for Optimizing Transformer Efficiency [5]\n 5. Making Transformer inference faster on GPUs[Blog]\n 6. Energy-efficient Inference Service of Transformer-based Deep Learning Models on GPUs [4]\n 7. Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs [TACO 2023 Ref 2]\n 8. hugging face https://huggingface.co/docs/transformers/performance\n 9. \n\n----------------------------------------\n\n\n# 1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]\n\n👍 👍 👍 👍\n\n\n# 4. Making Transformer inference faster on GPUs[Blog]\n\nhttps://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190",normalizedContent:" 1. hat: hardware-aware transformers for efficient natural language processing [mit 247]\n 2. turbotransformers: an efficient gpu serving system for transformer models [82]\n 3. improving the efficiency of transformers for resource-constrained devices [8]\n 4. bag of tricks for optimizing transformer efficiency [5]\n 5. making transformer inference faster on gpus[blog]\n 6. energy-efficient inference service of transformer-based deep learning models on gpus [4]\n 7. improving computation and memory efficiency for real-world transformer inference on gpus [taco 2023 ref 2]\n 8. hugging face https://huggingface.co/docs/transformers/performance\n 9. \n\n----------------------------------------\n\n\n# 1. hat: hardware-aware transformers for efficient natural language processing [mit 247]\n\n👍 👍 👍 👍\n\n\n# 4. making transformer inference faster on gpus[blog]\n\nhttps://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"topdown analysis",frontmatter:{title:"topdown analysis",date:"2024-07-15T15:32:49.000Z",permalink:"/pages/cc7036/"},regularPath:"/04.cpu/02.topdown.html",relativePath:"04.cpu/02.topdown.md",key:"v-3e3c1a05",path:"/pages/cc7036/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"two-test-1",frontmatter:{title:"two-test-1",date:"2022-07-18T17:23:23.000Z",permalink:"/pages/f07697/"},regularPath:"/04.cpu/1234.markdown.html",relativePath:"04.cpu/1234.markdown.md",key:"v-c1b2e1c6",path:"/pages/f07697/",headers:[{level:2,title:"为什么要使用 Markdown?",slug:"为什么要使用-markdown",normalizedTitle:"为什么要使用 markdown?",charIndex:429},{level:2,title:"Markdown 相关软件推荐",slug:"markdown-相关软件推荐",normalizedTitle:"markdown 相关软件推荐",charIndex:1004},{level:2,title:"1. 标题&目录",slug:"_1-标题-目录",normalizedTitle:"1. 标题&amp;目录",charIndex:null},{level:3,title:"1.1 标题",slug:"_1-1-标题",normalizedTitle:"1.1 标题",charIndex:1269},{level:3,title:"1.2 目录",slug:"_1-2-目录",normalizedTitle:"1.2 目录",charIndex:1470},{level:2,title:"2. 斜体&粗体",slug:"_2-斜体-粗体",normalizedTitle:"2. 斜体&amp;粗体",charIndex:null},{level:3,title:"2.1 斜体",slug:"_2-1-斜体",normalizedTitle:"2.1 斜体",charIndex:1662},{level:3,title:"2.2 粗体",slug:"_2-2-粗体",normalizedTitle:"2.2 粗体",charIndex:1844},{level:3,title:"2.3 粗斜体 (斜粗体)",slug:"_2-3-粗斜体-斜粗体",normalizedTitle:"2.3 粗斜体 (斜粗体)",charIndex:2046},{level:3,title:"2.4 斜体包含粗体",slug:"_2-4-斜体包含粗体",normalizedTitle:"2.4 斜体包含粗体",charIndex:2434},{level:3,title:"2.5 粗体包含斜体",slug:"_2-5-粗体包含斜体",normalizedTitle:"2.5 粗体包含斜体",charIndex:2973},{level:2,title:"3. 线",slug:"_3-线",normalizedTitle:"3. 线",charIndex:3505},{level:3,title:"3.1 水平分割线",slug:"_3-1-水平分割线",normalizedTitle:"3.1 水平分割线",charIndex:3515},{level:3,title:"3.2 文本删除线",slug:"_3-2-文本删除线",normalizedTitle:"3.2 文本删除线",charIndex:3676},{level:3,title:"3.3 文本下划线",slug:"_3-3-文本下划线",normalizedTitle:"3.3 文本下划线",charIndex:3779},{level:2,title:"4. 列表&引用",slug:"_4-列表-引用",normalizedTitle:"4. 列表&amp;引用",charIndex:null},{level:3,title:"4.1 有序列表",slug:"_4-1-有序列表",normalizedTitle:"4.1 有序列表",charIndex:3903},{level:3,title:"4.2 无序列表",slug:"_4-2-无序列表",normalizedTitle:"4.2 无序列表",charIndex:4966},{level:3,title:"4.3 引用",slug:"_4-3-引用",normalizedTitle:"4.3 引用",charIndex:5496},{level:3,title:"4.4 缩进&退格",slug:"_4-4-缩进-退格",normalizedTitle:"4.4 缩进&amp;退格",charIndex:null},{level:2,title:"5. 网页链接与图像",slug:"_5-网页链接与图像",normalizedTitle:"5. 网页链接与图像",charIndex:8422},{level:3,title:"5.1 网页链接",slug:"_5-1-网页链接",normalizedTitle:"5.1 网页链接",charIndex:8438},{level:3,title:"5.2 图像",slug:"_5-2-图像",normalizedTitle:"5.2 图像",charIndex:9013},{level:2,title:"6. 表格",slug:"_6-表格",normalizedTitle:"6. 表格",charIndex:9834},{level:3,title:"6.1 表格中文本内容的换行",slug:"_6-1-表格中文本内容的换行",normalizedTitle:"6.1 表格中文本内容的换行",charIndex:10206},{level:2,title:"7. 代码域",slug:"_7-代码域",normalizedTitle:"7. 代码域",charIndex:10549},{level:3,title:"7.1 行内代码",slug:"_7-1-行内代码",normalizedTitle:"7.1 行内代码",charIndex:10561},{level:3,title:"7.2 代码块",slug:"_7-2-代码块",normalizedTitle:"7.2 代码块",charIndex:10969},{level:3,title:"7.3 如何在行内代码里显示反引号",slug:"_7-3-如何在行内代码里显示反引号",normalizedTitle:"7.3 如何在行内代码里显示反引号",charIndex:14992},{level:2,title:"8. 任务列表（待办）",slug:"_8-任务列表-待办",normalizedTitle:"8. 任务列表（待办）",charIndex:15142},{level:3,title:"示范",slug:"示范-22",normalizedTitle:"示范",charIndex:1805},{level:3,title:"示范",slug:"示范-23",normalizedTitle:"示范",charIndex:1805},{level:2,title:"9. 注释",slug:"_9-注释",normalizedTitle:"9. 注释",charIndex:15992},{level:3,title:"示范",slug:"示范-只有切换至-编辑模式-才能看到喔",normalizedTitle:"示范",charIndex:1805},{level:2,title:"10. 变量",slug:"_10-变量",normalizedTitle:"10. 变量",charIndex:16421},{level:3,title:"10.1 网页链接变量",slug:"_10-1-网页链接变量",normalizedTitle:"10.1 网页链接变量",charIndex:16433},{level:3,title:"10.2 脚注",slug:"_10-2-脚注",normalizedTitle:"10.2 脚注",charIndex:16756},{level:2,title:"11. 拓展文本格式标记",slug:"_11-拓展文本格式标记",normalizedTitle:"11. 拓展文本格式标记",charIndex:17028},{level:3,title:"11.1 键盘文本",slug:"_11-1-键盘文本",normalizedTitle:"11.1 键盘文本",charIndex:17130},{level:3,title:"11.2 放大文本",slug:"_11-2-放大文本",normalizedTitle:"11.2 放大文本",charIndex:17465},{level:3,title:"11.3 缩小文本",slug:"_11-3-缩小文本",normalizedTitle:"11.3 缩小文本",charIndex:17707},{level:3,title:"11.4 多彩文本",slug:"_11-4-多彩文本",normalizedTitle:"11.4 多彩文本",charIndex:17949},{level:2,title:"12. 拓展文本显示效果",slug:"_12-拓展文本显示效果",normalizedTitle:"12. 拓展文本显示效果",charIndex:18979},{level:3,title:"12.1 文本高亮",slug:"_12-1-文本高亮",normalizedTitle:"12.1 文本高亮",charIndex:19120},{level:3,title:"12.2 上标",slug:"_12-2-上标",normalizedTitle:"12.2 上标",charIndex:19194},{level:3,title:"12.3 下标",slug:"_12-3-下标",normalizedTitle:"12.3 下标",charIndex:19368},{level:3,title:"12.4 Emoji 符号",slug:"_12-4-emoji-符号",normalizedTitle:"12.4 emoji 符号",charIndex:19547},{level:2,title:"13. 转义字符",slug:"_13-转义字符",normalizedTitle:"13. 转义字符",charIndex:19898},{level:3,title:"例1 以普通字符显示星号",slug:"例1-以普通字符显示星号",normalizedTitle:"例1 以普通字符显示星号",charIndex:20457},{level:3,title:"例2 表格内 单元格中的竖杠",slug:"例2-表格内-单元格中的竖杠",normalizedTitle:"例2 表格内 单元格中的竖杠",charIndex:20722},{level:3,title:"例3 不会变成代码的反引号",slug:"例3-不会变成代码的反引号",normalizedTitle:"例3 不会变成代码的反引号",charIndex:21280},{level:3,title:"例4 链接中的中括号",slug:"例4-链接中的中括号",normalizedTitle:"例4 链接中的中括号",charIndex:21554},{level:3,title:"例5 不是列表的连接符(横杠)",slug:"例5-不是列表的连接符-横杠",normalizedTitle:"例5 不是列表的连接符(横杠)",charIndex:21719},{level:3,title:"例6 不是标题的 \\#",slug:"例6-不是标题的",normalizedTitle:"例6 不是标题的 #",charIndex:22158},{level:3,title:"例7 不会注释的 \\%",slug:"例7-不会注释的",normalizedTitle:"例7 不会注释的 %",charIndex:22235},{level:3,title:"例8 木有链接的双链",slug:"例8-木有链接的双链",normalizedTitle:"例8 木有链接的双链",charIndex:22364},{level:3,title:"例9 页链接里 显示文本内的 中括号",slug:"例9-页链接里-显示文本内的-中括号",normalizedTitle:"例9 页链接里 显示文本内的 中括号",charIndex:22501},{level:3,title:"特殊情况 文本修饰的中括号",slug:"特殊情况-文本修饰的中括号",normalizedTitle:"特殊情况 文本修饰的中括号",charIndex:22675},{level:2,title:"14. 空格&换行&强制删除",slug:"_14-空格-换行-强制删除",normalizedTitle:"14. 空格&amp;换行&amp;强制删除",charIndex:null},{level:3,title:"14.1 空格",slug:"_14-1-空格",normalizedTitle:"14.1 空格",charIndex:22819},{level:3,title:"14.2 换行",slug:"_14-2-换行",normalizedTitle:"14.2 换行",charIndex:23070},{level:3,title:"14.3 强制删除",slug:"_14-3-强制删除",normalizedTitle:"14.3 强制删除",charIndex:23728},{level:2,title:"15. 嵌入",slug:"_15-嵌入",normalizedTitle:"15. 嵌入",charIndex:23900},{level:3,title:"15.1 嵌入音频",slug:"_15-1-嵌入音频",normalizedTitle:"15.1 嵌入音频",charIndex:24080},{level:3,title:"15.2 嵌入视频",slug:"_15-2-嵌入视频",normalizedTitle:"15.2 嵌入视频",charIndex:24341},{level:3,title:"15.3 嵌入页面",slug:"_15-3-嵌入页面",normalizedTitle:"15.3 嵌入页面",charIndex:24721},{level:2,title:"16. Latex 数学公式",slug:"_16-latex-数学公式",normalizedTitle:"16. latex 数学公式",charIndex:25471},{level:3,title:"16.1 行内公式",slug:"_16-1-行内公式",normalizedTitle:"16.1 行内公式",charIndex:25516},{level:3,title:"16.2 公式块",slug:"_16-2-公式块",normalizedTitle:"16.2 公式块",charIndex:25823},{level:2,title:"17. Mermaid",slug:"_17-mermaid",normalizedTitle:"17. mermaid",charIndex:27121},{level:3,title:"17.1 流程图",slug:"_17-1-流程图",normalizedTitle:"17.1 流程图",charIndex:27284},{level:3,title:"17.2 饼图",slug:"_17-2-饼图",normalizedTitle:"17.2 饼图",charIndex:28498},{level:3,title:"17.3 序列图 (时序图)",slug:"_17-3-序列图-时序图",normalizedTitle:"17.3 序列图 (时序图)",charIndex:28739},{level:3,title:"17.4 甘特图",slug:"_17-4-甘特图",normalizedTitle:"17.4 甘特图",charIndex:32416},{level:3,title:"17.5 类图",slug:"_17-5-类图",normalizedTitle:"17.5 类图",charIndex:32999},{level:2,title:"18. 标签 (Tag)",slug:"_18-标签-tag",normalizedTitle:"18. 标签 (tag)",charIndex:33887},{level:3,title:"关于空格",slug:"关于空格",normalizedTitle:"关于空格",charIndex:33986},{level:3,title:"关于数字",slug:"关于数字",normalizedTitle:"关于数字",charIndex:34197},{level:3,title:"标签的嵌套",slug:"标签的嵌套",normalizedTitle:"标签的嵌套",charIndex:34299},{level:3,title:"能被使用的符号",slug:"能被使用的符号",normalizedTitle:"能被使用的符号",charIndex:34521},{level:3,title:"如何让 \\# 不被识别",slug:"如何让-不被识别",normalizedTitle:"如何让 # 不被识别",charIndex:34585},{level:2,title:"19. 避免标识符的滥用",slug:"_19-避免标识符的滥用",normalizedTitle:"19. 避免标识符的滥用",charIndex:34678}],headersStr:"为什么要使用 Markdown? Markdown 相关软件推荐 1. 标题&目录 1.1 标题 1.2 目录 2. 斜体&粗体 2.1 斜体 2.2 粗体 2.3 粗斜体 (斜粗体) 2.4 斜体包含粗体 2.5 粗体包含斜体 3. 线 3.1 水平分割线 3.2 文本删除线 3.3 文本下划线 4. 列表&引用 4.1 有序列表 4.2 无序列表 4.3 引用 4.4 缩进&退格 5. 网页链接与图像 5.1 网页链接 5.2 图像 6. 表格 6.1 表格中文本内容的换行 7. 代码域 7.1 行内代码 7.2 代码块 7.3 如何在行内代码里显示反引号 8. 任务列表（待办） 示范 示范 9. 注释 示范 10. 变量 10.1 网页链接变量 10.2 脚注 11. 拓展文本格式标记 11.1 键盘文本 11.2 放大文本 11.3 缩小文本 11.4 多彩文本 12. 拓展文本显示效果 12.1 文本高亮 12.2 上标 12.3 下标 12.4 Emoji 符号 13. 转义字符 例1 以普通字符显示星号 例2 表格内 单元格中的竖杠 例3 不会变成代码的反引号 例4 链接中的中括号 例5 不是列表的连接符(横杠) 例6 不是标题的 \\# 例7 不会注释的 \\% 例8 木有链接的双链 例9 页链接里 显示文本内的 中括号 特殊情况 文本修饰的中括号 14. 空格&换行&强制删除 14.1 空格 14.2 换行 14.3 强制删除 15. 嵌入 15.1 嵌入音频 15.2 嵌入视频 15.3 嵌入页面 16. Latex 数学公式 16.1 行内公式 16.2 公式块 17. Mermaid 17.1 流程图 17.2 饼图 17.3 序列图 (时序图) 17.4 甘特图 17.5 类图 18. 标签 (Tag) 关于空格 关于数字 标签的嵌套 能被使用的符号 如何让 \\# 不被识别 19. 避免标识符的滥用",content:'这里是 two-test-1 的内容。\n\n\n\n以下Markdown内容转载自：Markdown超级教程 Obsidian版\n\n这里仅作为展示Vuepress解析Markdown效果的一个展示。\n\n\n# 什么是 Markdown?\n\n 1. Markdown 是一款轻量级标记语言，不同于HTML (Hypertext Markup Language)，Markdown 的语法非常简单，且容易上手\n 2. Markdown 以 纯文本格式 编写文档，依赖键盘而非鼠标，专注于写作本身，感受书写的魅力\n 3. Markdown 的通过添加一些简单的 标识符，让文本具有恰到好处的格式\n 4. Markdown 核心特征就是 删繁剪芜， 简扼 + 精炼\n 5. Markdown 是 笔记 与 网页文章 的最佳载体\n 6. Down 的核心：坐 下 来，就能把思维写 下 来\n    * 牛津高阶英汉双解词典第九版 中，关于 down 的释义：\n\n\n\n\n\n\n# 为什么要使用 Markdown?\n\n有朋友问我 ，Markdown 的效果 用Word 完全可以复现，甚至功能更多，那为何要用 Markdown 呢？\n\n答：\n\n * 功能多，不一定是好事\n   * 功能一多，选择就会变多，然后你会开始纠结……\n     * 这个字号是不是该大一点呢？\n     * 这个颜色好像有点不太搭呢？\n     * 这个粗体，是不是该再加点颜色呢？\n     * 这个图片的位置看起来有点不大对劲呢？\n   * 结果，写了半天，就憋出一点点东西\n     * 写出来的内容...好像...也不咋滴\n\nMD的优势：\n\n 1. Markdown 让我们免于 被繁杂臃肿的功能晃花了眼 的困扰\n 2. Markdown 让我们回归内容本身，拥抱笔记的内核，而非浮于表象的样式，写出高效精练的笔记！\n\n用 Markdown 写东西，记住一个原则\n\n> 能用10个字搞定的，绝不用11个字\n\n经常使用 Markdown 书写的朋友，也许会有一种奇妙的感触\n\n * 书写，会==倒逼==思维的跃进。像是有东西拽着你的思绪往前冲\n   * 倒逼：逆向逼迫，反向推动\n\n关于标识符的滥用\n\n这个其实是写在最后的，之所以放在这里，是因为它很重要！\n\n如果你有一定的MD语法基础，可以直接[[#19 避免标识符的滥用|点击跳转]]\n\n\n\n# Markdown 相关软件推荐\n\n * Markdown 书写软件 推荐：Typora 优秀的 MD网页文章 书写软件\n   * 点击跳转下载地址\n     * #提示 以前是免费的，现在收费了，不过是买断制\n * Markdown 笔记软件 推荐：Obsidian 银河系最强 MD+双向链 笔记软件\n   * 点击跳转下载地址\n\n\n\n\n\n\n# Markdown 语法\n\n * 提示1： 本教程推荐使用 Obsidian 打开阅读\n * 提示2： 下文提到的所有标识符都是 英文状态 的 ！\n\n\n# 1. 标题&目录\n\n\n\n# 1.1 标题\n\n * Markdown标题共有 六级，和 HTML 一样\n * 区分 一级标题 → 六级标题\n   * 标题 的格式：\n     * # × 标题级数 + 空格 + 文本内容\n\n这是一段普通的文本\n\n# 这是一级标题\n## 这是二级标题\n### 这是三级标题\n#### 这是四级标题\n##### 这是五级标题\n###### 这是六级标题\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n# 1.2 目录\n\n * 目录的 格式：\n   * 在文档的顶部 输入 [toc] ，会根据 标题 自动生成目录 ( Table of Content )\n * 不是所有 MD编辑器 都支持目录生成\n   * Obsidian 就不支持，不过 OB 是自带大纲的，就是目录的效果\n\n输入下方内容会生成一个目录：\n\n[toc]\n\n\n1\n2\n3\n\n\n\n\n\n\n\n# 2. 斜体&粗体\n\n\n\n# 2.1 斜体\n\n * 斜体 的格式：\n   1. * + 文本内容 + *\n   2. _ + 文本内容 + _ ( 下划线 )\n * 说明：\n   * 斜体文本，首尾只有 单个 标识符\n\n这是一段普通文本\n\n*这里是一段斜体文本*\n_这也是一段斜体文本_\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体文本 这也是一段斜体文本\n\n\n\n# 2.2 粗体\n\n * 粗体 的格式：\n   \n   1. ** + 文本内容 + **\n   2. __ + 文本内容 + __ (这里是两个 _ )\n\n * 说明：\n   \n   * 粗体文本，首尾各有 两个 标识符\n\n这是一段普通文本\n\n**这里是一段加粗文本**\n__这也是一段加粗文本__\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段加粗文本 这也是一段加粗文本\n\n\n\n# 2.3 粗斜体 (斜粗体)\n\n * 粗斜体 的格式：\n   \n   1. *** + 文本内容 + ***\n   2. ___ + 文本内容 + ___ （ 这里是3个 _ )\n   3. **_ + 文本内容 + _**\n   4. __* + 文本内容 + *__\n   5. *__ + 文本内容 + __*\n   6. _** + 文本内容 + **_\n\n * 说明：\n   \n   * 粗斜体文本，首尾各有 三个 标识符\n\n这是一段普通文本\n\n***粗斜体文本1***\n___粗斜体文本2___\n**_粗斜体文本3_**\n__*粗斜体文本4*__\n*__粗斜体文本5__*\n_**粗斜体文本6**_\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n这是一段普通文本\n\n粗斜体文本1 粗斜体文本2 粗斜体文本3 粗斜体文本4 粗斜体文本5 粗斜体文本6\n\n\n\n# 2.4 斜体包含粗体\n\n * 斜体中包含粗体 的格式：\n   \n   1. * + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + *\n   2. _ + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + _ （ 这里是两个 _ )\n   3. * + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + *\n   4. _ + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + _\n\n * 说明：\n   \n   * 斜体 中包含 粗体，其实就是嵌套的关系，外层 是 斜体，内层 是 粗体\n   * 外层是斜体，标识符是单个；内层是粗体，标识符是两个\n   * 因为 粗体 是被包裹在 斜体 中的，所以显示效果为 斜粗体\n\n这是一段普通文本\n\n*这里是一段斜体中**包含粗体**的文字*\n_这也是一段斜体中**包含粗体**的文字_\n*这又是一段斜体中__包含粗体__的文字*\n_这还是一段斜体中**包含粗体**的文字_\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体中包含粗体的文字 这也是一段斜体中包含粗体的文字 这又是一段斜体中__包含粗体__的文字 这还是一段斜体中包含粗体的文字\n\n\n\n# 2.5 粗体包含斜体\n\n * 粗体中包含斜体 的格式：\n   1. ** + 粗体文本 + * + 斜体文本 + * + 粗体文本 + **\n   2. __ + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + __ （ 这里是两个 _ )\n   3. ** + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + **\n   4. __ + 粗体文本 + * + 斜体文本 + * + 粗体文本 + __\n * 说明：\n   * 粗体 中包含 斜体，也就是嵌套的关系，外层 是 粗体，内层 是 斜体\n   * 外层是粗体，标识符是两个；内层是斜体，标识符是单个\n   * 因为 斜体 是被包裹在 粗体 中的，所以显示效果为 粗斜体\n\n这是一段普通文本\n\n**这里是一段粗体中*包含斜体*的文字**\n__这也是一段粗体中_包含斜体_的文字__\n**这又是一段粗体中_包含斜体_的文字**\n__这还是一段粗体中*包含斜体*的文字__\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段粗体中包含斜体的文字 这也是一段粗体中_包含斜体_的文字 这又是一段粗体中_包含斜体_的文字 这还是一段粗体中包含斜体的文字\n\n\n\n\n\n\n# 3. 线\n\n\n\n# 3.1 水平分割线\n\n * 水平分割线由至少 3 个 * 或 - 组成\n\n下面是一条水平分割线：\n---\n***\n\n\n1\n2\n3\n\n\n# 示范\n\n----------------------------------------\n\n----------------------------------------\n\n\n\n# 3.2 文本删除线\n\n * 删除线 的格式：\n   * ~~ + 文本内容 +~~ 首尾各加两个 ~ 波浪号\n\n~~这是一段加了删除线的文本~~\n\n\n1\n\n\n# 示范\n\n这是一段加了删除线的文本\n\n\n\n# 3.3 文本下划线\n\n * 下划线的格式，和 HTML 是一样的\n   * <u> + 文本内容 + </u>\n\n<u>这是一段加了下划线的文本</u>\n\n\n1\n\n\n# 示范\n\n这是一段加了下划线的文本\n\n\n\n\n\n\n# 4. 列表&引用\n\n\n\n# 4.1 有序列表\n\n * 有序列表 的格式：\n   \n   * 1. + 空格 + 文本内容\n\n * 说明：\n   \n   * 输入文本内容后，敲击 Enter 自动补全格式，并进入 下个 有序列表\n   * 若需要在同个列表内，增加 换行显示 的内容 (但不进入下个列表) 敲击 Shift + Enter ，即可另起一行输入文本\n   * 在有序列表的中间，插入一个新的列表，后面列表的 数字序号 会自动 递进 一层\n   * 即便在源代码模式中修改了数字序号，渲染界面依然是 依照顺序 显示的\n\n1. 这是第一个有序列表 \x3c!-- (Enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (Enter) --\x3e\n3. 这是第三个有序列表\n\n\n1. 这是第一个有序列表 \x3c!-- (Shift + Enter) --\x3e\n   这是同个列表下，另起一行的文本内容 \x3c!-- (Enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (Shift + Enter) --\x3e\n   这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 这是第一个有序列表\n\n 2. 这是第二个有序列表\n\n 3. 这是第三个有序列表\n\n 4. 这是第一个有序列表 这是同个列表下，另起一行的文本内容\n\n 5. 这是第二个有序列表 这是同个列表下，另起一行的文本内容\n\n# 补充\n\n * 由于有序列表存在强制排序性，它的数字序号必然是逐一递进的 若你希望内容前的数字，不依照递进顺序排序，或者以 整百，整十数 排序\n * 可以配合无序列表，在无序列表中输入：\n   * 数字 + . + 内容 #注意 点号 与 内容 之间，没有空格 (其实有空格也行，就是会感觉有点奇怪)\n\n- 10.这是无序列表下，整十数排列的内容\n- 20.这是无序列表下，整十数排列的内容\n- 30.这是无序列表下，整十数排列的内容\n\n\n- 100.这是无序列表下，整百数排列的内容\n- 200.这是无序列表下，整百数排列的内容\n- 300.这是无序列表下，整百数排列的内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n效果：\n\n * 10.这是无序列表下，整十数排列的内容\n * 20.这是无序列表下，整十数排列的内容\n * 30.这是无序列表下，整十数排列的内容\n\n\n * 100.这是无序列表下，整百数排列的内容\n * 200.这是无序列表下，整百数排列的内容\n * 300.这是无序列表下，整百数排列的内容\n\n\n\n# 4.2 无序列表\n\n * 无序列表 的格式：\n * - + 空格 + 文本内容\n * 说明：\n   * 输入文本内容后，敲击 Enter 自动补全格式，并进入 下个 无序列表\n   * 若需要在同个列表内，增加换行显示的内容 (但不进入下个列表) 敲击 Shift + Enter ，即可另起一行输入文本\n * 补充：\n   * 在Obsidian中，按下 Ctrl + Enter\n   * 即可快速生成一个无序列表\n\n- 这是第1个无序列表 \x3c!-- (Enter) --\x3e\n- 这是第2个无序列表 \x3c!-- (Enter) --\x3e\n- 这是第3个无序列表\n\n- 这是第一个无序列表 \x3c!-- (Shift + Enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n- 这是第二个无序列表 \x3c!-- (Shift + Enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n * 这是第1个无序列表\n * 这是第2个无序列表\n * 这是第3个无序列表\n\n\n * 这是第一个无序列表 这是同个列表下，另起一行的文本内容\n * 这是第二个无序列表 这是同个列表下，另起一行的文本内容\n\n\n\n# 4.3 引用\n\n * 引用 的格式：\n   * > + 文本内容 （不需要空格)\n * 说明：\n   * 同个引用段落内的换行直接敲击 Enter 即可\n   * 若需添加 第二个独立引用段落 ，连续敲击 两下 Enter 即可\n\n>这是第一段引用文本的第1行 \x3c!-- (Enter) --\x3e\n>这是第一段引用文本的第2行 \x3c!-- (Enter) --\x3e\n\x3c!-- (Enter) --\x3e\n>这是第二段引用文本的第1行 \x3c!-- (Enter) --\x3e\n>这是第二段引用文本内第2行\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n> 这是第一段引用文本的第1行 这是第一段引用文本的第2行\n\n> 这是第二段引用文本的第1行 这是第二段引用文本的第2行\n\n\n\n# 4.4 缩进&退格\n\n在列表和引用的书写过程中，我们需要利用 ==缩进== 与 ==退格== ，让文章肌理分明，更具层级\n\n * 缩进：\n   \n   1. Tab\n   2. Ctrl + [   (左中括号)\n\n * 退格：\n   \n   1. Shift + Tab\n   2. Ctrl + ] （右中括号）\n\n\n# 4.4.1 有序列表的缩&退\n\n1. 第一级有序列表1 \x3c!-- (Enter) --\x3e\n\t1. 第二级有序列表1    \x3c!-- 写文本之前，先( Tab 或 Ctrl + ] ) ；写完文本后，再(Enter) --\x3e\n\t2. 第二级有序列表2 \x3c!-- (Enter) --\x3e\n2. 第一级有序列表2    \x3c!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n * 补充说明：\n   * 有序列表的数字序号，即便你在源代码模式里 强行改掉 数字，它仍然会 依照顺序 显示\n\n# 示范\n\n 1. 第一级有序列表1\n    1. 第二级有序列表1\n    2. 第二级有序列表2\n 2. 第一级有序列表2\n\n\n# 4.4.2 无序列表的缩&退\n\n- 第一级无序列表1 \x3c!-- (Enter) --\x3e\n\t- 第二级无序列表1  \x3c!-- 写文本前，先( Tab 或 Ctrl + ] ) ；写完后，再(Enter) --\x3e\n\t- 第二级无序列表2 \x3c!-- (Enter) --\x3e\n- 第一级无序列表2  \x3c!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n * 第一级无序列表1\n   * 第二级无序列表1\n   * 第二级无序列表2\n * 第一级无序列表2\n\n\n# 4.4.3 引用的缩&退\n\n * 引用的 缩进 和列表 不同\n   * 引用需另起一行，并额外多打一个 > 来完成 缩进\n * 引用的 退格 与列表 相同\n   1. Shift + Tab\n   2. Ctrl + ] （右中括号）\n\n>第一级引用1 \x3c!-- (enter) --\x3e\n>>第二级引用1 \x3c!-- 先打1个 > (这里的第一个 > 是会自动补充的，只需额外增补1个即可) ，再(enter) --\x3e\n>>第二级引用2 \x3c!-- (enter) --\x3e\n>第一级引用2   \x3c!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n> 第一级引用1\n> \n> > 第二级引用1 第二级引用2\n> \n> 第一级引用2\n\n\n * 补充： 在 Obsidian 中，引用的退格是不太一样的\n * **Obsidian **中，如果想让已经缩进的引用 退回一层\n   * 得使用 Shift + Enter ，配合方向键，在多个 > 之间灵活断行 并在下一行 根据需要 选择性补充 >\n * 这个用文字比较难以描述，这里选择用2个带键位的 Gif图 来描述\n\nGif演示1：\n\n\n\n\n\n * 效果1：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> > \n> > 444\n> \n> 555\n\n\nGif演示2：\n\n\n\n\n\n * 效果2：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> \n> > 444\n> > \n> > > 555\n> \n> 666\n\n777\n\n\n# 4.4.4 有序&无序&引用 连续套娃\n\n * 有序列表、无序列表、引用 三者之间，可以相互嵌套\n * 核心键 ： Shift + Enter & Enter & Shift + Tab ( 或 Ctrl + [ )\n   * Shift + Enter 在切换格式的嵌套中，是 自带一层 缩进 效果的\n\n1. 第一级 有序列表1 \x3c!-- (Shift + Enter) --\x3e\n\t- 第二级 无序列表1 \x3c!-- (Shift + Enter) --\x3e\n\t\t>第三级 引用1  \x3c!-- (Enter) --\x3e\n\t\t\t- 第四级 无序列表2 \x3c!-- (Shift + Enter) --\x3e\n            \t1. 第五级 有序列表2 \x3c!-- (Enter) --\x3e\n            - 第四级 无序列表3   \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) ；写完后再 (Enter) --\x3e\n        >第三级 引用2  \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) ；写完后再 (Enter × 2) --\x3e\n    - 第二级 无序列表4  \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) --\x3e\n2. 第一级 有序列表3  \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 第一级 有序列表1\n    \n    * 第二级 无序列表1\n      \n      > 第三级 引用1\n      > \n      >  * 第四级 无序列表2\n      >    1. 第五级 有序列表2\n      >  * 第四级 无序列表3\n      > \n      > 第三级 引用2\n    \n    * 第二级 无序列表4\n\n 2. 第一级 有序列表3\n\n# 4.4.5 Obsidian 的一些缩退问题\n\n * Obsidian 在列表首行使用缩进的时候，后续的列表会出现一些问题\n   * Tab 和 Shift + tab 会无法 缩进 退格\n     * 可以使用 Ctrl + ] 与 Ctrl + [ 来解决问题\n\n- - 这是第一段就被缩进的列表\n\t- 这是第二段被再次缩进的列表  \x3c!-- 这里需按两次 Ctrl + ] ,Tab键是无效的 --\x3e\n  - 这是第三段列表  \x3c!-- Ctrl + [ --\x3e\n\n\n1\n2\n3\n\n * * 这是第一段就被缩进的列表 - 这是第二段被再次缩进的列表\n     * 这是第三段列表\n\n\n\n\n\n\n# 5. 网页链接与图像\n\n\n\n# 5.1 网页链接\n\n * 网页链接的 格式：\n   * [ + 显示文本内容 + ] + ( + 链接地址 + 空格 + " + 提示信息文本 + " + )\n * 说明：\n   * 显示文本内容，是在渲染界面实际 可见 的文本，用以 说明 链接\n   * 提示信息文本，需鼠标悬停于 显示文本内容 方可触发，用于增加额外提示信息\n     * #注意 "提示信息文本" 是可选项，一般不会填\n     * 一般来讲，需按住 Ctrl + 鼠标左键点击 才可跳转链接，不过也有 直接鼠标点击 就能跳转的\n\n[显示文本内容](链接地址 "提示信息文本")\n\n[百度一下，你就知道](http://www.baidu.com "按住Ctrl点击跳转百度")\n\n\n1\n2\n3\n\n\n示范：\n\n百度一下，你就知道\n\n\n# 5.1.1链接的加粗\n\n * 格式有两种：\n   \n   1. 把一对 ** 加在 ==显示文本内容==的首尾\n      \n      * 格式1：[**显示文本内容**](链接地址)\n      * 效果： 百度一下，你就知道\n   \n   2. 把一对 ** 加在 链接格式==整体== 的首尾\n      \n      * 格式2：**[显示文本内容](链接地址)**\n      * 效果： 百度一下，你就知道\n\n\n\n\n\n\n# 5.2 图像\n\n * 图像格式：\n   * 图像格式，就是在网页链接前面加个 ! (英文格式的)，! 代表 可见\n   * 图片的提示信息，和网页链接一样，写在 " " 内\n   * [ ] 方括号里的文字信息在 Markdown 没啥实质的作用，只是方便在源代码模式下，知道这个图片是什么，在渲染界面是不会显示的。有点类似于HTML img标签 里的 alt属性。\n\n![文字信息](图片链接 "提示文本信息")\n\n![湘湖1](https://z3.ax1x.com/2021/08/06/fuNkXq.jpg "湘湖一角")\n\n\n1\n2\n3\n\n\n * 补充：\n   \n   * 图像链接可以是本地的，也可以是在线的\n     * 本地图像直接 Ctrl + C 黏贴，Ctrl + V 复制 就可以\n     * 在线图像推荐使用 图床\n   * 调整图像的大小需要使用 HTML 和 CSS，在 Typora编辑器 中右键可以直接缩放图片 本质是转成了HTML的格式，最后会有一个 style="zoom: %;" ，这里数值可以自己修改\n   * 如果有使用 Obsidian 的朋友，在线图片链接是通用的。不过，因为 Obsidian 是双向链笔记 它的本地图片格式不太一样\n     * ![[图片名]]\n       * Obsidian 中的图片是以双链的格式引用在目标笔记中，用 ! 使它可见\n       * Obsidian的图片设置大小是用 | 分隔，后面写宽度数值，单位是px。 设定好宽度，高度会自动等比例调整\n         * ![[图片名|宽度数值]] - 若想自主调整图片宽高，则用： - ![[图片名|宽度数值x高度数值]] - #提示 这里的 x 是 英文字母x\n     * 如果是在线图床，需要调整图片大小：\n       * ![图床|宽度数值](链接地址)\n\n# 示范\n\n\n\n\n\n\n\n\n# 6. 表格\n\n * Markdown的表格，比HTML简单很多\n   * | 是构成表格的主要 框架\n   * - 区分 表头 和 表格主体\n   * : 控制 表格内 文本内容 的 对齐方式\n   * **Typora编辑器中 ** 输入 Ctrl + T 即可快速插入表格，自由定义样式\n\n|这里是表头1|这里是表头2|这里是表头3|\n|:-|:-:|-:|    \x3c!--区分表头和表格主体，:代表文本对齐方式，分别是左对齐，居中对齐，右对齐--\x3e\n|单元格数据1|单元格数据2|单元格数据3|\n|单元格数据4|单元格数据5|单元格数据6|\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这里是表头1   这里是表头2   这里是表头3\n单元格数据1   单元格数据2   单元格数据3\n单元格数据4   单元格数据5   单元格数据6\n\n\n\n# 6.1 表格中文本内容的换行\n\n * Mardown中表格，它的宽高是由 单元格数据内的文本内容 撑开 的\n * 当我们输入一段很长很长的文本，它所在的单元格会变得过宽\n\n如下图所示：\n\n表头1                                   表头2\n这是一段很长很长很长很长很长很长很长很长很长很长很长很长很长很长的文本   普通文本\n\n * 若想对一段长文本进行换行，可以在 中间 插入一个 <br> （ 换行标签 )\n\n| 表头1 |  表头2 |\n|:-:|:-:|\n|这是第一行文本<br>这是另起一行的文本|普通文本|\n\n\n1\n2\n3\n\n\n# 示范\n\n表头1         表头2\n这是第一行文本     普通文本\n这是另起一行的文本\n\n\n\n\n\n\n# 7. 代码域\n\n\n\n# 7.1 行内代码\n\n * 行内代码 的格式：\n   * 输入两个 ` 反引号 ，在中间写代码内容\n * 补充：\n   * 行内代码不一定非得写代码，也可以作为**着重标记**，突出显示内容\n   * 行内代码中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 所谓行内代码： 只要你的屏幕足够宽，它就不会换行\n\n`这是一段行内代码`\n\n`<table border="1" cellspacing="0" width="500" height="500">`\n\n`print("Hello, World!")`\n\n`这是一行突出显示的文本内容`\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 示范\n\n<table border="1" cellspacing="0" width="500" height="500">\n\n\nprint("Hello, World!")\n\n\n这是一行突出显示的文本内容\n\n\n\n# 7.2 代码块\n\n * 代码块 的格式：\n   1. 在首行和末行各加 三个 ` 反引号\n   * ``` + 语言种类 代码内容 ```\n   2. 在首行和末行各加 三个 ~ 波浪号\n      * ~~~ + 语言种类 代码内容 ~~~\n * 补充：\n   * 在代码块也不一定要写代码，可以写一段突出的文本内容，语言类型可以填写 txt 或者 干脆不写\n   * 代码块中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 在 Typora编辑器 ，用键盘按键脱离代码块区域，需输入： Ctrl + Enter\n\n```语言种类\n代码内容\n代码内容\n代码内容\n```\n\n下面是HTML代码块\n\n```html\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n```\n\n下面是CSS代码块\n\n```css\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%,red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n```\n\n下面是JavaScript代码块\n\n```js\n    // 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n        sum += arr.shift();\n        count++;\n        if (count % 5 === 0) {\n            newarr.push(sum / 5);\n            sum =  0;\n        }\n    }\n    console.log(newarr);\n\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, len = arr.length; i < len / 5; i++) {\n        let subarr = arr.splice(0, 5)\n        for (let j = 0, sum = 0; j < subarr.length; j++) {\n            sum += subarr[j];\n        }\n        newarr.push(sum / 5);\n    }\n    console.log(newarr);\n```\n\n\n下面是Python代码块\n\n```python\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "Good bye!"\n```\n\n下面是一块突出显示的文本\n\n```txt\n这是一段\n突出显示的\n文本内容\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n# 示范\n\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%, red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n// 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n\tsum += arr.shift();\n\tcount++;\n\tif (count % 5 === 0) {\n\t\tnewarr.push(sum / 5);\n\t\tsum =  0;\n\t}\n}\nconsole.log(newarr);\n\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, len = arr.length; i < len / 5; i++) {\n\tlet subarr = arr.splice(0, 5)\n\tfor (let j = 0, sum = 0; j < subarr.length; j++) {\n\t\tsum += subarr[j];\n\t}\n\tnewarr.push(sum / 5);\n}\nconsole.log(newarr);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "Good bye!"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n这是一段\n突出显示的\n文本内容\n\n\n1\n2\n3\n\n\n\n# 7.2.1 代码块的嵌套\n\n\n格式：\n\n * 使用4个 ` 包裹 3个 `\n\n# 示范\n\n````txt\n```js\n// 3. 输出 100以内(不包括100) 所有偶数的和\n// 这类求和问题的核心 ： 利用循环  (总和 = 旧数的和 + 新数)\n\nlet sum = 0;\n\nfor (let i = 1, sum = 0; i < 100; i++) {\n if (i % 2 == 0) {\n // 筛选偶数\n sum += i; // sum = sum + i // 累加偶数并赋值给sum\n // sum为(旧的，已经进入循环的数)的和，i 为新进入循环的数。当加到(最后一个新数i)时，sum就是最后的 总和\n }\n}\n\nconsole.log(sum); // 打印总和\n```\n````\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n如果要再套一层，就在最外层 加 5个 ` ，以此类推……\n\n\n\n# 7.3 如何在行内代码里显示反引号\n\n首尾各用 两个反引号`+ 空格 包裹\n\n格式：\n\n``+空格+带`的内容+空格+``  \x3c!-- 不要忘记前后的两个空格 --\x3e\n\n`` 这是一段能显示`反引号`的行内代码 ``\n\n\n1\n2\n3\n\n\n效果：\n\n这是一段能显示`反引号`的行内代码\n\n\n\n\n\n\n# 8. 任务列表（待办）\n\n * 任务列表 的格式：\n   \n   * - + 空格 +[ ] +空格 + 任务列表内容 ( 中括号[ ] 里面必须有个空格)\n   * 给待办任务列表打 √ ，变成 已办\n     1. 在渲染界面，直接鼠标左键点击框框\n     2. 在源代码界面，在中括号内输入 英文字母x\n        * 部分编辑器，在 中括号内 输入任意字符都可以打 √ ( 例如 Obsidian )\n\n * 补充：\n   \n   * 大部分 MD编辑器 支持输入第一个任务列表后，按下 Enter 进入下一行会 自动补全待办格式\n   * 在Obsidian中，连续输入两次 Ctrl + Enter ，即可生成一个待办列表\n     * 再输入一次 Ctrl + Enter ，会在待办列表 打 √\n\n * 格式：\n\n- [ ] 待办任务列表1\n- [ ] 待办任务列表2\n- [x] 已办任务列表1    \x3c!-- 英文字母X --\x3e\n- [x] 已办任务列表2\n\n\n1\n2\n3\n4\n\n\n\n# 示范\n\n * [ ] 待办任务列表1\n * [ ] 待办任务列表2\n * [x] 已办任务列表1\n * [x] 已办任务列表2\n\n\n * 在 Obsidian 中，可以利用 Ctrl + Enter ，快速生成任务列表\n   1. - + 空格 + Ctrl + Enter +待办文本内容\n   2. 待办文本内容 + Ctrl + Enter ×2   ( 输入文本后，连续2次 Ctrl + enter )\n\n\n * 任务列表也是可以缩进+退格的，操作跟 无序、有序列表一样\n\n\n# 示范\n\n * [ ] 第一级待办列表1\n   * [ ] 第二级待办列表1 另起一行的第二级待办列表1\n     * [x] 第三级已办列表1\n     * [x] 第三级已办列表2\n   * [ ] 第二级待办列表2 另起一行的第二级待办列表2\n * [ ] 第一级待办列表2\n\n\n\n\n\n\n# 9. 注释\n\nMarkdown 的 注释 和 HMTL 一样，注释的内容在 渲染界面 不可见 （部分编辑器可见)\n\n * 注释 的格式：\n   * \x3c!-- 这里是注释的内容 --\x3e\n     * 注释可以是单行，也可以是多行\n   * 如果有在使用 Obsidian 的，它的注释格式是不一样的\n     * %%这是Obsidian的注释内容%%\n\n\x3c!-- 这里是一行注释 --\x3e\n\n\x3c!--\n这里是\n一段\n假装有\n很多行的\n注释\n--\x3e\n\n%%这是一行Obsidian里的注释%%\n\n%%\n这里是\n一段\n假装有\n很多行的\nObsidian里的\n注释\n%%\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 示范 (只有切换至 编辑模式 才能看到喔)\n\n%%这是一行Obsidian里的注释%%\n\n%% 这里是 一段 假装有 很多行的 Obsidian里的 注释 %%\n\n\n\n\n\n\n# 10. 变量\n\n\n\n# 10.1 网页链接变量\n\n * 网页链接变量 的格式：\n   1. 首先输入\n      * [显示文本内容] + [变量名]\n        * 变量名可以自己取，没啥限制，任意字符都可以\n   2. 在文档任意一个区域，输入：\n      * [变量名] + : + 空格 + 链接地址 （这个**空格** 不打也没事)\n\n[百度一下，你就知道][度娘]\n[知乎-有问题，就会有答案][知乎]\n\n\x3c!-- 这里是变量区域 --\x3e\n[度娘]: http://www.baidu.com\n[知乎]: https://www.zhihu.com\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n百度一下，你就知道\n\n知乎-有问题，就会有答案\n\n\n\n# 10.2 脚注\n\n * 脚注 的格式：\n   * 在需要脚注的地方，输入：\n     * [^脚注代号] ( 脚注代号会直接显示在渲染界面 )\n       * 脚注代号可以随便命名，不过推荐使用 数字序号\n   * 在其他区域，输入：\n     * [^脚注代号] + : + 空格 + 脚注内容 （这个 空格 不打也没事)\n\n鲁迅原名是什么[^1] ，浙江哪里人[^2]\n\n\x3c!-- 这里是变量区域 --\x3e\n[^1]: 周树人\n[^2]: 绍兴人\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n鲁迅原名是什么^1，浙江哪里人^2\n\n\n\n\n\n\n# 11. 拓展文本格式标记\n\n * Markdown 想实现更多的文本显示效果，只能依赖HTML标记实现\n * 个人不是很推荐在 MD 中使用 HTML，不过一些简单的标记还是可以 轻度使用 的\n\n\n\n# 11.1 键盘文本\n\n * 键盘文本的 格式：\n   \n   * <kbd>键盘文本</kbd>\n   * <kbd>Ctrl</kbd> + <kbd>X</kbd>\n\n * 效果：\n   \n   * 键盘文本\n   * Ctrl + X ( 剪切 )\n\n * 说明：\n   \n   * 键盘文本也不一定非得是键盘按键，也可以作为着重文本突出显示\n     * 效果： 这也算一种着重文本的方式\n\n# 11.1.1 加粗键盘文本\n\n * 加粗键盘文本的格式有两种：\n   \n   * <kbd>**键盘文本**</kbd>\n   * **<kbd>ctrl + x</kbd>**\n\n * 效果：\n   \n   1. 键盘文本\n   2. ctrl + x\n\n\n\n# 11.2 放大文本\n\n * 放大文本 的格式：\n   \n   * 这是一段普通文本 <big>这是一段放大文本</big>\n\n * 效果：\n   \n   * 这是一段普通文本 这是一段放大文本\n\n# 11.2.1 放大粗体文本\n\n * 放大加粗文本的格式有两种：\n   1. **<big>这是一段放大粗体文本</big>**\n   2. <big>**这是一段放大粗体文本**</big>\n * 效果：\n   1. 这是一段放大粗体文本\n   2. 这是一段放大粗体文本\n\n\n\n# 11.3 缩小文本\n\n * 缩小文本 的格式：\n   * 这是一段普通文本 <small>这是一段缩小文本</small>\n * 效果：\n   * 这是一段普通文本 这是一段缩小文本\n\n# 11.3.1 缩小斜体文本\n\n * 斜体缩小文本 的格式有两种：\n   1. <small>*这是一段缩小斜体文本*</small>\n   2. *<small>这是一段缩小斜体文本</small>*\n * 效果：\n   1. 这是一段缩小斜体文本\n   2. 这是一段缩小斜体文本\n\n\n\n# 11.4 多彩文本\n\n * 多彩文本 的格式：\n   * <font color=orange>这是一段橘色文本</font>\n * 效果：\n   * 这是一段橘色文本\n     * color 里的颜色支持 英文单词，16进制，rgb，rgba\n\n\n# 11.4.1 多彩粗体文本\n\n * 只需要在上面示例的基础上，加上 加粗标识符，有两种格式：\n   1. 格式1： **<font color=teal>这是一段加粗的水鸭色文本</font>**\n      * 效果： 这是一段加粗的水鸭色文本\n   2. 格式2： <font color=teal>**这是一段加粗的水鸭色文本**</font>\n      * 效果： 这是一段加粗的水鸭色文本\n * 若上述混搭方法的样式失效 ，可以使用 纯HTML标记\n   * 格式： <strong style="color:teal;">这是一段加粗的水鸭色文本</strong> (标记略复杂，不是很推荐)\n   * 效果： 这是一段加粗的水鸭色文本\n\n\n# 11.4.2 多彩斜体文本\n\n * 跟多彩加粗文本完全一样，只需把首尾的 ** 换成 * 即可\n\n 1. 格式1： *<font color=teal>This is an italic teal text</font>*\n    * 效果： This is an italic teal text\n 2. 格式2： <font color=teal>*This is an italic teal text*</font>\n    * 效果： This is an italic teal text\n\n\n# 11.4.2 多彩粗斜体文本\n\n * 首尾换成 ***\n\n 1. 格式1： ***<font color=teal>This is a bold italic teal text</font>***\n    * 效果： This is a bold italic teal text\n 2. 格式2： <font color=teal>***This is a bold italic teal text***</font>\n    * 效果： This is a bold italic teal text\n\n\n#注意 多彩文本尽量慎用，Markdown 的核心就是 简洁精炼，注重 实质内容，而非花哨的 颜色样式\n\n\n\n\n\n\n# 12. 拓展文本显示效果\n\n * 拓展显示效果既不是原生 Markdown语法 支持的，也非 HTML标记，而是部分编辑器 提供的 额外标识符，属于拓展语法，旨在为 Markdown使用者 提供更多样式选择\n * 不同编辑器，支持不一样，这里以 Typora编辑器 为例\n\n\n\n# 12.1 文本高亮\n\n * 文本高亮 的格式：\n   * ==这里是一段高亮文本==\n * 效果：\n   * ==这里是一段高亮文本==\n\n\n\n# 12.2 上标\n\n * 用一对 ^ 包裹 (Shift+ 6)\n   * 格式： x^2^\n   * 效果： x^2^\n * Obsidian 没效果的，可以用后面会讲的 Latex\n * 或者，也可以使用 HTML标记\n   * <sup>这里是上标内容</sup>\n   * X<sup>2</sup>\n * 效果：\n   * X2\n\n\n\n# 12.3 下标\n\n * 用一对 ~ 包裹 (Shift + `)\n   * 格式： H~2~O\n   * 效果： H~2~O\n * Obsidian 没效果的，可以用后面会讲的 Latex\n * 或者，也可以使用 HTML标记\n   * <sub>这里是下标内容</sub>\n   * H<sub>2</sub>O\n * 效果：\n   * H2O\n\n\n\n# 12.4 Emoji 符号\n\n用一对 : 包裹，里面是 Emoji 符号的 语义化文本 ( Typora编辑器 中，输入 : 就会带提示器 )\n\n * 示例：\n   * :smile: :sweat: :cat: :woman_cartwheeling:\n * 效果：\n   * 😄 😓 🐱 🤸‍♀\n\n\n * 补充：\n   * 不支持上述方式的 MD编辑器或笔记软件，直接用 输入法 输入也是可以的\n   * Windows系统 用户 win + . 就可以输入 Emoji 了\n   * Obsidian 用户可以安装第三方插件来支持 Emoji 的输入，推荐两个\n     1. ==Emoji Shortcodes==\n     2. ==Emoji Toolbar==\n\n\n\n\n\n\n# 13. 转义字符\n\n * 在 Markdown 中，我们 通过 标识符 改变 文本显示效果\n * 现在我们希望它不作为标识符，而是 作为字符本身呈现出来 （不具备改变文本显示效果的功能，只是一个普通字符)\n   * 首先我们可以用前面介绍的 代码域 ，因为代码模式的显示效果就是源代码完全一致的\n   * 还有一种方法，可以利用转义字符，在这些标识符 前面 加上 反斜线 \\ ( 反斜线要紧贴在标识符前面，不能 有 空格 )\n     * 原理：\n       * \\ 的作用是让标识符 转义 变为一个普通字符，完成这个效果后，反斜线会自动隐藏\n       * 隐藏后的反斜线仅在源代码界面可见，在渲染界面不可见\n       * 反斜线只争对标识符起作用，其他字符添加 \\，\\ 不会自动隐藏\n     * 补充：\n       * 如果想给已经被加在标识符前面，会自动隐藏的 \\ 显示出来，可以在反斜线前面再加一个 \\ ，用它自己来转义自己\n         * 示例： 这里紧跟在标识符前面的反斜线\\\\*会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体*\n         * **效果： ** 这里紧跟在标识符前面的 反斜线\\会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体\n\n\n\n# 例1 以普通字符显示星号\n\n * 如何让被一对或多对 * 号 包裹的文本内容，能够正常显示 * ，且文本不改变格式\n   * \\*这段文本被一对星号包裹，但不会倾斜\\*\n     * 效果： *这段文本被1对星号包裹，但不会倾斜*\n   * \\*\\*这段文本被2对星号包裹，但不会加粗\\*\\*\n     * 效果： **这段文本被2对星号包裹，但不会加粗**\n   * \\*\\*\\*这段文本被3对星号包裹，但它既不倾斜也不加粗\\*\\*\\*\n     * 效果： ***这段文本被3对星号包裹，但它既不倾斜也不加粗***\n\n\n\n# 例2 表格内 单元格中的竖杠\n\n * 在表格中，使用 | 作为单元格的内容，但不会被识别为表格的结构，不会增加额外的单元格\n\n|表头1|表头2|\n|-|-|\n|这里的文本被\\|分隔|这里的文本也被\\|分隔|\n\n\n1\n2\n3\n\n * 效果：\n\n表头1         表头2\n这里的文本被|分隔   这里的文本也被|分隔\n\n\n#补充 该技巧可用于 Obsidian 表格内 双链的文本修饰\n\n文本修饰：\n\n在 双链[[ ]]内 以 | 引导的内容\n\n * 格式： [[链接的内容|文本修饰]]\n * 说明： 文本修饰是渲染界面实际显示的文本，便于更好地融入语境\n\n表格内的格式：\n\n在 | 前面加上 \\\n\n * [[表格内的链接内容\\|文本修饰]]\n\n示例：\n\n|                  表头1                  |                        表头2                        |\n|:---------------------------------------:|:---------------------------------------------------:|\n| [[#例2 表格内 单元格中的竖杠\\|单元格中的竖杠]] | [[#例3 不会变成代码的反引号\\|不会变成代码的反引号]] |\n\n\n1\n2\n3\n\n\n效果：\n\n表头1                           表头2\n[[#例2 表格内 单元格中的竖杠|单元格中的竖杠]]   [[#例3 不会变成代码的反引号|不会变成代码的反引号]]\n\n\n\n# 例3 不会变成代码的反引号\n\n使用 转义符号\\ 让 反引号` 变成普通字符，不再具有[[#7 1 行内代码|行内代码]]的标识符功能\n\n格式：\n\n\\`这段被反引号包裹的内容不会变成行内代码\\`\n\n效果：\n\n`这段被反引号包裹的内容不会变成行内代码`\n\n\n\n# 例4 链接中的中括号\n\n在 网页链接 的 显示文本内容 中，使用 中括号 [ ]\n\n * 在显示文本内容中，在其中一个中括号前面，加上转义符号 反斜杠 \\\n   * 格式： [链接里的 \\[中括号\\] 能被正常显示](https://www.runoob.com)\n   * 效果： 链接里的 [中括号] 能被正常显示\n\n\n\n# 例5 不是列表的连接符(横杠)\n\n * 引用一段话，一般会在换行之后，加上 - 出处\n * 因为 - 是标识符，会变成一个无序列表\n\n如下所示：\n\n> The Web, the Tree, and the String. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n> \n>  * 史蒂芬·平克\n\n * 解决方法：\n   \n   * 在 - 前面加上 转义符号 \\\n   \n   >The Web, the Tree, and the String.\n   >写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n   >\\- 史蒂芬·平克   \x3c!-- 加上转义符号 \\ , 不会变成无序列表 --\x3e\n   \n   \n   1\n   2\n   3\n   \n\n * 效果：\n\n> The Web, the Tree, and the String. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。 - 史蒂芬·平克\n\n\n\n# 例6 不是标题的 #\n\n让 # 不被识别为标题标识符\n\n格式：\n\n\\# 这里的内容不会被识别为标题\n\n效果：\n\n# 这里的内容不会被识别为标题\n\n\n\n# 例7 不会注释的 %\n\n在 Obsidian 中 注释是前后各两个 % 号\n\n使用 转义符号\\，让 %% 作为普通字符显示出来，不具备注释的功能\n\n * 格式： \\%\\%这里的内容可以被显示喔\\%\\%\n * 效果： %%这里的内容可以被显示喔%%\n\n\n\n# 例8 木有链接的双链\n\nObsidian 的双向链格式是2个方括号 [[ ]] (双方)，使用 转义符号\\，让 [ ] 不再具有 双链功能\n\n格式：\n\n\\[\\[这段文本被双方包裹，但不是一个双向链\\]\\]\n\n效果：\n\n[[这段文本被双方包裹，但不是一个双向链]]\n\n\n\n# 例9 页链接里 显示文本内的 中括号\n\n使用转义符号\\，让中括号可以作为显示文本 在[[#5 1 网页链接|网页链接]]中显示出来\n\n格式：\n\n[\\[这是一个带中括号的网页链接显示文本，点击会跳转至百度\\]](https://www.baidu.com/)\n\n\n1\n\n\n效果：\n\n[这是一个带中括号的网页链接显示文本，点击会跳转至百度]\n\n\n\n# 特殊情况 文本修饰的中括号\n\n文本修饰的 中括号[ ] 不需要使用 转义符号\\\n\n示范：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n效果：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n\n\n\n\n\n# 14. 空格&换行&强制删除\n\n\n\n# 14.1 空格\n\n * 在一些编辑器或者支持MD的笔记软件里，无论你打多少个空格，它只会显示单个 空格 的距离\n   * 可以使用 HTML中 空格 的 字符实体 —— &nbsp;\n   * 若要添加 多个 空格，就输入多个 —— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n * 格式：\n   * 这里有&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6个空格分隔\n * 效果：\n   * 这里有      6个空格分隔\n\n\n\n# 14.2 换行\n\n场景1：\n\n * 在一些编辑器或者支持MD的笔记软件里，无论你打多少个 回车，它只会显示单个 回车 的空行间距\n   * 可以使用之前表格里提到的 <br> 标签，在 单独一行 中使用，增加额外的空行间距\n   * 如果要增加 多个，就输入 多个 —— <br><br><br><br><br>\n   * #注意 当单独一行使用 <br> 标签的时候，如果前后有标题标识符或者列表标识符，确保 br元素 前后两行都是空白行\n\n格式：\n\n这里是第一段文本\n\n<br><br><br><br><br>     \x3c!-- 这里插入了5个空行间距 --\x3e\n\n这里是第二段文本\n\n\n1\n2\n3\n4\n5\n\n\n效果：\n\n这里是第一段文本\n\n\n\n\n\n\n\n\n这里是第二段文本\n\n\n\n\n\n场景2：\n\n * 在列表中也可以插入换行符\n\n- 这是一段无序列表\n  <br>     \x3c!-- 插入一个空行间距，需单独一行，上下不用预留空格 --\x3e\n  这是同一段无序列表中，空一行距离显示的内容\n- 这是第二段无序列表\n\n\n1\n2\n3\n4\n\n\n效果：\n\n * 这里是第一段无序列表\n   这里是同一段无序列表中，空一行距离显示的内容\n * 这里是第二段无序列表\n\n\n * 补充：\n   * 有一些MD编辑器或笔记软件，严格遵循MD的换行规则，你敲一个回车是没法换行的，必须在 行末 敲 2个空格，再按回车键\n     * 格式：\n       * 这里是一段想换行的文本空格 空格 Enter 这是换行后的文本\n\n\n\n# 14.3 强制删除\n\n * 很多编辑器都有英文标点自动补全功能，自动生成一对，光标落在中间 只想删除前面1个，却会把 一整对 都删掉\n * 在多个列表的嵌套中，也许会遇到一些 无法被删除 的 列表标识符\n * 解决方法： 使用 Shift + Backspace 即可强制删除\n   * Bcakspace   ( 退格键 )\n\n\n\n\n\n\n# 15. 嵌入\n\n * 嵌入都是依赖 HTML标签 实现的，嵌入的都是在线链接格式\n   * 如果是本地的，Obsidian 中音频是有自带的可录制的录音机插件的，其他的 音频、视频 直接复制黏贴就可以了，也可以直接拖拽到OB的笔记界面\n     * 其他的媒体文件在 Obsidian 也和图片一样，以双链的格式引用在目标笔记中，使用 ! 使它可见\n\n\n\n# 15.1 嵌入音频\n\n * 格式：\n   \n   * <audio controls="controls" preload="none" src="音频链接地址"></audio>\n\n * 示例：\n\n<audio controls="controls" preload="none" src="https://www.ldoceonline.com/media/english/exaProns/p008-001803372.mp3?version=1.2.37"></audio>\n\n\n1\n\n * 效果：\n\n\n\n\n\n# 15.2 嵌入视频\n\n * 格式：\n\n<video width="600" height="420" controls>\n  <source src="movie.mp4" type="video/mp4">\n  <source src="movie.ogg" type="video/ogg">\n  <source src="movie.webm" type="video/webm">\n</video>\n\n\n1\n2\n3\n4\n5\n\n * 说明：\n   * width ( 宽度 ) height ( 高度 ) ，可以自己设置，直接输入数字即可，单位默认是 px(像素) 也可以使用 百分比 width=100% 代表水平撑满整个窗口 height=50% 代表垂直撑满半个窗口\n   * Video标签 支持的视频格式 ：MP4 ogg webm\n\n\n\n# 15.3 嵌入页面\n\n * 格式： <iframe width=600 height=400 src="页面链接地址" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n<iframe width=600 height=400 src="https://www.runoob.com/html/html-tutorial.html" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 效果：\n\n\n * iframe标签 除了嵌入页面，也可以嵌入在线视频，主流的视频网站都会提供嵌入代码\n   \n   * 具体可以看这个 iframe视频嵌入教程\n   * B站 的视频，得在 // 前面补充 http:\n   * 不是所有的 编辑器和笔记软件 都支持这个\n\n * 示例：\n\n<iframe width=600 height=400 src="http://player.bilibili.com/player.html?aid=20190823&bvid=BV1yW411s7og&cid=32964980&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 宽高设置和前面的 video 一样\n\n\n * 效果：\n\n\n\n\n\n\n# 16. Latex 数学公式\n\n * 主要用于 数学公式 与 化学公式 的书写\n\n\n\n# 16.1 行内公式\n\n * 格式：\n   \n   * $ + 行内公式 + $\n\n\n * 示例：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $\\ce{CO2 + C -> 2 CO}$\n   * $\\ce{CO2 + C -> 2 CO}$\n   * $\\ce{2Mg + O2 ->[燃烧] 2 MgO}$\n\n\n * 效果：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $e^{i\\pi} + 1 = 0$\n   * $\\ce{CO2 + C -> 2 CO}$\n   * $\\ce{2Mg + O2 ->[燃烧] 2 MgO}$\n\n\n\n# 16.2 公式块\n\n * 格式：\n   * $$ 公式块 $$\n\n\n * 示例：\n\n% 化学公式\n$$\n\\ce{Zn^2+  <=>[+ 2OH-][+ 2H+]  $\\underset{\\text{amphoteres Hydroxid}}{\\ce{Zn(OH)2 v}}$  <=>[+ 2OH-][+ 2H+]  $\\underset{\\text{Hydroxozikat}}{\\ce{[Zn(OH)4]^2-}}$}\n$$\n\n\n1\n2\n3\n4\n\n\n% 麦克斯韦方程组\n$$\n\\begin{array}{lll}\n\\nabla\\times E &=& -\\;\\frac{\\partial{B}}{\\partial{t}}\n\\ \\nabla\\times H &=& \\frac{\\partial{D}}{\\partial{t}}+J\n\\ \\nabla\\cdot D &=& \\rho\n\\ \\nabla\\cdot B &=& 0\n\\ \\end{array}\n$$\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n% 薛定谔方程\n$$\ni\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + V \\psi\n$$\n\n\n1\n2\n3\n4\n\n\n * 效果：\n\n$$ % 化学公式 \\ce{Zn^2+ <=>[+ 2OH-][+ 2H+] $\\underset{\\text{amphoteres Hydroxid}}{\\ce{Zn(OH)2 v}}$ <=>[+ 2OH-][+ 2H+] $\\underset{\\text{Hydroxozikat}}{\\ce{[Zn(OH)4]^2-}}$} $$\n\n\n$$ % 麦克斯韦方程组 \\begin{array}{lll} \\nabla\\times E &=& -;\\frac{\\partial{B}}{\\partial{t}} \\ \\nabla\\times H &=& \\frac{\\partial{D}}{\\partial{t}}+J \\ \\nabla\\cdot D &=& \\rho \\ \\nabla\\cdot B &=& 0 \\ \\end{array} $$\n\n\n$$ i\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + V \\psi $$\n\n * 补充：\n   * 需要详细教程的，可戳下方链接\n   * Latex详细教程\n\n\n\n\n\n\n# 17. Mermaid\n\n * 一些 MD编辑器 和 笔记软件 支持通过 Mermaid 及其所提供的 编译器 来为用户提供图表的绘制功能\n\n * 这里只提供一些演示的图表，具体教程可戳下方\n   \n   * [[MOC Mermiad 教程 Obsidian版| Mermiad 超级教程 Obsidian版]]\n\n\n\n# 17.1 流程图\n\n\n源码1：\n\n```mermaid\ngraph TB\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点2\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点1\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n渲染1：\n\ngraph TB\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点1\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点2\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n源码2：\n\n```mermaid\ngraph LR\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n渲染2：\n\ngraph LR\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n# 17.2 饼图\n\n\n源码：\n\n```mermaid\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n渲染：\n\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n# 17.3 序列图 (时序图)\n\n\n源码：\n\n```mermaid\nsequenceDiagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant A as Aly\n        participant B as Bob\n        participant C as CofCai\n        %% 便签说明\n        Note left of A: 只复习了一部分\n        Note right of B: 没复习\n        Note over A,B: are contacting\n\n        A->>B: 明天是要考试吗？\n        B--\x3e>A: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par askA\n                C --\x3e> A:你复习好了吗？\n            and askB\n                C --\x3e> B:你复习好了吗？\n            and self\n                C ->>C:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/Every min\n            %% <br/>可以换行\n            C ->> C:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                A ->> C:复习了一点\n            else is common\n                B ->> C:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt Extra response\n                C ->> C:你们怎么不回答我\n            end\n        endsequenceDiagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant A as Aly\n        participant B as Bob\n        participant C as CofCai\n        %% 便签说明\n        Note left of A: 只复习了一部分\n        Note right of B: 没复习\n        Note over A,B: are contacting\n\n        A->>B: 明天是要考试吗？\n        B--\x3e>A: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par askA\n                C --\x3e> A:你复习好了吗？\n            and askB\n                C --\x3e> B:你复习好了吗？\n            and self\n                C ->>C:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/Every min\n            %% <br/>可以换行\n            C ->> C:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                A ->> C:复习了一点\n            else is common\n                B ->> C:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt Extra response\n                C ->> C:你们怎么不回答我\n            end\n        end\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\n\n渲染：\n\nsequenceDiagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant A as Aly\n        participant B as Bob\n        participant C as CofCai\n        %% 便签说明\n        Note left of A: 只复习了一部分\n        Note right of B: 没复习\n        Note over A,B: are contacting\n\n        A->>B: 明天是要考试吗？\n        B--\x3e>A: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par askA\n                C --\x3e> A:你复习好了吗？\n            and askB\n                C --\x3e> B:你复习好了吗？\n            and self\n                C ->>C:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/Every min\n            %% <br/>可以换行\n            C ->> C:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                A ->> C:复习了一点\n            else is common\n                B ->> C:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt Extra response\n                C ->> C:你们怎么不回答我\n            end\n        end\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n\n# 17.4 甘特图\n\n\n源码：\n\n```mermaid\ngantt\n    title A Gantt Diagram\n    dateFormat  YYYY-MM-DD\n    section Section\n    A task           :a1, 2014-01-01, 30d\n    Another task     :after a1  , 20d\n    section Another\n    Task in sec      :2014-01-12  , 12d\n    another task      : 24d\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n渲染：\n\ngantt\n    title A Gantt Diagram\n    dateFormat  YYYY-MM-DD\n    section Section\n    A task           :a1, 2014-01-01, 30d\n    Another task     :after a1  , 20d\n    section Another\n    Task in sec      :2014-01-12  , 12d\n    another task      : 24d\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 17.5 类图\n\n\n源码：\n\n```mermaid\nclassDiagram\n    Animal <|-- Duck\n    Animal <|-- Fish\n    Animal <|-- Zebra\n    Animal : +int age\n    Animal : +String gender\n    Animal: +isMammal()\n    Animal: +mate()\n    class Duck{\n      +String beakColor\n      +swim()\n      +quack()\n    }\n    class Fish{\n      -int sizeInFeet\n      -canEat()\n    }\n    class Zebra{\n      +bool is_wild\n      +run()\n    }\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n渲染：\n\nclassDiagram\n    Animal <|-- Duck\n    Animal <|-- Fish\n    Animal <|-- Zebra\n    Animal : +int age\n    Animal : +String gender\n    Animal: +isMammal()\n    Animal: +mate()\n    class Duck{\n      +String beakColor\n      +swim()\n      +quack()\n    }\n    class Fish{\n      -int sizeInFeet\n      -canEat()\n    }\n    class Zebra{\n      +bool is_wild\n      +run()\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n\n\n# 18. 标签 (Tag)\n\n * 标签是 Obsidian 特有的一个功能，标签可以通过点击唤起快速搜索 (搜索包含该标签的所有笔记)\n\n格式：\n\n * # + 标签名\n   * #标签名\n\n\n# 关于空格\n\n * 在一段正文文本的后面添加 Tag， # 的前面 需要有个空格\n   * 空格 + # + 标签名\n\n\n * # 与 标签名 之间，不能有空格，否则就变成 一级标题 了\n\n\n * 标签名的内部，不允许使用空格，若想区分标签中的词语，可使用以下三种方法：\n   1. 驼峰式大小写： #BlueTopaz\n   2. 下划线： #blue_topaz\n   3. 连字符： #blue-topaz\n\n\n\n# 关于数字\n\n * 标签内允许使用数字，但不能完全由数字组成\n   * #1984 ❌\n   * #1984Date ⭕\n   * #da_1984_te ⭕\n   * #date-1984 ⭕\n\n\n\n# 标签的嵌套\n\n在标签名内，使用 / 斜杠 可以实现标签的嵌套\n\n格式：\n\n * #主标签/子标签1\n * #主标签/子标签2\n * #主标签/子标签3\n\n嵌套标签可以像普通标签一样通过点击来唤起搜索，嵌套标签允许你选择搜索的层次。例如：\n\n * 搜索 #主标签 ，即可找到包含任意一个子标签的所有笔记\n   * 返回的结果会是上述的三个例子\n * 当你在一个主分类下设置了多个子分类，想找到这个主分类包含的所有内容时，该功能会很实用\n\n\n\n# 能被使用的符号\n\n综上所述，标签内能被使用的符号共有三种\n\n 1. _ 下划线\n 2. - 连字符\n 3. / 斜杠\n\n\n\n# 如何让 # 不被识别\n\n可以使用前面提到的转义符号 \\ 反斜杠，与上述的 转义标题 类似\n\n格式：\n\n\\#这里的内容不会被识别为标签\n\n效果：\n\n#这里的内容不会被识别为标签\n\n\n\n# 19. 避免标识符的滥用\n\n即使在 Markdown 中，也要尽量避免标识符的滥用\n\n比如我的这篇教程，就存在一定程度的滥用\n\n * 其实是因为我这篇是教学性质的，不太一样，有些不能避免\n   * (好吧，我就是在甩锅)\n\n标识符的本质是突出显示，代表重点\n\n * 一篇笔记里的某段文本，使用各式各样的的标识符，会造成重点不清晰\n\n有三种标识，慎用！\n\n 1. 词中对单个汉字的标识\n    1. 卧==虎==藏==龙==\n 2. 短语中对单个英语单词的标识\n    1. get a ==bang== out of\n 3. 标识符的多层嵌套\n    1. 我感觉快要==原地起飞==了\n\n原因：\n\n * 词义的割裂\n * 视觉的混乱\n * 不利于搜索\n   * 卧==虎==藏==龙==\n     * 搜 卧虎 -- 搜不到\n     * 搜 藏龙 -- 搜不到',normalizedContent:'这里是 two-test-1 的内容。\n\n\n\n以下markdown内容转载自：markdown超级教程 obsidian版\n\n这里仅作为展示vuepress解析markdown效果的一个展示。\n\n\n# 什么是 markdown?\n\n 1. markdown 是一款轻量级标记语言，不同于html (hypertext markup language)，markdown 的语法非常简单，且容易上手\n 2. markdown 以 纯文本格式 编写文档，依赖键盘而非鼠标，专注于写作本身，感受书写的魅力\n 3. markdown 的通过添加一些简单的 标识符，让文本具有恰到好处的格式\n 4. markdown 核心特征就是 删繁剪芜， 简扼 + 精炼\n 5. markdown 是 笔记 与 网页文章 的最佳载体\n 6. down 的核心：坐 下 来，就能把思维写 下 来\n    * 牛津高阶英汉双解词典第九版 中，关于 down 的释义：\n\n\n\n\n\n\n# 为什么要使用 markdown?\n\n有朋友问我 ，markdown 的效果 用word 完全可以复现，甚至功能更多，那为何要用 markdown 呢？\n\n答：\n\n * 功能多，不一定是好事\n   * 功能一多，选择就会变多，然后你会开始纠结……\n     * 这个字号是不是该大一点呢？\n     * 这个颜色好像有点不太搭呢？\n     * 这个粗体，是不是该再加点颜色呢？\n     * 这个图片的位置看起来有点不大对劲呢？\n   * 结果，写了半天，就憋出一点点东西\n     * 写出来的内容...好像...也不咋滴\n\nmd的优势：\n\n 1. markdown 让我们免于 被繁杂臃肿的功能晃花了眼 的困扰\n 2. markdown 让我们回归内容本身，拥抱笔记的内核，而非浮于表象的样式，写出高效精练的笔记！\n\n用 markdown 写东西，记住一个原则\n\n> 能用10个字搞定的，绝不用11个字\n\n经常使用 markdown 书写的朋友，也许会有一种奇妙的感触\n\n * 书写，会==倒逼==思维的跃进。像是有东西拽着你的思绪往前冲\n   * 倒逼：逆向逼迫，反向推动\n\n关于标识符的滥用\n\n这个其实是写在最后的，之所以放在这里，是因为它很重要！\n\n如果你有一定的md语法基础，可以直接[[#19 避免标识符的滥用|点击跳转]]\n\n\n\n# markdown 相关软件推荐\n\n * markdown 书写软件 推荐：typora 优秀的 md网页文章 书写软件\n   * 点击跳转下载地址\n     * #提示 以前是免费的，现在收费了，不过是买断制\n * markdown 笔记软件 推荐：obsidian 银河系最强 md+双向链 笔记软件\n   * 点击跳转下载地址\n\n\n\n\n\n\n# markdown 语法\n\n * 提示1： 本教程推荐使用 obsidian 打开阅读\n * 提示2： 下文提到的所有标识符都是 英文状态 的 ！\n\n\n# 1. 标题&目录\n\n\n\n# 1.1 标题\n\n * markdown标题共有 六级，和 html 一样\n * 区分 一级标题 → 六级标题\n   * 标题 的格式：\n     * # × 标题级数 + 空格 + 文本内容\n\n这是一段普通的文本\n\n# 这是一级标题\n## 这是二级标题\n### 这是三级标题\n#### 这是四级标题\n##### 这是五级标题\n###### 这是六级标题\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n# 1.2 目录\n\n * 目录的 格式：\n   * 在文档的顶部 输入 [toc] ，会根据 标题 自动生成目录 ( table of content )\n * 不是所有 md编辑器 都支持目录生成\n   * obsidian 就不支持，不过 ob 是自带大纲的，就是目录的效果\n\n输入下方内容会生成一个目录：\n\n[toc]\n\n\n1\n2\n3\n\n\n\n\n\n\n\n# 2. 斜体&粗体\n\n\n\n# 2.1 斜体\n\n * 斜体 的格式：\n   1. * + 文本内容 + *\n   2. _ + 文本内容 + _ ( 下划线 )\n * 说明：\n   * 斜体文本，首尾只有 单个 标识符\n\n这是一段普通文本\n\n*这里是一段斜体文本*\n_这也是一段斜体文本_\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体文本 这也是一段斜体文本\n\n\n\n# 2.2 粗体\n\n * 粗体 的格式：\n   \n   1. ** + 文本内容 + **\n   2. __ + 文本内容 + __ (这里是两个 _ )\n\n * 说明：\n   \n   * 粗体文本，首尾各有 两个 标识符\n\n这是一段普通文本\n\n**这里是一段加粗文本**\n__这也是一段加粗文本__\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段加粗文本 这也是一段加粗文本\n\n\n\n# 2.3 粗斜体 (斜粗体)\n\n * 粗斜体 的格式：\n   \n   1. *** + 文本内容 + ***\n   2. ___ + 文本内容 + ___ （ 这里是3个 _ )\n   3. **_ + 文本内容 + _**\n   4. __* + 文本内容 + *__\n   5. *__ + 文本内容 + __*\n   6. _** + 文本内容 + **_\n\n * 说明：\n   \n   * 粗斜体文本，首尾各有 三个 标识符\n\n这是一段普通文本\n\n***粗斜体文本1***\n___粗斜体文本2___\n**_粗斜体文本3_**\n__*粗斜体文本4*__\n*__粗斜体文本5__*\n_**粗斜体文本6**_\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n这是一段普通文本\n\n粗斜体文本1 粗斜体文本2 粗斜体文本3 粗斜体文本4 粗斜体文本5 粗斜体文本6\n\n\n\n# 2.4 斜体包含粗体\n\n * 斜体中包含粗体 的格式：\n   \n   1. * + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + *\n   2. _ + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + _ （ 这里是两个 _ )\n   3. * + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + *\n   4. _ + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + _\n\n * 说明：\n   \n   * 斜体 中包含 粗体，其实就是嵌套的关系，外层 是 斜体，内层 是 粗体\n   * 外层是斜体，标识符是单个；内层是粗体，标识符是两个\n   * 因为 粗体 是被包裹在 斜体 中的，所以显示效果为 斜粗体\n\n这是一段普通文本\n\n*这里是一段斜体中**包含粗体**的文字*\n_这也是一段斜体中**包含粗体**的文字_\n*这又是一段斜体中__包含粗体__的文字*\n_这还是一段斜体中**包含粗体**的文字_\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体中包含粗体的文字 这也是一段斜体中包含粗体的文字 这又是一段斜体中__包含粗体__的文字 这还是一段斜体中包含粗体的文字\n\n\n\n# 2.5 粗体包含斜体\n\n * 粗体中包含斜体 的格式：\n   1. ** + 粗体文本 + * + 斜体文本 + * + 粗体文本 + **\n   2. __ + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + __ （ 这里是两个 _ )\n   3. ** + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + **\n   4. __ + 粗体文本 + * + 斜体文本 + * + 粗体文本 + __\n * 说明：\n   * 粗体 中包含 斜体，也就是嵌套的关系，外层 是 粗体，内层 是 斜体\n   * 外层是粗体，标识符是两个；内层是斜体，标识符是单个\n   * 因为 斜体 是被包裹在 粗体 中的，所以显示效果为 粗斜体\n\n这是一段普通文本\n\n**这里是一段粗体中*包含斜体*的文字**\n__这也是一段粗体中_包含斜体_的文字__\n**这又是一段粗体中_包含斜体_的文字**\n__这还是一段粗体中*包含斜体*的文字__\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段粗体中包含斜体的文字 这也是一段粗体中_包含斜体_的文字 这又是一段粗体中_包含斜体_的文字 这还是一段粗体中包含斜体的文字\n\n\n\n\n\n\n# 3. 线\n\n\n\n# 3.1 水平分割线\n\n * 水平分割线由至少 3 个 * 或 - 组成\n\n下面是一条水平分割线：\n---\n***\n\n\n1\n2\n3\n\n\n# 示范\n\n----------------------------------------\n\n----------------------------------------\n\n\n\n# 3.2 文本删除线\n\n * 删除线 的格式：\n   * ~~ + 文本内容 +~~ 首尾各加两个 ~ 波浪号\n\n~~这是一段加了删除线的文本~~\n\n\n1\n\n\n# 示范\n\n这是一段加了删除线的文本\n\n\n\n# 3.3 文本下划线\n\n * 下划线的格式，和 html 是一样的\n   * <u> + 文本内容 + </u>\n\n<u>这是一段加了下划线的文本</u>\n\n\n1\n\n\n# 示范\n\n这是一段加了下划线的文本\n\n\n\n\n\n\n# 4. 列表&引用\n\n\n\n# 4.1 有序列表\n\n * 有序列表 的格式：\n   \n   * 1. + 空格 + 文本内容\n\n * 说明：\n   \n   * 输入文本内容后，敲击 enter 自动补全格式，并进入 下个 有序列表\n   * 若需要在同个列表内，增加 换行显示 的内容 (但不进入下个列表) 敲击 shift + enter ，即可另起一行输入文本\n   * 在有序列表的中间，插入一个新的列表，后面列表的 数字序号 会自动 递进 一层\n   * 即便在源代码模式中修改了数字序号，渲染界面依然是 依照顺序 显示的\n\n1. 这是第一个有序列表 \x3c!-- (enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (enter) --\x3e\n3. 这是第三个有序列表\n\n\n1. 这是第一个有序列表 \x3c!-- (shift + enter) --\x3e\n   这是同个列表下，另起一行的文本内容 \x3c!-- (enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (shift + enter) --\x3e\n   这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 这是第一个有序列表\n\n 2. 这是第二个有序列表\n\n 3. 这是第三个有序列表\n\n 4. 这是第一个有序列表 这是同个列表下，另起一行的文本内容\n\n 5. 这是第二个有序列表 这是同个列表下，另起一行的文本内容\n\n# 补充\n\n * 由于有序列表存在强制排序性，它的数字序号必然是逐一递进的 若你希望内容前的数字，不依照递进顺序排序，或者以 整百，整十数 排序\n * 可以配合无序列表，在无序列表中输入：\n   * 数字 + . + 内容 #注意 点号 与 内容 之间，没有空格 (其实有空格也行，就是会感觉有点奇怪)\n\n- 10.这是无序列表下，整十数排列的内容\n- 20.这是无序列表下，整十数排列的内容\n- 30.这是无序列表下，整十数排列的内容\n\n\n- 100.这是无序列表下，整百数排列的内容\n- 200.这是无序列表下，整百数排列的内容\n- 300.这是无序列表下，整百数排列的内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n效果：\n\n * 10.这是无序列表下，整十数排列的内容\n * 20.这是无序列表下，整十数排列的内容\n * 30.这是无序列表下，整十数排列的内容\n\n\n * 100.这是无序列表下，整百数排列的内容\n * 200.这是无序列表下，整百数排列的内容\n * 300.这是无序列表下，整百数排列的内容\n\n\n\n# 4.2 无序列表\n\n * 无序列表 的格式：\n * - + 空格 + 文本内容\n * 说明：\n   * 输入文本内容后，敲击 enter 自动补全格式，并进入 下个 无序列表\n   * 若需要在同个列表内，增加换行显示的内容 (但不进入下个列表) 敲击 shift + enter ，即可另起一行输入文本\n * 补充：\n   * 在obsidian中，按下 ctrl + enter\n   * 即可快速生成一个无序列表\n\n- 这是第1个无序列表 \x3c!-- (enter) --\x3e\n- 这是第2个无序列表 \x3c!-- (enter) --\x3e\n- 这是第3个无序列表\n\n- 这是第一个无序列表 \x3c!-- (shift + enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n- 这是第二个无序列表 \x3c!-- (shift + enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n * 这是第1个无序列表\n * 这是第2个无序列表\n * 这是第3个无序列表\n\n\n * 这是第一个无序列表 这是同个列表下，另起一行的文本内容\n * 这是第二个无序列表 这是同个列表下，另起一行的文本内容\n\n\n\n# 4.3 引用\n\n * 引用 的格式：\n   * > + 文本内容 （不需要空格)\n * 说明：\n   * 同个引用段落内的换行直接敲击 enter 即可\n   * 若需添加 第二个独立引用段落 ，连续敲击 两下 enter 即可\n\n>这是第一段引用文本的第1行 \x3c!-- (enter) --\x3e\n>这是第一段引用文本的第2行 \x3c!-- (enter) --\x3e\n\x3c!-- (enter) --\x3e\n>这是第二段引用文本的第1行 \x3c!-- (enter) --\x3e\n>这是第二段引用文本内第2行\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n> 这是第一段引用文本的第1行 这是第一段引用文本的第2行\n\n> 这是第二段引用文本的第1行 这是第二段引用文本的第2行\n\n\n\n# 4.4 缩进&退格\n\n在列表和引用的书写过程中，我们需要利用 ==缩进== 与 ==退格== ，让文章肌理分明，更具层级\n\n * 缩进：\n   \n   1. tab\n   2. ctrl + [   (左中括号)\n\n * 退格：\n   \n   1. shift + tab\n   2. ctrl + ] （右中括号）\n\n\n# 4.4.1 有序列表的缩&退\n\n1. 第一级有序列表1 \x3c!-- (enter) --\x3e\n\t1. 第二级有序列表1    \x3c!-- 写文本之前，先( tab 或 ctrl + ] ) ；写完文本后，再(enter) --\x3e\n\t2. 第二级有序列表2 \x3c!-- (enter) --\x3e\n2. 第一级有序列表2    \x3c!-- 写文本前，先 ( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n * 补充说明：\n   * 有序列表的数字序号，即便你在源代码模式里 强行改掉 数字，它仍然会 依照顺序 显示\n\n# 示范\n\n 1. 第一级有序列表1\n    1. 第二级有序列表1\n    2. 第二级有序列表2\n 2. 第一级有序列表2\n\n\n# 4.4.2 无序列表的缩&退\n\n- 第一级无序列表1 \x3c!-- (enter) --\x3e\n\t- 第二级无序列表1  \x3c!-- 写文本前，先( tab 或 ctrl + ] ) ；写完后，再(enter) --\x3e\n\t- 第二级无序列表2 \x3c!-- (enter) --\x3e\n- 第一级无序列表2  \x3c!-- 写文本前，先 ( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n * 第一级无序列表1\n   * 第二级无序列表1\n   * 第二级无序列表2\n * 第一级无序列表2\n\n\n# 4.4.3 引用的缩&退\n\n * 引用的 缩进 和列表 不同\n   * 引用需另起一行，并额外多打一个 > 来完成 缩进\n * 引用的 退格 与列表 相同\n   1. shift + tab\n   2. ctrl + ] （右中括号）\n\n>第一级引用1 \x3c!-- (enter) --\x3e\n>>第二级引用1 \x3c!-- 先打1个 > (这里的第一个 > 是会自动补充的，只需额外增补1个即可) ，再(enter) --\x3e\n>>第二级引用2 \x3c!-- (enter) --\x3e\n>第一级引用2   \x3c!-- 写文本前，先 ( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n> 第一级引用1\n> \n> > 第二级引用1 第二级引用2\n> \n> 第一级引用2\n\n\n * 补充： 在 obsidian 中，引用的退格是不太一样的\n * **obsidian **中，如果想让已经缩进的引用 退回一层\n   * 得使用 shift + enter ，配合方向键，在多个 > 之间灵活断行 并在下一行 根据需要 选择性补充 >\n * 这个用文字比较难以描述，这里选择用2个带键位的 gif图 来描述\n\ngif演示1：\n\n\n\n\n\n * 效果1：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> > \n> > 444\n> \n> 555\n\n\ngif演示2：\n\n\n\n\n\n * 效果2：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> \n> > 444\n> > \n> > > 555\n> \n> 666\n\n777\n\n\n# 4.4.4 有序&无序&引用 连续套娃\n\n * 有序列表、无序列表、引用 三者之间，可以相互嵌套\n * 核心键 ： shift + enter & enter & shift + tab ( 或 ctrl + [ )\n   * shift + enter 在切换格式的嵌套中，是 自带一层 缩进 效果的\n\n1. 第一级 有序列表1 \x3c!-- (shift + enter) --\x3e\n\t- 第二级 无序列表1 \x3c!-- (shift + enter) --\x3e\n\t\t>第三级 引用1  \x3c!-- (enter) --\x3e\n\t\t\t- 第四级 无序列表2 \x3c!-- (shift + enter) --\x3e\n            \t1. 第五级 有序列表2 \x3c!-- (enter) --\x3e\n            - 第四级 无序列表3   \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) ；写完后再 (enter) --\x3e\n        >第三级 引用2  \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) ；写完后再 (enter × 2) --\x3e\n    - 第二级 无序列表4  \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) --\x3e\n2. 第一级 有序列表3  \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 第一级 有序列表1\n    \n    * 第二级 无序列表1\n      \n      > 第三级 引用1\n      > \n      >  * 第四级 无序列表2\n      >    1. 第五级 有序列表2\n      >  * 第四级 无序列表3\n      > \n      > 第三级 引用2\n    \n    * 第二级 无序列表4\n\n 2. 第一级 有序列表3\n\n# 4.4.5 obsidian 的一些缩退问题\n\n * obsidian 在列表首行使用缩进的时候，后续的列表会出现一些问题\n   * tab 和 shift + tab 会无法 缩进 退格\n     * 可以使用 ctrl + ] 与 ctrl + [ 来解决问题\n\n- - 这是第一段就被缩进的列表\n\t- 这是第二段被再次缩进的列表  \x3c!-- 这里需按两次 ctrl + ] ,tab键是无效的 --\x3e\n  - 这是第三段列表  \x3c!-- ctrl + [ --\x3e\n\n\n1\n2\n3\n\n * * 这是第一段就被缩进的列表 - 这是第二段被再次缩进的列表\n     * 这是第三段列表\n\n\n\n\n\n\n# 5. 网页链接与图像\n\n\n\n# 5.1 网页链接\n\n * 网页链接的 格式：\n   * [ + 显示文本内容 + ] + ( + 链接地址 + 空格 + " + 提示信息文本 + " + )\n * 说明：\n   * 显示文本内容，是在渲染界面实际 可见 的文本，用以 说明 链接\n   * 提示信息文本，需鼠标悬停于 显示文本内容 方可触发，用于增加额外提示信息\n     * #注意 "提示信息文本" 是可选项，一般不会填\n     * 一般来讲，需按住 ctrl + 鼠标左键点击 才可跳转链接，不过也有 直接鼠标点击 就能跳转的\n\n[显示文本内容](链接地址 "提示信息文本")\n\n[百度一下，你就知道](http://www.baidu.com "按住ctrl点击跳转百度")\n\n\n1\n2\n3\n\n\n示范：\n\n百度一下，你就知道\n\n\n# 5.1.1链接的加粗\n\n * 格式有两种：\n   \n   1. 把一对 ** 加在 ==显示文本内容==的首尾\n      \n      * 格式1：[**显示文本内容**](链接地址)\n      * 效果： 百度一下，你就知道\n   \n   2. 把一对 ** 加在 链接格式==整体== 的首尾\n      \n      * 格式2：**[显示文本内容](链接地址)**\n      * 效果： 百度一下，你就知道\n\n\n\n\n\n\n# 5.2 图像\n\n * 图像格式：\n   * 图像格式，就是在网页链接前面加个 ! (英文格式的)，! 代表 可见\n   * 图片的提示信息，和网页链接一样，写在 " " 内\n   * [ ] 方括号里的文字信息在 markdown 没啥实质的作用，只是方便在源代码模式下，知道这个图片是什么，在渲染界面是不会显示的。有点类似于html img标签 里的 alt属性。\n\n![文字信息](图片链接 "提示文本信息")\n\n![湘湖1](https://z3.ax1x.com/2021/08/06/funkxq.jpg "湘湖一角")\n\n\n1\n2\n3\n\n\n * 补充：\n   \n   * 图像链接可以是本地的，也可以是在线的\n     * 本地图像直接 ctrl + c 黏贴，ctrl + v 复制 就可以\n     * 在线图像推荐使用 图床\n   * 调整图像的大小需要使用 html 和 css，在 typora编辑器 中右键可以直接缩放图片 本质是转成了html的格式，最后会有一个 style="zoom: %;" ，这里数值可以自己修改\n   * 如果有使用 obsidian 的朋友，在线图片链接是通用的。不过，因为 obsidian 是双向链笔记 它的本地图片格式不太一样\n     * ![[图片名]]\n       * obsidian 中的图片是以双链的格式引用在目标笔记中，用 ! 使它可见\n       * obsidian的图片设置大小是用 | 分隔，后面写宽度数值，单位是px。 设定好宽度，高度会自动等比例调整\n         * ![[图片名|宽度数值]] - 若想自主调整图片宽高，则用： - ![[图片名|宽度数值x高度数值]] - #提示 这里的 x 是 英文字母x\n     * 如果是在线图床，需要调整图片大小：\n       * ![图床|宽度数值](链接地址)\n\n# 示范\n\n\n\n\n\n\n\n\n# 6. 表格\n\n * markdown的表格，比html简单很多\n   * | 是构成表格的主要 框架\n   * - 区分 表头 和 表格主体\n   * : 控制 表格内 文本内容 的 对齐方式\n   * **typora编辑器中 ** 输入 ctrl + t 即可快速插入表格，自由定义样式\n\n|这里是表头1|这里是表头2|这里是表头3|\n|:-|:-:|-:|    \x3c!--区分表头和表格主体，:代表文本对齐方式，分别是左对齐，居中对齐，右对齐--\x3e\n|单元格数据1|单元格数据2|单元格数据3|\n|单元格数据4|单元格数据5|单元格数据6|\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这里是表头1   这里是表头2   这里是表头3\n单元格数据1   单元格数据2   单元格数据3\n单元格数据4   单元格数据5   单元格数据6\n\n\n\n# 6.1 表格中文本内容的换行\n\n * mardown中表格，它的宽高是由 单元格数据内的文本内容 撑开 的\n * 当我们输入一段很长很长的文本，它所在的单元格会变得过宽\n\n如下图所示：\n\n表头1                                   表头2\n这是一段很长很长很长很长很长很长很长很长很长很长很长很长很长很长的文本   普通文本\n\n * 若想对一段长文本进行换行，可以在 中间 插入一个 <br> （ 换行标签 )\n\n| 表头1 |  表头2 |\n|:-:|:-:|\n|这是第一行文本<br>这是另起一行的文本|普通文本|\n\n\n1\n2\n3\n\n\n# 示范\n\n表头1         表头2\n这是第一行文本     普通文本\n这是另起一行的文本\n\n\n\n\n\n\n# 7. 代码域\n\n\n\n# 7.1 行内代码\n\n * 行内代码 的格式：\n   * 输入两个 ` 反引号 ，在中间写代码内容\n * 补充：\n   * 行内代码不一定非得写代码，也可以作为**着重标记**，突出显示内容\n   * 行内代码中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 所谓行内代码： 只要你的屏幕足够宽，它就不会换行\n\n`这是一段行内代码`\n\n`<table border="1" cellspacing="0" width="500" height="500">`\n\n`print("hello, world!")`\n\n`这是一行突出显示的文本内容`\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 示范\n\n<table border="1" cellspacing="0" width="500" height="500">\n\n\nprint("hello, world!")\n\n\n这是一行突出显示的文本内容\n\n\n\n# 7.2 代码块\n\n * 代码块 的格式：\n   1. 在首行和末行各加 三个 ` 反引号\n   * ``` + 语言种类 代码内容 ```\n   2. 在首行和末行各加 三个 ~ 波浪号\n      * ~~~ + 语言种类 代码内容 ~~~\n * 补充：\n   * 在代码块也不一定要写代码，可以写一段突出的文本内容，语言类型可以填写 txt 或者 干脆不写\n   * 代码块中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 在 typora编辑器 ，用键盘按键脱离代码块区域，需输入： ctrl + enter\n\n```语言种类\n代码内容\n代码内容\n代码内容\n```\n\n下面是html代码块\n\n```html\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n```\n\n下面是css代码块\n\n```css\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%,red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n```\n\n下面是javascript代码块\n\n```js\n    // 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n        sum += arr.shift();\n        count++;\n        if (count % 5 === 0) {\n            newarr.push(sum / 5);\n            sum =  0;\n        }\n    }\n    console.log(newarr);\n\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, len = arr.length; i < len / 5; i++) {\n        let subarr = arr.splice(0, 5)\n        for (let j = 0, sum = 0; j < subarr.length; j++) {\n            sum += subarr[j];\n        }\n        newarr.push(sum / 5);\n    }\n    console.log(newarr);\n```\n\n\n下面是python代码块\n\n```python\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "good bye!"\n```\n\n下面是一块突出显示的文本\n\n```txt\n这是一段\n突出显示的\n文本内容\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n# 示范\n\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%, red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n// 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n\tsum += arr.shift();\n\tcount++;\n\tif (count % 5 === 0) {\n\t\tnewarr.push(sum / 5);\n\t\tsum =  0;\n\t}\n}\nconsole.log(newarr);\n\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, len = arr.length; i < len / 5; i++) {\n\tlet subarr = arr.splice(0, 5)\n\tfor (let j = 0, sum = 0; j < subarr.length; j++) {\n\t\tsum += subarr[j];\n\t}\n\tnewarr.push(sum / 5);\n}\nconsole.log(newarr);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "good bye!"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n这是一段\n突出显示的\n文本内容\n\n\n1\n2\n3\n\n\n\n# 7.2.1 代码块的嵌套\n\n\n格式：\n\n * 使用4个 ` 包裹 3个 `\n\n# 示范\n\n````txt\n```js\n// 3. 输出 100以内(不包括100) 所有偶数的和\n// 这类求和问题的核心 ： 利用循环  (总和 = 旧数的和 + 新数)\n\nlet sum = 0;\n\nfor (let i = 1, sum = 0; i < 100; i++) {\n if (i % 2 == 0) {\n // 筛选偶数\n sum += i; // sum = sum + i // 累加偶数并赋值给sum\n // sum为(旧的，已经进入循环的数)的和，i 为新进入循环的数。当加到(最后一个新数i)时，sum就是最后的 总和\n }\n}\n\nconsole.log(sum); // 打印总和\n```\n````\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n如果要再套一层，就在最外层 加 5个 ` ，以此类推……\n\n\n\n# 7.3 如何在行内代码里显示反引号\n\n首尾各用 两个反引号`+ 空格 包裹\n\n格式：\n\n``+空格+带`的内容+空格+``  \x3c!-- 不要忘记前后的两个空格 --\x3e\n\n`` 这是一段能显示`反引号`的行内代码 ``\n\n\n1\n2\n3\n\n\n效果：\n\n这是一段能显示`反引号`的行内代码\n\n\n\n\n\n\n# 8. 任务列表（待办）\n\n * 任务列表 的格式：\n   \n   * - + 空格 +[ ] +空格 + 任务列表内容 ( 中括号[ ] 里面必须有个空格)\n   * 给待办任务列表打 √ ，变成 已办\n     1. 在渲染界面，直接鼠标左键点击框框\n     2. 在源代码界面，在中括号内输入 英文字母x\n        * 部分编辑器，在 中括号内 输入任意字符都可以打 √ ( 例如 obsidian )\n\n * 补充：\n   \n   * 大部分 md编辑器 支持输入第一个任务列表后，按下 enter 进入下一行会 自动补全待办格式\n   * 在obsidian中，连续输入两次 ctrl + enter ，即可生成一个待办列表\n     * 再输入一次 ctrl + enter ，会在待办列表 打 √\n\n * 格式：\n\n- [ ] 待办任务列表1\n- [ ] 待办任务列表2\n- [x] 已办任务列表1    \x3c!-- 英文字母x --\x3e\n- [x] 已办任务列表2\n\n\n1\n2\n3\n4\n\n\n\n# 示范\n\n * [ ] 待办任务列表1\n * [ ] 待办任务列表2\n * [x] 已办任务列表1\n * [x] 已办任务列表2\n\n\n * 在 obsidian 中，可以利用 ctrl + enter ，快速生成任务列表\n   1. - + 空格 + ctrl + enter +待办文本内容\n   2. 待办文本内容 + ctrl + enter ×2   ( 输入文本后，连续2次 ctrl + enter )\n\n\n * 任务列表也是可以缩进+退格的，操作跟 无序、有序列表一样\n\n\n# 示范\n\n * [ ] 第一级待办列表1\n   * [ ] 第二级待办列表1 另起一行的第二级待办列表1\n     * [x] 第三级已办列表1\n     * [x] 第三级已办列表2\n   * [ ] 第二级待办列表2 另起一行的第二级待办列表2\n * [ ] 第一级待办列表2\n\n\n\n\n\n\n# 9. 注释\n\nmarkdown 的 注释 和 hmtl 一样，注释的内容在 渲染界面 不可见 （部分编辑器可见)\n\n * 注释 的格式：\n   * \x3c!-- 这里是注释的内容 --\x3e\n     * 注释可以是单行，也可以是多行\n   * 如果有在使用 obsidian 的，它的注释格式是不一样的\n     * %%这是obsidian的注释内容%%\n\n\x3c!-- 这里是一行注释 --\x3e\n\n\x3c!--\n这里是\n一段\n假装有\n很多行的\n注释\n--\x3e\n\n%%这是一行obsidian里的注释%%\n\n%%\n这里是\n一段\n假装有\n很多行的\nobsidian里的\n注释\n%%\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 示范 (只有切换至 编辑模式 才能看到喔)\n\n%%这是一行obsidian里的注释%%\n\n%% 这里是 一段 假装有 很多行的 obsidian里的 注释 %%\n\n\n\n\n\n\n# 10. 变量\n\n\n\n# 10.1 网页链接变量\n\n * 网页链接变量 的格式：\n   1. 首先输入\n      * [显示文本内容] + [变量名]\n        * 变量名可以自己取，没啥限制，任意字符都可以\n   2. 在文档任意一个区域，输入：\n      * [变量名] + : + 空格 + 链接地址 （这个**空格** 不打也没事)\n\n[百度一下，你就知道][度娘]\n[知乎-有问题，就会有答案][知乎]\n\n\x3c!-- 这里是变量区域 --\x3e\n[度娘]: http://www.baidu.com\n[知乎]: https://www.zhihu.com\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n百度一下，你就知道\n\n知乎-有问题，就会有答案\n\n\n\n# 10.2 脚注\n\n * 脚注 的格式：\n   * 在需要脚注的地方，输入：\n     * [^脚注代号] ( 脚注代号会直接显示在渲染界面 )\n       * 脚注代号可以随便命名，不过推荐使用 数字序号\n   * 在其他区域，输入：\n     * [^脚注代号] + : + 空格 + 脚注内容 （这个 空格 不打也没事)\n\n鲁迅原名是什么[^1] ，浙江哪里人[^2]\n\n\x3c!-- 这里是变量区域 --\x3e\n[^1]: 周树人\n[^2]: 绍兴人\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n鲁迅原名是什么^1，浙江哪里人^2\n\n\n\n\n\n\n# 11. 拓展文本格式标记\n\n * markdown 想实现更多的文本显示效果，只能依赖html标记实现\n * 个人不是很推荐在 md 中使用 html，不过一些简单的标记还是可以 轻度使用 的\n\n\n\n# 11.1 键盘文本\n\n * 键盘文本的 格式：\n   \n   * <kbd>键盘文本</kbd>\n   * <kbd>ctrl</kbd> + <kbd>x</kbd>\n\n * 效果：\n   \n   * 键盘文本\n   * ctrl + x ( 剪切 )\n\n * 说明：\n   \n   * 键盘文本也不一定非得是键盘按键，也可以作为着重文本突出显示\n     * 效果： 这也算一种着重文本的方式\n\n# 11.1.1 加粗键盘文本\n\n * 加粗键盘文本的格式有两种：\n   \n   * <kbd>**键盘文本**</kbd>\n   * **<kbd>ctrl + x</kbd>**\n\n * 效果：\n   \n   1. 键盘文本\n   2. ctrl + x\n\n\n\n# 11.2 放大文本\n\n * 放大文本 的格式：\n   \n   * 这是一段普通文本 <big>这是一段放大文本</big>\n\n * 效果：\n   \n   * 这是一段普通文本 这是一段放大文本\n\n# 11.2.1 放大粗体文本\n\n * 放大加粗文本的格式有两种：\n   1. **<big>这是一段放大粗体文本</big>**\n   2. <big>**这是一段放大粗体文本**</big>\n * 效果：\n   1. 这是一段放大粗体文本\n   2. 这是一段放大粗体文本\n\n\n\n# 11.3 缩小文本\n\n * 缩小文本 的格式：\n   * 这是一段普通文本 <small>这是一段缩小文本</small>\n * 效果：\n   * 这是一段普通文本 这是一段缩小文本\n\n# 11.3.1 缩小斜体文本\n\n * 斜体缩小文本 的格式有两种：\n   1. <small>*这是一段缩小斜体文本*</small>\n   2. *<small>这是一段缩小斜体文本</small>*\n * 效果：\n   1. 这是一段缩小斜体文本\n   2. 这是一段缩小斜体文本\n\n\n\n# 11.4 多彩文本\n\n * 多彩文本 的格式：\n   * <font color=orange>这是一段橘色文本</font>\n * 效果：\n   * 这是一段橘色文本\n     * color 里的颜色支持 英文单词，16进制，rgb，rgba\n\n\n# 11.4.1 多彩粗体文本\n\n * 只需要在上面示例的基础上，加上 加粗标识符，有两种格式：\n   1. 格式1： **<font color=teal>这是一段加粗的水鸭色文本</font>**\n      * 效果： 这是一段加粗的水鸭色文本\n   2. 格式2： <font color=teal>**这是一段加粗的水鸭色文本**</font>\n      * 效果： 这是一段加粗的水鸭色文本\n * 若上述混搭方法的样式失效 ，可以使用 纯html标记\n   * 格式： <strong style="color:teal;">这是一段加粗的水鸭色文本</strong> (标记略复杂，不是很推荐)\n   * 效果： 这是一段加粗的水鸭色文本\n\n\n# 11.4.2 多彩斜体文本\n\n * 跟多彩加粗文本完全一样，只需把首尾的 ** 换成 * 即可\n\n 1. 格式1： *<font color=teal>this is an italic teal text</font>*\n    * 效果： this is an italic teal text\n 2. 格式2： <font color=teal>*this is an italic teal text*</font>\n    * 效果： this is an italic teal text\n\n\n# 11.4.2 多彩粗斜体文本\n\n * 首尾换成 ***\n\n 1. 格式1： ***<font color=teal>this is a bold italic teal text</font>***\n    * 效果： this is a bold italic teal text\n 2. 格式2： <font color=teal>***this is a bold italic teal text***</font>\n    * 效果： this is a bold italic teal text\n\n\n#注意 多彩文本尽量慎用，markdown 的核心就是 简洁精炼，注重 实质内容，而非花哨的 颜色样式\n\n\n\n\n\n\n# 12. 拓展文本显示效果\n\n * 拓展显示效果既不是原生 markdown语法 支持的，也非 html标记，而是部分编辑器 提供的 额外标识符，属于拓展语法，旨在为 markdown使用者 提供更多样式选择\n * 不同编辑器，支持不一样，这里以 typora编辑器 为例\n\n\n\n# 12.1 文本高亮\n\n * 文本高亮 的格式：\n   * ==这里是一段高亮文本==\n * 效果：\n   * ==这里是一段高亮文本==\n\n\n\n# 12.2 上标\n\n * 用一对 ^ 包裹 (shift+ 6)\n   * 格式： x^2^\n   * 效果： x^2^\n * obsidian 没效果的，可以用后面会讲的 latex\n * 或者，也可以使用 html标记\n   * <sup>这里是上标内容</sup>\n   * x<sup>2</sup>\n * 效果：\n   * x2\n\n\n\n# 12.3 下标\n\n * 用一对 ~ 包裹 (shift + `)\n   * 格式： h~2~o\n   * 效果： h~2~o\n * obsidian 没效果的，可以用后面会讲的 latex\n * 或者，也可以使用 html标记\n   * <sub>这里是下标内容</sub>\n   * h<sub>2</sub>o\n * 效果：\n   * h2o\n\n\n\n# 12.4 emoji 符号\n\n用一对 : 包裹，里面是 emoji 符号的 语义化文本 ( typora编辑器 中，输入 : 就会带提示器 )\n\n * 示例：\n   * :smile: :sweat: :cat: :woman_cartwheeling:\n * 效果：\n   * 😄 😓 🐱 🤸‍♀\n\n\n * 补充：\n   * 不支持上述方式的 md编辑器或笔记软件，直接用 输入法 输入也是可以的\n   * windows系统 用户 win + . 就可以输入 emoji 了\n   * obsidian 用户可以安装第三方插件来支持 emoji 的输入，推荐两个\n     1. ==emoji shortcodes==\n     2. ==emoji toolbar==\n\n\n\n\n\n\n# 13. 转义字符\n\n * 在 markdown 中，我们 通过 标识符 改变 文本显示效果\n * 现在我们希望它不作为标识符，而是 作为字符本身呈现出来 （不具备改变文本显示效果的功能，只是一个普通字符)\n   * 首先我们可以用前面介绍的 代码域 ，因为代码模式的显示效果就是源代码完全一致的\n   * 还有一种方法，可以利用转义字符，在这些标识符 前面 加上 反斜线 \\ ( 反斜线要紧贴在标识符前面，不能 有 空格 )\n     * 原理：\n       * \\ 的作用是让标识符 转义 变为一个普通字符，完成这个效果后，反斜线会自动隐藏\n       * 隐藏后的反斜线仅在源代码界面可见，在渲染界面不可见\n       * 反斜线只争对标识符起作用，其他字符添加 \\，\\ 不会自动隐藏\n     * 补充：\n       * 如果想给已经被加在标识符前面，会自动隐藏的 \\ 显示出来，可以在反斜线前面再加一个 \\ ，用它自己来转义自己\n         * 示例： 这里紧跟在标识符前面的反斜线\\\\*会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体*\n         * **效果： ** 这里紧跟在标识符前面的 反斜线\\会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体\n\n\n\n# 例1 以普通字符显示星号\n\n * 如何让被一对或多对 * 号 包裹的文本内容，能够正常显示 * ，且文本不改变格式\n   * \\*这段文本被一对星号包裹，但不会倾斜\\*\n     * 效果： *这段文本被1对星号包裹，但不会倾斜*\n   * \\*\\*这段文本被2对星号包裹，但不会加粗\\*\\*\n     * 效果： **这段文本被2对星号包裹，但不会加粗**\n   * \\*\\*\\*这段文本被3对星号包裹，但它既不倾斜也不加粗\\*\\*\\*\n     * 效果： ***这段文本被3对星号包裹，但它既不倾斜也不加粗***\n\n\n\n# 例2 表格内 单元格中的竖杠\n\n * 在表格中，使用 | 作为单元格的内容，但不会被识别为表格的结构，不会增加额外的单元格\n\n|表头1|表头2|\n|-|-|\n|这里的文本被\\|分隔|这里的文本也被\\|分隔|\n\n\n1\n2\n3\n\n * 效果：\n\n表头1         表头2\n这里的文本被|分隔   这里的文本也被|分隔\n\n\n#补充 该技巧可用于 obsidian 表格内 双链的文本修饰\n\n文本修饰：\n\n在 双链[[ ]]内 以 | 引导的内容\n\n * 格式： [[链接的内容|文本修饰]]\n * 说明： 文本修饰是渲染界面实际显示的文本，便于更好地融入语境\n\n表格内的格式：\n\n在 | 前面加上 \\\n\n * [[表格内的链接内容\\|文本修饰]]\n\n示例：\n\n|                  表头1                  |                        表头2                        |\n|:---------------------------------------:|:---------------------------------------------------:|\n| [[#例2 表格内 单元格中的竖杠\\|单元格中的竖杠]] | [[#例3 不会变成代码的反引号\\|不会变成代码的反引号]] |\n\n\n1\n2\n3\n\n\n效果：\n\n表头1                           表头2\n[[#例2 表格内 单元格中的竖杠|单元格中的竖杠]]   [[#例3 不会变成代码的反引号|不会变成代码的反引号]]\n\n\n\n# 例3 不会变成代码的反引号\n\n使用 转义符号\\ 让 反引号` 变成普通字符，不再具有[[#7 1 行内代码|行内代码]]的标识符功能\n\n格式：\n\n\\`这段被反引号包裹的内容不会变成行内代码\\`\n\n效果：\n\n`这段被反引号包裹的内容不会变成行内代码`\n\n\n\n# 例4 链接中的中括号\n\n在 网页链接 的 显示文本内容 中，使用 中括号 [ ]\n\n * 在显示文本内容中，在其中一个中括号前面，加上转义符号 反斜杠 \\\n   * 格式： [链接里的 \\[中括号\\] 能被正常显示](https://www.runoob.com)\n   * 效果： 链接里的 [中括号] 能被正常显示\n\n\n\n# 例5 不是列表的连接符(横杠)\n\n * 引用一段话，一般会在换行之后，加上 - 出处\n * 因为 - 是标识符，会变成一个无序列表\n\n如下所示：\n\n> the web, the tree, and the string. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n> \n>  * 史蒂芬·平克\n\n * 解决方法：\n   \n   * 在 - 前面加上 转义符号 \\\n   \n   >the web, the tree, and the string.\n   >写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n   >\\- 史蒂芬·平克   \x3c!-- 加上转义符号 \\ , 不会变成无序列表 --\x3e\n   \n   \n   1\n   2\n   3\n   \n\n * 效果：\n\n> the web, the tree, and the string. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。 - 史蒂芬·平克\n\n\n\n# 例6 不是标题的 #\n\n让 # 不被识别为标题标识符\n\n格式：\n\n\\# 这里的内容不会被识别为标题\n\n效果：\n\n# 这里的内容不会被识别为标题\n\n\n\n# 例7 不会注释的 %\n\n在 obsidian 中 注释是前后各两个 % 号\n\n使用 转义符号\\，让 %% 作为普通字符显示出来，不具备注释的功能\n\n * 格式： \\%\\%这里的内容可以被显示喔\\%\\%\n * 效果： %%这里的内容可以被显示喔%%\n\n\n\n# 例8 木有链接的双链\n\nobsidian 的双向链格式是2个方括号 [[ ]] (双方)，使用 转义符号\\，让 [ ] 不再具有 双链功能\n\n格式：\n\n\\[\\[这段文本被双方包裹，但不是一个双向链\\]\\]\n\n效果：\n\n[[这段文本被双方包裹，但不是一个双向链]]\n\n\n\n# 例9 页链接里 显示文本内的 中括号\n\n使用转义符号\\，让中括号可以作为显示文本 在[[#5 1 网页链接|网页链接]]中显示出来\n\n格式：\n\n[\\[这是一个带中括号的网页链接显示文本，点击会跳转至百度\\]](https://www.baidu.com/)\n\n\n1\n\n\n效果：\n\n[这是一个带中括号的网页链接显示文本，点击会跳转至百度]\n\n\n\n# 特殊情况 文本修饰的中括号\n\n文本修饰的 中括号[ ] 不需要使用 转义符号\\\n\n示范：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n效果：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n\n\n\n\n\n# 14. 空格&换行&强制删除\n\n\n\n# 14.1 空格\n\n * 在一些编辑器或者支持md的笔记软件里，无论你打多少个空格，它只会显示单个 空格 的距离\n   * 可以使用 html中 空格 的 字符实体 —— &nbsp;\n   * 若要添加 多个 空格，就输入多个 —— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n * 格式：\n   * 这里有&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6个空格分隔\n * 效果：\n   * 这里有      6个空格分隔\n\n\n\n# 14.2 换行\n\n场景1：\n\n * 在一些编辑器或者支持md的笔记软件里，无论你打多少个 回车，它只会显示单个 回车 的空行间距\n   * 可以使用之前表格里提到的 <br> 标签，在 单独一行 中使用，增加额外的空行间距\n   * 如果要增加 多个，就输入 多个 —— <br><br><br><br><br>\n   * #注意 当单独一行使用 <br> 标签的时候，如果前后有标题标识符或者列表标识符，确保 br元素 前后两行都是空白行\n\n格式：\n\n这里是第一段文本\n\n<br><br><br><br><br>     \x3c!-- 这里插入了5个空行间距 --\x3e\n\n这里是第二段文本\n\n\n1\n2\n3\n4\n5\n\n\n效果：\n\n这里是第一段文本\n\n\n\n\n\n\n\n\n这里是第二段文本\n\n\n\n\n\n场景2：\n\n * 在列表中也可以插入换行符\n\n- 这是一段无序列表\n  <br>     \x3c!-- 插入一个空行间距，需单独一行，上下不用预留空格 --\x3e\n  这是同一段无序列表中，空一行距离显示的内容\n- 这是第二段无序列表\n\n\n1\n2\n3\n4\n\n\n效果：\n\n * 这里是第一段无序列表\n   这里是同一段无序列表中，空一行距离显示的内容\n * 这里是第二段无序列表\n\n\n * 补充：\n   * 有一些md编辑器或笔记软件，严格遵循md的换行规则，你敲一个回车是没法换行的，必须在 行末 敲 2个空格，再按回车键\n     * 格式：\n       * 这里是一段想换行的文本空格 空格 enter 这是换行后的文本\n\n\n\n# 14.3 强制删除\n\n * 很多编辑器都有英文标点自动补全功能，自动生成一对，光标落在中间 只想删除前面1个，却会把 一整对 都删掉\n * 在多个列表的嵌套中，也许会遇到一些 无法被删除 的 列表标识符\n * 解决方法： 使用 shift + backspace 即可强制删除\n   * bcakspace   ( 退格键 )\n\n\n\n\n\n\n# 15. 嵌入\n\n * 嵌入都是依赖 html标签 实现的，嵌入的都是在线链接格式\n   * 如果是本地的，obsidian 中音频是有自带的可录制的录音机插件的，其他的 音频、视频 直接复制黏贴就可以了，也可以直接拖拽到ob的笔记界面\n     * 其他的媒体文件在 obsidian 也和图片一样，以双链的格式引用在目标笔记中，使用 ! 使它可见\n\n\n\n# 15.1 嵌入音频\n\n * 格式：\n   \n   * <audio controls="controls" preload="none" src="音频链接地址"></audio>\n\n * 示例：\n\n<audio controls="controls" preload="none" src="https://www.ldoceonline.com/media/english/exaprons/p008-001803372.mp3?version=1.2.37"></audio>\n\n\n1\n\n * 效果：\n\n\n\n\n\n# 15.2 嵌入视频\n\n * 格式：\n\n<video width="600" height="420" controls>\n  <source src="movie.mp4" type="video/mp4">\n  <source src="movie.ogg" type="video/ogg">\n  <source src="movie.webm" type="video/webm">\n</video>\n\n\n1\n2\n3\n4\n5\n\n * 说明：\n   * width ( 宽度 ) height ( 高度 ) ，可以自己设置，直接输入数字即可，单位默认是 px(像素) 也可以使用 百分比 width=100% 代表水平撑满整个窗口 height=50% 代表垂直撑满半个窗口\n   * video标签 支持的视频格式 ：mp4 ogg webm\n\n\n\n# 15.3 嵌入页面\n\n * 格式： <iframe width=600 height=400 src="页面链接地址" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n<iframe width=600 height=400 src="https://www.runoob.com/html/html-tutorial.html" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 效果：\n\n\n * iframe标签 除了嵌入页面，也可以嵌入在线视频，主流的视频网站都会提供嵌入代码\n   \n   * 具体可以看这个 iframe视频嵌入教程\n   * b站 的视频，得在 // 前面补充 http:\n   * 不是所有的 编辑器和笔记软件 都支持这个\n\n * 示例：\n\n<iframe width=600 height=400 src="http://player.bilibili.com/player.html?aid=20190823&bvid=bv1yw411s7og&cid=32964980&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 宽高设置和前面的 video 一样\n\n\n * 效果：\n\n\n\n\n\n\n# 16. latex 数学公式\n\n * 主要用于 数学公式 与 化学公式 的书写\n\n\n\n# 16.1 行内公式\n\n * 格式：\n   \n   * $ + 行内公式 + $\n\n\n * 示例：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $\\ce{co2 + c -> 2 co}$\n   * $\\ce{co2 + c -> 2 co}$\n   * $\\ce{2mg + o2 ->[燃烧] 2 mgo}$\n\n\n * 效果：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $e^{i\\pi} + 1 = 0$\n   * $\\ce{co2 + c -> 2 co}$\n   * $\\ce{2mg + o2 ->[燃烧] 2 mgo}$\n\n\n\n# 16.2 公式块\n\n * 格式：\n   * $$ 公式块 $$\n\n\n * 示例：\n\n% 化学公式\n$$\n\\ce{zn^2+  <=>[+ 2oh-][+ 2h+]  $\\underset{\\text{amphoteres hydroxid}}{\\ce{zn(oh)2 v}}$  <=>[+ 2oh-][+ 2h+]  $\\underset{\\text{hydroxozikat}}{\\ce{[zn(oh)4]^2-}}$}\n$$\n\n\n1\n2\n3\n4\n\n\n% 麦克斯韦方程组\n$$\n\\begin{array}{lll}\n\\nabla\\times e &=& -\\;\\frac{\\partial{b}}{\\partial{t}}\n\\ \\nabla\\times h &=& \\frac{\\partial{d}}{\\partial{t}}+j\n\\ \\nabla\\cdot d &=& \\rho\n\\ \\nabla\\cdot b &=& 0\n\\ \\end{array}\n$$\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n% 薛定谔方程\n$$\ni\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + v \\psi\n$$\n\n\n1\n2\n3\n4\n\n\n * 效果：\n\n$$ % 化学公式 \\ce{zn^2+ <=>[+ 2oh-][+ 2h+] $\\underset{\\text{amphoteres hydroxid}}{\\ce{zn(oh)2 v}}$ <=>[+ 2oh-][+ 2h+] $\\underset{\\text{hydroxozikat}}{\\ce{[zn(oh)4]^2-}}$} $$\n\n\n$$ % 麦克斯韦方程组 \\begin{array}{lll} \\nabla\\times e &=& -;\\frac{\\partial{b}}{\\partial{t}} \\ \\nabla\\times h &=& \\frac{\\partial{d}}{\\partial{t}}+j \\ \\nabla\\cdot d &=& \\rho \\ \\nabla\\cdot b &=& 0 \\ \\end{array} $$\n\n\n$$ i\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + v \\psi $$\n\n * 补充：\n   * 需要详细教程的，可戳下方链接\n   * latex详细教程\n\n\n\n\n\n\n# 17. mermaid\n\n * 一些 md编辑器 和 笔记软件 支持通过 mermaid 及其所提供的 编译器 来为用户提供图表的绘制功能\n\n * 这里只提供一些演示的图表，具体教程可戳下方\n   \n   * [[moc mermiad 教程 obsidian版| mermiad 超级教程 obsidian版]]\n\n\n\n# 17.1 流程图\n\n\n源码1：\n\n```mermaid\ngraph tb\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点2\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点1\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n渲染1：\n\ngraph tb\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点1\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点2\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n源码2：\n\n```mermaid\ngraph lr\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n渲染2：\n\ngraph lr\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n# 17.2 饼图\n\n\n源码：\n\n```mermaid\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n渲染：\n\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n# 17.3 序列图 (时序图)\n\n\n源码：\n\n```mermaid\nsequencediagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant a as aly\n        participant b as bob\n        participant c as cofcai\n        %% 便签说明\n        note left of a: 只复习了一部分\n        note right of b: 没复习\n        note over a,b: are contacting\n\n        a->>b: 明天是要考试吗？\n        b--\x3e>a: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par aska\n                c --\x3e> a:你复习好了吗？\n            and askb\n                c --\x3e> b:你复习好了吗？\n            and self\n                c ->>c:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/every min\n            %% <br/>可以换行\n            c ->> c:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                a ->> c:复习了一点\n            else is common\n                b ->> c:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt extra response\n                c ->> c:你们怎么不回答我\n            end\n        endsequencediagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant a as aly\n        participant b as bob\n        participant c as cofcai\n        %% 便签说明\n        note left of a: 只复习了一部分\n        note right of b: 没复习\n        note over a,b: are contacting\n\n        a->>b: 明天是要考试吗？\n        b--\x3e>a: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par aska\n                c --\x3e> a:你复习好了吗？\n            and askb\n                c --\x3e> b:你复习好了吗？\n            and self\n                c ->>c:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/every min\n            %% <br/>可以换行\n            c ->> c:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                a ->> c:复习了一点\n            else is common\n                b ->> c:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt extra response\n                c ->> c:你们怎么不回答我\n            end\n        end\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\n\n渲染：\n\nsequencediagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant a as aly\n        participant b as bob\n        participant c as cofcai\n        %% 便签说明\n        note left of a: 只复习了一部分\n        note right of b: 没复习\n        note over a,b: are contacting\n\n        a->>b: 明天是要考试吗？\n        b--\x3e>a: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par aska\n                c --\x3e> a:你复习好了吗？\n            and askb\n                c --\x3e> b:你复习好了吗？\n            and self\n                c ->>c:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/every min\n            %% <br/>可以换行\n            c ->> c:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                a ->> c:复习了一点\n            else is common\n                b ->> c:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt extra response\n                c ->> c:你们怎么不回答我\n            end\n        end\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n\n# 17.4 甘特图\n\n\n源码：\n\n```mermaid\ngantt\n    title a gantt diagram\n    dateformat  yyyy-mm-dd\n    section section\n    a task           :a1, 2014-01-01, 30d\n    another task     :after a1  , 20d\n    section another\n    task in sec      :2014-01-12  , 12d\n    another task      : 24d\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n渲染：\n\ngantt\n    title a gantt diagram\n    dateformat  yyyy-mm-dd\n    section section\n    a task           :a1, 2014-01-01, 30d\n    another task     :after a1  , 20d\n    section another\n    task in sec      :2014-01-12  , 12d\n    another task      : 24d\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 17.5 类图\n\n\n源码：\n\n```mermaid\nclassdiagram\n    animal <|-- duck\n    animal <|-- fish\n    animal <|-- zebra\n    animal : +int age\n    animal : +string gender\n    animal: +ismammal()\n    animal: +mate()\n    class duck{\n      +string beakcolor\n      +swim()\n      +quack()\n    }\n    class fish{\n      -int sizeinfeet\n      -caneat()\n    }\n    class zebra{\n      +bool is_wild\n      +run()\n    }\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n渲染：\n\nclassdiagram\n    animal <|-- duck\n    animal <|-- fish\n    animal <|-- zebra\n    animal : +int age\n    animal : +string gender\n    animal: +ismammal()\n    animal: +mate()\n    class duck{\n      +string beakcolor\n      +swim()\n      +quack()\n    }\n    class fish{\n      -int sizeinfeet\n      -caneat()\n    }\n    class zebra{\n      +bool is_wild\n      +run()\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n\n\n# 18. 标签 (tag)\n\n * 标签是 obsidian 特有的一个功能，标签可以通过点击唤起快速搜索 (搜索包含该标签的所有笔记)\n\n格式：\n\n * # + 标签名\n   * #标签名\n\n\n# 关于空格\n\n * 在一段正文文本的后面添加 tag， # 的前面 需要有个空格\n   * 空格 + # + 标签名\n\n\n * # 与 标签名 之间，不能有空格，否则就变成 一级标题 了\n\n\n * 标签名的内部，不允许使用空格，若想区分标签中的词语，可使用以下三种方法：\n   1. 驼峰式大小写： #bluetopaz\n   2. 下划线： #blue_topaz\n   3. 连字符： #blue-topaz\n\n\n\n# 关于数字\n\n * 标签内允许使用数字，但不能完全由数字组成\n   * #1984 ❌\n   * #1984date ⭕\n   * #da_1984_te ⭕\n   * #date-1984 ⭕\n\n\n\n# 标签的嵌套\n\n在标签名内，使用 / 斜杠 可以实现标签的嵌套\n\n格式：\n\n * #主标签/子标签1\n * #主标签/子标签2\n * #主标签/子标签3\n\n嵌套标签可以像普通标签一样通过点击来唤起搜索，嵌套标签允许你选择搜索的层次。例如：\n\n * 搜索 #主标签 ，即可找到包含任意一个子标签的所有笔记\n   * 返回的结果会是上述的三个例子\n * 当你在一个主分类下设置了多个子分类，想找到这个主分类包含的所有内容时，该功能会很实用\n\n\n\n# 能被使用的符号\n\n综上所述，标签内能被使用的符号共有三种\n\n 1. _ 下划线\n 2. - 连字符\n 3. / 斜杠\n\n\n\n# 如何让 # 不被识别\n\n可以使用前面提到的转义符号 \\ 反斜杠，与上述的 转义标题 类似\n\n格式：\n\n\\#这里的内容不会被识别为标签\n\n效果：\n\n#这里的内容不会被识别为标签\n\n\n\n# 19. 避免标识符的滥用\n\n即使在 markdown 中，也要尽量避免标识符的滥用\n\n比如我的这篇教程，就存在一定程度的滥用\n\n * 其实是因为我这篇是教学性质的，不太一样，有些不能避免\n   * (好吧，我就是在甩锅)\n\n标识符的本质是突出显示，代表重点\n\n * 一篇笔记里的某段文本，使用各式各样的的标识符，会造成重点不清晰\n\n有三种标识，慎用！\n\n 1. 词中对单个汉字的标识\n    1. 卧==虎==藏==龙==\n 2. 短语中对单个英语单词的标识\n    1. get a ==bang== out of\n 3. 标识符的多层嵌套\n    1. 我感觉快要==原地起飞==了\n\n原因：\n\n * 词义的割裂\n * 视觉的混乱\n * 不利于搜索\n   * 卧==虎==藏==龙==\n     * 搜 卧虎 -- 搜不到\n     * 搜 藏龙 -- 搜不到',charsets:{cjk:!0},lastUpdated:"2024/07/16, 20:14:06"},{title:"How to run llama.cpp with gem5",frontmatter:{title:"How to run llama.cpp with gem5",date:"2024-01-21T23:32:49.000Z",permalink:"/pages/dc7037/"},regularPath:"/05.llm/03.gem5_LLAMA.html",relativePath:"05.llm/03.gem5_LLAMA.md",key:"v-259f44f9",path:"/pages/dc7037/",headersStr:null,content:'1) LLama.cpp\n\nllama support compilation of x86, arm and gpu.\n\na) github download llama.cpp\n\nhttps://github.com/ggerganov/llama.cpp.git\n\nb) gem5 support ARM architecture better, thus we compile llama.cpp with arm.\n\n\n\nThen we start to compile: make UNAME_M=aarch64\n\nthe compile tool chain is based on aarch64-linux-gnu-gc-10. It will generate "main" binary if compress successfully.\n\nuse file main to check the file:\n\n\n\nc) download a model to llama.cpp/models directory.\n\nHere I downloaded llama-2-7b-chat.Q2_K.gguf. It utilize 2bit quantization and only needs 3GB memory.\n\nGGML_TYPE_Q2_K - "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n\n\n\nd) then we can run the main binary and model, my prompt is "How are you".\n\n./main -m ./models/llama-2-7b-chat.Q2_K.gguf -p "How are you" -n 16\n\nThe last line in the following figure is the output.\n\n\n\n2) gem5\n\nafter we have build gem5 successfully, we can run the model with gem5.\n\nHere I plan to run with 8 core.\n\n> build/ARM/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $LLAMA_path/llama.cpp/main-arm \'--options=-m $LLAMA_path/llama-2-7b-chat.Q2_K.gguf -p Hi -n 16\' --cpu-type=ArmAtomicSimpleCPU --mem-size=8GB -n 8\n\n> The output is like the following:\n\n\n\nThe left several columns are output of LLAMA model. The followings are cpu ID with instruction executed.\n\nThe output of the model is "Hi，I\'m a 30-year-old male, and..."\n\nHowever, only 4 core has been used, since LLAMA.cpp default threads configuration is 4.\n\nThen we can configure the model with 8 thread.\n\n> build/ARM/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $LLAMA_path/llama.cpp/main-arm \'--options=-m $LLAMA_path/llama-2-7b-chat.Q2_K.gguf -p Hi -n 16 -t 8\' --cpu-type=ArmAtomicSimpleCPU --mem-size=8GB -n 8\n\n\n\nNow, you can see that by default, only CPU0 execute 2.9 billion instruction with the output of "Hi" in 8 core simulation.\n\nHowever, with default 4 core, it has to run 5.4 billion instruction to the same result. This complies to the number of cores runing parallely.',normalizedContent:'1) llama.cpp\n\nllama support compilation of x86, arm and gpu.\n\na) github download llama.cpp\n\nhttps://github.com/ggerganov/llama.cpp.git\n\nb) gem5 support arm architecture better, thus we compile llama.cpp with arm.\n\n\n\nthen we start to compile: make uname_m=aarch64\n\nthe compile tool chain is based on aarch64-linux-gnu-gc-10. it will generate "main" binary if compress successfully.\n\nuse file main to check the file:\n\n\n\nc) download a model to llama.cpp/models directory.\n\nhere i downloaded llama-2-7b-chat.q2_k.gguf. it utilize 2bit quantization and only needs 3gb memory.\n\nggml_type_q2_k - "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. block scales and mins are quantized with 4 bits. this ends up effectively using 2.5625 bits per weight (bpw)\n\n\n\nd) then we can run the main binary and model, my prompt is "how are you".\n\n./main -m ./models/llama-2-7b-chat.q2_k.gguf -p "how are you" -n 16\n\nthe last line in the following figure is the output.\n\n\n\n2) gem5\n\nafter we have build gem5 successfully, we can run the model with gem5.\n\nhere i plan to run with 8 core.\n\n> build/arm/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $llama_path/llama.cpp/main-arm \'--options=-m $llama_path/llama-2-7b-chat.q2_k.gguf -p hi -n 16\' --cpu-type=armatomicsimplecpu --mem-size=8gb -n 8\n\n> the output is like the following:\n\n\n\nthe left several columns are output of llama model. the followings are cpu id with instruction executed.\n\nthe output of the model is "hi，i\'m a 30-year-old male, and..."\n\nhowever, only 4 core has been used, since llama.cpp default threads configuration is 4.\n\nthen we can configure the model with 8 thread.\n\n> build/arm/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $llama_path/llama.cpp/main-arm \'--options=-m $llama_path/llama-2-7b-chat.q2_k.gguf -p hi -n 16 -t 8\' --cpu-type=armatomicsimplecpu --mem-size=8gb -n 8\n\n\n\nnow, you can see that by default, only cpu0 execute 2.9 billion instruction with the output of "hi" in 8 core simulation.\n\nhowever, with default 4 core, it has to run 5.4 billion instruction to the same result. this complies to the number of cores runing parallely.',charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"how llm works",frontmatter:{title:"how llm works",date:"2024-01-02T15:32:49.000Z",permalink:"/pages/dc7035/"},regularPath:"/05.llm/01.How_LLM_Works.html",relativePath:"05.llm/01.How_LLM_Works.md",key:"v-26a9c736",path:"/pages/dc7035/",headers:[{level:3,title:"1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2012]",slug:"_1-llm-as-os-agents-as-apps-envisioning-aios-agents-and-the-aios-agent-ecosystem-2012",normalizedTitle:"1. llm as os, agents as apps: envisioning aios, agents and the aios-agent ecosystem [2012]",charIndex:174},{level:3,title:"2. NVIDIA Mastering LLM Techniques",slug:"_2-nvidia-mastering-llm-techniques",normalizedTitle:"2. nvidia mastering llm techniques",charIndex:93},{level:3,title:"3. Finetuning",slug:"_3-finetuning",normalizedTitle:"3. finetuning",charIndex:2746},{level:3,title:"4. Function Calling",slug:"_4-function-calling",normalizedTitle:"4. function calling",charIndex:3035}],headersStr:"1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2012] 2. NVIDIA Mastering LLM Techniques 3. Finetuning 4. Function Calling",content:" 1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2023]\n 2. NVIDIA Mastering LLM Techniques\n\n----------------------------------------\n\n\n# 1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2012]\n\n👍 Analogy of LLM and OS.\n\nBlog: https://huggingface.co/blog/shivance/illustrated-llm-os\n\nYoutube: Andrej Karpathy https://www.youtube.com/watch?v=kCc8FmEb1nY\n\nParallel decoding(Multi threading): This is a technique that allows multiple decoding processes to occur simultaneously, which can speed up the decoding process. For example, instead of generating one token at a time, parallel decoding can generate several tokens in parallel, using different models or different parts of the same model. This can reduce the latency and increase the throughput of the decoding process. A recent paper by Apple researchers proposed a method called Parallel Speculative Sampling (PaSS) that introduces parallel decoding for LLMs, maintaining model quality while achieving remarkable speed. Related Paper: Accelerating LLM Inference with Staged Speculative Decoding\n\nEnsemble decoding(Multi processing): This is a technique that involves using multiple models to decode a single input sequence, which can improve the accuracy of the decoding process. For example, instead of relying on one model to generate the output, ensemble decoding can combine the outputs of several models, using methods such as voting, averaging, or reranking. This can increase the diversity and robustness of the decoding process. A common approach for ensemble decoding is to use models that have been trained with different architectures, hyperparameters, or data sources.\n\nSpeculative execution: This is a technique that involves predicting the outcome of a computation before it is actually executed, which can speed up the decoding process. For example, instead of waiting for the final hidden state of the model to generate the next token, speculative execution can use the early hidden states to predict the next token and execute the model in parallel on the predicted token. This can reduce the dependency between tokens and increase the parallelism of the decoding process. A recent paper by Berkeley researchers proposed a method called SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states.\n\nRelated Paper: SPEED: Speculative Pipelined Execution for Efficient Decoding\n\n\n# 2. NVIDIA Mastering LLM Techniques\n\nLink: https://developer.nvidia.com/blog/search-posts/?q=Mastering+LLM+Techniques\n\n 1. Customization\n 2. LLMOps\n 3. Training\n 4. Inference Optimization\n 5. \n\n\n# 3. Finetuning\n\n 1. How RLHF Preference Model Tuning Works (And How Things May Go Wrong) Blog: https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/\n\nPaper: RRHF: Rank Responses to Align Language Models with Human Feedback without tears 2.\n\n\n# 4. Function Calling\n\n 1. Blog: https://crunchingthedata.com/when-to-use-function-calling-for-llms/\n 2. Paper: An LLM Compiler for Parallel Function Calling",normalizedContent:" 1. llm as os, agents as apps: envisioning aios, agents and the aios-agent ecosystem [2023]\n 2. nvidia mastering llm techniques\n\n----------------------------------------\n\n\n# 1. llm as os, agents as apps: envisioning aios, agents and the aios-agent ecosystem [2012]\n\n👍 analogy of llm and os.\n\nblog: https://huggingface.co/blog/shivance/illustrated-llm-os\n\nyoutube: andrej karpathy https://www.youtube.com/watch?v=kcc8fmeb1ny\n\nparallel decoding(multi threading): this is a technique that allows multiple decoding processes to occur simultaneously, which can speed up the decoding process. for example, instead of generating one token at a time, parallel decoding can generate several tokens in parallel, using different models or different parts of the same model. this can reduce the latency and increase the throughput of the decoding process. a recent paper by apple researchers proposed a method called parallel speculative sampling (pass) that introduces parallel decoding for llms, maintaining model quality while achieving remarkable speed. related paper: accelerating llm inference with staged speculative decoding\n\nensemble decoding(multi processing): this is a technique that involves using multiple models to decode a single input sequence, which can improve the accuracy of the decoding process. for example, instead of relying on one model to generate the output, ensemble decoding can combine the outputs of several models, using methods such as voting, averaging, or reranking. this can increase the diversity and robustness of the decoding process. a common approach for ensemble decoding is to use models that have been trained with different architectures, hyperparameters, or data sources.\n\nspeculative execution: this is a technique that involves predicting the outcome of a computation before it is actually executed, which can speed up the decoding process. for example, instead of waiting for the final hidden state of the model to generate the next token, speculative execution can use the early hidden states to predict the next token and execute the model in parallel on the predicted token. this can reduce the dependency between tokens and increase the parallelism of the decoding process. a recent paper by berkeley researchers proposed a method called speed, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states.\n\nrelated paper: speed: speculative pipelined execution for efficient decoding\n\n\n# 2. nvidia mastering llm techniques\n\nlink: https://developer.nvidia.com/blog/search-posts/?q=mastering+llm+techniques\n\n 1. customization\n 2. llmops\n 3. training\n 4. inference optimization\n 5. \n\n\n# 3. finetuning\n\n 1. how rlhf preference model tuning works (and how things may go wrong) blog: https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/\n\npaper: rrhf: rank responses to align language models with human feedback without tears 2.\n\n\n# 4. function calling\n\n 1. blog: https://crunchingthedata.com/when-to-use-function-calling-for-llms/\n 2. paper: an llm compiler for parallel function calling",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Memory Usage in Training LLM",frontmatter:{title:"Memory Usage in Training LLM",date:"2024-05-29T23:32:49.000Z",permalink:"/pages/dc7038/"},regularPath:"/05.llm/04.mem_usage_llm.html",relativePath:"05.llm/04.mem_usage_llm.md",key:"v-0ee7ecc5",path:"/pages/dc7038/",headersStr:null,content:"1) Nvidia Paper on Traning LLM\nReducing Activation Recomputation in Large Transformer Models\n\n2) Blog Understanding and Estimating GPU Memory\nUnderstanding and Estimating GPU Memory Demands for Training LLMs in practice\n\n3) Blog Memory-Efficient Training\nMemory-Efficient Training of Large Language Models: Overcoming Constraints on Consumer GPUs for Large Neural Networks\n\n4）Stanford Paper Low-Memory Neural Network Training:A Technical Report\nLow-Memory Neural Network Training:A Technical Report\n\n5) Blog Gradient / Activation checkpointing\nhttps://iq.opengenus.org/gradient-checkpointing/\n\n6) Tianqi Chen Gradient Checkpointing Paper\nTraining Deep Nets with Sublinear Memory Cost\n\n7）UCSD Efficient Finetuning of LLMs\nhttps://cseweb.ucsd.edu/classes/wi24/cse234-a/slides/CSE234-GuestLecture-SumanthHegde.pdf",normalizedContent:"1) nvidia paper on traning llm\nreducing activation recomputation in large transformer models\n\n2) blog understanding and estimating gpu memory\nunderstanding and estimating gpu memory demands for training llms in practice\n\n3) blog memory-efficient training\nmemory-efficient training of large language models: overcoming constraints on consumer gpus for large neural networks\n\n4）stanford paper low-memory neural network training:a technical report\nlow-memory neural network training:a technical report\n\n5) blog gradient / activation checkpointing\nhttps://iq.opengenus.org/gradient-checkpointing/\n\n6) tianqi chen gradient checkpointing paper\ntraining deep nets with sublinear memory cost\n\n7）ucsd efficient finetuning of llms\nhttps://cseweb.ucsd.edu/classes/wi24/cse234-a/slides/cse234-guestlecture-sumanthhegde.pdf",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"how malloc works",frontmatter:{title:"how malloc works",date:"2024-01-02T15:32:49.000Z",permalink:"/pages/ec7035/"},regularPath:"/06.unix/01.malloc.html",relativePath:"06.unix/01.malloc.md",key:"v-e114a476",path:"/pages/ec7035/",headersStr:null,content:"malloc\n\n1 brk and mmap\nhttps://people.kth.se/~johanmon/courses/id2206/lectures/management-handout.pdf\n\n\nMalloc will invoke brk or mmap systemcall. The difference is based on\nFocus on Size:\n\nThe primary factor influencing malloc's choice is the requested memory size.\nbrk for Smaller Allocations:\nFor smaller allocations (often configurable through a threshold), malloc will likely use brk. brk is a system call that adjusts the program's data segment boundary. It's a relatively fast operation for requesting contiguous memory from the heap.\n\nmmap for Larger Allocations:\nWhen the requested memory size exceeds a certain threshold (often set by mallopt function), malloc might use mmap instead.\n\n2. Memory.\n\n\nEvery time you call sbrk，it will increase the brk.\n\n\n\n\n\n3. Code, Lib, Systemcall\n\n\n\n\n\n\n4. The Object in memory is organized by metadata and then data.\nThe metadata is size and bit.\n\n\nThe metadata is aligned by 16Byte. ------- This needs to be proved.\n\n\n\n\n\n5. Create Hooks for malloc.\nhttps://www.gnu.org/software/libc/manual/html_node/Hooks-for-Malloc.html\n\n/* Prototypes for __malloc_hook, __free_hook */\n#include <malloc.h>\n\n/* Prototypes for our hooks.  */\nstatic void my_init_hook (void);\nstatic void *my_malloc_hook (size_t, const void *);\nstatic void my_free_hook (void*, const void *);\n\nstatic void\nmy_init (void)\n{\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nstatic void *\nmy_malloc_hook (size_t size, const void *caller)\n{\n  void *result;\n  /* Restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* Call recursively */\n  result = malloc (size);\n  /* Save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call malloc, so protect it too. */\n  printf (\"malloc (%u) returns %p\\n\", (unsigned int) size, result);\n  /* Restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n  return result;\n}\n\nstatic void\nmy_free_hook (void *ptr, const void *caller)\n{\n  /* Restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* Call recursively */\n  free (ptr);\n  /* Save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call free, so protect it too. */\n  printf (\"freed pointer %p\\n\", ptr);\n  /* Restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nmain ()\n{\n  my_init ();\n  …\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n6. CppCon 2017 Memory Alloc”\n\nCppCon 2017: John Lakos “Local ('Arena') Memory Allocators (part 1 of 2)”",normalizedContent:"malloc\n\n1 brk and mmap\nhttps://people.kth.se/~johanmon/courses/id2206/lectures/management-handout.pdf\n\n\nmalloc will invoke brk or mmap systemcall. the difference is based on\nfocus on size:\n\nthe primary factor influencing malloc's choice is the requested memory size.\nbrk for smaller allocations:\nfor smaller allocations (often configurable through a threshold), malloc will likely use brk. brk is a system call that adjusts the program's data segment boundary. it's a relatively fast operation for requesting contiguous memory from the heap.\n\nmmap for larger allocations:\nwhen the requested memory size exceeds a certain threshold (often set by mallopt function), malloc might use mmap instead.\n\n2. memory.\n\n\nevery time you call sbrk，it will increase the brk.\n\n\n\n\n\n3. code, lib, systemcall\n\n\n\n\n\n\n4. the object in memory is organized by metadata and then data.\nthe metadata is size and bit.\n\n\nthe metadata is aligned by 16byte. ------- this needs to be proved.\n\n\n\n\n\n5. create hooks for malloc.\nhttps://www.gnu.org/software/libc/manual/html_node/hooks-for-malloc.html\n\n/* prototypes for __malloc_hook, __free_hook */\n#include <malloc.h>\n\n/* prototypes for our hooks.  */\nstatic void my_init_hook (void);\nstatic void *my_malloc_hook (size_t, const void *);\nstatic void my_free_hook (void*, const void *);\n\nstatic void\nmy_init (void)\n{\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nstatic void *\nmy_malloc_hook (size_t size, const void *caller)\n{\n  void *result;\n  /* restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* call recursively */\n  result = malloc (size);\n  /* save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call malloc, so protect it too. */\n  printf (\"malloc (%u) returns %p\\n\", (unsigned int) size, result);\n  /* restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n  return result;\n}\n\nstatic void\nmy_free_hook (void *ptr, const void *caller)\n{\n  /* restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* call recursively */\n  free (ptr);\n  /* save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call free, so protect it too. */\n  printf (\"freed pointer %p\\n\", ptr);\n  /* restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nmain ()\n{\n  my_init ();\n  …\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n6. cppcon 2017 memory alloc”\n\ncppcon 2017: john lakos “local ('arena') memory allocators (part 1 of 2)”",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"留言板",frontmatter:{title:"留言板",date:"2022-07-12T10:38:46.000Z",permalink:"/message-board"},regularPath:"/09.nine/01.%E7%95%99%E8%A8%80%E6%9D%BF.html",relativePath:"09.nine/01.留言板.md",key:"v-2e715932",path:"/message-board/",headersStr:null,content:"你可以在这里留下想说的内容。",normalizedContent:"你可以在这里留下想说的内容。",charsets:{cjk:!0},lastUpdated:"2024/07/16, 20:14:06"},{title:"template",frontmatter:{title:"template",date:"2020-04-05T10:38:46.000Z",permalink:"/pages/a1ccb2/"},regularPath:"/09.nine/02.template.html",relativePath:"09.nine/02.template.md",key:"v-bb1ed876",path:"/pages/a1ccb2/",headers:[{level:2,title:"Contents",slug:"contents",normalizedTitle:"contents",charIndex:28},{level:2,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:42},{level:2,title:"List of Papers",slug:"list-of-papers",normalizedTitle:"list of papers",charIndex:59},{level:2,title:"Paper Reviews",slug:"paper-reviews",normalizedTitle:"paper reviews",charIndex:5},{level:3,title:"Paper 1: Title of Paper 1",slug:"paper-1-title-of-paper-1",normalizedTitle:"paper 1: title of paper 1",charIndex:98},{level:3,title:"Paper 2: Title of Paper 2",slug:"paper-2-title-of-paper-2",normalizedTitle:"paper 2: title of paper 2",charIndex:130},{level:3,title:"Paper 3: Title of Paper 3",slug:"paper-3-title-of-paper-3",normalizedTitle:"paper 3: title of paper 3",charIndex:162}],headersStr:"Contents Introduction List of Papers Paper Reviews Paper 1: Title of Paper 1 Paper 2: Title of Paper 2 Paper 3: Title of Paper 3",content:"# 📚 Paper Reviews Blog\n\n\n# Contents\n\n 1. Introduction\n 2. List of Papers\n 3. Paper Reviews\n    * Paper 1: Title of Paper 1\n    * Paper 2: Title of Paper 2\n    * Paper 3: Title of Paper 3\n\n----------------------------------------\n\n\n# Introduction\n\nWelcome to my paper reviews blog! Here, I delve into various academic papers and provide insights, critiques, and summaries. Each paper offers a unique perspective on its subject matter, contributing to the ever-evolving landscape of research. Let's explore the latest findings and discussions in the academic world.\n\n\n# List of Papers\n\n * Title of Paper 1\n * Title of Paper 2\n * Title of Paper 3\n\n----------------------------------------\n\n\n# Paper Reviews\n\n\n# Paper 1: Title of Paper 1\n\n# Authors: Author Names\n\nAbstract: Brief summary of the paper's abstract.\n\nKeywords: Keywords, Keywords, Keywords\n\nIntroduction: Provide a summary of the introduction section.\n\nMethodology: Discuss the methodology used in the research.\n\nResults: Highlight the key findings of the study.\n\nDiscussion: Offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# Paper 2: Title of Paper 2\n\n# Authors: Author Names\n\nAbstract: Brief summary of the paper's abstract.\n\nKeywords: Keywords, Keywords, Keywords\n\nIntroduction: Provide a summary of the introduction section.\n\nMethodology: Discuss the methodology used in the research.\n\nResults: Highlight the key findings of the study.\n\nDiscussion: Offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# Paper 3: Title of Paper 3\n\n# Authors: Author Names\n\nAbstract: Brief summary of the paper's abstract.\n\nKeywords: Keywords, Keywords, Keywords\n\nIntroduction: Provide a summary of the introduction section.\n\nMethodology: Discuss the methodology used in the research.\n\nResults: Highlight the key findings of the study.\n\nDiscussion: Offer your thoughts and critiques on the paper.\n\n----------------------------------------",normalizedContent:"# 📚 paper reviews blog\n\n\n# contents\n\n 1. introduction\n 2. list of papers\n 3. paper reviews\n    * paper 1: title of paper 1\n    * paper 2: title of paper 2\n    * paper 3: title of paper 3\n\n----------------------------------------\n\n\n# introduction\n\nwelcome to my paper reviews blog! here, i delve into various academic papers and provide insights, critiques, and summaries. each paper offers a unique perspective on its subject matter, contributing to the ever-evolving landscape of research. let's explore the latest findings and discussions in the academic world.\n\n\n# list of papers\n\n * title of paper 1\n * title of paper 2\n * title of paper 3\n\n----------------------------------------\n\n\n# paper reviews\n\n\n# paper 1: title of paper 1\n\n# authors: author names\n\nabstract: brief summary of the paper's abstract.\n\nkeywords: keywords, keywords, keywords\n\nintroduction: provide a summary of the introduction section.\n\nmethodology: discuss the methodology used in the research.\n\nresults: highlight the key findings of the study.\n\ndiscussion: offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# paper 2: title of paper 2\n\n# authors: author names\n\nabstract: brief summary of the paper's abstract.\n\nkeywords: keywords, keywords, keywords\n\nintroduction: provide a summary of the introduction section.\n\nmethodology: discuss the methodology used in the research.\n\nresults: highlight the key findings of the study.\n\ndiscussion: offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# paper 3: title of paper 3\n\n# authors: author names\n\nabstract: brief summary of the paper's abstract.\n\nkeywords: keywords, keywords, keywords\n\nintroduction: provide a summary of the introduction section.\n\nmethodology: discuss the methodology used in the research.\n\nresults: highlight the key findings of the study.\n\ndiscussion: offer your thoughts and critiques on the paper.\n\n----------------------------------------",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"archives",frontmatter:{archivesPage:!0,title:"archives",permalink:"/archives/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-f8e02736",path:"/archives/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Home",frontmatter:{home:!0,heroText:"Notes in Computer System.",actionText:"Start →",actionLink:"/pages/f27694/",bannerBg:"none",postList:"simple"},regularPath:"/",relativePath:"index.md",key:"v-2f8a0a7e",path:"/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/07/16, 20:14:06"},{title:"Add pictures",frontmatter:{title:"Add pictures",date:"2023-05-08T00:00:00.000Z",permalink:"/pages/3deeec/"},regularPath:"/pictures/addPictures.html",relativePath:"pictures/addPictures.md",key:"v-01216c79",path:"/pages/3deeec/",headersStr:null,content:"Add pictures into this foler",normalizedContent:"add pictures into this foler",charsets:{},lastUpdated:"2024/07/16, 20:14:06"}],themeConfig:{nav:[{text:"Home",link:"/"},{text:"hbm",link:"/hbm/"},{text:"compiler",link:"/compiler/"},{text:"gpu",link:"/gpu/"},{text:"cpu",link:"/cpu/"},{text:"llm",link:"/llm/"},{text:"unix",link:"/unix/"},{text:"BBS",link:"/message-board/"},{text:"CSDN",link:"https://blog.csdn.net/hit_shaoqi"}],sidebarDepth:3,repo:"hitqshao/qishao-notes",searchMaxSuggestions:10,lastUpdated:"上次更新",editLinks:!0,docsDir:"docs",docsBranch:"main",editLinkText:"帮助我们改善此页面",searchPlaceholder:"按下 𝑺 搜索",category:!1,tag:!1,sidebar:{"/00.目录页/":[["00.Content.md","Content","/pages/f27694/"],["01.hbm.md","HBM","/hbm/"],["02.compiler.md","llvm & mlir","/compiler/"],["03.gpu.md","gpu","/gpu/"],["04.cpu.md","cpu","/cpu/"],["05.llm.md","llm","/llm/"],["06.unix.md","unix","/unix/"]],catalogue:{hbm:"/hbm/",compiler:"/compiler/",gpu:"/gpu/",cpu:"/cpu/",llm:"/llm/",unix:"/unix/"},"/01.hbm/":[["01.HBM_Paper_List.md","HBM Paper List","/pages/24769e/"],["02.hbm_dead_block_predictor.md","HBM Dead Block Predictor","/pages/2476af/"],["03.Dynamically_Adapting _Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.md","Dynamically Adapting  Page Migration Policies Based on Applications Memory Access Behaviors","/pages/24769f/"],["04.DRAM_PCM_NVM_Cache.md","DRAM PCM NVM Cache","/pages/24760e/"],["05.cache_mem_compression.md","Cache Memory Compression","/pages/2476bf/"],["06.memory ecc.md","memory-ecc","/pages/f07695/"],["07.hbm-latency.md","hbm-latency","/pages/f07696/"],["08.compression.md","compression","/pages/f07698/"],["09.compressibility_prediction.md","compressibility prediction","/pages/f07699/"],["10.software_memory_paper.md","memory management","/pages/f07692/"]],"/02.compiler/":[["01.llvm_frontend.md","llvm front end","/pages/000001/"],["02.GetStartedLLVMChap5Notes.md","Getting Started with LLVM Core Libraries-Notes Chap5 IR","/pages/000002/"],["03.GetStartedLLVMChap6Notes.md","Getting Started with LLVM Core Libraries-Notes Chap6 Backend","/pages/000003/"],["04. LearningLLVMDiary0.md","Learning LLVM Diary 00","/pages/000004/"],["05. addInstACE.md",'Add New Instruction "ACE" to LLVM',"/pages/000005/"],["06.Value&Use.md","How does LLVM perform instruction combine","/pages/000006/"],["07. UnderstaningLLVMwithSourceCode.md","Understand llvm with its source code Part 1","/pages/000007/"]],"/03.gpu/":[["01.operand_collector.md","operand-collector","/pages/cc7034/"],["02.warp_execution.md","GPU WARP Scheduler","/pages/2476ae/"],["03.Precise Exception.md","Precision Exception","/pages/14769f/"],["04.Unified_Memory.md","Unified Memory Paper List","/pages/44771e/"],["05.TensorCore.md","TensorCore Paper List","/pages/44871e/"],["06.MemoryBehaviour.md","Memory Behaviour Paper List","/pages/45871e/"],["07.GPUVirtualization.md","GPU Virtualization Paper List","/pages/45871f/"],["08.LLM.md","Large Language Model Paper List","/pages/458720/"],["09.Simulator.md","GPU Simulator","/pages/458721/"],["10. Architectural Survey.md","Architectural Survey","/pages/458722/"],["1234.TODO.md","TO READ","/pages/47871e/"]],"/04.cpu/":[["01.checkpoint.md","checkpoint","/pages/cc7035/"],["02.topdown.md","topdown analysis","/pages/cc7036/"],["03.loadstore.md","load store unit","/pages/cc7037/"],["1234.markdown.md","two-test-1","/pages/f07697/"]],"/05.llm/":[["01.How_LLM_Works.md","how llm works","/pages/dc7035/"],["02.LLM_HW_Opt.md","LLM Hardware Optimization","/pages/dc7036/"],["03.gem5_LLAMA.md","How to run llama.cpp with gem5","/pages/dc7037/"],["04.mem_usage_llm.md","Memory Usage in Training LLM","/pages/dc7038/"]],"/06.unix/":[["01.malloc.md","how malloc works","/pages/ec7035/"]],"/09.nine/":[["01.留言板.md","留言板","/message-board"],["02.template.md","template","/pages/a1ccb2/"]]},pageStyle:"line",updateBar:{showToArticle:!1},author:{name:"hitqishao",link:"https://github.com/hitqshao"},social:{icons:[{iconClass:"icon-github",title:"GitHub",link:"https://github.com/hitqshao"},{iconClass:"icon-youjian",title:"发邮件",link:"mailto:hitqshao@163.com"},{iconClass:"icon-gitee",title:"Gitee",link:"https://gitee.com/hitqshao"}]},footer:{createYear:2022,copyrightInfo:'Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a>'}}};var cc=t(118),lc=t(119),dc=t(13);var uc={computed:{$filterPosts(){return this.$site.pages.filter(e=>{const{frontmatter:{pageComponent:n,article:t,home:a}}=e;return!(n||!1===t||!0===a)})},$sortPosts(){return(e=this.$filterPosts).sort((e,n)=>{const t=e.frontmatter.sticky,a=n.frontmatter.sticky;return t&&a?t==a?Object(dc.a)(e,n):t-a:t&&!a?-1:!t&&a?1:Object(dc.a)(e,n)}),e;var e},$sortPostsByDate(){return(e=this.$filterPosts).sort((e,n)=>Object(dc.a)(e,n)),e;var e},$groupPosts(){return function(e){const n={},t={};for(let a=0,r=e.length;a<r;a++){const{frontmatter:{categories:r,tags:i}}=e[a];"array"===Object(dc.n)(r)&&r.forEach(t=>{t&&(n[t]||(n[t]=[]),n[t].push(e[a]))}),"array"===Object(dc.n)(i)&&i.forEach(n=>{n&&(t[n]||(t[n]=[]),t[n].push(e[a]))})}return{categories:n,tags:t}}(this.$sortPosts)},$categoriesAndTags(){return function(e){const n=[],t=[];for(let t in e.categories)n.push({key:t,length:e.categories[t].length});for(let n in e.tags)t.push({key:n,length:e.tags[n].length});return{categories:n,tags:t}}(this.$groupPosts)}}};Wt.component(cc.default),Wt.component(lc.default);function hc(e){return e.toString().padStart(2,"0")}t(268);Wt.component("Badge",()=>Promise.all([t.e(0),t.e(3)]).then(t.bind(null,464))),Wt.component("CodeBlock",()=>Promise.resolve().then(t.bind(null,118))),Wt.component("CodeGroup",()=>Promise.resolve().then(t.bind(null,119)));t(269);
/**
  * vue-class-component v7.2.6
  * (c) 2015-present Evan You
  * @license MIT
  */
function pc(e){return(pc="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}function mc(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function fc(e){return function(e){if(Array.isArray(e)){for(var n=0,t=new Array(e.length);n<e.length;n++)t[n]=e[n];return t}}(e)||function(e){if(Symbol.iterator in Object(e)||"[object Arguments]"===Object.prototype.toString.call(e))return Array.from(e)}(e)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance")}()}function gc(){return"undefined"!=typeof Reflect&&Reflect.defineMetadata&&Reflect.getOwnMetadataKeys}function yc(e,n){vc(e,n),Object.getOwnPropertyNames(n.prototype).forEach((function(t){vc(e.prototype,n.prototype,t)})),Object.getOwnPropertyNames(n).forEach((function(t){vc(e,n,t)}))}function vc(e,n,t){(t?Reflect.getOwnMetadataKeys(n,t):Reflect.getOwnMetadataKeys(n)).forEach((function(a){var r=t?Reflect.getOwnMetadata(a,n,t):Reflect.getOwnMetadata(a,n);t?Reflect.defineMetadata(a,r,e,t):Reflect.defineMetadata(a,r,e)}))}var bc={__proto__:[]}instanceof Array;function wc(e){return function(n,t,a){var r="function"==typeof n?n:n.constructor;r.__decorators__||(r.__decorators__=[]),"number"!=typeof a&&(a=void 0),r.__decorators__.push((function(n){return e(n,t,a)}))}}function kc(e,n){var t=n.prototype._init;n.prototype._init=function(){var n=this,t=Object.getOwnPropertyNames(e);if(e.$options.props)for(var a in e.$options.props)e.hasOwnProperty(a)||t.push(a);t.forEach((function(t){Object.defineProperty(n,t,{get:function(){return e[t]},set:function(n){e[t]=n},configurable:!0})}))};var a=new n;n.prototype._init=t;var r={};return Object.keys(a).forEach((function(e){void 0!==a[e]&&(r[e]=a[e])})),r}var _c=["data","beforeCreate","created","beforeMount","mounted","beforeDestroy","destroyed","beforeUpdate","updated","activated","deactivated","render","errorCaptured","serverPrefetch"];function xc(e){var n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};n.name=n.name||e._componentTag||e.name;var t=e.prototype;Object.getOwnPropertyNames(t).forEach((function(e){if("constructor"!==e)if(_c.indexOf(e)>-1)n[e]=t[e];else{var a=Object.getOwnPropertyDescriptor(t,e);void 0!==a.value?"function"==typeof a.value?(n.methods||(n.methods={}))[e]=a.value:(n.mixins||(n.mixins=[])).push({data:function(){return mc({},e,a.value)}}):(a.get||a.set)&&((n.computed||(n.computed={}))[e]={get:a.get,set:a.set})}})),(n.mixins||(n.mixins=[])).push({data:function(){return kc(this,e)}});var a=e.__decorators__;a&&(a.forEach((function(e){return e(n)})),delete e.__decorators__);var r=Object.getPrototypeOf(e.prototype),i=r instanceof Wt?r.constructor:Wt,o=i.extend(n);return Ac(o,e,i),gc()&&yc(o,e),o}var Cc={prototype:!0,arguments:!0,callee:!0,caller:!0};function Ac(e,n,t){Object.getOwnPropertyNames(n).forEach((function(a){if(!Cc[a]){var r=Object.getOwnPropertyDescriptor(e,a);if(!r||r.configurable){var i,o,s=Object.getOwnPropertyDescriptor(n,a);if(!bc){if("cid"===a)return;var c=Object.getOwnPropertyDescriptor(t,a);if(i=s.value,o=pc(i),null!=i&&("object"===o||"function"===o)&&c&&c.value===s.value)return}0,Object.defineProperty(e,a,s)}}}))}function Mc(e){return"function"==typeof e?xc(e):function(n){return xc(n,e)}}Mc.registerHooks=function(e){_c.push.apply(_c,fc(e))};var Sc=Mc;function Tc(e){return wc((function(n,t){void 0===n.inject&&(n.inject={}),Array.isArray(n.inject)||(n.inject[t]=e||t)}))}function Pc(e){var n=function(){var t=this,a="function"==typeof e?e.call(this):e;for(var r in(a=Object.create(a||null)).__reactiveInject__=this.__reactiveInject__||{},n.managed)a[n.managed[r]]=this[r];var i=function(e){a[n.managedReactive[e]]=o[e],Object.defineProperty(a.__reactiveInject__,n.managedReactive[e],{enumerable:!0,get:function(){return t[e]}})},o=this;for(var r in n.managedReactive)i(r);return a};return n.managed={},n.managedReactive={},n}function Ic(e){return"function"!=typeof e||!e.managed&&!e.managedReactive}var Dc="undefined"!=typeof Reflect&&void 0!==Reflect.getMetadata;function Lc(e,n,t){if(Dc&&!Array.isArray(e)&&"function"!=typeof e&&void 0===e.type){var a=Reflect.getMetadata("design:type",n,t);a!==Object&&(e.type=a)}}function Oc(e){return void 0===e&&(e={}),function(n,t){Lc(e,n,t),wc((function(n,t){(n.props||(n.props={}))[t]=e}))(n,t)}}function zc(e,n){void 0===n&&(n={});var t=n.deep,a=void 0!==t&&t,r=n.immediate,i=void 0!==r&&r;return wc((function(n,t){"object"!=typeof n.watch&&(n.watch=Object.create(null));var r=n.watch;"object"!=typeof r[e]||Array.isArray(r[e])?void 0===r[e]&&(r[e]=[]):r[e]=[r[e]],r[e].push({handler:t,deep:a,immediate:i})}))}var Rc=t(16);const Ec=(e,n)=>`${e}${Object(Rc.stringify)(n,{addQueryPrefix:!0})}`,Uc=(e,n)=>`${e.replace(/\/$/,"")}/${n.replace(/^\//,"")}`;var jc=t(115),Bc=t.n(jc);const Gc=e=>Bc()(e,"YYYY-MM-DD HH:mm:ss"),Nc=e=>(e.split("#")[0]||"").split("?")[0]||"";
/*!
 * vue-i18n v8.28.2 
 * (c) 2022 kazuya kawaguchi
 * Released under the MIT License.
 */
var $c=["compactDisplay","currency","currencyDisplay","currencySign","localeMatcher","notation","numberingSystem","signDisplay","style","unit","unitDisplay","useGrouping","minimumIntegerDigits","minimumFractionDigits","maximumFractionDigits","minimumSignificantDigits","maximumSignificantDigits"],qc=["dateStyle","timeStyle","calendar","localeMatcher","hour12","hourCycle","timeZone","formatMatcher","weekday","era","year","month","day","hour","minute","second","timeZoneName"];function Fc(e,n){"undefined"!=typeof console&&(console.warn("[vue-i18n] "+e),n&&console.warn(n.stack))}var Vc=Array.isArray;function Hc(e){return null!==e&&"object"==typeof e}function Wc(e){return"string"==typeof e}var Kc=Object.prototype.toString;function Yc(e){return"[object Object]"===Kc.call(e)}function Qc(e){return null==e}function Zc(e){return"function"==typeof e}function Xc(){for(var e=[],n=arguments.length;n--;)e[n]=arguments[n];var t=null,a=null;return 1===e.length?Hc(e[0])||Vc(e[0])?a=e[0]:"string"==typeof e[0]&&(t=e[0]):2===e.length&&("string"==typeof e[0]&&(t=e[0]),(Hc(e[1])||Vc(e[1]))&&(a=e[1])),{locale:t,params:a}}function Jc(e){return JSON.parse(JSON.stringify(e))}function el(e,n){return!!~e.indexOf(n)}var nl=Object.prototype.hasOwnProperty;function tl(e,n){return nl.call(e,n)}function al(e){for(var n=arguments,t=Object(e),a=1;a<arguments.length;a++){var r=n[a];if(null!=r){var i=void 0;for(i in r)tl(r,i)&&(Hc(r[i])?t[i]=al(t[i],r[i]):t[i]=r[i])}}return t}function rl(e,n){if(e===n)return!0;var t=Hc(e),a=Hc(n);if(!t||!a)return!t&&!a&&String(e)===String(n);try{var r=Vc(e),i=Vc(n);if(r&&i)return e.length===n.length&&e.every((function(e,t){return rl(e,n[t])}));if(r||i)return!1;var o=Object.keys(e),s=Object.keys(n);return o.length===s.length&&o.every((function(t){return rl(e[t],n[t])}))}catch(e){return!1}}function il(e){return null!=e&&Object.keys(e).forEach((function(n){"string"==typeof e[n]&&(e[n]=e[n].replace(/</g,"&lt;").replace(/>/g,"&gt;").replace(/"/g,"&quot;").replace(/'/g,"&apos;"))})),e}var ol={name:"i18n",functional:!0,props:{tag:{type:[String,Boolean,Object],default:"span"},path:{type:String,required:!0},locale:{type:String},places:{type:[Array,Object]}},render:function(e,n){var t=n.data,a=n.parent,r=n.props,i=n.slots,o=a.$i18n;if(o){var s=r.path,c=r.locale,l=r.places,d=i(),u=o.i(s,c,function(e){var n;for(n in e)if("default"!==n)return!1;return Boolean(n)}(d)||l?function(e,n){var t=n?function(e){0;return Array.isArray(e)?e.reduce(cl,{}):Object.assign({},e)}(n):{};if(!e)return t;var a=(e=e.filter((function(e){return e.tag||""!==e.text.trim()}))).every(ll);0;return e.reduce(a?sl:cl,t)}(d.default,l):d),h=r.tag&&!0!==r.tag||!1===r.tag?r.tag:"span";return h?e(h,t,u):u}}};function sl(e,n){return n.data&&n.data.attrs&&n.data.attrs.place&&(e[n.data.attrs.place]=n),e}function cl(e,n,t){return e[t]=n,e}function ll(e){return Boolean(e.data&&e.data.attrs&&e.data.attrs.place)}var dl,ul={name:"i18n-n",functional:!0,props:{tag:{type:[String,Boolean,Object],default:"span"},value:{type:Number,required:!0},format:{type:[String,Object]},locale:{type:String}},render:function(e,n){var t=n.props,a=n.parent,r=n.data,i=a.$i18n;if(!i)return null;var o=null,s=null;Wc(t.format)?o=t.format:Hc(t.format)&&(t.format.key&&(o=t.format.key),s=Object.keys(t.format).reduce((function(e,n){var a;return el($c,n)?Object.assign({},e,((a={})[n]=t.format[n],a)):e}),null));var c=t.locale||i.locale,l=i._ntp(t.value,c,o,s),d=l.map((function(e,n){var t,a=r.scopedSlots&&r.scopedSlots[e.type];return a?a(((t={})[e.type]=e.value,t.index=n,t.parts=l,t)):e.value})),u=t.tag&&!0!==t.tag||!1===t.tag?t.tag:"span";return u?e(u,{attrs:r.attrs,class:r.class,staticClass:r.staticClass},d):d}};function hl(e,n,t){fl(e,t)&&gl(e,n,t)}function pl(e,n,t,a){if(fl(e,t)){var r=t.context.$i18n;(function(e,n){var t=n.context;return e._locale===t.$i18n.locale})(e,t)&&rl(n.value,n.oldValue)&&rl(e._localeMessage,r.getLocaleMessage(r.locale))||gl(e,n,t)}}function ml(e,n,t,a){if(t.context){var r=t.context.$i18n||{};n.modifiers.preserve||r.preserveDirectiveContent||(e.textContent=""),e._vt=void 0,delete e._vt,e._locale=void 0,delete e._locale,e._localeMessage=void 0,delete e._localeMessage}else Fc("Vue instance does not exists in VNode context")}function fl(e,n){var t=n.context;return t?!!t.$i18n||(Fc("VueI18n instance does not exists in Vue instance"),!1):(Fc("Vue instance does not exists in VNode context"),!1)}function gl(e,n,t){var a,r,i=function(e){var n,t,a,r;Wc(e)?n=e:Yc(e)&&(n=e.path,t=e.locale,a=e.args,r=e.choice);return{path:n,locale:t,args:a,choice:r}}(n.value),o=i.path,s=i.locale,c=i.args,l=i.choice;if(o||s||c)if(o){var d=t.context;e._vt=e.textContent=null!=l?(a=d.$i18n).tc.apply(a,[o,l].concat(yl(s,c))):(r=d.$i18n).t.apply(r,[o].concat(yl(s,c))),e._locale=d.$i18n.locale,e._localeMessage=d.$i18n.getLocaleMessage(d.$i18n.locale)}else Fc("`path` is required in v-t directive");else Fc("value type not supported")}function yl(e,n){var t=[];return e&&t.push(e),n&&(Array.isArray(n)||Yc(n))&&t.push(n),t}function vl(e,n){void 0===n&&(n={bridge:!1}),vl.installed=!0;(dl=e).version&&Number(dl.version.split(".")[0]);(function(e){e.prototype.hasOwnProperty("$i18n")||Object.defineProperty(e.prototype,"$i18n",{get:function(){return this._i18n}}),e.prototype.$t=function(e){for(var n=[],t=arguments.length-1;t-- >0;)n[t]=arguments[t+1];var a=this.$i18n;return a._t.apply(a,[e,a.locale,a._getMessages(),this].concat(n))},e.prototype.$tc=function(e,n){for(var t=[],a=arguments.length-2;a-- >0;)t[a]=arguments[a+2];var r=this.$i18n;return r._tc.apply(r,[e,r.locale,r._getMessages(),this,n].concat(t))},e.prototype.$te=function(e,n){var t=this.$i18n;return t._te(e,t.locale,t._getMessages(),n)},e.prototype.$d=function(e){for(var n,t=[],a=arguments.length-1;a-- >0;)t[a]=arguments[a+1];return(n=this.$i18n).d.apply(n,[e].concat(t))},e.prototype.$n=function(e){for(var n,t=[],a=arguments.length-1;a-- >0;)t[a]=arguments[a+1];return(n=this.$i18n).n.apply(n,[e].concat(t))}})(dl),dl.mixin(function(e){function n(){this!==this.$root&&this.$options.__INTLIFY_META__&&this.$el&&this.$el.setAttribute("data-intlify",this.$options.__INTLIFY_META__)}return void 0===e&&(e=!1),e?{mounted:n}:{beforeCreate:function(){var e=this.$options;if(e.i18n=e.i18n||(e.__i18nBridge||e.__i18n?{}:null),e.i18n)if(e.i18n instanceof zl){if(e.__i18nBridge||e.__i18n)try{var n=e.i18n&&e.i18n.messages?e.i18n.messages:{};(e.__i18nBridge||e.__i18n).forEach((function(e){n=al(n,JSON.parse(e))})),Object.keys(n).forEach((function(t){e.i18n.mergeLocaleMessage(t,n[t])}))}catch(e){0}this._i18n=e.i18n,this._i18nWatcher=this._i18n.watchI18nData()}else if(Yc(e.i18n)){var t=this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof zl?this.$root.$i18n:null;if(t&&(e.i18n.root=this.$root,e.i18n.formatter=t.formatter,e.i18n.fallbackLocale=t.fallbackLocale,e.i18n.formatFallbackMessages=t.formatFallbackMessages,e.i18n.silentTranslationWarn=t.silentTranslationWarn,e.i18n.silentFallbackWarn=t.silentFallbackWarn,e.i18n.pluralizationRules=t.pluralizationRules,e.i18n.preserveDirectiveContent=t.preserveDirectiveContent),e.__i18nBridge||e.__i18n)try{var a=e.i18n&&e.i18n.messages?e.i18n.messages:{};(e.__i18nBridge||e.__i18n).forEach((function(e){a=al(a,JSON.parse(e))})),e.i18n.messages=a}catch(e){0}var r=e.i18n.sharedMessages;r&&Yc(r)&&(e.i18n.messages=al(e.i18n.messages,r)),this._i18n=new zl(e.i18n),this._i18nWatcher=this._i18n.watchI18nData(),(void 0===e.i18n.sync||e.i18n.sync)&&(this._localeWatcher=this.$i18n.watchLocale()),t&&t.onComponentInstanceCreated(this._i18n)}else 0;else this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof zl?this._i18n=this.$root.$i18n:e.parent&&e.parent.$i18n&&e.parent.$i18n instanceof zl&&(this._i18n=e.parent.$i18n)},beforeMount:function(){var e=this.$options;e.i18n=e.i18n||(e.__i18nBridge||e.__i18n?{}:null),e.i18n?(e.i18n instanceof zl||Yc(e.i18n))&&(this._i18n.subscribeDataChanging(this),this._subscribing=!0):(this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof zl||e.parent&&e.parent.$i18n&&e.parent.$i18n instanceof zl)&&(this._i18n.subscribeDataChanging(this),this._subscribing=!0)},mounted:n,beforeDestroy:function(){if(this._i18n){var e=this;this.$nextTick((function(){e._subscribing&&(e._i18n.unsubscribeDataChanging(e),delete e._subscribing),e._i18nWatcher&&(e._i18nWatcher(),e._i18n.destroyVM(),delete e._i18nWatcher),e._localeWatcher&&(e._localeWatcher(),delete e._localeWatcher)}))}}}}(n.bridge)),dl.directive("t",{bind:hl,update:pl,unbind:ml}),dl.component(ol.name,ol),dl.component(ul.name,ul),dl.config.optionMergeStrategies.i18n=function(e,n){return void 0===n?e:n}}var bl=function(){this._caches=Object.create(null)};bl.prototype.interpolate=function(e,n){if(!n)return[e];var t=this._caches[e];return t||(t=function(e){var n=[],t=0,a="";for(;t<e.length;){var r=e[t++];if("{"===r){a&&n.push({type:"text",value:a}),a="";var i="";for(r=e[t++];void 0!==r&&"}"!==r;)i+=r,r=e[t++];var o="}"===r,s=wl.test(i)?"list":o&&kl.test(i)?"named":"unknown";n.push({value:i,type:s})}else"%"===r?"{"!==e[t]&&(a+=r):a+=r}return a&&n.push({type:"text",value:a}),n}(e),this._caches[e]=t),function(e,n){var t=[],a=0,r=Array.isArray(n)?"list":Hc(n)?"named":"unknown";if("unknown"===r)return t;for(;a<e.length;){var i=e[a];switch(i.type){case"text":t.push(i.value);break;case"list":t.push(n[parseInt(i.value,10)]);break;case"named":"named"===r&&t.push(n[i.value]);break;case"unknown":0}a++}return t}(t,n)};var wl=/^(?:\d)+/,kl=/^(?:\w)+/;var _l=[];_l[0]={ws:[0],ident:[3,0],"[":[4],eof:[7]},_l[1]={ws:[1],".":[2],"[":[4],eof:[7]},_l[2]={ws:[2],ident:[3,0],0:[3,0],number:[3,0]},_l[3]={ident:[3,0],0:[3,0],number:[3,0],ws:[1,1],".":[2,1],"[":[4,1],eof:[7,1]},_l[4]={"'":[5,0],'"':[6,0],"[":[4,2],"]":[1,3],eof:8,else:[4,0]},_l[5]={"'":[4,0],eof:8,else:[5,0]},_l[6]={'"':[4,0],eof:8,else:[6,0]};var xl=/^\s?(?:true|false|-?[\d.]+|'[^']*'|"[^"]*")\s?$/;function Cl(e){if(null==e)return"eof";switch(e.charCodeAt(0)){case 91:case 93:case 46:case 34:case 39:return e;case 95:case 36:case 45:return"ident";case 9:case 10:case 13:case 160:case 65279:case 8232:case 8233:return"ws"}return"ident"}function Al(e){var n,t,a,r=e.trim();return("0"!==e.charAt(0)||!isNaN(e))&&(a=r,xl.test(a)?(t=(n=r).charCodeAt(0))!==n.charCodeAt(n.length-1)||34!==t&&39!==t?n:n.slice(1,-1):"*"+r)}var Ml=function(){this._cache=Object.create(null)};Ml.prototype.parsePath=function(e){var n=this._cache[e];return n||(n=function(e){var n,t,a,r,i,o,s,c=[],l=-1,d=0,u=0,h=[];function p(){var n=e[l+1];if(5===d&&"'"===n||6===d&&'"'===n)return l++,a="\\"+n,h[0](),!0}for(h[1]=function(){void 0!==t&&(c.push(t),t=void 0)},h[0]=function(){void 0===t?t=a:t+=a},h[2]=function(){h[0](),u++},h[3]=function(){if(u>0)u--,d=4,h[0]();else{if(u=0,void 0===t)return!1;if(!1===(t=Al(t)))return!1;h[1]()}};null!==d;)if(l++,"\\"!==(n=e[l])||!p()){if(r=Cl(n),8===(i=(s=_l[d])[r]||s.else||8))return;if(d=i[0],(o=h[i[1]])&&(a=void 0===(a=i[2])?n:a,!1===o()))return;if(7===d)return c}}(e))&&(this._cache[e]=n),n||[]},Ml.prototype.getPathValue=function(e,n){if(!Hc(e))return null;var t=this.parsePath(n);if(0===t.length)return null;for(var a=t.length,r=e,i=0;i<a;){var o=r[t[i]];if(null==o)return null;r=o,i++}return r};var Sl,Tl=/<\/?[\w\s="/.':;#-\/]+>/,Pl=/(?:@(?:\.[a-zA-Z]+)?:(?:[\w\-_|./]+|\([\w\-_:|./]+\)))/g,Il=/^@(?:\.([a-zA-Z]+))?:/,Dl=/[()]/g,Ll={upper:function(e){return e.toLocaleUpperCase()},lower:function(e){return e.toLocaleLowerCase()},capitalize:function(e){return""+e.charAt(0).toLocaleUpperCase()+e.substr(1)}},Ol=new bl,zl=function(e){var n=this;void 0===e&&(e={}),!dl&&"undefined"!=typeof window&&window.Vue&&vl(window.Vue);var t=e.locale||"en-US",a=!1!==e.fallbackLocale&&(e.fallbackLocale||"en-US"),r=e.messages||{},i=e.dateTimeFormats||e.datetimeFormats||{},o=e.numberFormats||{};this._vm=null,this._formatter=e.formatter||Ol,this._modifiers=e.modifiers||{},this._missing=e.missing||null,this._root=e.root||null,this._sync=void 0===e.sync||!!e.sync,this._fallbackRoot=void 0===e.fallbackRoot||!!e.fallbackRoot,this._fallbackRootWithEmptyString=void 0===e.fallbackRootWithEmptyString||!!e.fallbackRootWithEmptyString,this._formatFallbackMessages=void 0!==e.formatFallbackMessages&&!!e.formatFallbackMessages,this._silentTranslationWarn=void 0!==e.silentTranslationWarn&&e.silentTranslationWarn,this._silentFallbackWarn=void 0!==e.silentFallbackWarn&&!!e.silentFallbackWarn,this._dateTimeFormatters={},this._numberFormatters={},this._path=new Ml,this._dataListeners=new Set,this._componentInstanceCreatedListener=e.componentInstanceCreatedListener||null,this._preserveDirectiveContent=void 0!==e.preserveDirectiveContent&&!!e.preserveDirectiveContent,this.pluralizationRules=e.pluralizationRules||{},this._warnHtmlInMessage=e.warnHtmlInMessage||"off",this._postTranslation=e.postTranslation||null,this._escapeParameterHtml=e.escapeParameterHtml||!1,"__VUE_I18N_BRIDGE__"in e&&(this.__VUE_I18N_BRIDGE__=e.__VUE_I18N_BRIDGE__),this.getChoiceIndex=function(e,t){var a=Object.getPrototypeOf(n);if(a&&a.getChoiceIndex)return a.getChoiceIndex.call(n,e,t);var r,i;return n.locale in n.pluralizationRules?n.pluralizationRules[n.locale].apply(n,[e,t]):(r=e,i=t,r=Math.abs(r),2===i?r?r>1?1:0:1:r?Math.min(r,2):0)},this._exist=function(e,t){return!(!e||!t)&&(!Qc(n._path.getPathValue(e,t))||!!e[t])},"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||Object.keys(r).forEach((function(e){n._checkLocaleMessage(e,n._warnHtmlInMessage,r[e])})),this._initVM({locale:t,fallbackLocale:a,messages:r,dateTimeFormats:i,numberFormats:o})},Rl={vm:{configurable:!0},messages:{configurable:!0},dateTimeFormats:{configurable:!0},numberFormats:{configurable:!0},availableLocales:{configurable:!0},locale:{configurable:!0},fallbackLocale:{configurable:!0},formatFallbackMessages:{configurable:!0},missing:{configurable:!0},formatter:{configurable:!0},silentTranslationWarn:{configurable:!0},silentFallbackWarn:{configurable:!0},preserveDirectiveContent:{configurable:!0},warnHtmlInMessage:{configurable:!0},postTranslation:{configurable:!0},sync:{configurable:!0}};zl.prototype._checkLocaleMessage=function(e,n,t){var a=function(e,n,t,r){if(Yc(t))Object.keys(t).forEach((function(i){var o=t[i];Yc(o)?(r.push(i),r.push("."),a(e,n,o,r),r.pop(),r.pop()):(r.push(i),a(e,n,o,r),r.pop())}));else if(Vc(t))t.forEach((function(t,i){Yc(t)?(r.push("["+i+"]"),r.push("."),a(e,n,t,r),r.pop(),r.pop()):(r.push("["+i+"]"),a(e,n,t,r),r.pop())}));else if(Wc(t)){if(Tl.test(t)){var i="Detected HTML in message '"+t+"' of keypath '"+r.join("")+"' at '"+n+"'. Consider component interpolation with '<i18n>' to avoid XSS. See https://bit.ly/2ZqJzkp";"warn"===e?Fc(i):"error"===e&&function(e,n){"undefined"!=typeof console&&(console.error("[vue-i18n] "+e),n&&console.error(n.stack))}(i)}}};a(n,e,t,[])},zl.prototype._initVM=function(e){var n=dl.config.silent;dl.config.silent=!0,this._vm=new dl({data:e,__VUE18N__INSTANCE__:!0}),dl.config.silent=n},zl.prototype.destroyVM=function(){this._vm.$destroy()},zl.prototype.subscribeDataChanging=function(e){this._dataListeners.add(e)},zl.prototype.unsubscribeDataChanging=function(e){!function(e,n){if(e.delete(n));}(this._dataListeners,e)},zl.prototype.watchI18nData=function(){var e=this;return this._vm.$watch("$data",(function(){for(var n,t,a=(n=e._dataListeners,t=[],n.forEach((function(e){return t.push(e)})),t),r=a.length;r--;)dl.nextTick((function(){a[r]&&a[r].$forceUpdate()}))}),{deep:!0})},zl.prototype.watchLocale=function(e){if(e){if(!this.__VUE_I18N_BRIDGE__)return null;var n=this,t=this._vm;return this.vm.$watch("locale",(function(a){t.$set(t,"locale",a),n.__VUE_I18N_BRIDGE__&&e&&(e.locale.value=a),t.$forceUpdate()}),{immediate:!0})}if(!this._sync||!this._root)return null;var a=this._vm;return this._root.$i18n.vm.$watch("locale",(function(e){a.$set(a,"locale",e),a.$forceUpdate()}),{immediate:!0})},zl.prototype.onComponentInstanceCreated=function(e){this._componentInstanceCreatedListener&&this._componentInstanceCreatedListener(e,this)},Rl.vm.get=function(){return this._vm},Rl.messages.get=function(){return Jc(this._getMessages())},Rl.dateTimeFormats.get=function(){return Jc(this._getDateTimeFormats())},Rl.numberFormats.get=function(){return Jc(this._getNumberFormats())},Rl.availableLocales.get=function(){return Object.keys(this.messages).sort()},Rl.locale.get=function(){return this._vm.locale},Rl.locale.set=function(e){this._vm.$set(this._vm,"locale",e)},Rl.fallbackLocale.get=function(){return this._vm.fallbackLocale},Rl.fallbackLocale.set=function(e){this._localeChainCache={},this._vm.$set(this._vm,"fallbackLocale",e)},Rl.formatFallbackMessages.get=function(){return this._formatFallbackMessages},Rl.formatFallbackMessages.set=function(e){this._formatFallbackMessages=e},Rl.missing.get=function(){return this._missing},Rl.missing.set=function(e){this._missing=e},Rl.formatter.get=function(){return this._formatter},Rl.formatter.set=function(e){this._formatter=e},Rl.silentTranslationWarn.get=function(){return this._silentTranslationWarn},Rl.silentTranslationWarn.set=function(e){this._silentTranslationWarn=e},Rl.silentFallbackWarn.get=function(){return this._silentFallbackWarn},Rl.silentFallbackWarn.set=function(e){this._silentFallbackWarn=e},Rl.preserveDirectiveContent.get=function(){return this._preserveDirectiveContent},Rl.preserveDirectiveContent.set=function(e){this._preserveDirectiveContent=e},Rl.warnHtmlInMessage.get=function(){return this._warnHtmlInMessage},Rl.warnHtmlInMessage.set=function(e){var n=this,t=this._warnHtmlInMessage;if(this._warnHtmlInMessage=e,t!==e&&("warn"===e||"error"===e)){var a=this._getMessages();Object.keys(a).forEach((function(e){n._checkLocaleMessage(e,n._warnHtmlInMessage,a[e])}))}},Rl.postTranslation.get=function(){return this._postTranslation},Rl.postTranslation.set=function(e){this._postTranslation=e},Rl.sync.get=function(){return this._sync},Rl.sync.set=function(e){this._sync=e},zl.prototype._getMessages=function(){return this._vm.messages},zl.prototype._getDateTimeFormats=function(){return this._vm.dateTimeFormats},zl.prototype._getNumberFormats=function(){return this._vm.numberFormats},zl.prototype._warnDefault=function(e,n,t,a,r,i){if(!Qc(t))return t;if(this._missing){var o=this._missing.apply(null,[e,n,a,r]);if(Wc(o))return o}else 0;if(this._formatFallbackMessages){var s=Xc.apply(void 0,r);return this._render(n,i,s.params,n)}return n},zl.prototype._isFallbackRoot=function(e){return(this._fallbackRootWithEmptyString?!e:Qc(e))&&!Qc(this._root)&&this._fallbackRoot},zl.prototype._isSilentFallbackWarn=function(e){return this._silentFallbackWarn instanceof RegExp?this._silentFallbackWarn.test(e):this._silentFallbackWarn},zl.prototype._isSilentFallback=function(e,n){return this._isSilentFallbackWarn(n)&&(this._isFallbackRoot()||e!==this.fallbackLocale)},zl.prototype._isSilentTranslationWarn=function(e){return this._silentTranslationWarn instanceof RegExp?this._silentTranslationWarn.test(e):this._silentTranslationWarn},zl.prototype._interpolate=function(e,n,t,a,r,i,o){if(!n)return null;var s,c=this._path.getPathValue(n,t);if(Vc(c)||Yc(c))return c;if(Qc(c)){if(!Yc(n))return null;if(!Wc(s=n[t])&&!Zc(s))return null}else{if(!Wc(c)&&!Zc(c))return null;s=c}return Wc(s)&&(s.indexOf("@:")>=0||s.indexOf("@.")>=0)&&(s=this._link(e,n,s,a,"raw",i,o)),this._render(s,r,i,t)},zl.prototype._link=function(e,n,t,a,r,i,o){var s=t,c=s.match(Pl);for(var l in c)if(c.hasOwnProperty(l)){var d=c[l],u=d.match(Il),h=u[0],p=u[1],m=d.replace(h,"").replace(Dl,"");if(el(o,m))return s;o.push(m);var f=this._interpolate(e,n,m,a,"raw"===r?"string":r,"raw"===r?void 0:i,o);if(this._isFallbackRoot(f)){if(!this._root)throw Error("unexpected error");var g=this._root.$i18n;f=g._translate(g._getMessages(),g.locale,g.fallbackLocale,m,a,r,i)}f=this._warnDefault(e,m,f,a,Vc(i)?i:[i],r),this._modifiers.hasOwnProperty(p)?f=this._modifiers[p](f):Ll.hasOwnProperty(p)&&(f=Ll[p](f)),o.pop(),s=f?s.replace(d,f):s}return s},zl.prototype._createMessageContext=function(e,n,t,a){var r=this,i=Vc(e)?e:[],o=Hc(e)?e:{},s=this._getMessages(),c=this.locale;return{list:function(e){return i[e]},named:function(e){return o[e]},values:e,formatter:n,path:t,messages:s,locale:c,linked:function(e){return r._interpolate(c,s[c]||{},e,null,a,void 0,[e])}}},zl.prototype._render=function(e,n,t,a){if(Zc(e))return e(this._createMessageContext(t,this._formatter||Ol,a,n));var r=this._formatter.interpolate(e,t,a);return r||(r=Ol.interpolate(e,t,a)),"string"!==n||Wc(r)?r:r.join("")},zl.prototype._appendItemToChain=function(e,n,t){var a=!1;return el(e,n)||(a=!0,n&&(a="!"!==n[n.length-1],n=n.replace(/!/g,""),e.push(n),t&&t[n]&&(a=t[n]))),a},zl.prototype._appendLocaleToChain=function(e,n,t){var a,r=n.split("-");do{var i=r.join("-");a=this._appendItemToChain(e,i,t),r.splice(-1,1)}while(r.length&&!0===a);return a},zl.prototype._appendBlockToChain=function(e,n,t){for(var a=!0,r=0;r<n.length&&"boolean"==typeof a;r++){var i=n[r];Wc(i)&&(a=this._appendLocaleToChain(e,i,t))}return a},zl.prototype._getLocaleChain=function(e,n){if(""===e)return[];this._localeChainCache||(this._localeChainCache={});var t=this._localeChainCache[e];if(!t){n||(n=this.fallbackLocale),t=[];for(var a,r=[e];Vc(r);)r=this._appendBlockToChain(t,r,n);(r=Wc(a=Vc(n)?n:Hc(n)?n.default?n.default:null:n)?[a]:a)&&this._appendBlockToChain(t,r,null),this._localeChainCache[e]=t}return t},zl.prototype._translate=function(e,n,t,a,r,i,o){for(var s,c=this._getLocaleChain(n,t),l=0;l<c.length;l++){var d=c[l];if(!Qc(s=this._interpolate(d,e[d],a,r,i,o,[a])))return s}return null},zl.prototype._t=function(e,n,t,a){for(var r,i=[],o=arguments.length-4;o-- >0;)i[o]=arguments[o+4];if(!e)return"";var s=Xc.apply(void 0,i);this._escapeParameterHtml&&(s.params=il(s.params));var c=s.locale||n,l=this._translate(t,c,this.fallbackLocale,e,a,"string",s.params);if(this._isFallbackRoot(l)){if(!this._root)throw Error("unexpected error");return(r=this._root).$t.apply(r,[e].concat(i))}return l=this._warnDefault(c,e,l,a,i,"string"),this._postTranslation&&null!=l&&(l=this._postTranslation(l,e)),l},zl.prototype.t=function(e){for(var n,t=[],a=arguments.length-1;a-- >0;)t[a]=arguments[a+1];return(n=this)._t.apply(n,[e,this.locale,this._getMessages(),null].concat(t))},zl.prototype._i=function(e,n,t,a,r){var i=this._translate(t,n,this.fallbackLocale,e,a,"raw",r);if(this._isFallbackRoot(i)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.i(e,n,r)}return this._warnDefault(n,e,i,a,[r],"raw")},zl.prototype.i=function(e,n,t){return e?(Wc(n)||(n=this.locale),this._i(e,n,this._getMessages(),null,t)):""},zl.prototype._tc=function(e,n,t,a,r){for(var i,o=[],s=arguments.length-5;s-- >0;)o[s]=arguments[s+5];if(!e)return"";void 0===r&&(r=1);var c={count:r,n:r},l=Xc.apply(void 0,o);return l.params=Object.assign(c,l.params),o=null===l.locale?[l.params]:[l.locale,l.params],this.fetchChoice((i=this)._t.apply(i,[e,n,t,a].concat(o)),r)},zl.prototype.fetchChoice=function(e,n){if(!e||!Wc(e))return null;var t=e.split("|");return t[n=this.getChoiceIndex(n,t.length)]?t[n].trim():e},zl.prototype.tc=function(e,n){for(var t,a=[],r=arguments.length-2;r-- >0;)a[r]=arguments[r+2];return(t=this)._tc.apply(t,[e,this.locale,this._getMessages(),null,n].concat(a))},zl.prototype._te=function(e,n,t){for(var a=[],r=arguments.length-3;r-- >0;)a[r]=arguments[r+3];var i=Xc.apply(void 0,a).locale||n;return this._exist(t[i],e)},zl.prototype.te=function(e,n){return this._te(e,this.locale,this._getMessages(),n)},zl.prototype.getLocaleMessage=function(e){return Jc(this._vm.messages[e]||{})},zl.prototype.setLocaleMessage=function(e,n){"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||this._checkLocaleMessage(e,this._warnHtmlInMessage,n),this._vm.$set(this._vm.messages,e,n)},zl.prototype.mergeLocaleMessage=function(e,n){"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||this._checkLocaleMessage(e,this._warnHtmlInMessage,n),this._vm.$set(this._vm.messages,e,al(void 0!==this._vm.messages[e]&&Object.keys(this._vm.messages[e]).length?Object.assign({},this._vm.messages[e]):{},n))},zl.prototype.getDateTimeFormat=function(e){return Jc(this._vm.dateTimeFormats[e]||{})},zl.prototype.setDateTimeFormat=function(e,n){this._vm.$set(this._vm.dateTimeFormats,e,n),this._clearDateTimeFormat(e,n)},zl.prototype.mergeDateTimeFormat=function(e,n){this._vm.$set(this._vm.dateTimeFormats,e,al(this._vm.dateTimeFormats[e]||{},n)),this._clearDateTimeFormat(e,n)},zl.prototype._clearDateTimeFormat=function(e,n){for(var t in n){var a=e+"__"+t;this._dateTimeFormatters.hasOwnProperty(a)&&delete this._dateTimeFormatters[a]}},zl.prototype._localizeDateTime=function(e,n,t,a,r,i){for(var o=n,s=a[o],c=this._getLocaleChain(n,t),l=0;l<c.length;l++){var d=c[l];if(o=d,!Qc(s=a[d])&&!Qc(s[r]))break}if(Qc(s)||Qc(s[r]))return null;var u,h=s[r];if(i)u=new Intl.DateTimeFormat(o,Object.assign({},h,i));else{var p=o+"__"+r;(u=this._dateTimeFormatters[p])||(u=this._dateTimeFormatters[p]=new Intl.DateTimeFormat(o,h))}return u.format(e)},zl.prototype._d=function(e,n,t,a){if(!t)return(a?new Intl.DateTimeFormat(n,a):new Intl.DateTimeFormat(n)).format(e);var r=this._localizeDateTime(e,n,this.fallbackLocale,this._getDateTimeFormats(),t,a);if(this._isFallbackRoot(r)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.d(e,t,n)}return r||""},zl.prototype.d=function(e){for(var n=[],t=arguments.length-1;t-- >0;)n[t]=arguments[t+1];var a=this.locale,r=null,i=null;return 1===n.length?(Wc(n[0])?r=n[0]:Hc(n[0])&&(n[0].locale&&(a=n[0].locale),n[0].key&&(r=n[0].key)),i=Object.keys(n[0]).reduce((function(e,t){var a;return el(qc,t)?Object.assign({},e,((a={})[t]=n[0][t],a)):e}),null)):2===n.length&&(Wc(n[0])&&(r=n[0]),Wc(n[1])&&(a=n[1])),this._d(e,a,r,i)},zl.prototype.getNumberFormat=function(e){return Jc(this._vm.numberFormats[e]||{})},zl.prototype.setNumberFormat=function(e,n){this._vm.$set(this._vm.numberFormats,e,n),this._clearNumberFormat(e,n)},zl.prototype.mergeNumberFormat=function(e,n){this._vm.$set(this._vm.numberFormats,e,al(this._vm.numberFormats[e]||{},n)),this._clearNumberFormat(e,n)},zl.prototype._clearNumberFormat=function(e,n){for(var t in n){var a=e+"__"+t;this._numberFormatters.hasOwnProperty(a)&&delete this._numberFormatters[a]}},zl.prototype._getNumberFormatter=function(e,n,t,a,r,i){for(var o=n,s=a[o],c=this._getLocaleChain(n,t),l=0;l<c.length;l++){var d=c[l];if(o=d,!Qc(s=a[d])&&!Qc(s[r]))break}if(Qc(s)||Qc(s[r]))return null;var u,h=s[r];if(i)u=new Intl.NumberFormat(o,Object.assign({},h,i));else{var p=o+"__"+r;(u=this._numberFormatters[p])||(u=this._numberFormatters[p]=new Intl.NumberFormat(o,h))}return u},zl.prototype._n=function(e,n,t,a){if(!zl.availabilities.numberFormat)return"";if(!t)return(a?new Intl.NumberFormat(n,a):new Intl.NumberFormat(n)).format(e);var r=this._getNumberFormatter(e,n,this.fallbackLocale,this._getNumberFormats(),t,a),i=r&&r.format(e);if(this._isFallbackRoot(i)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.n(e,Object.assign({},{key:t,locale:n},a))}return i||""},zl.prototype.n=function(e){for(var n=[],t=arguments.length-1;t-- >0;)n[t]=arguments[t+1];var a=this.locale,r=null,i=null;return 1===n.length?Wc(n[0])?r=n[0]:Hc(n[0])&&(n[0].locale&&(a=n[0].locale),n[0].key&&(r=n[0].key),i=Object.keys(n[0]).reduce((function(e,t){var a;return el($c,t)?Object.assign({},e,((a={})[t]=n[0][t],a)):e}),null)):2===n.length&&(Wc(n[0])&&(r=n[0]),Wc(n[1])&&(a=n[1])),this._n(e,a,r,i)},zl.prototype._ntp=function(e,n,t,a){if(!zl.availabilities.numberFormat)return[];if(!t)return(a?new Intl.NumberFormat(n,a):new Intl.NumberFormat(n)).formatToParts(e);var r=this._getNumberFormatter(e,n,this.fallbackLocale,this._getNumberFormats(),t,a),i=r&&r.formatToParts(e);if(this._isFallbackRoot(i)){if(!this._root)throw Error("unexpected error");return this._root.$i18n._ntp(e,n,t,a)}return i||[]},Object.defineProperties(zl.prototype,Rl),Object.defineProperty(zl,"availabilities",{get:function(){if(!Sl){var e="undefined"!=typeof Intl;Sl={dateTimeFormat:e&&void 0!==Intl.DateTimeFormat,numberFormat:e&&void 0!==Intl.NumberFormat}}return Sl}}),zl.install=vl,zl.version="8.28.2";var El=zl;
/*!
 * vssue - A vue-powered issue-based comment plugin
 *
 * @version v1.4.8
 * @link https://vssue.js.org
 * @license MIT
 * @copyright 2018-2021 meteorlxy
 */
/*! *****************************************************************************
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use
this file except in compliance with the License. You may obtain a copy of the
License at http://www.apache.org/licenses/LICENSE-2.0

THIS CODE IS PROVIDED ON AN *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
MERCHANTABLITY OR NON-INFRINGEMENT.

See the Apache Version 2.0 License for specific language governing permissions
and limitations under the License.
***************************************************************************** */function Ul(e,n,t,a){var r,i=arguments.length,o=i<3?n:null===a?a=Object.getOwnPropertyDescriptor(n,t):a;if("object"==typeof Reflect&&"function"==typeof Reflect.decorate)o=Reflect.decorate(e,n,t,a);else for(var s=e.length-1;s>=0;s--)(r=e[s])&&(o=(i<3?r(o):i>3?r(n,t,o):r(n,t))||o);return i>3&&o&&Object.defineProperty(n,t,o),o}var jl=Wt.extend({name:"Iconfont"});function Bl(e,n,t,a,r,i,o,s,c,l){"boolean"!=typeof o&&(c=s,s=o,o=!1);const d="function"==typeof t?t.options:t;let u;if(e&&e.render&&(d.render=e.render,d.staticRenderFns=e.staticRenderFns,d._compiled=!0,r&&(d.functional=!0)),a&&(d._scopeId=a),i?(u=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),n&&n.call(this,c(e)),e&&e._registeredComponents&&e._registeredComponents.add(i)},d._ssrRegister=u):n&&(u=o?function(e){n.call(this,l(e,this.$root.$options.shadowRoot))}:function(e){n.call(this,s(e))}),u)if(d.functional){const e=d.render;d.render=function(n,t){return u.call(t),e(n,t)}}else{const e=d.beforeCreate;d.beforeCreate=e?[].concat(e,u):[u]}return t}"undefined"!=typeof navigator&&/msie [6-9]\\b/.test(navigator.userAgent.toLowerCase());const Gl=Bl({render:function(e,n){var t=n._c;return t("svg",{directives:[{name:"show",rawName:"v-show",value:!1,expression:"false"}]},[t("symbol",{attrs:{id:"vssue-icon-bitbucket",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M579.5522464 489.45249493q4.8371808 38.38537173-30.81752427 61.55702827t-67.95459093 3.66689493q-23.79580907-10.37653333-32.6119616-35.34262826t-0.31207573-50.01020907 31.67573333-35.34262827q21.92335253-11.00068587 44.1587808-7.33379093t39.00952427 21.61127573 16.77409493 41.1160384zM647.19476053 476.65737173q-8.50407573-65.22392427-68.8908192-99.9424t-120.07131413-7.9579424q-38.38537173 17.08617173-61.24495253 53.9111616t-21.0651424 78.95527574q2.41859093 55.4715424 47.20152426 94.48106666t100.87862827 34.1723424q55.4715424-4.8371808 92.60860907-51.18049493t30.50544746-102.43900907zM792.93434133 146.32472427q-12.17097173-16.4620192-34.1723424-27.15062827t-35.34262826-13.41927573-43.30057174-7.64586667q-177.33729493-28.63299093-345.00022826 1.24830507-26.2144 4.29104747-40.25782827 7.33379093t-33.54819093 13.41927573-30.50544747 26.2144q18.2564576 17.08617173 46.34331413 27.6967616t44.78293334 13.41927574 53.36502826 7.02171413q138.95192427 17.71032427 273.06666667 0.62415253 38.38537173-4.8371808 54.53531413-7.33379093t44.1587808-13.1072 45.7191616-28.32091413zM827.65281813 777.10872427q-4.8371808 15.83786667-9.44030506 46.65539093t-8.50407574 51.18049493-17.39824746 42.6764192-35.34262827 34.4064q-52.4288 29.2571424-115.46819093 43.61264747t-123.1140576 13.41927573-122.8019808-11.3127616q-28.0088384-4.8371808-49.69813334-11.00068586t-46.65539093-16.4620192-44.4708576-26.52647574-31.67573333-37.4491424q-15.21371413-58.51428587-34.71847574-177.96144746l3.66689494-9.7523808 11.00068586-5.46133334q135.9091808 90.1900192 308.72137174 90.1900192t309.34552426-90.1900192q12.79512427 3.66689493 14.5895616 14.04342827t-3.0427424 27.46270507-4.8371808 22.54750506zM937.97175147 191.41973333q-15.83786667 101.8148576-67.64251414 399.22346667-3.0427424 18.2564576-16.4620192 34.1723424t-26.52647573 24.3419424-33.23611413 18.88060907q-153.61950507 76.7707424-371.8387808 53.67710506-151.12289493-16.4620192-240.14262827-84.72868586-9.12822827-7.33379093-15.52579093-16.1499424t-10.37653334-21.2992-5.46133333-20.75306667-3.66689493-24.10788587-3.3548192-21.2992q-5.46133333-30.50544747-16.1499424-91.43832426t-17.08617174-98.4600384-14.35550506-89.8779424-13.41927574-96.27550507q1.7944384-15.83786667 10.68860907-29.5692192t19.19268587-22.8595808 27.46270506-18.2564576 28.0088384-13.73135253 29.2571424-11.3127616q76.22460907-28.0088384 190.75657174-39.00952427 231.0144-22.54750507 412.01859093 30.50544747 94.48106667 28.0088384 131.072 74.35215253 9.7523808 12.17097173 10.0644576 31.0515808t-3.3548192 32.9240384z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitea",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M184.31868985 236.10860742C106.94832667 235.94086648 3.32655508 285.13080468 9.02973665 408.46209936c8.93218827 192.65010787 206.32096845 210.5144844 285.20099725 212.06608453 8.63864186 36.14810496 101.48307766 160.77938883 170.21479898 167.32127321h301.09442177c180.57278288-11.99345499 315.77172611-546.07960359 215.54670217-548.09249109-165.7696721 7.79993906-264.02374305 11.74184405-348.27147151 12.41280591v166.69224585l-26.25140843-11.61603761-0.16773997-154.99233728c-96.70246985-0.04193548-181.83083757-4.52899687-343.4069947-12.49667687-20.21274496-0.12580547-48.39316992-3.5644886-78.67035236-3.64835859z m10.94507577 68.14462849h9.22573371c10.98701124 98.75729283 28.85138778 156.50200291 64.99949274 244.73357185-92.25734394-10.90314029-170.75995634-37.69970509-185.18564974-137.75698809-7.46445813-51.78991757 17.69663558-105.84433456 110.96042329-107.01851827z m358.83913087 97.07988723c6.29027343 0.08386999 12.70635233 1.25805468 18.74501482 4.02577499l31.40943263 13.54505513-22.51917887 41.05451824a28.18042496 25.03528825 0 0 0-10.10637297 1.59353561 28.18042496 25.03528825 0 0 0-16.98373825 32.038459 28.18042496 25.03528825 0 0 0 4.69673781 7.29671718l-38.83195528 70.70267333a28.18042496 25.03528825 0 0 0-9.30960467 1.59353659 28.18042496 25.03528825 0 0 0-16.98373825 32.038459 28.18042496 25.03528825 0 0 0 36.06423497 15.09665623 28.18042496 25.03528825 0 0 0 16.94180276-32.08039449 28.18042496 25.03528825 0 0 0-6.62575434-9.22573468l37.82551056-68.85752581a28.18042496 25.03528825 0 0 0 12.28700044-1.25805469 28.18042496 25.03528825 0 0 0 8.93218826-4.69673783c14.59343435 6.12253248 26.54495386 11.11281671 35.14166122 15.34826717 12.91602778 6.37414341 17.48696012 10.60959485 18.87082027 15.30633169 1.38386015 4.61286685-0.12580547 13.50312062-7.42252263 29.10299872-5.45157063 11.61603859-14.46762889 28.09655497-25.11915823 47.51253164a28.18042496 25.03528825 0 0 0-10.52572486 1.59353659 28.18042496 25.03528825 0 0 0-16.98373826 32.038459 28.18042496 25.03528825 0 0 0 36.06423498 15.09665623 28.18042496 25.03528825 0 0 0 16.94180278-32.03845901 28.18042496 25.03528825 0 0 0-5.74511608-8.47090188c10.52572388-19.20630122 19.58371762-35.72875308 25.41270465-48.14155897 7.88380904-16.85793279 11.99345499-29.39654416 8.38703091-41.51580463-3.60642311-12.11926046-14.67730434-20.0030695-29.35460966-27.25785217-9.6450856-4.73867233-21.68047607-9.77089106-36.06423399-15.80955357a28.18042496 25.03528825 0 0 0-1.59353562-10.022502 28.18042496 25.03528825 0 0 0-6.08059796-8.7644483l22.14176246-40.38355541 122.61839638 52.96410227c22.14176247 9.6031511 31.2836262 33.12877372 20.54822685 52.8382968l-84.28966393 154.32137544c-10.77733482 19.66758857-37.23841869 27.80300855-59.38018118 18.24179293l-173.48574115-74.98005927c-22.14176247-9.5612156-31.32556167-33.12877372-20.54822687-52.83829679l84.28966395-154.27943995c7.38058716-13.54505513 22.22563246-21.59660511 37.951317-22.22563246h2.68384935z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitee",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M978.404275 409.561604H455.061338c-25.117645 0-45.499734 20.382089-45.499734 45.499734l-0.031997 113.781333c0 25.117645 20.350092 45.499734 45.499734 45.531731h318.594132c25.117645 0 45.499734 20.382089 45.499734 45.499735v22.749867a136.5312 136.5312 0 0 1-136.5312 136.5312H250.248539a45.499734 45.499734 0 0 1-45.499734-45.499734V341.343999a136.5312 136.5312 0 0 1 136.5312-136.5312L978.308284 204.780802c25.117645 0 45.499734-20.350092 45.499734-45.467738L1023.904009 45.531731h0.031997A45.499734 45.499734 0 0 0 978.468269 0h-0.031997L341.343999 0.031997C152.84967 0.031997 0.031997 152.84967 0.031997 341.343999v637.092273c0 25.117645 20.382089 45.499734 45.499734 45.499734h671.233072a307.171203 307.171203 0 0 0 307.171203-307.171203v-261.671468c0-25.117645-20.382089-45.499734-45.499734-45.499734z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-github",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M512 20.4425c-278.334 0-504 225.6345-504 504 0 222.6735 144.4275 411.6105 344.673 478.233 25.2 4.662 34.461-10.9305 34.461-24.255 0-12.0015-0.4725-51.723-0.693-93.8385-140.238 30.492-169.8165-59.472-169.8165-59.472-22.932-58.2435-55.944-73.7415-55.944-73.7415-45.738-31.2795 3.465-30.6495 3.465-30.6495 50.589 3.5595 77.238 51.9435 77.238 51.9435 44.9505 77.049 117.9045 54.7785 146.664 41.895 4.5045-32.571 17.577-54.81 32.004-67.41-111.951-12.726-229.635-55.9755-229.635-249.0705 0-55.0305 19.6875-99.981 51.9435-135.2925-5.229-12.6945-22.491-63.945 4.8825-133.371 0 0 42.336-13.545 138.6315 51.66 40.194-11.1825 83.3175-16.758 126.1575-16.9785 42.8085 0.189 85.9635 5.796 126.252 16.9785 96.201-65.205 138.4425-51.66 138.4425-51.66 27.4365 69.426 10.1745 120.6765 4.9455 133.371 32.319 35.28 51.8805 80.262 51.8805 135.2925 0 193.5675-117.9045 236.187-230.139 248.6925 18.081 15.6555 34.1775 46.305 34.1775 93.3345 0 67.4415-0.5985 121.716-0.5985 138.3165 0 13.419 9.072 29.1375 34.6185 24.192 200.151-66.717 344.3895-255.5595 344.3895-478.17 0-278.3655-225.666-504-504-504z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitlab",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M66.61375986 405.11600042L512.11376028 976.03999972 23.84576 621.65599958a39.312 39.312 0 0 1-14.07600042-43.30799944l56.8080007-173.26800028z m259.88400014 0h371.26800014L512.14975986 976.03999972zM215.11376 60.88400042l111.384 344.232H66.61375986l111.384-344.232a19.72800014 19.72800014 0 0 1 37.11600014 0z m742.49999972 344.232l56.8080007 173.2679993a39.23999986 39.23999986 0 0 1-14.07600042 43.30800042l-488.26800028 354.38400014 445.50000042-570.92400028z m0 0h-259.88400014l111.384-344.232a19.72800014 19.72800014 0 0 1 37.11600014 0z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-loading",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M843.307 742.24c0 3.217 2.607 5.824 5.824 5.824s5.824-2.607 5.824-5.824a5.823 5.823 0 0 0-5.824-5.824 5.823 5.823 0 0 0-5.824 5.824zM714.731 874.912c0 6.398 5.186 11.584 11.584 11.584s11.584-5.186 11.584-11.584-5.186-11.584-11.584-11.584-11.584 5.186-11.584 11.584zM541.419 943.2c0 9.614 7.794 17.408 17.408 17.408s17.408-7.794 17.408-17.408-7.794-17.408-17.408-17.408-17.408 7.794-17.408 17.408z m-186.56-9.152c0 12.795 10.373 23.168 23.168 23.168s23.168-10.373 23.168-23.168-10.373-23.168-23.168-23.168-23.168 10.373-23.168 23.168zM189.355 849.12c0 16.012 12.98 28.992 28.992 28.992s28.992-12.98 28.992-28.992-12.98-28.992-28.992-28.992-28.992 12.98-28.992 28.992zM74.731 704.736c0 19.228 15.588 34.816 34.816 34.816s34.816-15.588 34.816-34.816-15.588-34.816-34.816-34.816-34.816 15.588-34.816 34.816z m-43.008-177.28c0 22.41 18.166 40.576 40.576 40.576s40.576-18.166 40.576-40.576-18.166-40.576-40.576-40.576-40.576 18.166-40.576 40.576z m35.392-176.128c0 25.626 20.774 46.4 46.4 46.4s46.4-20.774 46.4-46.4c0-25.626-20.774-46.4-46.4-46.4-25.626 0-46.4 20.774-46.4 46.4z m106.176-142.016c0 28.843 23.381 52.224 52.224 52.224s52.224-23.381 52.224-52.224c0-28.843-23.381-52.224-52.224-52.224-28.843 0-52.224 23.381-52.224 52.224z m155.904-81.344c0 32.024 25.96 57.984 57.984 57.984s57.984-25.96 57.984-57.984-25.96-57.984-57.984-57.984-57.984 25.96-57.984 57.984z m175.104-5.056c0 35.24 28.568 63.808 63.808 63.808s63.808-28.568 63.808-63.808c0-35.24-28.568-63.808-63.808-63.808-35.24 0-63.808 28.568-63.808 63.808z m160.32 72.128c0 38.421 31.147 69.568 69.568 69.568s69.568-31.147 69.568-69.568-31.147-69.568-69.568-69.568-69.568 31.147-69.568 69.568z m113.92 135.488c0 41.638 33.754 75.392 75.392 75.392s75.392-33.754 75.392-75.392-33.754-75.392-75.392-75.392-75.392 33.754-75.392 75.392z m45.312 175.488c0 44.854 36.362 81.216 81.216 81.216s81.216-36.362 81.216-81.216c0-44.854-36.362-81.216-81.216-81.216-44.854 0-81.216 36.362-81.216 81.216z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-like",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M885.9 533.7c16.8-22.2 26.1-49.4 26.1-77.7 0-44.9-25.1-87.4-65.5-111.1a67.67 67.67 0 0 0-34.3-9.3H572.4l6-122.9c1.4-29.7-9.1-57.9-29.5-79.4-20.5-21.5-48.1-33.4-77.9-33.4-52 0-98 35-111.8 85.1l-85.9 311H144c-17.7 0-32 14.3-32 32v364c0 17.7 14.3 32 32 32h601.3c9.2 0 18.2-1.8 26.5-5.4 47.6-20.3 78.3-66.8 78.3-118.4 0-12.6-1.8-25-5.4-37 16.8-22.2 26.1-49.4 26.1-77.7 0-12.6-1.8-25-5.4-37 16.8-22.2 26.1-49.4 26.1-77.7-0.2-12.6-2-25.1-5.6-37.1zM184 852V568h81v284h-81z m636.4-353l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 16.5-7.2 32.2-19.6 43l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 16.5-7.2 32.2-19.6 43l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 22.4-13.2 42.6-33.6 51.8H329V564.8l99.5-360.5c5.2-18.9 22.5-32.2 42.2-32.3 7.6 0 15.1 2.2 21.1 6.7 9.9 7.4 15.2 18.6 14.6 30.5l-9.6 198.4h314.4C829 418.5 840 436.9 840 456c0 16.5-7.2 32.1-19.6 43z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-unlike",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M885.9 490.3c3.6-12 5.4-24.4 5.4-37 0-28.3-9.3-55.5-26.1-77.7 3.6-12 5.4-24.4 5.4-37 0-28.3-9.3-55.5-26.1-77.7 3.6-12 5.4-24.4 5.4-37 0-51.6-30.7-98.1-78.3-118.4-8.3-3.6-17.2-5.4-26.5-5.4H144c-17.7 0-32 14.3-32 32v364c0 17.7 14.3 32 32 32h129.3l85.8 310.8C372.9 889 418.9 924 470.9 924c29.7 0 57.4-11.8 77.9-33.4 20.5-21.5 31-49.7 29.5-79.4l-6-122.9h239.9c12.1 0 23.9-3.2 34.3-9.3 40.4-23.5 65.5-66.1 65.5-111 0-28.3-9.3-55.5-26.1-77.7zM184 456V172h81v284h-81z m627.2 160.4H496.8l9.6 198.4c0.6 11.9-4.7 23.1-14.6 30.5-6.1 4.5-13.6 6.8-21.1 6.7-19.6-0.1-36.9-13.4-42.2-32.3L329 459.2V172h415.4c20.4 9.2 33.6 29.4 33.6 51.8 0 9.7-2.3 18.9-6.9 27.3l-13.9 25.4 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 9.7-2.3 18.9-6.9 27.3l-13.9 25.4 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 9.7-2.3 18.9-6.9 27.3l-14 25.5 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 19.1-11 37.5-28.8 48.4z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-heart",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M923 283.6c-13.4-31.1-32.6-58.9-56.9-82.8-24.3-23.8-52.5-42.4-84-55.5-32.5-13.5-66.9-20.3-102.4-20.3-49.3 0-97.4 13.5-139.2 39-10 6.1-19.5 12.8-28.5 20.1-9-7.3-18.5-14-28.5-20.1-41.8-25.5-89.9-39-139.2-39-35.5 0-69.9 6.8-102.4 20.3-31.4 13-59.7 31.7-84 55.5-24.4 23.9-43.5 51.7-56.9 82.8-13.9 32.3-21 66.6-21 101.9 0 33.3 6.8 68 20.3 103.3 11.3 29.5 27.5 60.1 48.2 91 32.8 48.9 77.9 99.9 133.9 151.6 92.8 85.7 184.7 144.9 188.6 147.3l23.7 15.2c10.5 6.7 24 6.7 34.5 0l23.7-15.2c3.9-2.5 95.7-61.6 188.6-147.3 56-51.7 101.1-102.7 133.9-151.6 20.7-30.9 37-61.5 48.2-91 13.5-35.3 20.3-70 20.3-103.3 0.1-35.3-7-69.6-20.9-101.9zM512 814.8S156 586.7 156 385.5C156 283.6 240.3 201 344.3 201c73.1 0 136.5 40.8 167.7 100.4C543.2 241.8 606.6 201 679.7 201c104 0 188.3 82.6 188.3 184.5 0 201.2-356 429.3-356 429.3z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-edit",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M723.2 917.76H286.72c-65.28 0-118.4-51.2-118.4-113.92V261.76C168.32 198.4 221.44 147.2 286.72 147.2h375.04c17.92 0 32 14.08 32 32s-14.08 32-32 32H286.72c-30.08 0-54.4 22.4-54.4 49.92v542.08c0 27.52 24.32 49.92 54.4 49.92H723.2c30.08 0 54.4-22.4 54.4-49.92V440.32c0-17.92 14.08-32 32-32s32 14.08 32 32v363.52c0 62.72-53.12 113.92-118.4 113.92z"}}),n._v(" "),t("path",{attrs:{d:"M499.84 602.24c-7.68 0-14.72-2.56-21.12-7.68-13.44-11.52-14.72-32-3.2-45.44L780.16 198.4c11.52-13.44 32-14.72 45.44-3.2s14.72 32 3.2 45.44L524.16 591.36c-6.4 7.04-15.36 10.88-24.32 10.88z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-delete",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M677.647059 256l0-90.352941c0-37.436235-23.461647-60.235294-61.771294-60.235294L408.094118 105.411765c-38.249412 0-61.741176 22.799059-61.741176 60.235294l0 90.352941-180.705882 0 0 60.235294 60.235294 0 0 512c0 54.272 33.972706 90.352941 90.352941 90.352941l391.529412 0c55.085176 0 90.352941-33.490824 90.352941-90.352941l0-512 60.235294 0 0-60.235294L677.647059 256zM406.588235 165.647059l210.823529 0-1.264941 90.352941L406.588235 256 406.588235 165.647059zM737.882353 858.352941l-451.764706 0 0-542.117647 451.764706 0L737.882353 858.352941zM466.823529 376.470588l-58.729412 0-1.505882 391.529412 60.235294 0L466.823529 376.470588zM617.411765 376.470588l-60.235294 0 0 391.529412 60.235294 0L617.411765 376.470588z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-reply",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M426.666667 384 426.666667 213.333333 128 512 426.666667 810.666667 426.666667 635.733333C640 635.733333 789.333333 704 896 853.333333 853.333333 640 725.333333 426.666667 426.666667 384Z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-error",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M512 720m-48 0a48 48 0 1 0 96 0 48 48 0 1 0-96 0Z"}}),n._v(" "),t("path",{attrs:{d:"M480 416v184c0 4.4 3.6 8 8 8h48c4.4 0 8-3.6 8-8V416c0-4.4-3.6-8-8-8h-48c-4.4 0-8 3.6-8 8z"}}),n._v(" "),t("path",{attrs:{d:"M955.7 856l-416-720c-6.2-10.7-16.9-16-27.7-16s-21.6 5.3-27.7 16l-416 720C56 877.4 71.4 904 96 904h832c24.6 0 40-26.6 27.7-48z m-783.5-27.9L512 239.9l339.8 588.2H172.2z"}})])])},staticRenderFns:[]},void 0,jl,void 0,!0,void 0,!1,void 0,void 0,void 0);const Nl=Bl({},void 0,Wt.extend({name:"TransitionFade",functional:!0,props:{group:{type:Boolean,required:!1,default:!1},tag:{type:String,required:!1,default:"div"}},render:(e,{props:n,children:t})=>e(n.group?"TransitionGroup":"Transition",{props:{name:"fade",mode:"out-in",appear:!0,tag:n.tag}},t)}),void 0,void 0,void 0,!1,void 0,void 0,void 0);const $l=Bl({},void 0,Wt.extend({name:"VssueIcon",functional:!0,props:{name:{type:String,required:!0},title:{type:String,required:!1,default:null}},render:(e,{props:n,data:t})=>e("svg",Object.assign(Object.assign({},t),{class:["vssue-icon","vssue-icon-"+n.name],attrs:{"aria-hidden":"true"}}),[e("title",n.title),e("use",{attrs:{"xlink:href":"#vssue-icon-"+n.name}})])}),void 0,void 0,void 0,!1,void 0,void 0,void 0);let ql=class extends Wt{constructor(){super(...arguments),this.editMode=!1,this.editContent=this.comment.contentRaw,this.creatingReactions=[],this.isPutingComment=!1,this.isDeletingComment=!1}get currentUser(){return this.vssue.user?this.vssue.user.username:null}get content(){return this.comment.content}get author(){return this.comment.author}get createdAt(){return Gc(this.comment.createdAt)}get updatedAt(){return Gc(this.comment.updatedAt)}get showReactions(){return Boolean(this.vssue.API&&this.vssue.API.platform.meta.reactable&&this.comment.reactions&&!this.editMode)}get reactionKeys(){return["heart","like","unlike"]}get editContentRows(){return this.editContent.split("\n").length-1}get editInputRows(){return this.editContentRows<3?5:this.editContentRows+2}async postReaction({reaction:e}){try{if(this.creatingReactions.includes(e))return;this.creatingReactions.push(e);await this.vssue.postCommentReaction({commentId:this.comment.id,reaction:e})||this.vssue.$emit("error",new Error(this.vssue.$t("reactionGiven",{reaction:this.vssue.$t(e)})));const n=await this.vssue.getCommentReactions({commentId:this.comment.id});n&&(this.comment.reactions=n)}finally{this.creatingReactions.splice(this.creatingReactions.findIndex(n=>n===e),1)}}enterEdit(){this.editMode=!0,this.$nextTick(()=>{this.$refs.input.focus()})}resetEdit(){this.editMode=!1,this.editContent=this.comment.contentRaw}async putComment(){try{if(this.vssue.isPending)return;if(this.editContent!==this.comment.contentRaw){this.isPutingComment=!0,this.vssue.isUpdatingComment=!0;const e=await this.vssue.putComment({commentId:this.comment.id,content:this.editContent});e&&this.vssue.comments.data.splice(this.vssue.comments.data.findIndex(e=>e.id===this.comment.id),1,e)}this.editMode=!1}finally{this.isPutingComment=!1,this.vssue.isUpdatingComment=!1}}async deleteComment(){try{if(this.vssue.isPending)return;if(!window.confirm(this.vssue.$t("deleteConfirm")))return;this.isDeletingComment=!0,this.vssue.isUpdatingComment=!0;await this.vssue.deleteComment({commentId:this.comment.id})?(this.vssue.comments.count-=1,this.vssue.comments.data.length>1&&this.vssue.comments.data.splice(this.vssue.comments.data.findIndex(e=>e.id===this.comment.id),1),this.vssue.query.page>1&&this.vssue.query.page>Math.ceil(this.vssue.comments.count/this.vssue.query.perPage)?this.vssue.query.page-=1:await this.vssue.getComments()):this.vssue.$emit("error",new Error(this.vssue.$t("deleteFailed")))}finally{this.isDeletingComment=!1,this.vssue.isUpdatingComment=!1}}};Ul([Oc({type:Object,required:!0})],ql.prototype,"comment",void 0),Ul([Tc()],ql.prototype,"vssue",void 0),ql=Ul([Sc({components:{VssueIcon:$l}})],ql);const Fl=Bl({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-comment",class:{"vssue-comment-edit-mode":e.editMode,"vssue-comment-disabled":e.isDeletingComment||e.isPutingComment}},[t("div",{staticClass:"vssue-comment-avatar"},[t("a",{attrs:{href:e.author.homepage,title:e.author.username,target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:e.author.avatar,alt:e.author.username}})])]),e._v(" "),t("div",{staticClass:"vssue-comment-body"},[e._t("body",[t("div",{staticClass:"vssue-comment-header"},[t("span",{staticClass:"vssue-comment-author"},[t("a",{attrs:{href:e.author.homepage,title:e.author.username,target:"_blank",rel:"noopener noreferrer"}},[e._v("\n            "+e._s(e.author.username)+"\n          ")])]),e._v(" "),t("span",{staticClass:"vssue-comment-created-at"},[e._v("\n          "+e._s(e.createdAt)+"\n        ")])]),e._v(" "),t("div",{staticClass:"vssue-comment-main"},[e.editMode?t("textarea",{directives:[{name:"model",rawName:"v-model",value:e.editContent,expression:"editContent"}],ref:"input",staticClass:"vssue-edit-comment-input",attrs:{rows:e.editInputRows},domProps:{value:e.editContent},on:{keyup:function(n){return!n.type.indexOf("key")&&e._k(n.keyCode,"enter",13,n.key,"Enter")?null:n.ctrlKey?e.putComment():null},input:function(n){n.target.composing||(e.editContent=n.target.value)}}}):t("article",{staticClass:"markdown-body",domProps:{innerHTML:e._s(e.content)}})]),e._v(" "),t("div",{staticClass:"vssue-comment-footer"},[e.editMode?t("span",{staticClass:"vssue-comment-hint"},[e._v("\n          "+e._s(e.vssue.$t("editMode"))+"\n        ")]):e._e(),e._v(" "),e.showReactions?t("span",{staticClass:"vssue-comment-reactions"},e._l(e.reactionKeys,(function(n){return t("span",{key:n,staticClass:"vssue-comment-reaction",attrs:{title:e.vssue.$t(e.creatingReactions.includes(n)?"loading":n)},on:{click:function(t){return e.postReaction({reaction:n})}}},[t("VssueIcon",{attrs:{name:e.creatingReactions.includes(n)?"loading":n,title:e.vssue.$t(e.creatingReactions.includes(n)?"loading":n)}}),e._v(" "),t("span",{staticClass:"vssue-comment-reaction-number"},[e._v("\n              "+e._s(e.comment.reactions[n])+"\n            ")])],1)})),0):e._e(),e._v(" "),t("span",{staticClass:"vssue-comment-operations"},[e.comment.author.username===e.currentUser&&e.editMode?t("span",{staticClass:"vssue-comment-operation",class:{"vssue-comment-operation-muted":e.isPutingComment},attrs:{title:e.vssue.$t(e.isPutingComment?"loading":"submit")},on:{click:function(n){return e.putComment()}}},[t("VssueIcon",{directives:[{name:"show",rawName:"v-show",value:e.isPutingComment,expression:"isPutingComment"}],attrs:{name:"loading",title:e.vssue.$t("loading")}}),e._v("\n\n            "+e._s(e.vssue.$t("submit"))+"\n          ")],1):e._e(),e._v(" "),e.comment.author.username===e.currentUser&&e.editMode?t("span",{staticClass:"vssue-comment-operation vssue-comment-operation-muted",attrs:{title:e.vssue.$t("cancel")},on:{click:function(n){return e.resetEdit()}}},[e._v("\n            "+e._s(e.vssue.$t("cancel"))+"\n          ")]):e._e(),e._v(" "),e.comment.author.username===e.currentUser?t("span",{directives:[{name:"show",rawName:"v-show",value:!e.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(n){return e.enterEdit()}}},[t("VssueIcon",{attrs:{name:"edit",title:e.vssue.$t("edit")}})],1):e._e(),e._v(" "),e.comment.author.username===e.currentUser||e.vssue.isAdmin?t("span",{directives:[{name:"show",rawName:"v-show",value:!e.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(n){return e.deleteComment()}}},[t("VssueIcon",{attrs:{name:e.isDeletingComment?"loading":"delete",title:e.vssue.$t(e.isDeletingComment?"loading":"delete")}})],1):e._e(),e._v(" "),t("span",{directives:[{name:"show",rawName:"v-show",value:!e.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(n){return e.vssue.$emit("reply-comment",e.comment)}}},[t("VssueIcon",{attrs:{name:"reply",title:e.vssue.$t("reply")}})],1)])])])],2)])},staticRenderFns:[]},void 0,ql,void 0,!1,void 0,!1,void 0,void 0,void 0);let Vl=class extends Wt{get disabled(){return this.vssue.isPending}get pageCount(){const e=Math.ceil(this.vssue.comments.count/this.vssue.comments.perPage);return e>1?e:1}get perPageOptions(){const e=[5,10,20,50];return!e.includes(this.vssue.options.perPage)&&this.vssue.options.perPage<100&&e.push(this.vssue.options.perPage),e.sort((e,n)=>e-n)}get page(){return this.vssue.query.page>this.pageCount?this.pageCount:this.vssue.query.page}set page(e){e>0&&e<=this.pageCount&&(this.vssue.query.page=e)}get perPage(){return this.vssue.query.perPage}set perPage(e){this.perPageOptions.includes(e)&&(this.vssue.query.perPage=e)}};Ul([Tc()],Vl.prototype,"vssue",void 0),Vl=Ul([Sc({components:{VssueIcon:$l}})],Vl);const Hl=Bl({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-pagination"},[t("div",{staticClass:"vssue-pagination-per-page"},[t("label",[t("select",{directives:[{name:"model",rawName:"v-model",value:e.perPage,expression:"perPage"}],staticClass:"vssue-pagination-select",attrs:{disabled:e.disabled},on:{change:function(n){var t=Array.prototype.filter.call(n.target.options,(function(e){return e.selected})).map((function(e){return"_value"in e?e._value:e.value}));e.perPage=n.target.multiple?t:t[0]}}},e._l(e.perPageOptions,(function(n){return t("option",{key:n,domProps:{value:n}},[e._v("\n          "+e._s(n)+"\n        ")])})),0),e._v(" "),t("span",[e._v("\n        "+e._s(e.vssue.$t("perPage"))+"\n      ")])]),e._v(" "),e.vssue.API.platform.meta.sortable?t("span",{class:{"vssue-pagination-link":!0,disabled:e.disabled},attrs:{title:e.vssue.$t("sort")},on:{click:function(n){e.vssue.query.sort="asc"===e.vssue.query.sort?"desc":"asc"}}},[e._v("\n      "+e._s("asc"===e.vssue.query.sort?"↑":"↓")+"\n    ")]):e._e()]),e._v(" "),t("div",{staticClass:"vssue-pagination-page"},[t("span",{class:{"vssue-pagination-link":!0,disabled:1===e.page||e.disabled},attrs:{title:e.vssue.$t("prev")},domProps:{textContent:e._s("<")},on:{click:function(n){e.page-=1}}}),e._v(" "),t("label",[t("span",[e._v("\n        "+e._s(e.vssue.$t("page"))+"\n      ")]),e._v(" "),t("select",{directives:[{name:"show",rawName:"v-show",value:e.pageCount>1,expression:"pageCount > 1"},{name:"model",rawName:"v-model",value:e.page,expression:"page"}],staticClass:"vssue-pagination-select",attrs:{disabled:e.disabled},on:{change:function(n){var t=Array.prototype.filter.call(n.target.options,(function(e){return e.selected})).map((function(e){return"_value"in e?e._value:e.value}));e.page=n.target.multiple?t:t[0]}}},e._l(e.pageCount,(function(n){return t("option",{key:n,domProps:{value:n}},[e._v("\n          "+e._s(n)+"\n        ")])})),0),e._v(" "),t("span",{directives:[{name:"show",rawName:"v-show",value:e.pageCount<2,expression:"pageCount < 2"}],domProps:{textContent:e._s(e.page)}}),e._v(" "),t("span",{domProps:{textContent:e._s(" / "+e.pageCount+" ")}})]),e._v(" "),t("span",{class:{"vssue-pagination-link":!0,disabled:e.page===e.pageCount||e.disabled},attrs:{title:e.vssue.$t("next")},domProps:{textContent:e._s(">")},on:{click:function(n){e.page+=1}}})])])},staticRenderFns:[]},void 0,Vl,void 0,!1,void 0,!1,void 0,void 0,void 0);let Wl=class extends Wt{};Ul([Tc()],Wl.prototype,"vssue",void 0),Wl=Ul([Sc({components:{TransitionFade:Nl,VssueComment:Fl,VssuePagination:Hl}})],Wl);const Kl=Bl({render:function(){var e=this.$createElement,n=this._self._c||e;return n("div",{staticClass:"vssue-comments"},[n("VssuePagination"),this._v(" "),n("TransitionFade",{attrs:{group:""}},this._l(this.vssue.comments.data,(function(e){return n("VssueComment",{key:e.id,attrs:{comment:e}})})),1),this._v(" "),n("VssuePagination",{directives:[{name:"show",rawName:"v-show",value:this.vssue.comments.data.length>5,expression:"vssue.comments.data.length > 5"}]})],1)},staticRenderFns:[]},void 0,Wl,void 0,!1,void 0,!1,void 0,void 0,void 0);const Yl=Bl({},void 0,Wt.extend({name:"VssueIcon",functional:!0,props:{type:{type:String,required:!1,default:"default"}},render:(e,{props:n,data:t,children:a})=>e("button",Object.assign(Object.assign({},t),{class:["vssue-button","vssue-button-"+n.type]}),a)}),void 0,void 0,void 0,!1,void 0,void 0,void 0);let Ql=class extends Wt{constructor(){super(...arguments),this.content=""}get user(){return this.vssue.user}get platform(){return this.vssue.API&&this.vssue.API.platform.name}get isInputDisabled(){return this.loading||null===this.user||null===this.vssue.issue}get isSubmitDisabled(){return""===this.content||this.vssue.isPending||null===this.vssue.issue}get loading(){return this.vssue.isCreatingComment}get contentRows(){return this.content.split("\n").length-1}get inputRows(){return this.contentRows<3?5:this.contentRows+2}created(){this.vssue.$on("reply-comment",e=>{const n=e.contentRaw.replace(/\n/g,"\n> "),t=`@${e.author.username}\n\n> ${n}\n\n`;this.content=this.content.concat(t),this.focus()})}beforeDestroy(){this.vssue.$off("reply-comment")}focus(){this.$refs.input.focus()}async submit(){this.isSubmitDisabled||(await this.vssue.postComment({content:this.content}),this.content="",await this.vssue.getComments())}};Ul([Tc()],Ql.prototype,"vssue",void 0),Ql=Ul([Sc({components:{VssueButton:Yl,VssueIcon:$l}})],Ql);const Zl=Bl({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-new-comment"},[t("div",{staticClass:"vssue-comment-avatar"},[e.user?t("a",{attrs:{href:e.user.homepage,title:e.user.username,target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:e.user.avatar,alt:e.user.username}})]):t("VssueIcon",{attrs:{name:e.platform.toLowerCase(),title:e.vssue.$t("loginToComment",{platform:e.platform})},on:{click:function(n){return e.vssue.login()}}})],1),e._v(" "),t("div",{staticClass:"vssue-new-comment-body"},[t("textarea",{directives:[{name:"model",rawName:"v-model",value:e.content,expression:"content"}],ref:"input",staticClass:"vssue-new-comment-input",attrs:{rows:e.inputRows,disabled:e.isInputDisabled,placeholder:e.vssue.$t(e.user?"placeholder":"noLoginPlaceHolder"),spellcheck:!1,"aria-label":"leave a comment"},domProps:{value:e.content},on:{keyup:function(n){return!n.type.indexOf("key")&&e._k(n.keyCode,"enter",13,n.key,"Enter")?null:n.ctrlKey?e.submit():null},input:function(n){n.target.composing||(e.content=n.target.value)}}})]),e._v(" "),t("div",{staticClass:"vssue-new-comment-footer"},[e.user?t("span",{staticClass:"vssue-current-user"},[t("span",[e._v(e._s(e.vssue.$t("currentUser"))+" - "+e._s(e.user.username)+" - ")]),e._v(" "),t("a",{staticClass:"vssue-logout",on:{click:function(n){return e.vssue.logout()}}},[e._v("\n        "+e._s(e.vssue.$t("logout"))+"\n      ")])]):t("span",{staticClass:"vssue-current-user"},[e._v("\n      "+e._s(e.vssue.$t("loginToComment",{platform:e.platform}))+"\n    ")]),e._v(" "),t("div",{staticClass:"vssue-new-comment-operations"},[e.user?t("VssueButton",{staticClass:"vssue-button-submit-comment",attrs:{type:"primary",disabled:e.isSubmitDisabled},on:{click:function(n){return e.submit()}}},[t("VssueIcon",{directives:[{name:"show",rawName:"v-show",value:e.loading,expression:"loading"}],attrs:{name:"loading"}}),e._v("\n\n        "+e._s(e.vssue.$t(e.loading?"submitting":"submitComment"))+"\n      ")],1):t("VssueButton",{staticClass:"vssue-button-login",attrs:{type:"primary",title:e.vssue.$t("loginToComment",{platform:e.platform})},on:{click:function(n){return e.vssue.login()}}},[e._v("\n        "+e._s(e.vssue.$t("login",{platform:e.platform}))+"\n      ")])],1)])])},staticRenderFns:[]},void 0,Ql,void 0,!1,void 0,!1,void 0,void 0,void 0);let Xl=class extends Wt{constructor(){super(...arguments),this.progress={show:!1,percent:0,timer:null,speed:200},this.alert={show:!1,message:null,timer:null}}onLoadingCommentsChange(e){this.vssue.comments&&(e?this.progressStart():this.progressDone())}created(){this.vssue.$on("error",e=>this.alertShow(e.message))}beforeDestroy(){this.vssue.$off("error"),null!==this.progress.timer&&window.clearTimeout(this.progress.timer),null!==this.alert.timer&&window.clearTimeout(this.alert.timer)}progressStart(){this.progress.show=!0,this.progress.percent=0,this.progress.timer=window.setInterval(()=>{this.progress.percent+=5,this.progress.percent>94&&null!==this.progress.timer&&window.clearInterval(this.progress.timer)},this.progress.speed)}progressDone(){this.progress.percent=100,null!==this.progress.timer&&window.clearTimeout(this.progress.timer),this.progress.timer=null,window.setTimeout(()=>{this.progress.show=!1},this.progress.speed)}alertShow(e){this.alert.show=!0,this.alert.message=e,null!==this.alert.timer&&window.clearTimeout(this.alert.timer),this.alert.timer=window.setTimeout(()=>{this.alertHide()},3e3)}alertHide(){this.alert.show=!1,null!==this.alert.timer&&window.clearTimeout(this.alert.timer),this.alert.timer=null}};Ul([Tc()],Xl.prototype,"vssue",void 0),Ul([zc("vssue.isLoadingComments")],Xl.prototype,"onLoadingCommentsChange",null),Xl=Ul([Sc({components:{TransitionFade:Nl}})],Xl);const Jl=Bl({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-notice"},[t("div",{directives:[{name:"show",rawName:"v-show",value:e.progress.show,expression:"progress.show"}],staticClass:"vssue-progress",style:{width:e.progress.percent+"%",transition:"all "+e.progress.speed+"ms linear"}}),e._v(" "),t("TransitionFade",[t("div",{directives:[{name:"show",rawName:"v-show",value:e.alert.show,expression:"alert.show"}],staticClass:"vssue-alert",domProps:{textContent:e._s(e.alert.message)},on:{click:function(n){return e.alertHide()}}})])],1)},staticRenderFns:[]},void 0,Xl,void 0,!1,void 0,!1,void 0,void 0,void 0);let ed=class extends Wt{get status(){return this.vssue.isFailed?"failed":this.vssue.isInitializing?"initializing":this.vssue.isIssueNotCreated&&!this.vssue.isCreatingIssue?this.vssue.isAdmin||!this.vssue.isLogined?"issueNotCreated":"failed":this.vssue.isLoginRequired?"loginRequired":!this.vssue.comments||this.vssue.isCreatingIssue?"loadingComments":0===this.vssue.comments.data.length?"noComments":null}handleClick(){"issueNotCreated"===this.status?this.vssue.postIssue():"loginRequired"===this.status&&this.vssue.login()}};Ul([Tc()],ed.prototype,"vssue",void 0),ed=Ul([Sc({components:{TransitionFade:Nl,VssueIcon:$l}})],ed);const nd=Bl({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("TransitionFade",[e.status?t("div",{key:e.status,staticClass:"vssue-status"},[["failed","loadingComments","initializing"].includes(e.status)?t("VssueIcon",{attrs:{name:"failed"===e.status?"error":"loading"}}):e._e(),e._v(" "),t("p",{staticClass:"vssue-status-info"},[t(["issueNotCreated","loginRequired"].includes(e.status)?"a":"span",{tag:"Component",on:{click:e.handleClick}},[e._v("\n        "+e._s(e.vssue.$t(e.status))+"\n      ")])],1)],1):e._e()])},staticRenderFns:[]},void 0,ed,void 0,!1,void 0,!1,void 0,void 0,void 0);let td=class extends Wt{};Ul([Tc()],td.prototype,"vssue",void 0),td=Ul([Sc({components:{TransitionFade:Nl,VssueIcon:$l,VssueComments:Kl,VssueNewComment:Zl,VssueNotice:Jl,VssueStatus:nd}})],td);const ad=Bl({render:function(){var e=this.$createElement,n=this._self._c||e;return n("TransitionFade",[this.vssue.isInitializing?n("VssueStatus"):n("div",{staticClass:"vssue-body"},[this.vssue.API?n("VssueNewComment"):this._e(),this._v(" "),n("VssueNotice"),this._v(" "),n("TransitionFade",[this.vssue.comments&&this.vssue.comments.data.length>0?n("VssueComments"):n("VssueStatus")],1)],1)],1)},staticRenderFns:[]},void 0,td,void 0,!1,void 0,!1,void 0,void 0,void 0);let rd=class extends Wt{};Ul([Tc()],rd.prototype,"vssue",void 0),rd=Ul([Sc],rd);const id=Bl({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-header"},[t("a",{staticClass:"vssue-header-comments-count",attrs:{href:e.vssue.issue?e.vssue.issue.link:null,target:"_blank",rel:"noopener noreferrer"}},[t("span",[e._v("\n      "+e._s(e.vssue.comments?e.vssue.$tc("comments",e.vssue.comments.count,{count:e.vssue.comments.count}):e.vssue.$tc("comments",0))+"\n    ")])]),e._v(" "),t("span",{staticClass:"vssue-header-powered-by"},[t("span",[e._v("Powered by")]),e._v(" "),e.vssue.API?t("span",[t("a",{attrs:{href:e.vssue.API.platform.link,title:e.vssue.API.platform.name+" API "+e.vssue.API.platform.version,target:"_blank",rel:"noopener noreferrer"}},[e._v("\n        "+e._s(e.vssue.API.platform.name)+"\n      ")]),e._v(" "),t("span",[e._v("&")])]):e._e(),e._v(" "),t("a",{attrs:{href:"https://github.com/meteorlxy/vssue",title:"Vssue v"+e.vssue.version,target:"_blank",rel:"noopener noreferrer"}},[e._v("\n      Vssue\n    ")])])])},staticRenderFns:[]},void 0,rd,void 0,!1,void 0,!1,void 0,void 0,void 0),od={login:"Login with {platform}",logout:"Logout",currentUser:"Current User",loading:"Loading",submit:"Submit",submitting:"Submitting",submitComment:"Submit Comment",cancel:"Cancel",edit:"Edit",editMode:"Edit Mode",delete:"Delete",reply:"Reply",heart:"Heart",like:"Like",unlike:"Unlike",perPage:"Comments per page",sort:"Click to change the sort direction",page:"Page",prev:"Previous Page",next:"Next Page",comments:"Comments | {count} Comment | {count} Comments",loginToComment:"Login with {platform} account to leave a comment",placeholder:"Leave a comment. Styling with Markdown is supported. Ctrl + Enter to submit.",noLoginPlaceHolder:"Login to leave a comment. Styling with Markdown is supported. ",failed:"Failed to load comments",initializing:"Initializing...",issueNotCreated:"Click to create issue",loadingComments:"Loading comments...",loginRequired:"Login to view comments",noComments:"No comments yet. Leave the first comment !",reactionGiven:"Already given '{reaction}' reaction",deleteConfirm:"Confirm to delete this comment ?",deleteFailed:"Failed to delete comment"},sd={login:"使用 {platform} 登录",logout:"退出登录",currentUser:"当前用户",loading:"加载中",submit:"提交",submitting:"发表中",submitComment:"发表评论",cancel:"取消",edit:"编辑",editMode:"编辑模式",delete:"删除",reply:"回复",heart:"喜欢",like:"赞",unlike:"踩",perPage:"每页评论数",sort:"点击改变排序方式",page:"页数",prev:"上一页",next:"下一页",comments:"评论 | {count} 条评论 | {count} 条评论",loginToComment:"使用 {platform} 帐号登录后发表评论",placeholder:"留下你的评论丨支持 Markdown 语法丨Ctrl + Enter 发表评论",noLoginPlaceHolder:"登录后才能发表评论丨支持 Markdown 语法",failed:"评论加载失败",initializing:"正在初始化...",issueNotCreated:"点击创建 Issue",loadingComments:"正在加载评论...",loginRequired:"登录后查看评论",noComments:"还没有评论，来发表第一条评论吧！",reactionGiven:"已经添加过 '{reaction}' 了",deleteConfirm:"确认要删除该评论吗？",deleteFailed:"评论删除失败"},cd={login:"Entrar com {platform}",logout:"Sair",currentUser:"Usuário Atual",loading:"Carregando",submit:"Enviar",submitting:"Enviando",submitComment:"Enviar Comentário",cancel:"Cancelar",edit:"Editar",editMode:"Modo de Edição",delete:"Apagar",reply:"Responder",heart:"Heart",like:"Like",unlike:"Unlike",perPage:"Comentários por página",sort:"Clique para alterar a ordenação",page:"Página",prev:"Página Anterior",next:"Próxima Página",comments:"Comentários | {count} Comentário | {count} Comentários",loginToComment:"Entre com uma conta {platform} para deixar um comentário",placeholder:"Deixe um comentário. Estilos com Markdown suportados. Ctrl + Enter para enviar.",noLoginPlaceHolder:"Entre para deixar um comentário. Estilos com Markdown suportados. ",failed:"Falha ao carregar comentários",initializing:"Inicializando...",issueNotCreated:"Click to create issue",loadingComments:"Carregando comentários...",loginRequired:"Entrar para visualizar comentários",noComments:"Nenhum comentário. Deixe o primeiro comentário!",reactionGiven:"Já reagiu com '{reaction}'",deleteConfirm:"Apagar este comentário?",deleteFailed:"Falha ao apagar comentário"},ld={login:"{platform} でログイン",logout:"ログアウト",currentUser:"現在のユーザー",loading:"読み込み中",submit:"送信",submitting:"送信中",submitComment:"コメントを送信",cancel:"キャンセル",edit:"編集",editMode:"編集モード",delete:"削除",reply:"返信",heart:"ハート",like:"高評価",unlike:"低評価",perPage:"コメント/ページ",sort:"並び順を変更するにはクリックしてください",page:"ページ",prev:"前のページ",next:"次のページ",comments:"コメント | {count} コメント | {count} コメント",loginToComment:"コメントを残すには {platform} アカウントでログインしてください。",placeholder:"コメントを残してください。Markdown 記法をサポートしています。 Ctrl + Enter で送信できます。",noLoginPlaceHolder:"コメントを残すにはログインしてください。マークダウン記法をサポートしています。",failed:"コメントの読み込みに失敗しました",initializing:"初期化中...",issueNotCreated:"Click to create issue",loadingComments:"コメントの読み込み中...",loginRequired:"コメントを見るにはログインしてください",noComments:"まだコメントがありません。最初のコメントを残しましょう！",reactionGiven:"既に '{reaction}' のリアクションをしています",deleteConfirm:"本当にコメントを削除してもいいですか？",deleteFailed:"コメントの削除に失敗しました"},dd={login:"התחברו עם {platform}",logout:"התנתקו",currentUser:"משתמש/ת נוכחי/ת",loading:"טוען",submit:"שליחה",submitting:"שולח",submitComment:"שליחת תגובה",cancel:"ביטל",edit:"עריכה",editMode:"מצב עריכה",delete:"מחיקה",reply:"תשובה",heart:"לב",like:"לייק",unlike:"אנלייק",perPage:"תגובות לדף",sort:"לחצו כדי לשנות את כיוון המיון",page:"דף",prev:"הדף הקודם",next:"הדף הבא",comments:"תגובות | {count} תגובה | {count} תגובות",loginToComment:"התחברו עם חשבון {platform} כדי להשאיר תגובה",placeholder:"השאירו תגובה. יש תמיכה בעיצוב בעזרת Markdown. Ctrl + Enter כדי לשלוח.",noLoginPlaceHolder:"התחברו כדי להשאיר תגובה. יש תמיכה בעיצוב בעזרת Markdown. ",failed:"כשלון בטעינת התגובות",initializing:"מאתחל...",issueNotCreated:"לחצו ליצירת issue",loadingComments:"טוען תגובות...",loginRequired:"התחברו כדי לצפות בתגובות",noComments:"עדיין אין תגובות. השאירו תגובה ראשונה !",reactionGiven:"כבר ניתן חיווי '{reaction}'",deleteConfirm:"בטוחים במחיקת התגובה ?",deleteFailed:"כשלון במחיקת התגובה"};Object.prototype.hasOwnProperty.call(Wt,"$i18n")||Wt.use(El);const ud=new El({locale:"en",fallbackLocale:"en",messages:{en:od,"en-US":od,zh:sd,"zh-CN":sd,pt:cd,"pt-BR":cd,ja:ld,"ja-JP":ld,he:dd,"he-IL":dd}});let hd=class extends Wt{constructor(){super(...arguments),this.title=e=>`${e.prefix}${document.title}`,this.issueId=null,this.options=null,this.API=null,this.accessToken=null,this.user=null,this.issue=null,this.comments=null,this.query={page:1,perPage:10,sort:"desc"},this.isInitializing=!0,this.isIssueNotCreated=!1,this.isLoginRequired=!1,this.isFailed=!1,this.isCreatingIssue=!1,this.isLoadingComments=!1,this.isCreatingComment=!1,this.isUpdatingComment=!1}get version(){return"1.4.8"}get issueTitle(){return null===this.options?"":"function"==typeof this.title?this.title(this.options):`${this.options.prefix}${this.title}`}get isPending(){return this.isLoadingComments||this.isCreatingComment||this.isUpdatingComment}get isLogined(){return null!==this.accessToken&&null!==this.user}get isAdmin(){return null!==this.options&&null!==this.accessToken&&null!==this.user&&(this.user.username===this.options.owner||this.options.admins.includes(this.user.username))}get accessTokenKey(){return this.API?`Vssue.${this.API.platform.name.toLowerCase()}.access_token`:""}onQueryPerPageChange(){this.query.page=1,this.getComments()}onQueryChange(){this.getComments()}setOptions(e){this.options=Object.assign({labels:["Vssue"],state:"Vssue",prefix:"[Vssue]",admins:[],perPage:10,proxy:e=>"https://cors-anywhere.azm.workers.dev/"+e,issueContent:({url:e})=>e,autoCreateIssue:!1},e);const n=["api","owner","repo","clientId"];for(const e of n)this.options[e]||console.warn(`[Vssue] the option '${e}' is required`);if(this.options.locale)this.$i18n.locale=this.options.locale;else{const e=Object.keys(this.$i18n.messages),n=window.navigator.languages;this.$i18n.locale=n.filter(n=>e.includes(n)).shift()||"en"}}async init(){try{await this.initStore(),await this.initComments()}catch(e){e.response&&[401,403].includes(e.response.status)?this.isLoginRequired=!0:this.isFailed=!0,console.error(e)}}async initStore(){try{if(!this.options)throw new Error("Options are required to initialize Vssue");this.API=null,this.accessToken=null,this.user=null,this.issue=null,this.comments=null,this.query={page:1,perPage:this.options.perPage,sort:"desc"},this.isInitializing=!0,this.isIssueNotCreated=!1,this.isLoginRequired=!1,this.isFailed=!1,this.isCreatingIssue=!1,this.isLoadingComments=!1,this.isCreatingComment=!1,this.isUpdatingComment=!1;const e=this.options.api;this.API=new e({baseURL:this.options.baseURL,labels:this.options.labels,state:this.options.state,owner:this.options.owner,repo:this.options.repo,clientId:this.options.clientId,clientSecret:this.options.clientSecret,proxy:this.options.proxy}),await this.handleAuth()}finally{this.isInitializing=!1}}async initComments(){if(this.API&&this.options)if(this.issueId){const[e,n]=await Promise.all([this.API.getIssue({accessToken:this.accessToken,issueId:this.issueId}),this.API.getComments({accessToken:this.accessToken,issueId:this.issueId,query:this.query})]);this.issue=e,this.comments=n}else this.issue=await this.API.getIssue({accessToken:this.accessToken,issueTitle:this.issueTitle}),null===this.issue?(this.isIssueNotCreated=!0,this.options.autoCreateIssue&&await this.postIssue()):await this.getComments()}async postIssue(){if(this.API&&this.options&&!this.issue&&!this.issueId&&(this.isLogined||this.login(),this.isAdmin))try{this.isCreatingIssue=!0;const e=await this.API.postIssue({title:this.issueTitle,content:await this.options.issueContent({options:this.options,url:Nc(window.location.href)}),accessToken:this.accessToken});this.issue=e,this.isIssueNotCreated=!1,await this.getComments()}catch(e){this.isFailed=!0}finally{this.isCreatingIssue=!1}}async getComments(){try{if(!this.API||!this.issue||this.isLoadingComments)return;this.isLoadingComments=!0;const e=await this.API.getComments({accessToken:this.accessToken,issueId:this.issue.id,query:this.query});return this.comments=e,this.query.page!==e.page&&(this.query.page=e.page),this.query.perPage!==e.perPage&&(this.query.perPage=e.perPage),e}catch(e){if(!e.response||![401,403].includes(e.response.status)||this.isLogined)throw this.$emit("error",e),e;this.isLoginRequired=!0}finally{this.isLoadingComments=!1}}async postComment({content:e}){try{if(!this.API||!this.issue||this.isCreatingComment)return;this.isCreatingComment=!0;return await this.API.postComment({accessToken:this.accessToken,content:e,issueId:this.issue.id})}catch(e){throw this.$emit("error",e),e}finally{this.isCreatingComment=!1}}async putComment({commentId:e,content:n}){try{if(!this.API||!this.issue)return;return await this.API.putComment({accessToken:this.accessToken,issueId:this.issue.id,commentId:e,content:n})}catch(e){throw this.$emit("error",e),e}}async deleteComment({commentId:e}){try{if(!this.API||!this.issue)return;return await this.API.deleteComment({accessToken:this.accessToken,issueId:this.issue.id,commentId:e})}catch(e){throw this.$emit("error",e),e}}async getCommentReactions({commentId:e}){try{if(!this.API||!this.issue)return;return await this.API.getCommentReactions({accessToken:this.accessToken,issueId:this.issue.id,commentId:e})}catch(e){throw this.$emit("error",e),e}}async postCommentReaction({commentId:e,reaction:n}){try{if(!this.API||!this.issue)return!1;return await this.API.postCommentReaction({accessToken:this.accessToken,issueId:this.issue.id,commentId:e,reaction:n})}catch(e){throw this.$emit("error",e),e}}login(){this.API&&this.API.redirectAuth()}logout(){this.setAccessToken(null),this.user=null}async handleAuth(){if(!this.API)return;const e=await this.API.handleAuth();e?(this.setAccessToken(e),this.user=await this.API.getUser({accessToken:e})):this.getAccessToken()?this.user=await this.API.getUser({accessToken:this.accessToken}):(this.setAccessToken(null),this.user=null)}getAccessToken(){return this.accessToken=window.localStorage.getItem(this.accessTokenKey),this.accessToken}setAccessToken(e){null===e?window.localStorage.removeItem(this.accessTokenKey):window.localStorage.setItem(this.accessTokenKey,e),this.accessToken=e}};Ul([zc("query.perPage")],hd.prototype,"onQueryPerPageChange",null),Ul([zc("query.page"),zc("query.sort")],hd.prototype,"onQueryChange",null),hd=Ul([Sc({i18n:ud})],hd);var pd=hd;let md=class extends Wt{constructor(){super(...arguments),this.vssue=new pd}onOptionsChange(e){this.vssue.setOptions(e)}mounted(){null!==this.title&&(this.vssue.title=this.title),null!==this.issueId&&(this.vssue.issueId=this.issueId),this.vssue.setOptions(this.options),this.vssue.init()}};var fd;Ul([Oc({type:[String,Function],required:!1,default:null})],md.prototype,"title",void 0),Ul([Oc({type:[String,Number],required:!1,default:null})],md.prototype,"issueId",void 0),Ul([Oc({type:Object,required:!1,default:()=>({})})],md.prototype,"options",void 0),Ul([(fd="vssue",wc((function(e,n){var t=e.provide;Ic(t)&&(t=e.provide=Pc(t)),t.managed[n]=fd||n})))],md.prototype,"vssue",void 0),Ul([zc("options",{deep:!0})],md.prototype,"onOptionsChange",null),md=Ul([Sc({components:{Iconfont:Gl,VssueBody:ad,VssueHeader:id}})],md);const gd=Bl({render:function(){var e=this.$createElement,n=this._self._c||e;return n("div",{staticClass:"vssue"},[n("Iconfont"),this._v(" "),n("VssueHeader"),this._v(" "),n("VssueBody")],1)},staticRenderFns:[]},void 0,md,void 0,!1,void 0,!1,void 0,void 0,void 0);var yd=t(116),vd=t.n(yd);function bd(e){return{username:e.login,avatar:e.avatar_url,homepage:e.html_url}}function wd(e){return{id:e.number,title:e.title,content:e.body,link:e.html_url}}function kd(e){return{like:e["+1"],unlike:e[-1],heart:e.heart}}function _d(e){return{id:e.id,content:e.body_html,contentRaw:e.body,author:bd(e.user),createdAt:e.created_at,updatedAt:e.updated_at,reactions:kd(e.reactions)}}function xd(e){return"like"===e?"+1":"unlike"===e?"-1":e}class Cd{constructor({baseURL:e="https://github.com",owner:n,repo:t,labels:a,clientId:r,clientSecret:i,state:o,proxy:s}){if(void 0===i||void 0===s)throw new Error("clientSecret and proxy is required for GitHub V3");this.baseURL=e,this.owner=n,this.repo=t,this.labels=a,this.clientId=r,this.clientSecret=i,this.state=o,this.proxy=s,this.$http=vd.a.create({baseURL:"https://github.com"===e?"https://api.github.com":Uc(e,"api/v3"),headers:{Accept:"application/vnd.github.v3+json"}}),this.$http.interceptors.response.use(e=>e.data&&e.data.error?Promise.reject(new Error(e.data.error_description)):e,e=>(void 0===e.response&&"Network Error"===e.message&&(e.response={status:403}),Promise.reject(e)))}get platform(){return{name:"GitHub",link:this.baseURL,version:"v3",meta:{reactable:!0,sortable:!1}}}redirectAuth(){window.location.href=Ec(Uc(this.baseURL,"login/oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,scope:"public_repo",state:this.state})}async handleAuth(){const e=(n=window.location.search,Object(Rc.parse)(n,{ignoreQueryPrefix:!0}));var n;if(e.code){if(e.state!==this.state)return null;const n=e.code;delete e.code,delete e.state;const t=Ec(Nc(window.location.href),e)+window.location.hash;window.history.replaceState(null,"",t);return await this.getAccessToken({code:n})}return null}async getAccessToken({code:e}){const n=Uc(this.baseURL,"login/oauth/access_token"),t="function"==typeof this.proxy?this.proxy(n):this.proxy,{data:a}=await this.$http.post(t,{client_id:this.clientId,client_secret:this.clientSecret,code:e},{headers:{Accept:"application/json"}});return a.access_token}async getUser({accessToken:e}){const{data:n}=await this.$http.get("user",{headers:{Authorization:"token "+e}});return bd(n)}async getIssue({accessToken:e,issueId:n,issueTitle:t}){const a={};if(e&&(a.headers={Authorization:"token "+e}),!n){a.params={q:[`"${t}"`,"is:issue","in:title",`repo:${this.owner}/${this.repo}`,"is:public",...this.labels.map(e=>"label:"+e)].join(" "),timestamp:Date.now()};const{data:e}=await this.$http.get("search/issues",a);return e.items.map(wd).find(e=>e.title===t)||null}try{a.params={timestamp:Date.now()};const{data:e}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/${n}`,a);return wd(e)}catch(e){if(e.response&&404===e.response.status)return null;throw e}}async postIssue({accessToken:e,title:n,content:t}){const{data:a}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues`,{title:n,body:t,labels:this.labels},{headers:{Authorization:"token "+e}});return wd(a)}async getComments({accessToken:e,issueId:n,query:{page:t=1,perPage:a=10}={}}){const r={params:{timestamp:Date.now()}},i={params:{page:t,per_page:a,timestamp:Date.now()},headers:{Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}};e&&(r.headers={Authorization:"token "+e},i.headers.Authorization="token "+e);const[o,s]=await Promise.all([this.$http.get(`repos/${this.owner}/${this.repo}/issues/${n}`,r),this.$http.get(`repos/${this.owner}/${this.repo}/issues/${n}/comments`,i)]),c=s.headers.link||null,l=/rel="next"/.test(c)?Number(c.replace(/^.*[^_]page=(\d*).*rel="next".*$/,"$1"))-1:/rel="prev"/.test(c)?Number(c.replace(/^.*[^_]page=(\d*).*rel="prev".*$/,"$1"))+1:1,d=c?Number(c.replace(/^.*per_page=(\d*).*$/,"$1")):a;return{count:Number(o.data.comments),page:l,perPage:d,data:s.data.map(_d)}}async postComment({accessToken:e,issueId:n,content:t}){const{data:a}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/${n}/comments`,{body:t},{headers:{Authorization:"token "+e,Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}});return _d(a)}async putComment({accessToken:e,commentId:n,content:t}){const{data:a}=await this.$http.patch(`repos/${this.owner}/${this.repo}/issues/comments/${n}`,{body:t},{headers:{Authorization:"token "+e,Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}});return _d(a)}async deleteComment({accessToken:e,commentId:n}){const{status:t}=await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${n}`,{headers:{Authorization:"token "+e}});return 204===t}async getCommentReactions({accessToken:e,commentId:n}){const{data:t}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/comments/${n}`,{params:{timestamp:Date.now()},headers:{Authorization:"token "+e,Accept:"application/vnd.github.squirrel-girl-preview"}});return kd(t.reactions)}async postCommentReaction({accessToken:e,commentId:n,reaction:t}){const a=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/comments/${n}/reactions`,{content:xd(t)},{headers:{Authorization:"token "+e,Accept:"application/vnd.github.squirrel-girl-preview"}});return 200===a.status?this.deleteCommentReaction({accessToken:e,commentId:n,reactionId:a.data.id}):201===a.status}async deleteCommentReaction({accessToken:e,commentId:n,reactionId:t}){return 204===(await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${n}/reactions/${t}`,{headers:{Authorization:"token "+e,Accept:"application/vnd.github.squirrel-girl-preview"}})).status}}var Ad=t(117),Md=t.n(Ad),Sd=(t(325),{components:{VssueComponent:gd},data:()=>({isLoad:!1}),watch:{"$page.key"(e,n){e&&n!==e&&this.initialize()}},mounted(){this.$nextTick(()=>{this.initialize()})},methods:{initialize(){this.$el.remove(),this.isLoad=!1,setTimeout(()=>{if(this.needComment(this.$frontmatter)){document.querySelector("main").appendChild(this.$el),this.title=Md.a.render("[Comment]<%- frontmatter.title %>",{frontmatter:this.$frontmatter}),this.options={api:Cd,autoCreateIssue:!0,clientId:"adb9fb0ac1159e00ce7f",clientSecret:"27da8dc85f808c2bd1b6e44da5ae69c4ddf17d8d",owner:"eryajf",repo:"qishao-notes"},this.isLoad=!0}},1e3)},needComment:e=>!1!==e.comment&&!1!==e.comments}}),Td=Object(rc.a)(Sd,(function(){var e=this._self._c;return this.isLoad?e("div",[e("VssueComponent",{attrs:{title:this.title,options:this.options}})],1):this._e()}),[],!1,null,null,null).exports,Pd=[({Vue:e,options:n,router:t,siteData:a})=>{},({Vue:e,options:n,router:t,siteData:a})=>{a.pages.map(e=>{const{frontmatter:{date:n,author:t}}=e;"string"==typeof n&&"Z"===n.charAt(n.length-1)&&(e.frontmatter.date=function(e){e instanceof Date||(e=new Date(e));return`${e.getUTCFullYear()}-${hc(e.getUTCMonth()+1)}-${hc(e.getUTCDate())} ${hc(e.getUTCHours())}:${hc(e.getUTCMinutes())}:${hc(e.getUTCSeconds())}`}(n)),t?e.author=t:a.themeConfig.author&&(e.author=a.themeConfig.author)}),e.mixin(uc)},{},({Vue:e})=>{e.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},{},({Vue:e})=>{e.component("Vssue",Td)}],Id=["Vssue"];class Dd extends class{constructor(){this.store=new Wt({data:{state:{}}})}$get(e){return this.store.state[e]}$set(e,n){Wt.set(this.store.state,e,n)}$emit(...e){this.store.$emit(...e)}$on(...e){this.store.$on(...e)}}{}Object.assign(Dd.prototype,{getPageAsyncComponent:ss,getLayoutAsyncComponent:cs,getAsyncComponent:ls,getVueComponent:ds});var Ld={install(e){const n=new Dd;e.$vuepress=n,e.prototype.$vuepress=n}};function Od(e,n){return e.options.routes.filter(e=>e.path.toLowerCase()===n.toLowerCase()).length>0}var zd={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(e){const n=this.pageKey||this.$parent.$page.key;return hs("pageKey",n),Wt.component(n)||Wt.component(n,ss(n)),Wt.component(n)?e(n):e("")}},Rd={functional:!0,props:{slotKey:String,required:!0},render:(e,{props:n,slots:t})=>e("div",{class:["content__"+n.slotKey]},t()[n.slotKey])},Ed={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Ud=(t(326),t(327),Object(rc.a)(Ed,(function(){var e=this._self._c;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),jd={functional:!0,render(e,{parent:n,children:t}){if(n._isMounted)return t;n.$once("hook:mounted",()=>{n.$forceUpdate()})}};Wt.config.productionTip=!1,Wt.use(Vo),Wt.use(Ld),Wt.mixin(function(e,n,t=Wt){!function(e){e.locales&&Object.keys(e.locales).forEach(n=>{e.locales[n].path=n});Object.freeze(e)}(n),t.$vuepress.$set("siteData",n);const a=new(e(t.$vuepress.$get("siteData"))),r=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(a)),i={};return Object.keys(r).reduce((e,n)=>(n.startsWith("$")&&(e[n]=r[n].get),e),i),{computed:i}}(e=>class{setPage(e){this.__page=e}get $site(){return e}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:e={}}=this.$site;let n,t;for(const a in e)"/"===a?t=e[a]:0===this.$page.path.indexOf(a)&&(n=e[a]);return n||t||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:e}=this.$page.frontmatter;return"string"==typeof e&&e}get $title(){const e=this.$page,{metaTitle:n}=this.$page.frontmatter;if("string"==typeof n)return n;const t=this.$siteTitle,a=e.frontmatter.home?null:e.frontmatter.title||e.title;return t?a?a+" | "+t:t:a||"VuePress"}get $description(){const e=function(e){if(e){const n=e.filter(e=>"description"===e.name)[0];if(n)return n.content}}(this.$page.frontmatter.meta);return e||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(e,n){for(let t=0;t<e.length;t++){const a=e[t];if(a.path.toLowerCase()===n.toLowerCase())return a}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},sc)),Wt.component("Content",zd),Wt.component("ContentSlotsDistributor",Rd),Wt.component("OutboundLink",Ud),Wt.component("ClientOnly",jd),Wt.component("Layout",cs("Layout")),Wt.component("NotFound",cs("NotFound")),Wt.prototype.$withBase=function(e){const n=this.$site.base;return"/"===e.charAt(0)?n+e.slice(1):e},window.__VUEPRESS__={version:"1.8.0",hash:"7c02228"},async function(e){const n="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:sc.routerBase||sc.base,t=new Vo({base:n,mode:"history",fallback:!1,routes:oc,scrollBehavior:(e,n,t)=>t||(e.hash?!Wt.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(e.hash)}:{x:0,y:0})});!function(e){e.beforeEach((n,t,a)=>{if(Od(e,n.path))a();else if(/(\/|\.html)$/.test(n.path))if(/\/$/.test(n.path)){const t=n.path.replace(/\/$/,"")+".html";Od(e,t)?a(t):a()}else a();else{const t=n.path+"/",r=n.path+".html";Od(e,r)?a(r):Od(e,t)?a(t):a()}})}(t);const a={};try{await Promise.all(Pd.filter(e=>"function"==typeof e).map(n=>n({Vue:Wt,options:a,router:t,siteData:sc,isServer:e})))}catch(e){console.error(e)}return{app:new Wt(Object.assign(a,{router:t,render:e=>e("div",{attrs:{id:"app"}},[e("RouterView",{ref:"layout"}),e("div",{class:"global-ui"},Id.map(n=>e(n)))])})),router:t}}(!1).then(({app:e,router:n})=>{n.onReady(()=>{e.$mount("#app")})})}]);