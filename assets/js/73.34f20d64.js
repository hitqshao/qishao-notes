(window.webpackJsonp=window.webpackJsonp||[]).push([[73],{482:function(e,t,a){"use strict";a.r(t);var r=a(5),o=Object(r.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"software-engineering-assignment-qi-shao"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#software-engineering-assignment-qi-shao"}},[e._v("#")]),e._v(" Software Engineering Assignment - Qi Shao")]),e._v(" "),t("h2",{attrs:{id:"introduction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),t("p",[e._v("My research focuses on computer microarchitecture and memory subsystem. We try to improve the performance of CPU/GPU/Memory to achieve better IPC (Instruction per Cycle) as to CPU and higher throughput for GPU. Since it is unmanageable to program from silicon level, like Verilog or VHDL language. Researchers prefer to use simulators to simulate the function of CPU/GPU at cycle accurate level to value the idea.")]),e._v(" "),t("p",[e._v("I have been working in industry for years, using these simulators. Similar to methods mentioned in the lectures, we use regression test, combining both unit test and benchmark test to check whether added function is correct. Our team manager also make the rule that "),t("em",[e._v("if someone modify the code that slow down the runtime of simulator, he has to buy coffee for everyone in the team")]),e._v(". It is a soft way to prevent programmer from adding low-performance code.")]),e._v(" "),t("p",[e._v("We also follow the Google's C++ programming style guideline.")]),e._v(" "),t("h2",{attrs:{id:"robert-s-lecture-and-my-research-work-experience"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#robert-s-lecture-and-my-research-work-experience"}},[e._v("#")]),e._v(" Robert's Lecture and My Research & Work Experience")]),e._v(" "),t("h3",{attrs:{id:"v-model-my-research"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#v-model-my-research"}},[e._v("#")]),e._v(" V-Model & My Research")]),e._v(" "),t("p",[e._v("In the lecture, we disscussed about the flavor of SE/Processes V Model. The V Model has differente layers from upper-level "),t("em",[e._v("Requirements")]),e._v(" to bottom-level "),t("em",[e._v("Coding")]),e._v(". As to my research, there are already plenty of microarchitecture simulators.")]),e._v(" "),t("ul",[t("li",[e._v("Gem5/GPGPU-sim: Cycle Accurate Simualtor")]),e._v(" "),t("li",[e._v("Ramulator: Trace Driven Simulator")]),e._v(" "),t("li",[e._v("Zsim/Pin: Instrumental-based Simulator")])]),e._v(" "),t("p",[e._v("These simulators is build based on different requirments. Cycle-accurate simulator usually has better simulation accuracy, but lower speed. Ramulator is faster and Zsim/Pin is fastest since they run on real processor, just instrumenting code. However in the paper, I can always come accross that the results of some papers are simulated based on the simulator that does not support the function, in which it failed in the "),t("em",[e._v("Validation")]),e._v(" Part.")]),e._v(" "),t("p",[e._v("As to bottom-level coding, it is sensible that cycle-accurate simulator has most workload of coding. It has to simulate at cycle & instruction level. However, meanwhile since CPU&GPU design company like Intel or NVIDIA or AMD, they just disclose their design at very high-level block design. The detailed design of each unit in cycle accurate simulator is based on estimation the structure of each unit after runing benchmark on CPU & GPU.")]),e._v(" "),t("p",[e._v("In my research, I also met some paper that they claimed that they found previous cited work has miscoding some unit, so that the cited performance cannot be trusted.")]),e._v(" "),t("h3",{attrs:{id:"code-review-practices-my-work-experience"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#code-review-practices-my-work-experience"}},[e._v("#")]),e._v(" Code Review practices & My Work Experience")]),e._v(" "),t("p",[e._v("When I was in industry, we also follow the guideline of Google C++ programming style. But we didn't follow Google's code review. During the literature, I found that this part very interesting.")]),e._v(" "),t("p",[e._v("As a freshgraduate student, I also found that I could benefit a lot from reviewing code from senior or professionals. And based on this observation, during my research, when I first stepped into new area, for example compiler, like LLVM, I tends to go back to the initial commit of LLVM project and learn how the initional function was added into LLVM Project. It is the idea of "),t("em",[e._v("Education Maintaining norms")]),e._v(" mentioned in the paper. Another interesting finding is that as employer stays in Google longer, commemnts per change also decrease, and converge to each change 2 comments.")]),e._v(" "),t("p",[e._v("Another finding is similar to "),t("em",[e._v("finding5 code review at Google still faces breakdonws")]),e._v(". When I start to learn LLVM framework, in the beginning of user manual or guideline, one of the advantage of LLVM compared to GCC is the simplicty of LLVM. However, as years of development of LLVM, I feels the learning curve of LLVM is steeper than I imagine. And after new function are merged, the turtorial blogs are not updated. If I just follow the tutorial blogs, the function that I implement will not work. This is the mismatch of latest code and stale tutorial.")]),e._v(" "),t("h3",{attrs:{id:"verification-my-work-experience"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#verification-my-work-experience"}},[e._v("#")]),e._v(" Verification & My Work Experience")]),e._v(" "),t("p",[e._v("In the verification process, I have worked in some company that the programmer is also the verifier. They design test cases for their own code. An anology could be that an athelete is also the referee of the game. The company has follow this routine for a while, but in the end we found that there are always bugs that the programmer cannot found by themselves. It is since that the cases that the programers will always work. They are blind to the test cases that would trigger the bug. If they realized it, they would have fixed it. In the end, the company decide to manage a new team of verifiers that focus on testing the program.")]),e._v(" "),t("h2",{attrs:{id:"guest-s-lecture-and-my-research-work-experience"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#guest-s-lecture-and-my-research-work-experience"}},[e._v("#")]),e._v(" Guest's Lecture and My Research & Work Experience")]),e._v(" "),t("h3",{attrs:{id:"saab-survery-of-software-engineering-relevance-changing-with-ai-ml"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#saab-survery-of-software-engineering-relevance-changing-with-ai-ml"}},[e._v("#")]),e._v(" SAAB Survery of Software Engineering relevance changing with AI/ML")]),e._v(" "),t("p",[e._v("It is surprising that most of the answers are \"We dont know\". I also didn't use AI to generate programming code, since I feels that I have been trained to learn how to write efficient program and the AI could not achieve, until last year. Last year during the WASP conference, I found that most of the students have been using copilot. And nowadays that cursor, a new programming assistant tool is become popular. After the WASP conference, I tried Chat-GPT to write demo code for LLVM function and it works well. Sometimes, the code does not work, but at least, it provide a demo framework or an idea or suggest for you to program. As to me, if I am very familiar with some framework, for example, simulators, I will write code and modify it by myself, since I am better than chat-GPT or at least I dont need to understand the chat-gpt's code and debug. But as to new framework that I dont know, I will ask chatgpt to present some demo. It boosts the progress of my learning new framework.")]),e._v(" "),t("p",[e._v("After using chatgpt to generate code, I register WASP Natural Language Processing Code to learn why Transformers works in programming area. It is sensible that transformers at good at predict next token and it is also trained with public opensource frameworks. In this way,it could understand the programming style and generate code. And we can also view programming language as a universal language to communicate ideas. Thus, if it could works in chat robots, it will also work in programming.")]),e._v(" "),t("h3",{attrs:{id:"saab-safe-critical-consideration-with-ai"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#saab-safe-critical-consideration-with-ai"}},[e._v("#")]),e._v(" SAAB Safe-critical Consideration with AI")]),e._v(" "),t("p",[e._v("Usually AI tools like chatgpt are on-line, and we could use chatgpt to generate programs and we could remarks on the result to provide feedback to chatgpt about the quality of code. Thus it is a online feedback loop. As to Saab ATM, I have two considerations:")]),e._v(" "),t("ul",[t("li",[e._v("Before we introduce AI to generate test cases for code in SAAB, how could we promise the black-box test cases are enough to cover all safe-critiacal cases. And after we use AI, how we keep the same promise.")]),e._v(" "),t("li",[e._v("As a commercial company of SAAB, if they use chatgpt or some other AI tool to generate test cases or program demos, how could AI tool provider provide that the tool is safe from not generating risky code?")])]),e._v(" "),t("h2",{attrs:{id:"cain-papers"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cain-papers"}},[e._v("#")]),e._v(" CAIN Papers")]),e._v(" "),t("p",[e._v("My research topic is about improving performance of CPU & GPU, mainly with compression technique. Among the CAIN papers, I choose two paper related to memory compression.")]),e._v(" "),t("h3",{attrs:{id:"paper-1-towards-understanding-model-quantization-for-reliable-deep-naural-network-deployment-1"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#paper-1-towards-understanding-model-quantization-for-reliable-deep-naural-network-deployment-1"}},[e._v("#")]),e._v(' Paper 1: "Towards Understanding Model Quantization for Reliable Deep Naural Network Deployment" [^1]')]),e._v(" "),t("p",[e._v("This paper characterize the behaviour of quantization models. They mainly focused on the following four question and based on results of study, they provides answers to each of them.")]),e._v(" "),t("h4",{attrs:{id:"impact-of-data-distribution-shift-to-compressed-models"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#impact-of-data-distribution-shift-to-compressed-models"}},[e._v("#")]),e._v(" Impact of Data Distribution Shift to Compressed Models")]),e._v(" "),t("p",[e._v("The shift of dataset distribution could influence the performance of compressed model. And they choose two types of distribution shift datasets to evalute: dataset based on image transformation and dataset reported in literature. They compare the performance in terms of accuracy difference and lable difference.")]),e._v(" "),t("p",[e._v("They found that compressed model generated by CoreML is better than TensorflowLite during deployment. And the accurancy difference increaes by up 2.25% compared compressed model with original models.")]),e._v(" "),t("h4",{attrs:{id:"impact-of-training-strategy"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#impact-of-training-strategy"}},[e._v("#")]),e._v(" Impact of Training Strategy")]),e._v(" "),t("p",[e._v("They evalute different traning strategy: standard traning, quantization-aware traning, adversarial training and mixedup traning. In the study, they found quantization-aware traning is more stable.")]),e._v(" "),t("h4",{attrs:{id:"the-characteristics-on-which-original-and-compressed-models-disagree"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#the-characteristics-on-which-original-and-compressed-models-disagree"}},[e._v("#")]),e._v(" The characteristics on which original and compressed models disagree")]),e._v(" "),t("p",[e._v("They try to figure out the characteristic that original and compressed model most likely to disagree.  The conclustion is that the result is closer top1 and top2 output probilities. A better metric would be "),t("em",[e._v("Margin")]),e._v(" to distinguish disagreement and normal inputs.")]),e._v(" "),t("h4",{attrs:{id:"impact-of-retraining"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#impact-of-retraining"}},[e._v("#")]),e._v(" Impact of retraining")]),e._v(" "),t("p",[e._v("To my supervise, they found that retraining fails to reduce the total number of disagreements. It can also introduce many new disagreements.")]),e._v(" "),t("h3",{attrs:{id:"paper-2-the-impact-of-knowledge-distillation-on-the-energy-consumption-and-runtime-efficiency-of-nlp-models-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#paper-2-the-impact-of-knowledge-distillation-on-the-energy-consumption-and-runtime-efficiency-of-nlp-models-2"}},[e._v("#")]),e._v(' Paper 2: "The Impact of Knowledge Distillation on the Energy Consumption and Runtime Efficiency of NLP Models"  [^2]')]),e._v(" "),t("p",[e._v("In this paper, they study the impact of knowledge distillation in terms of energy consumption and runtime efficicency. They evaluate different models, BERT, Distilled-BERT, GPT-2 and Distilled-GPT2.")]),e._v(" "),t("h4",{attrs:{id:"energy-consumption"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#energy-consumption"}},[e._v("#")]),e._v(" Energy Consumption")]),e._v(" "),t("p",[e._v("As to distilled-GPT2, it experience 28.1% decrease in energy comsumption compared to GPT2 and Distilled-BERT 43.96%.")]),e._v(" "),t("h4",{attrs:{id:"inference-time"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#inference-time"}},[e._v("#")]),e._v(" Inference Time")]),e._v(" "),t("p",[e._v("Distilled-BERT and BERT takes inference time as 674ms and 891 ms, correspondingly. And GPT2 tooks 798ms, compared with Distilled-GPT 555ms.")]),e._v(" "),t("h4",{attrs:{id:"utilization"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#utilization"}},[e._v("#")]),e._v(" Utilization")]),e._v(" "),t("p",[e._v("Distilled-BERT has reduction in CPU utilization about 0.10% compared with BERT and reduction of memory utilization as 0.14%.")]),e._v(" "),t("p",[e._v("Distilled-GPT2 has lower 0.79% memory utilization and 0.13% lower CPU utilization.")]),e._v(" "),t("h4",{attrs:{id:"conclusion"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[e._v("#")]),e._v(" Conclusion")]),e._v(" "),t("p",[e._v("Compared with BERT, Distilled-BERT is more energy-efficient. And Distilled-GPT2 has better inference time and memory usage compared with GPT2.")]),e._v(" "),t("h2",{attrs:{id:"references"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#references"}},[e._v("#")]),e._v(" References")]),e._v(" "),t("p",[e._v("[^1]: Towards Understanding Model Quantization for Reliable Deep Naural Network Deployment")]),e._v(" "),t("p",[e._v("[^2]: The Impact of Knowledge Distillation on the Energy Consumption and Runtime Efficiency of NLP Models")])])}),[],!1,null,null,null);t.default=o.exports}}]);