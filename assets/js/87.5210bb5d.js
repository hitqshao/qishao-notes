(window.webpackJsonp=window.webpackJsonp||[]).push([[87],{549:function(t,a,e){"use strict";e.r(a);var i=e(8),r=Object(i.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"memory-optimizations"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#memory-optimizations"}},[t._v("#")]),t._v(" "),a("strong",[t._v("Memory Optimizations")])]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("Activation Checkpointing")]),a("br"),t._v("\nRecomputation during backward pass.")]),t._v(" "),a("li",[a("strong",[t._v("Quantization-Aware Training (QAT)")]),a("br"),t._v("\nTrain with INT8/FP8 precision.")]),t._v(" "),a("li",[a("strong",[t._v("Dynamic Memory Allocation")]),a("br"),t._v("\nBuffer reuse to avoid fragmentation.")]),t._v(" "),a("li",[a("strong",[t._v("Low-Rank Gradient Projection (GaLore)")]),a("br"),t._v(" "),a("strong",[t._v("NEW")]),t._v(" Compress gradients via low-rank approximations during training.")])]),t._v(" "),a("h2",{attrs:{id:"c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources"}},[t._v("#")]),t._v(" [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources")]),t._v(" "),a("ul",[a("li",[t._v("Use SGD instead of Adam for fine-tuning weights.")]),t._v(" "),a("li",[t._v("Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.")]),t._v(" "),a("li",[t._v("SGD also avoid state memory of ADAM.")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/c1e7d018-84e2-49f7-bd74-cdf4edaa0343",alt:"image"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/a98a808f-1cb1-4432-b0d0-7c51e9f4ee48",alt:"image"}})])])}),[],!1,null,null,null);a.default=r.exports}}]);