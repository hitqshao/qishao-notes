(window.webpackJsonp=window.webpackJsonp||[]).push([[89],{541:function(t,e,i){"use strict";i.r(e);var a=i(8),n=Object(a.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("ol",[e("li",[t._v("[139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization")]),t._v(" "),e("li",[t._v("[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models")]),t._v(" "),e("li",[t._v("[26] FP8-LM: Training FP8 Large Language Models üëç from Microsoft")]),t._v(" "),e("li",[t._v("[139] FP8 Formats for Deep Learning")]),t._v(" "),e("li",[t._v("[41] With Shared Microexponents, A Little Shifting Goes a Long Way üëç from Meta, Microsoft üëç")]),t._v(" "),e("li",[t._v("[34] Stable and low-precision training for large-scale vision-language models üëç\n"),e("em",[t._v("mentioned in Deepseek paper mixed precision training section. Not read yet.")])]),t._v(" "),e("li",[t._v("[Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization")]),t._v(" "),e("li",[t._v("[47] Microscaling Data Formats for Deep Learning")]),t._v(" "),e("li",[t._v("[Unread 2 Y2024] To FP8 and Back Again: Quantifying the Effects of Reducing Precision on LLM Training Stability")]),t._v(" "),e("li",[t._v("[145] Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model")]),t._v(" "),e("li",[t._v("[Unread 27]Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference")]),t._v(" "),e("li",[t._v("[45] PB-LLM: Partially Binarized Large Language Models")])]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization"}},[t._v("#")]),t._v(" 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization")]),t._v(" "),e("h3",{attrs:{id:"problems"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#problems"}},[t._v("#")]),t._v(" Problems")]),t._v(" "),e("p",[t._v("The problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:")]),t._v(" "),e("ul",[e("li",[t._v("High Dynamic Range of Activations\n"),e("ul",[e("li",[t._v("The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.")]),t._v(" "),e("li",[t._v("This means that the values within these tensors vary significantly in magnitude.")]),t._v(" "),e("li",[t._v("Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.")]),t._v(" "),e("li",[t._v("Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.")])])]),t._v(" "),e("li",[t._v("Presence of Structured Outliers\n"),e("ul",[e("li",[t._v("The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).")]),t._v(" "),e("li",[t._v("These outliers are not random; they appear to be correlated with specific input tokens and embedding dimensions.")]),t._v(" "),e("li",[t._v("Further analysis revealed that these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP]).")]),t._v(" "),e("li",[t._v("While this attention behavior might be beneficial for the model's performance, the outliers that cause it also create challenges for quantization.")])])]),t._v(" "),e("li",[t._v("Sensitivity to Quantization Noise\n"),e("ul",[e("li",[t._v("Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.")]),t._v(" "),e("li",[t._v("Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.")]),t._v(" "),e("li",[t._v("This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.")])])])]),t._v(" "),e("h3",{attrs:{id:"solutions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#solutions"}},[t._v("#")]),t._v(" Solutions")]),t._v(" "),e("p",[t._v("solutions proposed in the paper:")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("Mixed-precision PTQ")]),t._v(" "),e("ul",[e("li",[t._v("The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.")]),t._v(" "),e("li",[t._v("To address this, they proposed using a higher bit-width (16-bit) for the more sensitive activation tensors, particularly the residual sum after the feed-forward network (FFN).")]),t._v(" "),e("li",[t._v("This higher bit-width allows for more accurate representation of both the FFN's input and output, minimizing potential errors.")]),t._v(" "),e("li",[t._v("Additionally, they explored using low-bit (2-4) quantization for weights and token embeddings, which can significantly reduce model size without much accuracy loss.")])])]),t._v(" "),e("li",[e("p",[t._v("Per-embedding-group PTQ")]),t._v(" "),e("ul",[e("li",[t._v("The authors identified that outliers in the activation tensors primarily reside in a few specific embedding dimensions.")]),t._v(" "),e("li",[t._v("To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.")]),t._v(" "),e("li",[t._v("This method involves splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group.")]),t._v(" "),e("li",[t._v("To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.")]),t._v(" "),e("li",[t._v("This approach effectively handles outliers without significantly increasing computational overhead.")])])]),t._v(" "),e("li",[e("p",[t._v("Quantization-aware training (QAT)")]),t._v(" "),e("ul",[e("li",[t._v("The authors also explored QAT, where the model is trained with simulated quantization operations.")]),t._v(" "),e("li",[t._v("This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.")]),t._v(" "),e("li",[t._v("During QAT, they used learnable ranges for both weights and activations, further enhancing the model's adaptability to quantization.")])])])]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-43-integer-or-floating-point-new-outlooks-for-low-bit-quantization-on-large-language-models"}},[t._v("#")]),t._v(" 2.[43] Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models")]),t._v(" "),e("h3",{attrs:{id:"key-takeaways-in-three-sentences"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#key-takeaways-in-three-sentences"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Key Takeaways in Three Sentences")])]),t._v(" "),e("ol",[e("li",[t._v("The study demonstrates that "),e("strong",[t._v("low-bit floating-point formats, particularly FP8, provide superior quantization accuracy for LLMs compared to INT8")]),t._v(" , with comparable hardware efficiency at 8-bit precision.")]),t._v(" "),e("li",[t._v("The "),e("strong",[t._v("Mixture of Formats Quantization (MoFQ) approach optimally selects between INT and FP per layer")]),t._v(" , improving accuracy without increasing computational overhead.")]),t._v(" "),e("li",[t._v("MoFQ achieves "),e("strong",[t._v("state-of-the-art results in both W4-only and W8A8 quantization")]),t._v(" , outperforming existing methods like GPTQ, AWQ, LLM.int8(), and SmoothQuant while maintaining "),e("strong",[t._v("efficient inference speed")]),t._v(" .")])]),t._v(" "),e("h3",{attrs:{id:"abstract"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#abstract"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Abstract")])]),t._v(" "),e("p",[t._v("The study finds that optimal quantization formats vary across layers in LLMs, leading to the "),e("strong",[t._v("Mixture of Formats Quantization (MoFQ)")]),t._v("  approach, which selects the best format per layer."),e("br"),t._v("\nMoFQ achieves superior or comparable performance to current quantization methods for weight-only (W-only) and weight-activation (WA) quantization without additional hardware overhead.")]),t._v(" "),e("h3",{attrs:{id:"introduction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Introduction")])]),t._v(" "),e("p",[t._v("Quantization minimizes LLMs' size and inference costs, with prior work focusing on low-bit integer formats."),e("br"),t._v("\nHowever, as LLMs grow, integer quantization becomes less effective, requiring optimizations or alternatives. Low-bit floating-point formats have emerged as viable alternatives, with FP8 already supported in NVIDIA‚Äôs H100 GPUs.")]),t._v(" "),e("p",[t._v("The study:")]),t._v(" "),e("ol",[e("li",[t._v("Compares INT and FP formats in terms of hardware efficiency and quantization error.")]),t._v(" "),e("li",[t._v("Proposes "),e("strong",[t._v("Mixture of Formats Quantization (MoFQ)")]),t._v(" , selecting the best format per layer.")]),t._v(" "),e("li",[t._v("Implements an inference system for W-only quantization, maintaining performance parity with INT-based systems.")])]),t._v(" "),e("h3",{attrs:{id:"background-and-related-works"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#background-and-related-works"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Background and Related Works")])]),t._v(" "),e("p",[e("strong",[t._v("Integer vs. Floating-Point Formats")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Integer (INT)")]),t._v(" : Uniformly distributed values.")]),t._v(" "),e("li",[e("strong",[t._v("Floating-Point (FP)")]),t._v(" : Non-uniform distribution, allowing higher precision for small values but reduced precision for large values.")]),t._v(" "),e("li",[e("strong",[t._v("Hardware efficiency")]),t._v(" : FP operations typically cost more than INT, but at 8-bit, FP8 and INT8 MAC (Multiply-Accumulate) units have nearly identical area costs.")])]),t._v(" "),e("p",[e("strong",[t._v("Post-Training Quantization (PTQ) for LLMs")])]),t._v(" "),e("p",[t._v("Two main PTQ strategies:")]),t._v(" "),e("ol",[e("li",[e("strong",[t._v("Weight-Only (W-only) Quantization:")]),t._v("  Applies to weights only, e.g., W4A16.")]),t._v(" "),e("li",[e("strong",[t._v("Weight-Activation (WA) Quantization:")]),t._v("  Quantizes both weights and activations, e.g., W8A8.")])]),t._v(" "),e("p",[t._v("State-of-the-art (SOTA) methods:")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("LLM.int8()")]),t._v(" : Uses mixed precision (INT8+FP16).")]),t._v(" "),e("li",[e("strong",[t._v("SmoothQuant")]),t._v(" : Redistributes quantization difficulty from activations to weights.")]),t._v(" "),e("li",[e("strong",[t._v("GPTQ & AWQ")]),t._v(" : Use second-order information and pre-scaling techniques to improve quantization.")])]),t._v(" "),e("h3",{attrs:{id:"comparative-analysis-of-int-and-fp-formats"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#comparative-analysis-of-int-and-fp-formats"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Comparative Analysis of INT and FP Formats")])]),t._v(" "),e("p",[e("strong",[t._v("A. Hardware Cost of INT vs. FP MAC Units")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("At 8-bit precision, FP8 and INT8 MACs require nearly the same hardware area")]),t._v(" , aligning with H100 GPU capabilities.")])]),t._v(" "),e("p",[e("strong",[t._v("B. Quantization Error Comparison")])]),t._v(" "),e("ol",[e("li",[e("strong",[t._v("4-bit Weight-Only (W4) Quantization")]),t._v("  (LLaMA-65B model):")])]),t._v(" "),e("ul",[e("li",[t._v("Some layers perform better with INT4, while others favor FP4, indicating "),e("strong",[t._v("layer-dependent format preference")]),t._v(" .")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/97be028e-fdb3-4883-a92e-b784d1ff1f87",alt:"image"}})]),t._v(" "),e("ol",{attrs:{start:"2"}},[e("li",[e("strong",[t._v("8-bit Weight-Activation (W8A8) Quantization")]),t._v(" :")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Weights")]),t._v(" : INT8 generally has lower quantization error.")]),t._v(" "),e("li",[e("strong",[t._v("Activations")]),t._v(" : FP8 shows "),e("strong",[t._v("better robustness")]),t._v("  for dynamic activation tensors.")]),t._v(" "),e("li",[t._v("Best choice: INT8 for weights, FP8 for activations‚Äîbut hardware constraints necessitate using the same format per layer.")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/7af41adf-5821-4c0b-8599-1385c1982c87",alt:"image"}})]),t._v(" "),e("h3",{attrs:{id:"exploiting-int-and-fp-complementarity"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#exploiting-int-and-fp-complementarity"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Exploiting INT and FP Complementarity")])]),t._v(" "),e("p",[e("strong",[t._v("A. Improved Low-Bit FP4 Format")])]),t._v(" "),e("ul",[e("li",[t._v("IEEE floating-point format reserves exponent values for NaN and Inf.")]),t._v(" "),e("li",[e("strong",[t._v("Reallocating NaN & Inf to normalized numbers improves FP4 precision")]),t._v("  by 35%.")])]),t._v(" "),e("p",[e("strong",[t._v("B. Mixture of Formats Quantization (MoFQ)")])]),t._v(" "),e("ul",[e("li",[t._v("Selects the best quantization format (INT or FP) "),e("strong",[t._v("per layer")]),t._v("  based on quantization error.")]),t._v(" "),e("li",[t._v("Works for both "),e("strong",[t._v("W-only and WA quantization")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Algorithm")]),t._v(" : Iterates through layers, computes quantization error for INT and FP, and selects the lower-error format.")])]),t._v(" "),e("p",[e("strong",[t._v("C. Low-Bit W-Only Inference System")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("INT4 and FP4 require conversion to FP16 before computation")]),t._v("  due to FP16 activations.")]),t._v(" "),e("li",[e("strong",[t._v("W8A8 quantization")]),t._v(" : FP16 activations are converted to FP8 or INT8 based on next-layer format selection.")]),t._v(" "),e("li",[e("strong",[t._v("No additional hardware overhead for FP-based or MoFQ-based inference")]),t._v("  compared to INT-based quantization.")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/7d5e0e92-deda-4e40-8775-b428fff2ca0c",alt:"image"}})]),t._v(" "),e("h3",{attrs:{id:"conclusion"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Conclusion")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Comparative study")]),t._v(" : INT and FP formats have complementary strengths.")]),t._v(" "),e("li",[e("strong",[t._v("Key finding")]),t._v(" : "),e("strong",[t._v("FP8 and INT8 MAC units have similar hardware costs at low-bit quantization")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("MoFQ method")]),t._v(" :\n"),e("ul",[e("li",[t._v("Selects the best quantization format "),e("strong",[t._v("per layer")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Achieves state-of-the-art accuracy")]),t._v("  in W4-only and W8A8 quantization.")]),t._v(" "),e("li",[e("strong",[t._v("No additional inference latency or hardware overhead")]),t._v(" .")])])])]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"_26-fp8-lm-training-fp8-large-language-models"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_26-fp8-lm-training-fp8-large-language-models"}},[t._v("#")]),t._v(" [26] FP8-LM: Training FP8 Large Language Models")]),t._v(" "),e("h3",{attrs:{id:"abstract-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#abstract-2"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Abstract")])]),t._v(" "),e("p",[t._v("The paper explores "),e("strong",[t._v("FP8 low-bit data formats")]),t._v("  for training large language models (LLMs), significantly reducing "),e("strong",[t._v("memory usage and computation costs")]),t._v("  while maintaining accuracy.")]),t._v(" "),e("p",[t._v("The authors introduce an "),e("strong",[t._v("FP8 automatic mixed-precision training framework")]),t._v("  with three levels of FP8 utilization, improving mixed-precision and distributed parallel training. "),e("strong",[t._v("Key results")]),t._v("  show that training the "),e("strong",[t._v("GPT-175B model on an H100 GPU platform")]),t._v("  using FP8:")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Reduces memory usage by 39%")])]),t._v(" "),e("li",[e("strong",[t._v("Speeds up training by 75% compared to BF16 (Megatron-LM)")])]),t._v(" "),e("li",[e("strong",[t._v("Outperforms Nvidia Transformer Engine by 37%")]),t._v("\nThe "),e("strong",[t._v("FP8 training methodology is generalizable")]),t._v("  to fine-tuning, instruction tuning, and reinforcement learning with human feedback (RLHF). The framework is "),e("strong",[t._v("open-sourced")]),t._v("  at "),e("a",{attrs:{href:"https://github.com/Azure/MS-AMP",target:"_blank",rel:"noopener noreferrer"}},[t._v("aka.ms/MS.AMP"),e("OutboundLink")],1),t._v(" .")])]),t._v(" "),e("h3",{attrs:{id:"_1-introduction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-introduction"}},[t._v("#")]),t._v(" "),e("strong",[t._v("1. Introduction")])]),t._v(" "),e("p",[t._v("LLMs have demonstrated exceptional performance in various domains but are extremely expensive to train."),e("br"),t._v("\nThe cost of training models like "),e("strong",[t._v("GPT-3 (175B) or PaLM (540B)")]),t._v("  is enormous, requiring "),e("strong",[t._v("thousands of GPUs or TPUs")]),t._v(" ."),e("br"),t._v("\nLow-precision training is a "),e("strong",[t._v("promising solution")]),t._v("  as it:")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Increases speed")])]),t._v(" "),e("li",[e("strong",[t._v("Reduces memory usage")])]),t._v(" "),e("li",[e("strong",[t._v("Minimizes communication overhead")])])]),t._v(" "),e("p",[t._v("Most existing frameworks, such as "),e("strong",[t._v("Megatron-LM, MetaSeq, and Colossal-AI")]),t._v(" , use "),e("strong",[t._v("FP32, FP16, or BF16 mixed-precision training")]),t._v(" , but "),e("strong",[t._v("FP8 offers significant efficiency gains")]),t._v(" :")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("2√ó speed-up")])]),t._v(" "),e("li",[e("strong",[t._v("50%-75% memory and communication savings")])])]),t._v(" "),e("h4",{attrs:{id:"challenges-of-fp8-training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#challenges-of-fp8-training"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Challenges of FP8 Training")])]),t._v(" "),e("ol",[e("li",[e("strong",[t._v("Data underflow/overflow issues")]),t._v("  due to FP8‚Äôs limited dynamic range.")]),t._v(" "),e("li",[e("strong",[t._v("Numerical instabilities and divergence")]),t._v("  during training.")])]),t._v(" "),e("h4",{attrs:{id:"proposed-fp8-mixed-precision-framework"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#proposed-fp8-mixed-precision-framework"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Proposed FP8 Mixed-Precision Framework")])]),t._v(" "),e("ul",[e("li",[t._v("Introduces "),e("strong",[t._v("three levels of FP8 utilization")]),t._v("  (gradients, optimizer states, and distributed learning).")]),t._v(" "),e("li",[t._v("Uses "),e("strong",[t._v("precision decoupling")]),t._v("  and "),e("strong",[t._v("automatic scaling")]),t._v("  to mitigate numerical instability.")]),t._v(" "),e("li",[t._v("Achieves "),e("strong",[t._v("29%-39% memory savings")]),t._v("  and "),e("strong",[t._v("63%-65% communication cost reductions")]),t._v(" .")])]),t._v(" "),e("h3",{attrs:{id:"_2-fp8-llm-training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-fp8-llm-training"}},[t._v("#")]),t._v(" "),e("strong",[t._v("2. FP8 LLM Training")])]),t._v(" "),e("h4",{attrs:{id:"_2-1-fp8-gradient-and-all-reduce-communication"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-fp8-gradient-and-all-reduce-communication"}},[t._v("#")]),t._v(" "),e("strong",[t._v("2.1 FP8 Gradient and All-Reduce Communication")])]),t._v(" "),e("ul",[e("li",[t._v("Traditional mixed-precision training uses "),e("strong",[t._v("FP16/FP32 for gradients")]),t._v(" , leading to high communication costs.")]),t._v(" "),e("li",[t._v("Applying "),e("strong",[t._v("FP8 directly to gradients")]),t._v("  results in "),e("strong",[t._v("loss of accuracy")]),t._v("  due to underflow/overflow.")]),t._v(" "),e("li",[t._v("The paper proposes an "),e("strong",[t._v("automatic scaling technique")]),t._v("  to adapt scaling factors dynamically, preventing numerical instability.")])]),t._v(" "),e("h4",{attrs:{id:"_2-2-fp8-optimizer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-fp8-optimizer"}},[t._v("#")]),t._v(" "),e("strong",[t._v("2.2 FP8 Optimizer")])]),t._v(" "),e("ul",[e("li",[t._v("The "),e("strong",[t._v("Adam optimizer")]),t._v("  typically consumes "),e("strong",[t._v("16 bytes per parameter")]),t._v("  due to high-precision storage of gradients and optimizer states.")]),t._v(" "),e("li",[t._v("The proposed "),e("strong",[t._v("FP8 optimizer")]),t._v("  stores:\n"),e("ul",[e("li",[t._v("FP8 first-order moment")]),t._v(" "),e("li",[t._v("FP16 master weights (with tensor scaling)")]),t._v(" "),e("li",[t._v("FP16 second-order moment")])])]),t._v(" "),e("li",[t._v("This reduces "),e("strong",[t._v("memory consumption from 16 bytes to 6 bytes per parameter")]),t._v("  (2.6√ó savings).")])]),t._v(" "),e("blockquote",[e("p",[t._v("My main takeaway is that direction of gradient matters, instead of magnitude.")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/aa8bdd9a-c5ea-4910-bfdf-836eca32f9b5",alt:"image"}})]),t._v(" "),e("h4",{attrs:{id:"_2-3-fp8-distributed-parallel-training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-fp8-distributed-parallel-training"}},[t._v("#")]),t._v(" "),e("strong",[t._v("2.3 FP8 Distributed Parallel Training")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Tensor Parallelism")]),t._v(" : Uses "),e("strong",[t._v("FP8 for weight and activation tensors")]),t._v(" , reducing compute and communication overhead.")]),t._v(" "),e("li",[e("strong",[t._v("Sequence Parallelism")]),t._v(" : Converts activation tensors to "),e("strong",[t._v("FP8 before communication")]),t._v(" , reducing costs.")]),t._v(" "),e("li",[e("strong",[t._v("ZeRO (Zero Redundancy Optimizer) Support")]),t._v(" : Distributes "),e("strong",[t._v("full tensors")]),t._v("  across devices while preserving "),e("strong",[t._v("FP8 scaling factors")]),t._v(" .")])]),t._v(" "),e("h3",{attrs:{id:"_3-experimentation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-experimentation"}},[t._v("#")]),t._v(" "),e("strong",[t._v("3. Experimentation")])]),t._v(" "),e("h4",{attrs:{id:"_3-1-experimental-setup"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-experimental-setup"}},[t._v("#")]),t._v(" "),e("strong",[t._v("3.1 Experimental Setup")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Training Dataset")]),t._v(" : Collected from "),e("strong",[t._v("CommonCrawl, The Pile, C4, OpenWebText, Wikipedia, RedPajama")]),t._v(" , and other curated sources.")]),t._v(" "),e("li",[e("strong",[t._v("Model Configuration")]),t._v(" : Uses a "),e("strong",[t._v("decoder-only Transformer")]),t._v("  architecture (like GPT-3), with "),e("strong",[t._v("RoPE embeddings and Flash Attention")]),t._v(" .")])]),t._v(" "),e("h4",{attrs:{id:"_3-2-main-results"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-main-results"}},[t._v("#")]),t._v(" "),e("strong",[t._v("3.2 Main Results")])]),t._v(" "),e("h5",{attrs:{id:"model-performance"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#model-performance"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Model Performance")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Loss curves of FP8 models match BF16 models")]),t._v(" , confirming "),e("strong",[t._v("accuracy preservation")])]),t._v(" "),e("li",[e("strong",[t._v("Zero-shot evaluations")]),t._v("  on "),e("strong",[t._v("Lambada, HellaSwag, BoolQ, PIQA, COPA")]),t._v("  show "),e("strong",[t._v("comparable performance between FP8 and BF16")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Fine-tuning (SFT & RLHF)")]),t._v(" : FP8 achieves:\n"),e("ul",[e("li",[t._v("27% faster fine-tuning")]),t._v(" "),e("li",[t._v("32% reduction in model weight memory")]),t._v(" "),e("li",[t._v("62% optimizer state memory savings")])])])]),t._v(" "),e("p",[e("strong",[t._v("System Performance")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Memory reduction")]),t._v(" : FP8 achieves "),e("strong",[t._v("28%-39% lower memory usage")]),t._v("  than BF16.")]),t._v(" "),e("li",[e("strong",[t._v("Training speed improvement")]),t._v(" :\n"),e("ul",[e("li",[t._v("75% faster training for GPT-175B")]),t._v(" "),e("li",[t._v("37% faster than Nvidia Transformer Engine")])])]),t._v(" "),e("li",[e("strong",[t._v("Communication efficiency")]),t._v(" :\n"),e("ul",[e("li",[t._v("63%-65% reduction in weight gradient communication")]),t._v(" "),e("li",[t._v("34% lower activation-related communication costs")])])])]),t._v(" "),e("h4",{attrs:{id:"_3-3-ablation-study"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-ablation-study"}},[t._v("#")]),t._v(" "),e("strong",[t._v("3.3 Ablation Study")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Gradient Scaling")]),t._v(" : "),e("strong",[t._v("Automatic scaling")]),t._v("  reduces "),e("strong",[t._v("underflow/overflow errors")]),t._v(" , improving training stability.")]),t._v(" "),e("li",[e("strong",[t._v("Optimizer Precision")]),t._v(" :\n"),e("ul",[e("li",[t._v("FP16 master weights outperform FP8 master weights in accuracy preservation.")]),t._v(" "),e("li",[t._v("FP8 first-order gradient moment is viable, but FP8 second-order moment leads to divergence.")])])]),t._v(" "),e("li",[e("strong",[t._v("Parallelism Optimization")]),t._v(" :\n"),e("ul",[e("li",[e("strong",[t._v("FP8 sequence and tensor parallelism")]),t._v("  reduce communication costs by "),e("strong",[t._v("34%")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("FP8 ZeRO")]),t._v("  maintains a balanced GPU memory load while saving memory.")])])])]),t._v(" "),e("h3",{attrs:{id:"_4-related-work"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-related-work"}},[t._v("#")]),t._v(" "),e("strong",[t._v("4. Related Work")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Mixed-Precision Training")]),t._v(" : Prior work focused on "),e("strong",[t._v("FP16/BF16")]),t._v(" , but "),e("strong",[t._v("FP8 remains underexplored")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Low-Precision LLM Training")]),t._v(" :\n"),e("ul",[e("li",[e("strong",[t._v("OPT, Bloom, Gopher, Chinchilla")]),t._v("  used "),e("strong",[t._v("BF16")]),t._v("  for better numerical stability.")]),t._v(" "),e("li",[t._v("FP8 support was limited before Nvidia Hopper GPUs.")]),t._v(" "),e("li",[t._v("This work provides the "),e("strong",[t._v("first systematic FP8 training framework")]),t._v("  for "),e("strong",[t._v("pre-training and fine-tuning LLMs")]),t._v(" .")])])])]),t._v(" "),e("h3",{attrs:{id:"_5-conclusion"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-conclusion"}},[t._v("#")]),t._v(" "),e("strong",[t._v("5. Conclusion")])]),t._v(" "),e("ul",[e("li",[t._v("Introduces a "),e("strong",[t._v("new FP8 mixed-precision training framework")]),t._v("  with "),e("strong",[t._v("automatic scaling")]),t._v("  and "),e("strong",[t._v("precision decoupling")]),t._v(" .")]),t._v(" "),e("li",[t._v("Achieves "),e("strong",[t._v("significant reductions in memory, compute, and communication costs")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Maintains model accuracy")]),t._v("  across "),e("strong",[t._v("GPT models from 125M to 175B parameters")]),t._v(" .")]),t._v(" "),e("li",[t._v("Demonstrates "),e("strong",[t._v("versatility")]),t._v("  in pre-training, instruction tuning, and RLHF.")]),t._v(" "),e("li",[e("strong",[t._v("Future work")]),t._v("  includes scaling to even larger models, training multi-modal models, and deploying FP8 LLMs on edge devices.")])]),t._v(" "),e("h3",{attrs:{id:"key-summary-in-3-sentences"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#key-summary-in-3-sentences"}},[t._v("#")]),t._v(" "),e("em",[t._v("Key Summary in 3 Sentences")]),t._v("*")]),t._v(" "),e("p",[t._v("This paper introduces an "),e("strong",[t._v("FP8 mixed-precision training framework")]),t._v("  that reduces memory consumption by "),e("strong",[t._v("39%")]),t._v(" , speeds up training by "),e("strong",[t._v("75%")]),t._v(" , and "),e("strong",[t._v("outperforms Nvidia Transformer Engine by 37%")]),t._v("  while maintaining LLM accuracy."),e("br"),t._v("\nThe framework uses "),e("strong",[t._v("automatic scaling and precision decoupling")]),t._v("  to stabilize training, supports "),e("strong",[t._v("FP8 optimizers and distributed training")]),t._v(" , and generalizes to "),e("strong",[t._v("fine-tuning and reinforcement learning with human feedback (RLHF)")]),t._v(" ."),e("br"),t._v("\nThese findings establish "),e("strong",[t._v("FP8 as the next-generation precision format for training LLMs")]),t._v(" , significantly lowering costs while preserving model performance.")]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"_139-fp8-formats-for-deep-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_139-fp8-formats-for-deep-learning"}},[t._v("#")]),t._v(" [139] FP8 Formats for Deep Learning")]),t._v(" "),e("h3",{attrs:{id:"_1-introduction-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-introduction-2"}},[t._v("#")]),t._v(" "),e("strong",[t._v("1. Introduction")])]),t._v(" "),e("p",[t._v("Deep learning models require increasing computational resources, necessitating lower-precision formats for efficiency.")]),t._v(" "),e("p",[t._v("FP8 is a "),e("strong",[t._v("natural evolution")]),t._v("  from FP16 and BF16, reducing "),e("strong",[t._v("compute and memory costs")]),t._v("  while maintaining "),e("strong",[t._v("accuracy comparable to FP16")]),t._v(" .")]),t._v(" "),e("h3",{attrs:{id:"key-contributions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#key-contributions"}},[t._v("#")]),t._v(" Key contributions:")]),t._v(" "),e("ul",[e("li",[e("p",[e("strong",[t._v("Two FP8 formats:")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("E4M3")]),t._v(" : 4-bit exponent, 3-bit mantissa (for weights and activations).")]),t._v(" "),e("li",[e("strong",[t._v("E5M2")]),t._v(" : 5-bit exponent, 2-bit mantissa (for gradients).")])])]),t._v(" "),e("li",[e("p",[e("strong",[t._v("Training and inference in FP8")]),t._v("  match FP16/BF16 accuracy across CNNs, RNNs, and Transformers.")])]),t._v(" "),e("li",[e("p",[e("strong",[t._v("Post-training quantization (PTQ)")]),t._v("  using FP8 "),e("strong",[t._v("outperforms int8")]),t._v("  while preserving model accuracy.")])])]),t._v(" "),e("h3",{attrs:{id:"_2-aspects-of-fp8-usage-in-deep-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-aspects-of-fp8-usage-in-deep-learning"}},[t._v("#")]),t._v(" "),e("strong",[t._v("2. Aspects of FP8 Usage in Deep Learning")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("FP8 computations")]),t._v("  will be performed in "),e("strong",[t._v("higher precision (FP16/FP32)")]),t._v(" , with final results cast back to FP8.")]),t._v(" "),e("li",[e("strong",[t._v("Scaling factors")]),t._v("  are applied to "),e("strong",[t._v("optimize FP8 precision")]),t._v(" , similar to "),e("strong",[t._v("loss-scaling in FP16 mixed precision")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Handling of special values (NaNs, Infs) is modified in E4M3")]),t._v("  to increase dynamic range.")])]),t._v(" "),e("h3",{attrs:{id:"_3-fp8-binary-interchange-format"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-fp8-binary-interchange-format"}},[t._v("#")]),t._v(" "),e("strong",[t._v("3. FP8 Binary Interchange Format")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/dc054ba5-6892-4c63-888e-ef24c9789d8a",alt:"image"}})]),t._v(" "),e("p",[t._v("FP8 includes "),e("strong",[t._v("two encodings")]),t._v(" :")]),t._v(" "),e("ul",[e("li",[e("p",[e("strong",[t._v("E4M3")]),t._v(" :")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Used for weights and activations")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("No representation for infinities")]),t._v("  (max value: "),e("strong",[t._v("448")]),t._v(" ).")]),t._v(" "),e("li",[e("strong",[t._v("Single NaN representation to extend range")]),t._v(" .")])])]),t._v(" "),e("li",[e("p",[e("strong",[t._v("E5M2")]),t._v(" :")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Used for gradients")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Standard IEEE-like format")]),t._v(" , supporting "),e("strong",[t._v("NaNs and infinities")]),t._v(" .")]),t._v(" "),e("li",[t._v("Larger range (up to "),e("strong",[t._v("57,344")]),t._v(" ).")])])])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/ac2dbf07-fd59-4902-a55b-0bb4b0385408",alt:"image"}})]),t._v(" "),e("h4",{attrs:{id:"_3-1-special-value-representations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-special-value-representations"}},[t._v("#")]),t._v(" "),e("strong",[t._v("3.1 Special Value Representations")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("E4M3 removes infinities")]),t._v("  and limits NaNs to a "),e("strong",[t._v("single pattern")]),t._v(" , extending its "),e("strong",[t._v("dynamic range")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("E5M2 follows IEEE-754")]),t._v(" , allowing "),e("strong",[t._v("straightforward conversion from FP16")]),t._v(" .")])]),t._v(" "),e("h4",{attrs:{id:"_3-2-exponent-bias"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-exponent-bias"}},[t._v("#")]),t._v(" "),e("strong",[t._v("3.2 Exponent Bias")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("E4M3 bias = 7, E5M2 bias = 15")]),t._v("  (matching IEEE-style representation).")]),t._v(" "),e("li",[t._v("Some models require "),e("strong",[t._v("per-tensor scaling")]),t._v("  rather than a fixed exponent bias (Figure 2).")])]),t._v(" "),e("h3",{attrs:{id:"_4-empirical-results"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-empirical-results"}},[t._v("#")]),t._v(" "),e("strong",[t._v("4. Empirical Results")])]),t._v(" "),e("h4",{attrs:{id:"_4-1-training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-training"}},[t._v("#")]),t._v(" "),e("strong",[t._v("4.1 Training")])]),t._v(" "),e("ul",[e("li",[t._v("FP8 training achieves "),e("strong",[t._v("accuracy comparable to FP16/BF16")]),t._v("  across CNNs, RNNs, and Transformers.")]),t._v(" "),e("li",[e("strong",[t._v("Image Classification")]),t._v(" :\n"),e("ul",[e("li",[t._v("FP8 accuracy is "),e("strong",[t._v("within statistical variation")]),t._v("  of FP16 for most CNNs (ResNet, MobileNet, VGG, etc.).")])])]),t._v(" "),e("li",[e("strong",[t._v("Language Translation")]),t._v(" :\n"),e("ul",[e("li",[t._v("FP8 BLEU scores "),e("strong",[t._v("match FP16")]),t._v("  for Transformer and GNMT models.")])])]),t._v(" "),e("li",[e("strong",[t._v("NLP Models (Table 4, Figure 1)")]),t._v(" :\n"),e("ul",[e("li",[t._v("GPT models (126M to 175B parameters) trained in FP8 "),e("strong",[t._v("match FP16 in perplexity")]),t._v(" .")])])])]),t._v(" "),e("h4",{attrs:{id:"_4-2-inference"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-inference"}},[t._v("#")]),t._v(" "),e("strong",[t._v("4.2 Inference")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("FP8 post-training quantization (PTQ) outperforms int8")]),t._v(" , retaining "),e("strong",[t._v("full precision accuracy")]),t._v("  for:\n"),e("ul",[e("li",[t._v("BERT (F1 score on SQuAD).")]),t._v(" "),e("li",[t._v("GPT-3 (perplexity on Wikitext103).")])])]),t._v(" "),e("li",[e("strong",[t._v("FP8-trained models require no additional quantization steps")]),t._v(" , simplifying deployment.")])]),t._v(" "),e("h4",{attrs:{id:"_4-3-per-tensor-scaling"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-per-tensor-scaling"}},[t._v("#")]),t._v(" "),e("strong",[t._v("4.3 Per-Tensor Scaling")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Fixed exponent bias fails")]),t._v("  when additional tensors (e.g., residuals) are stored in FP8.")]),t._v(" "),e("li",[e("strong",[t._v("Per-tensor scaling maintains accuracy")]),t._v(" , making FP8 viable for "),e("strong",[t._v("expanded use beyond GEMMs")]),t._v(" .")])]),t._v(" "),e("h3",{attrs:{id:"_5-conclusions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-conclusions"}},[t._v("#")]),t._v(" "),e("strong",[t._v("5. Conclusions")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("FP8 formats (E4M3, E5M2) efficiently reduce training and inference costs while maintaining accuracy")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("FP8 training is on par with FP16/BF16")]),t._v(" , without hyperparameter changes.")]),t._v(" "),e("li",[e("strong",[t._v("FP8 simplifies inference")]),t._v("  by eliminating the need for quantization-aware training (QAT) required for int8.")]),t._v(" "),e("li",[e("strong",[t._v("Future work")]),t._v(" : Expanding FP8 usage to "),e("strong",[t._v("more tensor types and operations")]),t._v("  beyond matrix multiplications.")])]),t._v(" "),e("h3",{attrs:{id:"key-takeaways-in-three-sentences-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#key-takeaways-in-three-sentences-2"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Key Takeaways in Three Sentences")])]),t._v(" "),e("p",[t._v("FP8 formats (E4M3 for weights/activations, E5M2 for gradients) "),e("strong",[t._v("significantly reduce computation and memory overhead")]),t._v("  while maintaining "),e("strong",[t._v("accuracy equivalent to FP16/BF16")]),t._v("  across CNNs, RNNs, and Transformer models.")]),t._v(" "),e("p",[e("strong",[t._v("Post-training quantization (PTQ) with FP8 outperforms int8")]),t._v(" , allowing for "),e("strong",[t._v("simpler and more effective deployment")]),t._v("  of trained models. The study "),e("strong",[t._v("validates FP8 training up to 175B parameters")]),t._v(" , proving its scalability for large-scale deep learning applications.")]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"y2025-paretoq-scaling-laws-in-extremely-low-bit-llm-quantization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#y2025-paretoq-scaling-laws-in-extremely-low-bit-llm-quantization"}},[t._v("#")]),t._v(" [Y2025] ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization")]),t._v(" "),e("h3",{attrs:{id:"abstract-3"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#abstract-3"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Abstract")])]),t._v(" "),e("p",[t._v("The study introduces "),e("strong",[t._v("ParetoQ")]),t._v(" , a unified framework for evaluating extremely low-bit quantization (1-bit to 4-bit) in large language models (LLMs)."),e("br"),t._v("\nIt identifies a "),e("strong",[t._v("learning transition")]),t._v("  between 2-bit and 3-bit models, with 3-bit and higher retaining pre-trained distributions, while 2-bit and below undergo drastic representation changes."),e("br"),t._v("\nBy optimizing training strategies and quantization functions, "),e("strong",[t._v("ParetoQ achieves state-of-the-art (SOTA) performance")]),t._v("  across multiple bit-widths."),e("br"),t._v("\nNotably, a "),e("strong",[t._v("ternary (1.58-bit) 600M model surpasses a previous 3B ternary model")]),t._v(" , using only one-fifth of the parameters."),e("br"),t._v("\nThe study finds that "),e("strong",[t._v("2-bit quantization offers a superior balance between accuracy, model size, and hardware efficiency")]),t._v("  compared to 4-bit and binary quantization.")]),t._v(" "),e("h3",{attrs:{id:"_1-introduction-3"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-introduction-3"}},[t._v("#")]),t._v(" "),e("strong",[t._v("1. Introduction")])]),t._v(" "),e("p",[t._v("As models scale, lower-precision computation is gaining traction due to "),e("strong",[t._v("memory savings and computational efficiency")]),t._v(" ."),e("br"),t._v("\nPrior studies suggest conflicting optimal quantization levels (1.58-bit, 2-bit, 4-bit, etc.), but "),e("strong",[t._v("no unified framework existed")]),t._v("  to systematically compare their effectiveness.")]),t._v(" "),e("h3",{attrs:{id:"key-questions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#key-questions"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Key Questions:")])]),t._v(" "),e("ul",[e("li",[t._v("What is the optimal trade-off between bit-width and model size?")]),t._v(" "),e("li",[t._v("How does quantization impact "),e("strong",[t._v("scaling laws")]),t._v("  in low-bit settings?")]),t._v(" "),e("li",[t._v("What training strategies and quantization functions yield "),e("strong",[t._v("Pareto-optimal results")]),t._v(" ?")])]),t._v(" "),e("h3",{attrs:{id:"paretoq-approach"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#paretoq-approach"}},[t._v("#")]),t._v(" "),e("strong",[t._v("ParetoQ Approach:")])]),t._v(" "),e("ul",[e("li",[t._v("Incorporates "),e("strong",[t._v("five key dimensions")]),t._v(" : model size ("),e("strong",[t._v("N")]),t._v(" ), token count ("),e("strong",[t._v("D")]),t._v(" ), quantization precision ("),e("strong",[t._v("P")]),t._v(" ), training strategy ("),e("strong",[t._v("S")]),t._v(" ), and quantization function ("),e("strong",[t._v("F")]),t._v(" ).")]),t._v(" "),e("li",[t._v("Identifies "),e("strong",[t._v("bit-specific training schemes and quantization functions")]),t._v(" .")]),t._v(" "),e("li",[t._v("Establishes that "),e("strong",[t._v("binary quantization significantly degrades accuracy")]),t._v(" , while "),e("strong",[t._v("ternary (1.58-bit), 2-bit, and 3-bit quantization match or exceed 4-bit performance")]),t._v(" .")])]),t._v(" "),e("h3",{attrs:{id:"_2-a-better-qat-scheduling-strategy-for-extreme-low-bit-llms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-a-better-qat-scheduling-strategy-for-extreme-low-bit-llms"}},[t._v("#")]),t._v(" "),e("strong",[t._v("2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMs")])]),t._v(" "),e("h4",{attrs:{id:"_2-1-training-budget-allocation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-training-budget-allocation"}},[t._v("#")]),t._v(" "),e("strong",[t._v("2.1 Training Budget Allocation")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Post-Training Quantization (PTQ)")]),t._v("  is easier to implement but "),e("strong",[t._v("performs poorly")]),t._v("  below 4-bit.")]),t._v(" "),e("li",[e("strong",[t._v("Quantization-Aware Training (QAT)")]),t._v("  integrates quantization during training, "),e("strong",[t._v("improving low-bit performance")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Optimal budget split:")]),t._v(" "),e("strong",[t._v("90% full-precision training, 10% QAT fine-tuning")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Finding-1:")]),t._v(" "),e("strong",[t._v("QAT fine-tuning outperforms both PTQ and QAT from scratch")]),t._v(" , achieving the best trade-off between accuracy and efficiency.")])]),t._v(" "),e("h4",{attrs:{id:"_2-2-fine-tuning-characteristics"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-fine-tuning-characteristics"}},[t._v("#")]),t._v(" "),e("strong",[t._v("2.2 Fine-tuning Characteristics")])]),t._v(" "),e("ul",[e("li",[t._v("Fine-tuning "),e("strong",[t._v("improves accuracy across all bit-widths")]),t._v(" , including binary and ternary models.")]),t._v(" "),e("li",[e("strong",[t._v("Lower-bit models (‚â§2-bit) require more fine-tuning (30B tokens), while 3-bit and 4-bit saturate at 10B tokens")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Finding-2:")]),t._v(" "),e("strong",[t._v("Bit-width transition effect:")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("3-bit & 4-bit recover near full precision with fine-tuning")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("1-bit to 2-bit undergo substantial weight transformations")]),t._v(" , requiring more tokens.")]),t._v(" "),e("li",[t._v('QAT serves as "compensation" for 3-bit+ but "reconstruction" for ‚â§2-bit models.')])])])]),t._v(" "),e("h3",{attrs:{id:"_3-a-hitchhiker-s-guide-to-quantization-method-choices"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-a-hitchhiker-s-guide-to-quantization-method-choices"}},[t._v("#")]),t._v(" "),e("strong",[t._v("3. A Hitchhiker‚Äôs Guide to Quantization Method Choices")])]),t._v(" "),e("h4",{attrs:{id:"_3-1-trade-offs-in-low-bit-quantization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-trade-offs-in-low-bit-quantization"}},[t._v("#")]),t._v(" "),e("strong",[t._v("3.1 Trade-offs in Low-bit Quantization")])]),t._v(" "),e("ol",[e("li",[e("strong",[t._v("Range Clipping")]),t._v(" : Lower-bit quantization suffers from outlier effects, requiring range clipping or "),e("strong",[t._v("learnable scales")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Quantization Grids")]),t._v(" :")])]),t._v(" "),e("ul",[e("li",[t._v("Binary & Ternary require balanced levels.")]),t._v(" "),e("li",[t._v("2-bit prefers symmetric distribution*.")]),t._v(" "),e("li",[t._v('3-bit and 4-bit benefit from including "0" in quantization levels.')])]),t._v(" "),e("h4",{attrs:{id:"_3-2-introducing-paretoq"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-introducing-paretoq"}},[t._v("#")]),t._v(" "),e("strong",[t._v("3.2 Introducing ParetoQ")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Combines the best quantization functions per bit-width")]),t._v(" :\n"),e("ul",[e("li",[e("strong",[t._v("1-bit")]),t._v(" : Elastic Binarization.")]),t._v(" "),e("li",[e("strong",[t._v("1.58-bit, 2-bit")]),t._v(" : "),e("strong",[t._v("Stretched Elastic Quant (SEQ)")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("3-bit, 4-bit")]),t._v(" : "),e("strong",[t._v("Learned Step Size Quantization (LSQ)")]),t._v(" .")])])]),t._v(" "),e("li",[e("strong",[t._v("Finding-3:")]),t._v("  No single best function for all bit-widths.\n"),e("strong",[t._v("Learnable range settings outperform fixed statistical methods")]),t._v(" , especially for "),e("strong",[t._v("sub-4-bit quantization")]),t._v(" .")])]),t._v(" "),e("h3",{attrs:{id:"_4-pareto-optimality-of-extremely-low-bit-llms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-pareto-optimality-of-extremely-low-bit-llms"}},[t._v("#")]),t._v(" "),e("strong",[t._v("4. Pareto-Optimality of Extremely Low-Bit LLMs")])]),t._v(" "),e("h4",{attrs:{id:"_4-1-accuracy-compression-trade-off"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-accuracy-compression-trade-off"}},[t._v("#")]),t._v(" "),e("strong",[t._v("4.1 Accuracy-Compression Trade-off")])]),t._v(" "),e("ul",[e("li",[t._v("Ternary (1.58-bit), 2-bit, and 3-bit outperform 4-bit in accuracy-size efficiency.")]),t._v(" "),e("li",[t._v("2-bit and ternary quantization sit on the Pareto frontier.")])]),t._v(" "),e("h4",{attrs:{id:"_4-2-hardware-constraints"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-hardware-constraints"}},[t._v("#")]),t._v(" "),e("strong",[t._v("4.2 Hardware Constraints")])]),t._v(" "),e("ul",[e("li",[t._v("Ternary (1.58-bit) appears efficient but is difficult to implement due to indexing overhead.")]),t._v(" "),e("li",[t._v("2-bit is more hardware-friendly**  due to **simpler storage and arithmetic operations.")])]),t._v(" "),e("h4",{attrs:{id:"_4-3-accuracy-speed-trade-off"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-accuracy-speed-trade-off"}},[t._v("#")]),t._v(" "),e("strong",[t._v("4.3 Accuracy-Speed Trade-off")])]),t._v(" "),e("ul",[e("li",[t._v("2-bit achieves higher speed at the same accuracy as 4-bit.")]),t._v(" "),e("li",[t._v("2-bit kernels are significantly faster than 4-bit kernels in large matrix multiplications.")])]),t._v(" "),e("h3",{attrs:{id:"_6-related-work"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_6-related-work"}},[t._v("#")]),t._v(" "),e("strong",[t._v("6. Related Work")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Early quantization research")]),t._v("  focused on "),e("strong",[t._v("8-bit and 4-bit LLMs")]),t._v(" .")]),t._v(" "),e("li",[t._v("Recent "),e("strong",[t._v("sub-4-bit research")]),t._v("  explored "),e("strong",[t._v("ternary, 2-bit, and 1-bit models")]),t._v(" , but lacked a "),e("strong",[t._v("unified comparison framework")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("ParetoQ is the first study to systematically compare sub-4-bit quantization schemes")]),t._v(" .")])]),t._v(" "),e("p",[e("strong",[t._v("7. Conclusions")])]),t._v(" "),e("ul",[e("li",[t._v("ParetoQ unifies training and quantization schemes across five bit-widths.")]),t._v(" "),e("li",[t._v("Ternary (1.58-bit), 2-bit, and 3-bit quantization outperform 4-bit in the accuracy-size trade-off.")]),t._v(" "),e("li",[t._v("2-bit is the most practical choice due to its hardware efficiency.")]),t._v(" "),e("li",[t._v("First framework that ensures fair comparisons across different quantization methods.")])]),t._v(" "),e("p",[e("strong",[t._v("Key Takeaways (3 Sentences)")])]),t._v(" "),e("ol",[e("li",[t._v("ParetoQ introduces a unified framework for extreme low-bit quantization, identifying 2-bit as the most efficient trade-off between accuracy, memory, and computational speed.")]),t._v(" "),e("li",[t._v("A clear transition exists between 2-bit and 3-bit models, where lower-bit quantization requires substantial fine-tuning to compensate for drastic weight transformations.")]),t._v(" "),e("li",[t._v("With its optimized quantization functions and training schemes, ParetoQ achieves state-of-the-art performance across all bit-widths, surpassing previous specialized methods.")])]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"_47-microscaling-data-formats-for-deep-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_47-microscaling-data-formats-for-deep-learning"}},[t._v("#")]),t._v(" [47] Microscaling Data Formats for Deep Learning")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/e1533c6c-54ca-47f8-946a-2d6fe7d08aef",alt:"image"}})]),t._v(" "),e("h3",{attrs:{id:"introduction-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#introduction-2"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Introduction")])]),t._v(" "),e("p",[t._v("The rapid advancement of deep learning models has led to increased computational and storage costs."),e("br"),t._v("\nOne approach to mitigating these costs is reducing bit-width precision in data formats, moving beyond traditional "),e("strong",[t._v("FP32")]),t._v("  to lower-bit formats such as "),e("strong",[t._v("FP16, BFloat16, FP8, and INT8")]),t._v("."),e("br"),t._v("\nHowever, per-tensor scaling in low-bit-width formats struggles with dynamic range limitations."),e("br"),t._v(" "),e("strong",[t._v("Microscaling (MX) data formats")]),t._v("  introduce "),e("strong",[t._v("per-block scaling factors")]),t._v("  to enhance efficiency, maintain accuracy, and ease deployment in AI hardware.")]),t._v(" "),e("h3",{attrs:{id:"microscaling-mx-data-formats"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#microscaling-mx-data-formats"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Microscaling (MX) Data Formats")])]),t._v(" "),e("p",[t._v("MX formats encode numerical values in "),e("strong",[t._v("fixed-size blocks")]),t._v(" , where each block consists of:")]),t._v(" "),e("ul",[e("li",[t._v("A "),e("strong",[t._v("shared scaling factor (X)")])]),t._v(" "),e("li",[t._v("Multiple "),e("strong",[t._v("narrow bit-width elements (Pi)")])])]),t._v(" "),e("p",[t._v("This approach "),e("strong",[t._v("extends the dynamic range")]),t._v("  beyond what per-tensor scaling allows, making sub-8-bit computations feasible."),e("br"),t._v("\nMX formats are "),e("strong",[t._v("hardware-efficient")]),t._v("  while minimizing accuracy loss and "),e("strong",[t._v("ensuring seamless adoption in existing AI frameworks")]),t._v(".")]),t._v(" "),e("h3",{attrs:{id:"concrete-mx-formats"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#concrete-mx-formats"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Concrete MX Formats")])]),t._v(" "),e("p",[t._v("MX formats are categorized based on "),e("strong",[t._v("block size, scale format, and element bit-width")]),t._v(".")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Format")]),t._v(" "),e("th",[t._v("Block Size")]),t._v(" "),e("th",[t._v("Scale Data Format")]),t._v(" "),e("th",[t._v("Scale Bits")]),t._v(" "),e("th",[t._v("Element Format")]),t._v(" "),e("th",[t._v("Element Bit-width")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("MXFP8")]),t._v(" "),e("td",[t._v("32")]),t._v(" "),e("td",[t._v("E8M0")]),t._v(" "),e("td",[t._v("8")]),t._v(" "),e("td",[t._v("FP8 (E4M3/E5M2)")]),t._v(" "),e("td",[t._v("8")])]),t._v(" "),e("tr",[e("td",[t._v("MXFP6")]),t._v(" "),e("td",[t._v("32")]),t._v(" "),e("td",[t._v("E8M0")]),t._v(" "),e("td",[t._v("8")]),t._v(" "),e("td",[t._v("FP6 (E2M3/E3M2)")]),t._v(" "),e("td",[t._v("6")])]),t._v(" "),e("tr",[e("td",[t._v("MXFP4")]),t._v(" "),e("td",[t._v("32")]),t._v(" "),e("td",[t._v("E8M0")]),t._v(" "),e("td",[t._v("8")]),t._v(" "),e("td",[t._v("FP4 (E2M1)")]),t._v(" "),e("td",[t._v("4")])]),t._v(" "),e("tr",[e("td",[t._v("MXINT8")]),t._v(" "),e("td",[t._v("32")]),t._v(" "),e("td",[t._v("E8M0")]),t._v(" "),e("td",[t._v("8")]),t._v(" "),e("td",[t._v("INT8")]),t._v(" "),e("td",[t._v("8")])])])]),t._v(" "),e("h3",{attrs:{id:"scalar-float-to-mx-format-conversion"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#scalar-float-to-mx-format-conversion"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Scalar Float to MX Format Conversion")])]),t._v(" "),e("p",[t._v("To convert floating-point data to an MX format, the "),e("strong",[t._v("shared scaling factor (X)")]),t._v("  is computed based on the largest absolute value in a block."),e("br"),t._v("\nEach element is then "),e("strong",[t._v("normalized using X and quantized")]),t._v("  to the desired format. The conversion follows a "),e("strong",[t._v("quantization algorithm")]),t._v("  that:")]),t._v(" "),e("ol",[e("li",[e("strong",[t._v("Determines the scaling exponent")]),t._v("  from the maximum value in the block.")]),t._v(" "),e("li",[e("strong",[t._v("Computes X as a power of two")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("Quantizes elements (Pi) based on X")]),t._v(" .")])]),t._v(" "),e("h3",{attrs:{id:"compute-flow-and-training-pipeline"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#compute-flow-and-training-pipeline"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Compute Flow and Training Pipeline")])]),t._v(" "),e("p",[t._v("For deep learning workloads, "),e("strong",[t._v("dot-product operations (e.g., matrix multiplication, convolutions) are performed in MX formats")]),t._v(" , while "),e("strong",[t._v("non-dot operations (e.g., activations, normalization, residual add) remain in higher-precision formats like FP32 or BFloat16")]),t._v(" ."),e("br"),t._v("\nTraining involves keeping a "),e("strong",[t._v("master FP32 copy of weights")]),t._v("  while performing compute-intensive operations in MX formats."),e("br"),t._v(" "),e("strong",[t._v("Quantization-aware fine-tuning")]),t._v("  is often required for best accuracy, particularly for lower-bit formats like MXFP6 and MXFP4.")]),t._v(" "),e("p",[e("strong",[t._v("Experimental Results")]),t._v(" "),e("strong",[t._v("Inference")]),t._v(" MX data formats were tested across "),e("strong",[t._v("language models, vision transformers, speech recognition, and recommendation models")]),t._v(".")]),t._v(" "),e("p",[e("strong",[t._v("Direct-Cast Inference (No Fine-Tuning)")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("MXINT8 performs nearly identically to FP32")]),t._v("  across all tasks, making it a "),e("strong",[t._v("drop-in replacement")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("MXFP8 and MXFP6 maintain good accuracy")]),t._v(" , but MXFP6 requires fine-tuning for best results.")]),t._v(" "),e("li",[e("strong",[t._v("MXFP4 suffers from significant accuracy loss")]),t._v(" , especially in complex models.")])]),t._v(" "),e("p",[e("strong",[t._v("Post-Training Quantization (PTQ) with Error Diffusion")])]),t._v(" "),e("ul",[e("li",[t._v("Error diffusion (similar to "),e("strong",[t._v("GPFQ-based post-training quantization")]),t._v(" ) helps recover accuracy.")]),t._v(" "),e("li",[e("strong",[t._v("MXFP6 achieves results close to FP32")]),t._v("  after error diffusion.")]),t._v(" "),e("li",[e("strong",[t._v("MXFP4 remains significantly worse than FP32, limiting its practical use in inference")]),t._v(" .")])]),t._v(" "),e("p",[e("strong",[t._v("Finetuned Inference")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("MXFP6 achieves FP32-level accuracy after fine-tuning")]),t._v(" .")]),t._v(" "),e("li",[e("strong",[t._v("MXFP4 improves slightly but still lags behind")]),t._v(" .")]),t._v(" "),e("li",[t._v("MXINT8 continues to serve as the most effective "),e("strong",[t._v("low-friction alternative to FP32")]),t._v(" .")])]),t._v(" "),e("p",[e("strong",[t._v("Generative Model Inference (GPT-3, LLaMA)")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("MXINT8 closely matches FP32 performance")]),t._v("  on GPT-3 and LLaMA.")]),t._v(" "),e("li",[e("strong",[t._v("MXFP6 and MXFP8 perform well in most tasks")]),t._v(" , but some degradation is observed in complex benchmarks.")]),t._v(" "),e("li",[e("strong",[t._v("MXFP4 shows noticeable loss")]),t._v(" , especially in "),e("strong",[t._v("zero-shot settings")]),t._v(" .")])]),t._v(" "),e("p",[e("strong",[t._v("Training with MX Formats")]),t._v("\nFor the "),e("strong",[t._v("first time, MX formats enable sub-8-bit training of large-scale transformers")]),t._v("  with minimal accuracy loss.")]),t._v(" "),e("p",[e("strong",[t._v("Training with MXFP6")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("MXFP6 (E3M2) trains models with no accuracy drop compared to FP32")]),t._v(" .")]),t._v(" "),e("li",[t._v("This represents the "),e("strong",[t._v("first demonstration of 6-bit training for large transformer models")]),t._v("  without modifications to the training recipe.")]),t._v(" "),e("li",[e("strong",[t._v("Hyperparameters remain unchanged from FP32 training")]),t._v(", making MXFP6 a practical choice.")])]),t._v(" "),e("p",[e("strong",[t._v("Training with MXFP4 + MXFP6")])]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("MXFP4 weights combined with MXFP6 activations/gradients")]),t._v("  yield slightly worse performance but remain viable for training.")]),t._v(" "),e("li",[e("strong",[t._v("Loss curves show only a minor increase in training loss")]),t._v(" , proving feasibility.")])]),t._v(" "),e("h3",{attrs:{id:"conclusion-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#conclusion-2"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Conclusion")])]),t._v(" "),e("p",[t._v("Microscaling (MX) data formats introduce "),e("strong",[t._v("per-block scaling")]),t._v("  to "),e("strong",[t._v("reduce bit-width")]),t._v("  while maintaining "),e("strong",[t._v("high accuracy, hardware efficiency, and seamless integration")]),t._v(" .")]),t._v(" "),e("h3",{attrs:{id:"key-findings"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#key-findings"}},[t._v("#")]),t._v(" Key findings:")]),t._v(" "),e("ol",[e("li",[t._v("MXINT8 is an effective drop-in replacement for FP32 inference.")]),t._v(" "),e("li",[t._v("MXFP6 enables sub-8-bit inference and training with minimal accuracy loss.")]),t._v(" "),e("li",[t._v("MXFP4 combined with MXFP6 remains viable for training but suffers in inference.")]),t._v(" "),e("li",[t._v("First-ever demonstration of training large generative models using 6-bit weights, activations, and gradients** .\nMX formats offer a compelling path toward "),e("strong",[t._v("lower precision deep learning")]),t._v("  without sacrificing model quality.")])]),t._v(" "),e("h3",{attrs:{id:"three-sentence-summary"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#three-sentence-summary"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Three-Sentence Summary")])]),t._v(" "),e("p",[t._v("Microscaling (MX) data formats introduce "),e("strong",[t._v("per-block scaling factors")]),t._v(" , improving the efficiency and accuracy of sub-8-bit deep learning computations. "),e("strong",[t._v("MXINT8 serves as a near-lossless drop-in replacement for FP32 inference")]),t._v(" , while "),e("strong",[t._v("MXFP6 enables large-scale deep learning models to be trained with sub-8-bit precision without altering training recipes")]),t._v(" . The results demonstrate that "),e("strong",[t._v("MX formats significantly reduce computational and storage costs while maintaining model performance")]),t._v(" , making them a strong alternative to traditional floating-point formats.")])])}),[],!1,null,null,null);e.default=n.exports}}]);