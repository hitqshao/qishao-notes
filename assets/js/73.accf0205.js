(window.webpackJsonp=window.webpackJsonp||[]).push([[73],{517:function(t,e,a){"use strict";a.r(e);var i=a(8),n=Object(i.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("ol",[e("li",[t._v("Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective")])]),t._v(" "),e("hr"),t._v(" "),e("h3",{attrs:{id:"_1-large-language-model-inference-acceleration-a-comprehensive-hardware-perspective"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-large-language-model-inference-acceleration-a-comprehensive-hardware-perspective"}},[t._v("#")]),t._v(" 1. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective")]),t._v(" "),e("h4",{attrs:{id:"optimizations-on-hardware-platforms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#optimizations-on-hardware-platforms"}},[t._v("#")]),t._v(" Optimizations on Hardware Platforms")]),t._v(" "),e("h5",{attrs:{id:"quantization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#quantization"}},[t._v("#")]),t._v(" Quantization")]),t._v(" "),e("p",[t._v("Data Format")]),t._v(" "),e("ul",[e("li",[t._v("Uniform Quantization")]),t._v(" "),e("li",[t._v("Non-uniform Quantization")])]),t._v(" "),e("p",[t._v("Granularity")]),t._v(" "),e("ul",[e("li",[t._v("group-wise: Group-wise granularity is a coarser approach where multiple channels or layers are quantized with the same parameters.")]),t._v(" "),e("li",[t._v("channel-wise: Channel-wise granularity involves quantizing each channel individually within the model.")]),t._v(" "),e("li",[t._v("tensor-wise: Tensor-wise granularity is the most detailed approach, where each tensor (such as weight tensors or activation tensors) is quantized separately.")])]),t._v(" "),e("p",[e("strong",[t._v("Weight-Only Quantization")])]),t._v(" "),e("p",[t._v("Uniform & Norn Uniform")]),t._v(" "),e("p",[t._v("Matrix decomposition quantization is a specialized method where a large matrix is approximated by the product of several smaller matrices.")]),t._v(" "),e("p",[e("strong",[t._v("Weight-Activation Quantization")])]),t._v(" "),e("p",[t._v("Quantization include the activations generated during model inference.")]),t._v(" "),e("p",[t._v("In this method, both the weights and the activations at each layer are quantized to lower precision formats.")]),t._v(" "),e("p",[t._v("This reduces memory bandwidth requirements and enhances inference speed.")]),t._v(" "),e("p",[t._v("The challenge with weight-activation quantization is to manage the trade-off between quantization errors and model accuracy.")]),t._v(" "),e("p",[t._v("Techniques such as dynamic range quantization or specific quantization schemes are used to balance precision and computational efficiency.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/c7bae637-2223-4d24-88de-c89d3252e34a",alt:"image"}})]),t._v(" "),e("h4",{attrs:{id:"weight-only-quantization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#weight-only-quantization"}},[t._v("#")]),t._v(" Weight-Only Quantization")]),t._v(" "),e("p",[t._v("Shen et al. [97] leverage Intel Neural Compressor to automate the INT4 quantization process with negligible accuracy loss, supporting various quantization recipes such as "),e("strong",[t._v("GPTQ")]),t._v(", "),e("strong",[t._v("AWQ")]),t._v(" and "),e("strong",[t._v("TEQ")]),t._v(".")]),t._v(" "),e("p",[t._v("Due to the overheads of weight dequantization from integer to floating, T-MAC leverages lookup tables (LUTs) for efficient low-bit LLM inference on edge CPUs, circumventing the need for dequantization and mixed precision matrix multiplication.")]),t._v(" "),e("p",[e("strong",[t._v("GPTQ")]),t._v("\nGPTQ is an one-shot weight quantization method based on approximate second-order information and error compensation, that is both highly-accurate and highly-efficient."),e("br"),t._v("\nIt can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3-bit or 4-bit per weight, with negligible accuracy degradation relative to the uncompressed baseline.")]),t._v(" "),e("p",[e("strong",[t._v("AWQ")]),t._v("\nAWQ is based on the observation that protecting 1% of salient weights whose activations are extremely large can greatly reduce quantization error."),e("br"),t._v("\nIt first searches for the optimal per-channel scaling and then multiplies the salient weights with the per-channel scalings."),e("br"),t._v("\nIt also reduces the bitwidth down to 3 or 4 bits per weight.")]),t._v(" "),e("p",[e("strong",[t._v("SpQR")]),t._v("\nTo further reduce the accuracy loss for smaller models in the 1-10B parameter range, SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision like half data type (16-bit), while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs.")]),t._v(" "),e("p",[e("strong",[t._v("SqueezeLLM")]),t._v("\nSqueezeLLM proposes a sensitivity-based non-uniform quantization method, which searches for the optimal bit precision assignment based on second-order information."),e("br"),t._v("\nIt also applies dense and sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format.")]),t._v(" "),e("p",[e("strong",[t._v("LLM-MQ")]),t._v("\nLLM-MQ proposes sensitivity-based precision allocation to assign the proper bitwidth for each layer within the given budget for weight memory based on their first-order information and quantization error.\nIt also develops an efficient CUDA core kernels to accelerate LLMs by fusing the dequantization and general matrix-vector multiplication (GEMV).")]),t._v(" "),e("p",[e("strong",[t._v("APTQ")]),t._v("\nAPTQ proposes an attention-aware 2/4-bit mixed-precision quantization for LLMs, which considers not only the second-order information of each layer’s weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model.")]),t._v(" "),e("p",[e("strong",[t._v("LUT-GEMM")]),t._v("\nLUT-GEMM proposes an efficient LUT-based GPU kernel for quantized matrix multiplication, which not only eliminates the resource-intensive dequantization process but also reduces computational costs compared to previous kernels for weight-only quantization.")]),t._v(" "),e("p",[e("strong",[t._v("FLUTE")]),t._v("\nFLUTE is a flexible lookup table engine for LUT-quantized LLMs, which uses offline restructuring of the quantized weight matrix to minimize bit manipulations associated with unpacking, and vectorization and duplication of the lookup table to mitigate shared memory bandwidth constraints.")]),t._v(" "),e("h4",{attrs:{id:"weight-activation-quantization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#weight-activation-quantization"}},[t._v("#")]),t._v(" Weight-Activation Quantization")]),t._v(" "),e("p",[t._v("In addition to hardware units that support FP16 computations, NVIDIA GPUs also provide hardware units that support INT4, INT8, and FP8 computations."),e("br"),t._v("\nThe number of these computation units can be 2× and 4× greater than FP16 on each chip."),e("br"),t._v("\nCompared to weight-only quantization, weight-activation quantization can utilize INT4, INT8, and FP8 computations, thereby maximizing the peak computational performance of the GPU.\\")]),t._v(" "),e("p",[e("strong",[t._v("Since the prefill phase in LLM inference is compute-bound, weight-activation quantization can significantly enhance performance during this stage.")])]),t._v(" "),e("p",[t._v("LLM.int8 uses vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features.")]),t._v(" "),e("p",[t._v("SmoothQuant enables 8-bit weight and 8-bit activation (W8A8) quantization for LLMs.")]),t._v(" "),e("p",[t._v("QUIK is for the first time, that the majority of inference computations for LLMs can be performed with both weights and activations being cast to 4 bits.")]),t._v(" "),e("p",[t._v("Prevalent quantization schemes (e.g., W8A8) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.")]),t._v(" "),e("p",[t._v("Atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.")]),t._v(" "),e("h4",{attrs:{id:"quantitative-comparison"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#quantitative-comparison"}},[t._v("#")]),t._v(" Quantitative Comparison")]),t._v(" "),e("p",[t._v("For CPUs, power consumption ranges from 3W to 385W, with inference speeds between 3 tokens/s and 50 tokens/s, located in the bottom part of the figure.")]),t._v(" "),e("p",[t._v("For GPUs, power consumption ranges from 40W to 450W, with inference speeds between 18 tokens/s and 194 tokens/s, situated in the upper right part of the figure.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/65194934-51e3-4bcc-a3ca-1c86fdd6ea23",alt:"image"}})]),t._v(" "),e("h4",{attrs:{id:"sparsity"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#sparsity"}},[t._v("#")]),t._v(" Sparsity")]),t._v(" "),e("p",[t._v("Sparsity patterns can be categorized into random and structured sparsity as shown in Figure 5.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/4cae57a4-8489-43c3-96f3-7f279f959787",alt:"image"}})]),t._v(" "),e("p",[t._v("Random pattern involves a random distribution of zero elements within the matrix, achieving higher accuracy but potentially lower speed for computation."),e("br"),t._v("\nStructured pattern applies a specific pattern to the sparsity, improving computational efficiency by aligning with hardware optimizations."),e("br"),t._v("\nWithin structured sparsity, common patterns include block-wise sparsity, N:M sparsity, channel-wise sparsity and some combinations of structured pattern sparsity.")]),t._v(" "),e("h4",{attrs:{id:"gpu"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gpu"}},[t._v("#")]),t._v(" GPU")]),t._v(" "),e("h5",{attrs:{id:"weight-sparsity"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#weight-sparsity"}},[t._v("#")]),t._v(" Weight Sparsity")]),t._v(" "),e("h5",{attrs:{id:"attention-sparsity"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#attention-sparsity"}},[t._v("#")]),t._v(" Attention Sparsity")]),t._v(" "),e("p",[e("strong",[t._v("Static Sparisty")])]),t._v(" "),e("ul",[e("li",[t._v("Sparse Transformer")]),t._v(" "),e("li",[t._v("StreamingLLM")]),t._v(" "),e("li",[t._v("Bigbird")]),t._v(" "),e("li",[t._v("Longformer")])]),t._v(" "),e("p",[t._v("Above schemes use the naumal combination of global and local patterns to replace the full attention patterns.")]),t._v(" "),e("p",[t._v("*"),e("em",[t._v("Dynamic Sparsity")])]),t._v(" "),e("ul",[e("li",[t._v("Adaptive Sparse Attention")]),t._v(" "),e("li",[t._v("Reformer")]),t._v(" "),e("li",[t._v("Sparse Flash Attention")]),t._v(" "),e("li",[t._v("Sparse Sinkhorn Attention")]),t._v(" "),e("li",[t._v("H2O Heavy Hitters")])])])}),[],!1,null,null,null);e.default=n.exports}}]);