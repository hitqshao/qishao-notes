(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(e){function n(n){for(var a,o,s=n[0],c=n[1],l=n[2],h=0,u=[];h<s.length;h++)o=s[h],Object.prototype.hasOwnProperty.call(i,o)&&i[o]&&u.push(i[o][0]),i[o]=0;for(a in c)Object.prototype.hasOwnProperty.call(c,a)&&(e[a]=c[a]);for(d&&d(n);u.length;)u.shift()();return r.push.apply(r,l||[]),t()}function t(){for(var e,n=0;n<r.length;n++){for(var t=r[n],a=!0,s=1;s<t.length;s++){var c=t[s];0!==i[c]&&(a=!1)}a&&(r.splice(n--,1),e=o(o.s=t[0]))}return e}var a={},i={1:0},r=[];function o(n){if(a[n])return a[n].exports;var t=a[n]={i:n,l:!1,exports:{}};return e[n].call(t.exports,t,t.exports,o),t.l=!0,t.exports}o.e=function(e){var n=[],t=i[e];if(0!==t)if(t)n.push(t[2]);else{var a=new Promise((function(n,a){t=i[e]=[n,a]}));n.push(t[2]=a);var r,s=document.createElement("script");s.charset="utf-8",s.timeout=120,o.nc&&s.setAttribute("nonce",o.nc),s.src=function(e){return o.p+"assets/js/"+({}[e]||e)+"."+{2:"75973713",3:"858ca26e",4:"424759af",5:"1d4e5a23",6:"938d7909",7:"ff277606",8:"9d9bbbd6",9:"3863430d",10:"ff60f5f4",11:"35356818",12:"97db4364",13:"117c9063",14:"d32097b1",15:"00157499",16:"1544c7aa",17:"be625961",18:"c1ff1604",19:"c41dccff",20:"51f7fa25",21:"b5ba1720",22:"aec55c0f",23:"00322b09",24:"ae95a93e",25:"e5add587",26:"9a920fc3",27:"e8d36ebd",28:"07f576d2",29:"2eae07d2",30:"7d0df21b",31:"0c998a26",32:"87750de9",33:"19f8db5c",34:"ffcf141e",35:"c89af1a2",36:"0db28e45",37:"dc096ab8",38:"2817b3b0",39:"3132cade",40:"a6a3704a",41:"f1295dc5",42:"34443c90",43:"bae3965b",44:"5d199f6c",45:"9425ffca",46:"2fd6db52",47:"58077ec3",48:"ff27c193",49:"a856fa03",50:"7ac4262b",51:"a8863a2a",52:"14ecc07a",53:"d19a80b0",54:"6c1cf823",55:"619bdce4",56:"f1bda6f9",57:"4666474f",58:"96826837",59:"6a407e53",60:"24fc7298",61:"708554c8",62:"7e9836c2",63:"d5776276",64:"a8e64870",65:"81cb45ba",66:"ca5f408a",67:"7a91b641",68:"6152cb0e"}[e]+".js"}(e);var c=new Error;r=function(n){s.onerror=s.onload=null,clearTimeout(l);var t=i[e];if(0!==t){if(t){var a=n&&("load"===n.type?"missing":n.type),r=n&&n.target&&n.target.src;c.message="Loading chunk "+e+" failed.\n("+a+": "+r+")",c.name="ChunkLoadError",c.type=a,c.request=r,t[1](c)}i[e]=void 0}};var l=setTimeout((function(){r({type:"timeout",target:s})}),12e4);s.onerror=s.onload=r,document.head.appendChild(s)}return Promise.all(n)},o.m=e,o.c=a,o.d=function(e,n,t){o.o(e,n)||Object.defineProperty(e,n,{enumerable:!0,get:t})},o.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},o.t=function(e,n){if(1&n&&(e=o(e)),8&n)return e;if(4&n&&"object"==typeof e&&e&&e.__esModule)return e;var t=Object.create(null);if(o.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:e}),2&n&&"string"!=typeof e)for(var a in e)o.d(t,a,function(n){return e[n]}.bind(null,a));return t},o.n=function(e){var n=e&&e.__esModule?function(){return e.default}:function(){return e};return o.d(n,"a",n),n},o.o=function(e,n){return Object.prototype.hasOwnProperty.call(e,n)},o.p="/qishao-notes/",o.oe=function(e){throw console.error(e),e};var s=window.webpackJsonp=window.webpackJsonp||[],c=s.push.bind(s);s.push=n,s=s.slice();for(var l=0;l<s.length;l++)n(s[l]);var d=c;r.push([130,0]),t()}([function(e,n,t){"use strict";var a=function(e){return e&&e.Math===Math&&e};e.exports=a("object"==typeof globalThis&&globalThis)||a("object"==typeof window&&window)||a("object"==typeof self&&self)||a("object"==typeof global&&global)||a("object"==typeof this&&this)||function(){return this}()||Function("return this")()},function(e,n,t){"use strict";var a=t(105),i=Object.prototype.toString;function r(e){return"[object Array]"===i.call(e)}function o(e){return void 0===e}function s(e){return null!==e&&"object"==typeof e}function c(e){if("[object Object]"!==i.call(e))return!1;var n=Object.getPrototypeOf(e);return null===n||n===Object.prototype}function l(e){return"[object Function]"===i.call(e)}function d(e,n){if(null!=e)if("object"!=typeof e&&(e=[e]),r(e))for(var t=0,a=e.length;t<a;t++)n.call(null,e[t],t,e);else for(var i in e)Object.prototype.hasOwnProperty.call(e,i)&&n.call(null,e[i],i,e)}e.exports={isArray:r,isArrayBuffer:function(e){return"[object ArrayBuffer]"===i.call(e)},isBuffer:function(e){return null!==e&&!o(e)&&null!==e.constructor&&!o(e.constructor)&&"function"==typeof e.constructor.isBuffer&&e.constructor.isBuffer(e)},isFormData:function(e){return"undefined"!=typeof FormData&&e instanceof FormData},isArrayBufferView:function(e){return"undefined"!=typeof ArrayBuffer&&ArrayBuffer.isView?ArrayBuffer.isView(e):e&&e.buffer&&e.buffer instanceof ArrayBuffer},isString:function(e){return"string"==typeof e},isNumber:function(e){return"number"==typeof e},isObject:s,isPlainObject:c,isUndefined:o,isDate:function(e){return"[object Date]"===i.call(e)},isFile:function(e){return"[object File]"===i.call(e)},isBlob:function(e){return"[object Blob]"===i.call(e)},isFunction:l,isStream:function(e){return s(e)&&l(e.pipe)},isURLSearchParams:function(e){return"undefined"!=typeof URLSearchParams&&e instanceof URLSearchParams},isStandardBrowserEnv:function(){return("undefined"==typeof navigator||"ReactNative"!==navigator.product&&"NativeScript"!==navigator.product&&"NS"!==navigator.product)&&("undefined"!=typeof window&&"undefined"!=typeof document)},forEach:d,merge:function e(){var n={};function t(t,a){c(n[a])&&c(t)?n[a]=e(n[a],t):c(t)?n[a]=e({},t):r(t)?n[a]=t.slice():n[a]=t}for(var a=0,i=arguments.length;a<i;a++)d(arguments[a],t);return n},extend:function(e,n,t){return d(n,(function(n,i){e[i]=t&&"function"==typeof n?a(n,t):n})),e},trim:function(e){return e.trim?e.trim():e.replace(/^\s+|\s+$/g,"")},stripBOM:function(e){return 65279===e.charCodeAt(0)&&(e=e.slice(1)),e}}},function(e,n,t){"use strict";var a="object"==typeof document&&document.all;e.exports=void 0===a&&void 0!==a?function(e){return"function"==typeof e||e===a}:function(e){return"function"==typeof e}},function(e,n,t){"use strict";var a=t(31),i=Function.prototype,r=i.call,o=a&&i.bind.bind(r,r);e.exports=a?o:function(e){return function(){return r.apply(e,arguments)}}},function(e,n,t){"use strict";e.exports=function(e){try{return!!e()}catch(e){return!0}}},function(e,n,t){"use strict";function a(e,n,t,a,i,r,o,s){var c,l="function"==typeof e?e.options:e;if(n&&(l.render=n,l.staticRenderFns=t,l._compiled=!0),a&&(l.functional=!0),r&&(l._scopeId="data-v-"+r),o?(c=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),i&&i.call(this,e),e&&e._registeredComponents&&e._registeredComponents.add(o)},l._ssrRegister=c):i&&(c=s?function(){i.call(this,(l.functional?this.parent:this).$root.$options.shadowRoot)}:i),c)if(l.functional){l._injectStyles=c;var d=l.render;l.render=function(e,n){return c.call(n),d(e,n)}}else{var h=l.beforeCreate;l.beforeCreate=h?[].concat(h,c):[c]}return{exports:e,options:l}}t.d(n,"a",(function(){return a}))},function(e,n,t){"use strict";var a=t(4);e.exports=!a((function(){return 7!==Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(e,n){var t=Array.isArray;e.exports=t},function(e,n,t){"use strict";var a=t(2);e.exports=function(e){return"object"==typeof e?null!==e:a(e)}},function(e,n,t){var a=t(79),i="object"==typeof self&&self&&self.Object===Object&&self,r=a||i||Function("return this")();e.exports=r},function(e,n,t){"use strict";var a=t(3),i=t(36),r=a({}.hasOwnProperty);e.exports=Object.hasOwn||function(e,n){return r(i(e),n)}},function(e,n,t){var a=t(189),i=t(192);e.exports=function(e,n){var t=i(e,n);return a(t)?t:void 0}},function(e,n,t){var a=t(292),i=t(103),r=/[T ]/,o=/:/,s=/^(\d{2})$/,c=[/^([+-]\d{2})$/,/^([+-]\d{3})$/,/^([+-]\d{4})$/],l=/^(\d{4})/,d=[/^([+-]\d{4})/,/^([+-]\d{5})/,/^([+-]\d{6})/],h=/^-(\d{2})$/,u=/^-?(\d{3})$/,p=/^-?(\d{2})-?(\d{2})$/,m=/^-?W(\d{2})$/,f=/^-?W(\d{2})-?(\d{1})$/,g=/^(\d{2}([.,]\d*)?)$/,y=/^(\d{2}):?(\d{2}([.,]\d*)?)$/,b=/^(\d{2}):?(\d{2}):?(\d{2}([.,]\d*)?)$/,v=/([Z+-].*)$/,w=/^(Z)$/,_=/^([+-])(\d{2})$/,k=/^([+-])(\d{2}):?(\d{2})$/;function x(e,n,t){n=n||0,t=t||0;var a=new Date(0);a.setUTCFullYear(e,0,4);var i=7*n+t+1-(a.getUTCDay()||7);return a.setUTCDate(a.getUTCDate()+i),a}e.exports=function(e,n){if(i(e))return new Date(e.getTime());if("string"!=typeof e)return new Date(e);var t=(n||{}).additionalDigits;t=null==t?2:Number(t);var C=function(e){var n,t={},a=e.split(r);o.test(a[0])?(t.date=null,n=a[0]):(t.date=a[0],n=a[1]);if(n){var i=v.exec(n);i?(t.time=n.replace(i[1],""),t.timezone=i[1]):t.time=n}return t}(e),P=function(e,n){var t,a=c[n],i=d[n];if(t=l.exec(e)||i.exec(e)){var r=t[1];return{year:parseInt(r,10),restDateString:e.slice(r.length)}}if(t=s.exec(e)||a.exec(e)){var o=t[1];return{year:100*parseInt(o,10),restDateString:e.slice(o.length)}}return{year:null}}(C.date,t),T=P.year,M=function(e,n){if(null===n)return null;var t,a,i,r;if(0===e.length)return(a=new Date(0)).setUTCFullYear(n),a;if(t=h.exec(e))return a=new Date(0),i=parseInt(t[1],10)-1,a.setUTCFullYear(n,i),a;if(t=u.exec(e)){a=new Date(0);var o=parseInt(t[1],10);return a.setUTCFullYear(n,0,o),a}if(t=p.exec(e)){a=new Date(0),i=parseInt(t[1],10)-1;var s=parseInt(t[2],10);return a.setUTCFullYear(n,i,s),a}if(t=m.exec(e))return r=parseInt(t[1],10)-1,x(n,r);if(t=f.exec(e)){r=parseInt(t[1],10)-1;var c=parseInt(t[2],10)-1;return x(n,r,c)}return null}(P.restDateString,T);if(M){var A,S=M.getTime(),I=0;if(C.time&&(I=function(e){var n,t,a;if(n=g.exec(e))return(t=parseFloat(n[1].replace(",",".")))%24*36e5;if(n=y.exec(e))return t=parseInt(n[1],10),a=parseFloat(n[2].replace(",",".")),t%24*36e5+6e4*a;if(n=b.exec(e)){t=parseInt(n[1],10),a=parseInt(n[2],10);var i=parseFloat(n[3].replace(",","."));return t%24*36e5+6e4*a+1e3*i}return null}(C.time)),C.timezone)A=6e4*function(e){var n,t;if(n=w.exec(e))return 0;if(n=_.exec(e))return t=60*parseInt(n[2],10),"+"===n[1]?-t:t;if(n=k.exec(e))return t=60*parseInt(n[2],10)+parseInt(n[3],10),"+"===n[1]?-t:t;return 0}(C.timezone);else{var L=S+I,D=new Date(L);A=a(D);var z=new Date(L);z.setDate(D.getDate()+1);var U=a(z)-a(D);U>0&&(A+=U)}return new Date(S+I+A)}return new Date(e)}},function(e,n,t){"use strict";t.d(n,"e",(function(){return a})),t.d(n,"b",(function(){return r})),t.d(n,"j",(function(){return o})),t.d(n,"g",(function(){return c})),t.d(n,"h",(function(){return l})),t.d(n,"i",(function(){return d})),t.d(n,"c",(function(){return h})),t.d(n,"f",(function(){return u})),t.d(n,"l",(function(){return p})),t.d(n,"m",(function(){return m})),t.d(n,"d",(function(){return g})),t.d(n,"k",(function(){return y})),t.d(n,"n",(function(){return b})),t.d(n,"a",(function(){return w}));t(29);const a=/#.*$/,i=/\.(md|html)$/,r=/\/$/,o=/^[a-z]+:/i;function s(e){return decodeURI(e).replace(a,"").replace(i,"")}function c(e){return o.test(e)}function l(e){return/^mailto:/.test(e)}function d(e){return/^tel:/.test(e)}function h(e){if(c(e))return e;if(!e)return"404";const n=e.match(a),t=n?n[0]:"",i=s(e);return r.test(i)?e:i+".html"+t}function u(e,n){const t=e.hash,i=function(e){const n=e&&e.match(a);if(n)return n[0]}(n);if(i&&t!==i)return!1;return s(e.path)===s(n)}function p(e,n,t){if(c(n))return{type:"external",path:n};t&&(n=function(e,n,t){const a=e.charAt(0);if("/"===a)return e;if("?"===a||"#"===a)return n+e;const i=n.split("/");t&&i[i.length-1]||i.pop();const r=e.replace(/^\//,"").split("/");for(let e=0;e<r.length;e++){const n=r[e];".."===n?i.pop():"."!==n&&i.push(n)}""!==i[0]&&i.unshift("");return i.join("/")}(n,t));const a=s(n);for(let n=0;n<e.length;n++)if(s(e[n].regularPath)===a)return Object.assign({},e[n],{type:"page",path:h(e[n].path)});return console.error(`[vuepress] No matching page found for sidebar item "${n}"`),{}}function m(e,n,t,a){const{pages:i,themeConfig:r}=t,o=a&&r.locales&&r.locales[a]||r;if("auto"===(e.frontmatter.sidebar||o.sidebar||r.sidebar))return f(e);const s=o.sidebar||r.sidebar;if(s){const{base:t,config:a}=function(e,n){if(Array.isArray(n))return{base:"/",config:n};for(const a in n)if(0===(t=e,/(\.html|\/)$/.test(t)?t:t+"/").indexOf(encodeURI(a)))return{base:a,config:n[a]};var t;return{}}(n,s);return"auto"===a?f(e):a?a.map(e=>function e(n,t,a,i=1){if("string"==typeof n)return p(t,n,a);if(Array.isArray(n))return Object.assign(p(t,n[0],a),{title:n[1]});{i>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const r=n.children||[];return 0===r.length&&n.path?Object.assign(p(t,n.path,a),{title:n.title}):{type:"group",path:n.path,title:n.title,sidebarDepth:n.sidebarDepth,initialOpenGroupIndex:n.initialOpenGroupIndex,children:r.map(n=>e(n,t,a,i+1)),collapsable:!1!==n.collapsable}}}(e,i,t)):[]}return[]}function f(e){const n=g(e.headers||[]);return[{type:"group",collapsable:!1,title:e.title,path:null,children:n.map(n=>({type:"auto",title:n.title,basePath:e.path,path:e.path+"#"+n.slug,children:n.children||[]}))}]}function g(e){let n;return(e=e.map(e=>Object.assign({},e))).forEach(e=>{2===e.level?n=e:n&&(n.children||(n.children=[])).push(e)}),e.filter(e=>2===e.level)}function y(e){return Object.assign(e,{type:e.items&&e.items.length?"links":"link"})}function b(e){return Object.prototype.toString.call(e).match(/\[object (.*?)\]/)[1].toLowerCase()}function v(e){let n=e.frontmatter.date||e.lastUpdated||new Date,t=new Date(n);return"Invalid Date"==t&&n&&(t=new Date(n.replace(/-/g,"/"))),t.getTime()}function w(e,n){return v(n)-v(e)}},function(e,n){e.exports=function(e){return null!=e&&"object"==typeof e}},function(e,n,t){"use strict";var a=t(273),i=t(274),r=t(275),o=t(276),s=t(100),c=t(20),l=t(277),d=Function,h=function(e){try{return d('"use strict"; return ('+e+").constructor;")()}catch(e){}},u=Object.getOwnPropertyDescriptor;if(u)try{u({},"")}catch(e){u=null}var p=function(){throw new c},m=u?function(){try{return p}catch(e){try{return u(arguments,"callee").get}catch(e){return p}}}():p,f=t(278)(),g=t(280)(),y=Object.getPrototypeOf||(g?function(e){return e.__proto__}:null),b={},v="undefined"!=typeof Uint8Array&&y?y(Uint8Array):void 0,w={__proto__:null,"%AggregateError%":"undefined"==typeof AggregateError?void 0:AggregateError,"%Array%":Array,"%ArrayBuffer%":"undefined"==typeof ArrayBuffer?void 0:ArrayBuffer,"%ArrayIteratorPrototype%":f&&y?y([][Symbol.iterator]()):void 0,"%AsyncFromSyncIteratorPrototype%":void 0,"%AsyncFunction%":b,"%AsyncGenerator%":b,"%AsyncGeneratorFunction%":b,"%AsyncIteratorPrototype%":b,"%Atomics%":"undefined"==typeof Atomics?void 0:Atomics,"%BigInt%":"undefined"==typeof BigInt?void 0:BigInt,"%BigInt64Array%":"undefined"==typeof BigInt64Array?void 0:BigInt64Array,"%BigUint64Array%":"undefined"==typeof BigUint64Array?void 0:BigUint64Array,"%Boolean%":Boolean,"%DataView%":"undefined"==typeof DataView?void 0:DataView,"%Date%":Date,"%decodeURI%":decodeURI,"%decodeURIComponent%":decodeURIComponent,"%encodeURI%":encodeURI,"%encodeURIComponent%":encodeURIComponent,"%Error%":a,"%eval%":eval,"%EvalError%":i,"%Float32Array%":"undefined"==typeof Float32Array?void 0:Float32Array,"%Float64Array%":"undefined"==typeof Float64Array?void 0:Float64Array,"%FinalizationRegistry%":"undefined"==typeof FinalizationRegistry?void 0:FinalizationRegistry,"%Function%":d,"%GeneratorFunction%":b,"%Int8Array%":"undefined"==typeof Int8Array?void 0:Int8Array,"%Int16Array%":"undefined"==typeof Int16Array?void 0:Int16Array,"%Int32Array%":"undefined"==typeof Int32Array?void 0:Int32Array,"%isFinite%":isFinite,"%isNaN%":isNaN,"%IteratorPrototype%":f&&y?y(y([][Symbol.iterator]())):void 0,"%JSON%":"object"==typeof JSON?JSON:void 0,"%Map%":"undefined"==typeof Map?void 0:Map,"%MapIteratorPrototype%":"undefined"!=typeof Map&&f&&y?y((new Map)[Symbol.iterator]()):void 0,"%Math%":Math,"%Number%":Number,"%Object%":Object,"%parseFloat%":parseFloat,"%parseInt%":parseInt,"%Promise%":"undefined"==typeof Promise?void 0:Promise,"%Proxy%":"undefined"==typeof Proxy?void 0:Proxy,"%RangeError%":r,"%ReferenceError%":o,"%Reflect%":"undefined"==typeof Reflect?void 0:Reflect,"%RegExp%":RegExp,"%Set%":"undefined"==typeof Set?void 0:Set,"%SetIteratorPrototype%":"undefined"!=typeof Set&&f&&y?y((new Set)[Symbol.iterator]()):void 0,"%SharedArrayBuffer%":"undefined"==typeof SharedArrayBuffer?void 0:SharedArrayBuffer,"%String%":String,"%StringIteratorPrototype%":f&&y?y(""[Symbol.iterator]()):void 0,"%Symbol%":f?Symbol:void 0,"%SyntaxError%":s,"%ThrowTypeError%":m,"%TypedArray%":v,"%TypeError%":c,"%Uint8Array%":"undefined"==typeof Uint8Array?void 0:Uint8Array,"%Uint8ClampedArray%":"undefined"==typeof Uint8ClampedArray?void 0:Uint8ClampedArray,"%Uint16Array%":"undefined"==typeof Uint16Array?void 0:Uint16Array,"%Uint32Array%":"undefined"==typeof Uint32Array?void 0:Uint32Array,"%URIError%":l,"%WeakMap%":"undefined"==typeof WeakMap?void 0:WeakMap,"%WeakRef%":"undefined"==typeof WeakRef?void 0:WeakRef,"%WeakSet%":"undefined"==typeof WeakSet?void 0:WeakSet};if(y)try{null.error}catch(e){var _=y(y(e));w["%Error.prototype%"]=_}var k={__proto__:null,"%ArrayBufferPrototype%":["ArrayBuffer","prototype"],"%ArrayPrototype%":["Array","prototype"],"%ArrayProto_entries%":["Array","prototype","entries"],"%ArrayProto_forEach%":["Array","prototype","forEach"],"%ArrayProto_keys%":["Array","prototype","keys"],"%ArrayProto_values%":["Array","prototype","values"],"%AsyncFunctionPrototype%":["AsyncFunction","prototype"],"%AsyncGenerator%":["AsyncGeneratorFunction","prototype"],"%AsyncGeneratorPrototype%":["AsyncGeneratorFunction","prototype","prototype"],"%BooleanPrototype%":["Boolean","prototype"],"%DataViewPrototype%":["DataView","prototype"],"%DatePrototype%":["Date","prototype"],"%ErrorPrototype%":["Error","prototype"],"%EvalErrorPrototype%":["EvalError","prototype"],"%Float32ArrayPrototype%":["Float32Array","prototype"],"%Float64ArrayPrototype%":["Float64Array","prototype"],"%FunctionPrototype%":["Function","prototype"],"%Generator%":["GeneratorFunction","prototype"],"%GeneratorPrototype%":["GeneratorFunction","prototype","prototype"],"%Int8ArrayPrototype%":["Int8Array","prototype"],"%Int16ArrayPrototype%":["Int16Array","prototype"],"%Int32ArrayPrototype%":["Int32Array","prototype"],"%JSONParse%":["JSON","parse"],"%JSONStringify%":["JSON","stringify"],"%MapPrototype%":["Map","prototype"],"%NumberPrototype%":["Number","prototype"],"%ObjectPrototype%":["Object","prototype"],"%ObjProto_toString%":["Object","prototype","toString"],"%ObjProto_valueOf%":["Object","prototype","valueOf"],"%PromisePrototype%":["Promise","prototype"],"%PromiseProto_then%":["Promise","prototype","then"],"%Promise_all%":["Promise","all"],"%Promise_reject%":["Promise","reject"],"%Promise_resolve%":["Promise","resolve"],"%RangeErrorPrototype%":["RangeError","prototype"],"%ReferenceErrorPrototype%":["ReferenceError","prototype"],"%RegExpPrototype%":["RegExp","prototype"],"%SetPrototype%":["Set","prototype"],"%SharedArrayBufferPrototype%":["SharedArrayBuffer","prototype"],"%StringPrototype%":["String","prototype"],"%SymbolPrototype%":["Symbol","prototype"],"%SyntaxErrorPrototype%":["SyntaxError","prototype"],"%TypedArrayPrototype%":["TypedArray","prototype"],"%TypeErrorPrototype%":["TypeError","prototype"],"%Uint8ArrayPrototype%":["Uint8Array","prototype"],"%Uint8ClampedArrayPrototype%":["Uint8ClampedArray","prototype"],"%Uint16ArrayPrototype%":["Uint16Array","prototype"],"%Uint32ArrayPrototype%":["Uint32Array","prototype"],"%URIErrorPrototype%":["URIError","prototype"],"%WeakMapPrototype%":["WeakMap","prototype"],"%WeakSetPrototype%":["WeakSet","prototype"]},x=t(52),C=t(282),P=x.call(Function.call,Array.prototype.concat),T=x.call(Function.apply,Array.prototype.splice),M=x.call(Function.call,String.prototype.replace),A=x.call(Function.call,String.prototype.slice),S=x.call(Function.call,RegExp.prototype.exec),I=/[^%.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|%$))/g,L=/\\(\\)?/g,D=function(e){var n=A(e,0,1),t=A(e,-1);if("%"===n&&"%"!==t)throw new s("invalid intrinsic syntax, expected closing `%`");if("%"===t&&"%"!==n)throw new s("invalid intrinsic syntax, expected opening `%`");var a=[];return M(e,I,(function(e,n,t,i){a[a.length]=t?M(i,L,"$1"):n||e})),a},z=function(e,n){var t,a=e;if(C(k,a)&&(a="%"+(t=k[a])[0]+"%"),C(w,a)){var i=w[a];if(i===b&&(i=function e(n){var t;if("%AsyncFunction%"===n)t=h("async function () {}");else if("%GeneratorFunction%"===n)t=h("function* () {}");else if("%AsyncGeneratorFunction%"===n)t=h("async function* () {}");else if("%AsyncGenerator%"===n){var a=e("%AsyncGeneratorFunction%");a&&(t=a.prototype)}else if("%AsyncIteratorPrototype%"===n){var i=e("%AsyncGenerator%");i&&y&&(t=y(i.prototype))}return w[n]=t,t}(a)),void 0===i&&!n)throw new c("intrinsic "+e+" exists, but is not available. Please file an issue!");return{alias:t,name:a,value:i}}throw new s("intrinsic "+e+" does not exist!")};e.exports=function(e,n){if("string"!=typeof e||0===e.length)throw new c("intrinsic name must be a non-empty string");if(arguments.length>1&&"boolean"!=typeof n)throw new c('"allowMissing" argument must be a boolean');if(null===S(/^%?[^%]*%?$/,e))throw new s("`%` may not be present anywhere but at the beginning and end of the intrinsic name");var t=D(e),a=t.length>0?t[0]:"",i=z("%"+a+"%",n),r=i.name,o=i.value,l=!1,d=i.alias;d&&(a=d[0],T(t,P([0,1],d)));for(var h=1,p=!0;h<t.length;h+=1){var m=t[h],f=A(m,0,1),g=A(m,-1);if(('"'===f||"'"===f||"`"===f||'"'===g||"'"===g||"`"===g)&&f!==g)throw new s("property names with quotes must have matching quotes");if("constructor"!==m&&p||(l=!0),C(w,r="%"+(a+="."+m)+"%"))o=w[r];else if(null!=o){if(!(m in o)){if(!n)throw new c("base intrinsic for "+e+" exists, but the property is not available.");return}if(u&&h+1>=t.length){var y=u(o,m);o=(p=!!y)&&"get"in y&&!("originalValue"in y.get)?y.get:o[m]}else p=C(o,m),o=o[m];p&&!l&&(w[r]=o)}}return o}},function(e,n,t){"use strict";var a=t(271),i=t(290),r=t(54);e.exports={formats:r,parse:i,stringify:a}},function(e,n,t){var a=t(19),i=t(174),r=t(175),o=a?a.toStringTag:void 0;e.exports=function(e){return null==e?void 0===e?"[object Undefined]":"[object Null]":o&&o in Object(e)?i(e):r(e)}},function(e,n,t){"use strict";var a=t(6),i=t(21),r=t(39);e.exports=a?function(e,n,t){return i.f(e,n,r(1,t))}:function(e,n,t){return e[n]=t,e}},function(e,n,t){var a=t(9).Symbol;e.exports=a},function(e,n,t){"use strict";e.exports=TypeError},function(e,n,t){"use strict";var a=t(6),i=t(74),r=t(125),o=t(57),s=t(64),c=TypeError,l=Object.defineProperty,d=Object.getOwnPropertyDescriptor;n.f=a?r?function(e,n,t){if(o(e),n=s(n),o(t),"function"==typeof e&&"prototype"===n&&"value"in t&&"writable"in t&&!t.writable){var a=d(e,n);a&&a.writable&&(e[n]=t.value,t={configurable:"configurable"in t?t.configurable:a.configurable,enumerable:"enumerable"in t?t.enumerable:a.enumerable,writable:!1})}return l(e,n,t)}:l:function(e,n,t){if(o(e),n=s(n),o(t),i)try{return l(e,n,t)}catch(e){}if("get"in t||"set"in t)throw new c("Accessors not supported");return"value"in t&&(e[n]=t.value),e}},function(e,n,t){"use strict";var a=t(3),i=a({}.toString),r=a("".slice);e.exports=function(e){return r(i(e),8,-1)}},function(e,n,t){var a=t(179),i=t(180),r=t(181),o=t(182),s=t(183);function c(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}c.prototype.clear=a,c.prototype.delete=i,c.prototype.get=r,c.prototype.has=o,c.prototype.set=s,e.exports=c},function(e,n,t){var a=t(81);e.exports=function(e,n){for(var t=e.length;t--;)if(a(e[t][0],n))return t;return-1}},function(e,n,t){var a=t(11)(Object,"create");e.exports=a},function(e,n,t){var a=t(201);e.exports=function(e,n){var t=e.__data__;return a(n)?t["string"==typeof n?"string":"hash"]:t.map}},function(e,n,t){var a=t(50);e.exports=function(e){if("string"==typeof e||a(e))return e;var n=e+"";return"0"==n&&1/e==-1/0?"-0":n}},function(e,n,t){var a,i;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(i="function"==typeof(a=function(){var e,n,t={version:"0.2.0"},a=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function i(e,n,t){return e<n?n:e>t?t:e}function r(e){return 100*(-1+e)}t.configure=function(e){var n,t;for(n in e)void 0!==(t=e[n])&&e.hasOwnProperty(n)&&(a[n]=t);return this},t.status=null,t.set=function(e){var n=t.isStarted();e=i(e,a.minimum,1),t.status=1===e?null:e;var c=t.render(!n),l=c.querySelector(a.barSelector),d=a.speed,h=a.easing;return c.offsetWidth,o((function(n){""===a.positionUsing&&(a.positionUsing=t.getPositioningCSS()),s(l,function(e,n,t){var i;return(i="translate3d"===a.positionUsing?{transform:"translate3d("+r(e)+"%,0,0)"}:"translate"===a.positionUsing?{transform:"translate("+r(e)+"%,0)"}:{"margin-left":r(e)+"%"}).transition="all "+n+"ms "+t,i}(e,d,h)),1===e?(s(c,{transition:"none",opacity:1}),c.offsetWidth,setTimeout((function(){s(c,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){t.remove(),n()}),d)}),d)):setTimeout(n,d)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var e=function(){setTimeout((function(){t.status&&(t.trickle(),e())}),a.trickleSpeed)};return a.trickle&&e(),this},t.done=function(e){return e||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(e){var n=t.status;return n?("number"!=typeof e&&(e=(1-n)*i(Math.random()*n,.1,.95)),n=i(n+e,0,.994),t.set(n)):t.start()},t.trickle=function(){return t.inc(Math.random()*a.trickleRate)},e=0,n=0,t.promise=function(a){return a&&"resolved"!==a.state()?(0===n&&t.start(),e++,n++,a.always((function(){0==--n?(e=0,t.done()):t.set((e-n)/e)})),this):this},t.render=function(e){if(t.isRendered())return document.getElementById("nprogress");l(document.documentElement,"nprogress-busy");var n=document.createElement("div");n.id="nprogress",n.innerHTML=a.template;var i,o=n.querySelector(a.barSelector),c=e?"-100":r(t.status||0),d=document.querySelector(a.parent);return s(o,{transition:"all 0 linear",transform:"translate3d("+c+"%,0,0)"}),a.showSpinner||(i=n.querySelector(a.spinnerSelector))&&u(i),d!=document.body&&l(d,"nprogress-custom-parent"),d.appendChild(n),n},t.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(a.parent),"nprogress-custom-parent");var e=document.getElementById("nprogress");e&&u(e)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var e=document.body.style,n="WebkitTransform"in e?"Webkit":"MozTransform"in e?"Moz":"msTransform"in e?"ms":"OTransform"in e?"O":"";return n+"Perspective"in e?"translate3d":n+"Transform"in e?"translate":"margin"};var o=function(){var e=[];function n(){var t=e.shift();t&&t(n)}return function(t){e.push(t),1==e.length&&n()}}(),s=function(){var e=["Webkit","O","Moz","ms"],n={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(e,n){return n.toUpperCase()})),n[t]||(n[t]=function(n){var t=document.body.style;if(n in t)return n;for(var a,i=e.length,r=n.charAt(0).toUpperCase()+n.slice(1);i--;)if((a=e[i]+r)in t)return a;return n}(t))}function a(e,n,a){n=t(n),e.style[n]=a}return function(e,n){var t,i,r=arguments;if(2==r.length)for(t in n)void 0!==(i=n[t])&&n.hasOwnProperty(t)&&a(e,t,i);else a(e,r[1],r[2])}}();function c(e,n){return("string"==typeof e?e:h(e)).indexOf(" "+n+" ")>=0}function l(e,n){var t=h(e),a=t+n;c(t,n)||(e.className=a.substring(1))}function d(e,n){var t,a=h(e);c(e,n)&&(t=a.replace(" "+n+" "," "),e.className=t.substring(1,t.length-1))}function h(e){return(" "+(e.className||"")+" ").replace(/\s+/gi," ")}function u(e){e&&e.parentNode&&e.parentNode.removeChild(e)}return t})?a.call(n,t,n,e):a)||(e.exports=i)},function(e,n,t){"use strict";var a=t(30),i=t(36),r=t(37),o=t(168),s=t(170);a({target:"Array",proto:!0,arity:1,forced:t(4)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(e){return e instanceof TypeError}}()},{push:function(e){var n=i(this),t=r(n),a=arguments.length;s(t+a);for(var c=0;c<a;c++)n[t]=arguments[c],t++;return o(n,t),t}})},function(e,n,t){"use strict";var a=t(0),i=t(62).f,r=t(18),o=t(121),s=t(42),c=t(75),l=t(148);e.exports=function(e,n){var t,d,h,u,p,m=e.target,f=e.global,g=e.stat;if(t=f?a:g?a[m]||s(m,{}):a[m]&&a[m].prototype)for(d in n){if(u=n[d],h=e.dontCallGetSet?(p=i(t,d))&&p.value:t[d],!l(f?d:m+(g?".":"#")+d,e.forced)&&void 0!==h){if(typeof u==typeof h)continue;c(u,h)}(e.sham||h&&h.sham)&&r(u,"sham",!0),o(t,d,u,e)}}},function(e,n,t){"use strict";var a=t(4);e.exports=!a((function(){var e=function(){}.bind();return"function"!=typeof e||e.hasOwnProperty("prototype")}))},function(e,n,t){"use strict";var a=t(58),i=t(40);e.exports=function(e){return a(i(e))}},function(e,n,t){"use strict";var a=t(0),i=t(2),r=function(e){return i(e)?e:void 0};e.exports=function(e,n){return arguments.length<2?r(a[e]):a[e]&&a[e][n]}},function(e,n,t){"use strict";var a=t(2),i=t(135),r=TypeError;e.exports=function(e){if(a(e))return e;throw new r(i(e)+" is not a function")}},function(e,n,t){"use strict";var a=t(0),i=t(71),r=t(10),o=t(73),s=t(68),c=t(67),l=a.Symbol,d=i("wks"),h=c?l.for||l:l&&l.withoutSetter||o;e.exports=function(e){return r(d,e)||(d[e]=s&&r(l,e)?l[e]:h("Symbol."+e)),d[e]}},function(e,n,t){"use strict";var a=t(40),i=Object;e.exports=function(e){return i(a(e))}},function(e,n,t){"use strict";var a=t(146);e.exports=function(e){return a(e.length)}},function(e,n,t){"use strict";var a=t(31),i=Function.prototype.call;e.exports=a?i.bind(i):function(){return i.apply(i,arguments)}},function(e,n,t){"use strict";e.exports=function(e,n){return{enumerable:!(1&e),configurable:!(2&e),writable:!(4&e),value:n}}},function(e,n,t){"use strict";var a=t(63),i=TypeError;e.exports=function(e){if(a(e))throw new i("Can't call method on "+e);return e}},function(e,n,t){"use strict";var a=t(72),i=t(0),r=t(42),o=e.exports=i["__core-js_shared__"]||r("__core-js_shared__",{});(o.versions||(o.versions=[])).push({version:"3.38.1",mode:a?"pure":"global",copyright:"© 2014-2024 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.38.1/LICENSE",source:"https://github.com/zloirock/core-js"})},function(e,n,t){"use strict";var a=t(0),i=Object.defineProperty;e.exports=function(e,n){try{i(a,e,{value:n,configurable:!0,writable:!0})}catch(t){a[e]=n}return n}},function(e,n,t){var a=t(173),i=t(14),r=Object.prototype,o=r.hasOwnProperty,s=r.propertyIsEnumerable,c=a(function(){return arguments}())?a:function(e){return i(e)&&o.call(e,"callee")&&!s.call(e,"callee")};e.exports=c},function(e,n,t){var a=t(11)(t(9),"Map");e.exports=a},function(e,n){e.exports=function(e){var n=typeof e;return null!=e&&("object"==n||"function"==n)}},function(e,n,t){var a=t(193),i=t(200),r=t(202),o=t(203),s=t(204);function c(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}c.prototype.clear=a,c.prototype.delete=i,c.prototype.get=r,c.prototype.has=o,c.prototype.set=s,e.exports=c},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e){t[++n]=e})),t}},function(e,n){e.exports=function(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=9007199254740991}},function(e,n,t){var a=t(7),i=t(50),r=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,o=/^\w*$/;e.exports=function(e,n){if(a(e))return!1;var t=typeof e;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=e&&!i(e))||(o.test(e)||!r.test(e)||null!=n&&e in Object(n))}},function(e,n,t){var a=t(17),i=t(14);e.exports=function(e){return"symbol"==typeof e||i(e)&&"[object Symbol]"==a(e)}},function(e,n){e.exports=function(e){return e}},function(e,n,t){"use strict";var a=t(281);e.exports=Function.prototype.bind||a},function(e,n,t){"use strict";var a=t(15)("%Object.defineProperty%",!0)||!1;if(a)try{a({},"a",{value:1})}catch(e){a=!1}e.exports=a},function(e,n,t){"use strict";var a=String.prototype.replace,i=/%20/g,r="RFC1738",o="RFC3986";e.exports={default:o,formatters:{RFC1738:function(e){return a.call(e,i,"+")},RFC3986:function(e){return String(e)}},RFC1738:r,RFC3986:o}},function(e,n,t){var a=t(297);e.exports=function(e){return a(e,{weekStartsOn:1})}},function(e,n,t){"use strict";var a=t(1),i=t(309),r=t(107),o={"Content-Type":"application/x-www-form-urlencoded"};function s(e,n){!a.isUndefined(e)&&a.isUndefined(e["Content-Type"])&&(e["Content-Type"]=n)}var c,l={transitional:{silentJSONParsing:!0,forcedJSONParsing:!0,clarifyTimeoutError:!1},adapter:(("undefined"!=typeof XMLHttpRequest||"undefined"!=typeof process&&"[object process]"===Object.prototype.toString.call(process))&&(c=t(108)),c),transformRequest:[function(e,n){return i(n,"Accept"),i(n,"Content-Type"),a.isFormData(e)||a.isArrayBuffer(e)||a.isBuffer(e)||a.isStream(e)||a.isFile(e)||a.isBlob(e)?e:a.isArrayBufferView(e)?e.buffer:a.isURLSearchParams(e)?(s(n,"application/x-www-form-urlencoded;charset=utf-8"),e.toString()):a.isObject(e)||n&&"application/json"===n["Content-Type"]?(s(n,"application/json"),function(e,n,t){if(a.isString(e))try{return(n||JSON.parse)(e),a.trim(e)}catch(e){if("SyntaxError"!==e.name)throw e}return(t||JSON.stringify)(e)}(e)):e}],transformResponse:[function(e){var n=this.transitional,t=n&&n.silentJSONParsing,i=n&&n.forcedJSONParsing,o=!t&&"json"===this.responseType;if(o||i&&a.isString(e)&&e.length)try{return JSON.parse(e)}catch(e){if(o){if("SyntaxError"===e.name)throw r(e,this,"E_JSON_PARSE");throw e}}return e}],timeout:0,xsrfCookieName:"XSRF-TOKEN",xsrfHeaderName:"X-XSRF-TOKEN",maxContentLength:-1,maxBodyLength:-1,validateStatus:function(e){return e>=200&&e<300}};l.headers={common:{Accept:"application/json, text/plain, */*"}},a.forEach(["delete","get","head"],(function(e){l.headers[e]={}})),a.forEach(["post","put","patch"],(function(e){l.headers[e]=a.merge(o)})),e.exports=l},function(e,n,t){"use strict";var a=t(8),i=String,r=TypeError;e.exports=function(e){if(a(e))return e;throw new r(i(e)+" is not an object")}},function(e,n,t){"use strict";var a=t(3),i=t(4),r=t(22),o=Object,s=a("".split);e.exports=i((function(){return!o("z").propertyIsEnumerable(0)}))?function(e){return"String"===r(e)?s(e,""):o(e)}:o},function(e,n,t){"use strict";e.exports={}},function(e,n){e.exports=function(e){return e.webpackPolyfill||(e.deprecate=function(){},e.paths=[],e.children||(e.children=[]),Object.defineProperty(e,"loaded",{enumerable:!0,get:function(){return e.l}}),Object.defineProperty(e,"id",{enumerable:!0,get:function(){return e.i}}),e.webpackPolyfill=1),e}},function(e,n){var t=/^\s+|\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,i=/^0b[01]+$/i,r=/^0o[0-7]+$/i,o=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,c="object"==typeof self&&self&&self.Object===Object&&self,l=s||c||Function("return this")(),d=Object.prototype.toString,h=Math.max,u=Math.min,p=function(){return l.Date.now()};function m(e){var n=typeof e;return!!e&&("object"==n||"function"==n)}function f(e){if("number"==typeof e)return e;if(function(e){return"symbol"==typeof e||function(e){return!!e&&"object"==typeof e}(e)&&"[object Symbol]"==d.call(e)}(e))return NaN;if(m(e)){var n="function"==typeof e.valueOf?e.valueOf():e;e=m(n)?n+"":n}if("string"!=typeof e)return 0===e?e:+e;e=e.replace(t,"");var s=i.test(e);return s||r.test(e)?o(e.slice(2),s?2:8):a.test(e)?NaN:+e}e.exports=function(e,n,t){var a,i,r,o,s,c,l=0,d=!1,g=!1,y=!0;if("function"!=typeof e)throw new TypeError("Expected a function");function b(n){var t=a,r=i;return a=i=void 0,l=n,o=e.apply(r,t)}function v(e){return l=e,s=setTimeout(_,n),d?b(e):o}function w(e){var t=e-c;return void 0===c||t>=n||t<0||g&&e-l>=r}function _(){var e=p();if(w(e))return k(e);s=setTimeout(_,function(e){var t=n-(e-c);return g?u(t,r-(e-l)):t}(e))}function k(e){return s=void 0,y&&a?b(e):(a=i=void 0,o)}function x(){var e=p(),t=w(e);if(a=arguments,i=this,c=e,t){if(void 0===s)return v(c);if(g)return s=setTimeout(_,n),b(c)}return void 0===s&&(s=setTimeout(_,n)),o}return n=f(n)||0,m(t)&&(d=!!t.leading,r=(g="maxWait"in t)?h(f(t.maxWait)||0,n):r,y="trailing"in t?!!t.trailing:y),x.cancel=function(){void 0!==s&&clearTimeout(s),l=0,a=c=i=s=void 0},x.flush=function(){return void 0===s?o:k(p())},x}},function(e,n,t){"use strict";var a=t(6),i=t(38),r=t(132),o=t(39),s=t(32),c=t(64),l=t(10),d=t(74),h=Object.getOwnPropertyDescriptor;n.f=a?h:function(e,n){if(e=s(e),n=c(n),d)try{return h(e,n)}catch(e){}if(l(e,n))return o(!i(r.f,e,n),e[n])}},function(e,n,t){"use strict";e.exports=function(e){return null==e}},function(e,n,t){"use strict";var a=t(133),i=t(65);e.exports=function(e){var n=a(e,"string");return i(n)?n:n+""}},function(e,n,t){"use strict";var a=t(33),i=t(2),r=t(66),o=t(67),s=Object;e.exports=o?function(e){return"symbol"==typeof e}:function(e){var n=a("Symbol");return i(n)&&r(n.prototype,s(e))}},function(e,n,t){"use strict";var a=t(3);e.exports=a({}.isPrototypeOf)},function(e,n,t){"use strict";var a=t(68);e.exports=a&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(e,n,t){"use strict";var a=t(69),i=t(4),r=t(0).String;e.exports=!!Object.getOwnPropertySymbols&&!i((function(){var e=Symbol("symbol detection");return!r(e)||!(Object(e)instanceof Symbol)||!Symbol.sham&&a&&a<41}))},function(e,n,t){"use strict";var a,i,r=t(0),o=t(70),s=r.process,c=r.Deno,l=s&&s.versions||c&&c.version,d=l&&l.v8;d&&(i=(a=d.split("."))[0]>0&&a[0]<4?1:+(a[0]+a[1])),!i&&o&&(!(a=o.match(/Edge\/(\d+)/))||a[1]>=74)&&(a=o.match(/Chrome\/(\d+)/))&&(i=+a[1]),e.exports=i},function(e,n,t){"use strict";var a=t(0).navigator,i=a&&a.userAgent;e.exports=i?String(i):""},function(e,n,t){"use strict";var a=t(41);e.exports=function(e,n){return a[e]||(a[e]=n||{})}},function(e,n,t){"use strict";e.exports=!1},function(e,n,t){"use strict";var a=t(3),i=0,r=Math.random(),o=a(1..toString);e.exports=function(e){return"Symbol("+(void 0===e?"":e)+")_"+o(++i+r,36)}},function(e,n,t){"use strict";var a=t(6),i=t(4),r=t(124);e.exports=!a&&!i((function(){return 7!==Object.defineProperty(r("div"),"a",{get:function(){return 7}}).a}))},function(e,n,t){"use strict";var a=t(10),i=t(141),r=t(62),o=t(21);e.exports=function(e,n,t){for(var s=i(n),c=o.f,l=r.f,d=0;d<s.length;d++){var h=s[d];a(e,h)||t&&a(t,h)||c(e,h,l(n,h))}}},function(e,n,t){"use strict";var a=t(145);e.exports=function(e){var n=+e;return n!=n||0===n?0:a(n)}},function(e,n,t){"use strict";var a=t(156),i=t(8),r=t(40),o=t(157);e.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var e,n=!1,t={};try{(e=a(Object.prototype,"__proto__","set"))(t,[]),n=t instanceof Array}catch(e){}return function(t,a){return r(t),o(a),i(t)?(n?e(t,a):t.__proto__=a,t):t}}():void 0)},function(e,n){e.exports=function(e,n){for(var t=-1,a=n.length,i=e.length;++t<a;)e[i+t]=n[t];return e}},function(e,n){var t="object"==typeof global&&global&&global.Object===Object&&global;e.exports=t},function(e,n,t){var a=t(23),i=t(184),r=t(185),o=t(186),s=t(187),c=t(188);function l(e){var n=this.__data__=new a(e);this.size=n.size}l.prototype.clear=i,l.prototype.delete=r,l.prototype.get=o,l.prototype.has=s,l.prototype.set=c,e.exports=l},function(e,n){e.exports=function(e,n){return e===n||e!=e&&n!=n}},function(e,n,t){var a=t(17),i=t(45);e.exports=function(e){if(!i(e))return!1;var n=a(e);return"[object Function]"==n||"[object GeneratorFunction]"==n||"[object AsyncFunction]"==n||"[object Proxy]"==n}},function(e,n){var t=Function.prototype.toString;e.exports=function(e){if(null!=e){try{return t.call(e)}catch(e){}try{return e+""}catch(e){}}return""}},function(e,n,t){var a=t(205),i=t(14);e.exports=function e(n,t,r,o,s){return n===t||(null==n||null==t||!i(n)&&!i(t)?n!=n&&t!=t:a(n,t,r,o,e,s))}},function(e,n,t){var a=t(86),i=t(208),r=t(87);e.exports=function(e,n,t,o,s,c){var l=1&t,d=e.length,h=n.length;if(d!=h&&!(l&&h>d))return!1;var u=c.get(e),p=c.get(n);if(u&&p)return u==n&&p==e;var m=-1,f=!0,g=2&t?new a:void 0;for(c.set(e,n),c.set(n,e);++m<d;){var y=e[m],b=n[m];if(o)var v=l?o(b,y,m,n,e,c):o(y,b,m,e,n,c);if(void 0!==v){if(v)continue;f=!1;break}if(g){if(!i(n,(function(e,n){if(!r(g,n)&&(y===e||s(y,e,t,o,c)))return g.push(n)}))){f=!1;break}}else if(y!==b&&!s(y,b,t,o,c)){f=!1;break}}return c.delete(e),c.delete(n),f}},function(e,n,t){var a=t(46),i=t(206),r=t(207);function o(e){var n=-1,t=null==e?0:e.length;for(this.__data__=new a;++n<t;)this.add(e[n])}o.prototype.add=o.prototype.push=i,o.prototype.has=r,e.exports=o},function(e,n){e.exports=function(e,n){return e.has(n)}},function(e,n,t){var a=t(218),i=t(224),r=t(92);e.exports=function(e){return r(e)?a(e):i(e)}},function(e,n,t){(function(e){var a=t(9),i=t(220),r=n&&!n.nodeType&&n,o=r&&"object"==typeof e&&e&&!e.nodeType&&e,s=o&&o.exports===r?a.Buffer:void 0,c=(s?s.isBuffer:void 0)||i;e.exports=c}).call(this,t(60)(e))},function(e,n){var t=/^(?:0|[1-9]\d*)$/;e.exports=function(e,n){var a=typeof e;return!!(n=null==n?9007199254740991:n)&&("number"==a||"symbol"!=a&&t.test(e))&&e>-1&&e%1==0&&e<n}},function(e,n,t){var a=t(221),i=t(222),r=t(223),o=r&&r.isTypedArray,s=o?i(o):a;e.exports=s},function(e,n,t){var a=t(82),i=t(48);e.exports=function(e){return null!=e&&i(e.length)&&!a(e)}},function(e,n,t){var a=t(11)(t(9),"Set");e.exports=a},function(e,n,t){var a=t(45);e.exports=function(e){return e==e&&!a(e)}},function(e,n){e.exports=function(e,n){return function(t){return null!=t&&(t[e]===n&&(void 0!==n||e in Object(t)))}}},function(e,n,t){var a=t(97),i=t(27);e.exports=function(e,n){for(var t=0,r=(n=a(n,e)).length;null!=e&&t<r;)e=e[i(n[t++])];return t&&t==r?e:void 0}},function(e,n,t){var a=t(7),i=t(49),r=t(235),o=t(238);e.exports=function(e,n){return a(e)?e:i(e,n)?[e]:r(o(e))}},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";e.exports=SyntaxError},function(e,n,t){"use strict";var a=t(15)("%Object.getOwnPropertyDescriptor%",!0);if(a)try{a([],"length")}catch(e){a=null}e.exports=a},function(e,n,t){"use strict";var a=t(54),i=Object.prototype.hasOwnProperty,r=Array.isArray,o=function(){for(var e=[],n=0;n<256;++n)e.push("%"+((n<16?"0":"")+n.toString(16)).toUpperCase());return e}(),s=function(e,n){for(var t=n&&n.plainObjects?Object.create(null):{},a=0;a<e.length;++a)void 0!==e[a]&&(t[a]=e[a]);return t};e.exports={arrayToObject:s,assign:function(e,n){return Object.keys(n).reduce((function(e,t){return e[t]=n[t],e}),e)},combine:function(e,n){return[].concat(e,n)},compact:function(e){for(var n=[{obj:{o:e},prop:"o"}],t=[],a=0;a<n.length;++a)for(var i=n[a],o=i.obj[i.prop],s=Object.keys(o),c=0;c<s.length;++c){var l=s[c],d=o[l];"object"==typeof d&&null!==d&&-1===t.indexOf(d)&&(n.push({obj:o,prop:l}),t.push(d))}return function(e){for(;e.length>1;){var n=e.pop(),t=n.obj[n.prop];if(r(t)){for(var a=[],i=0;i<t.length;++i)void 0!==t[i]&&a.push(t[i]);n.obj[n.prop]=a}}}(n),e},decode:function(e,n,t){var a=e.replace(/\+/g," ");if("iso-8859-1"===t)return a.replace(/%[0-9a-f]{2}/gi,unescape);try{return decodeURIComponent(a)}catch(e){return a}},encode:function(e,n,t,i,r){if(0===e.length)return e;var s=e;if("symbol"==typeof e?s=Symbol.prototype.toString.call(e):"string"!=typeof e&&(s=String(e)),"iso-8859-1"===t)return escape(s).replace(/%u[0-9a-f]{4}/gi,(function(e){return"%26%23"+parseInt(e.slice(2),16)+"%3B"}));for(var c="",l=0;l<s.length;l+=1024){for(var d=s.length>=1024?s.slice(l,l+1024):s,h=[],u=0;u<d.length;++u){var p=d.charCodeAt(u);45===p||46===p||95===p||126===p||p>=48&&p<=57||p>=65&&p<=90||p>=97&&p<=122||r===a.RFC1738&&(40===p||41===p)?h[h.length]=d.charAt(u):p<128?h[h.length]=o[p]:p<2048?h[h.length]=o[192|p>>6]+o[128|63&p]:p<55296||p>=57344?h[h.length]=o[224|p>>12]+o[128|p>>6&63]+o[128|63&p]:(u+=1,p=65536+((1023&p)<<10|1023&d.charCodeAt(u)),h[h.length]=o[240|p>>18]+o[128|p>>12&63]+o[128|p>>6&63]+o[128|63&p])}c+=h.join("")}return c},isBuffer:function(e){return!(!e||"object"!=typeof e)&&!!(e.constructor&&e.constructor.isBuffer&&e.constructor.isBuffer(e))},isRegExp:function(e){return"[object RegExp]"===Object.prototype.toString.call(e)},maybeMap:function(e,n){if(r(e)){for(var t=[],a=0;a<e.length;a+=1)t.push(n(e[a]));return t}return n(e)},merge:function e(n,t,a){if(!t)return n;if("object"!=typeof t){if(r(n))n.push(t);else{if(!n||"object"!=typeof n)return[n,t];(a&&(a.plainObjects||a.allowPrototypes)||!i.call(Object.prototype,t))&&(n[t]=!0)}return n}if(!n||"object"!=typeof n)return[n].concat(t);var o=n;return r(n)&&!r(t)&&(o=s(n,a)),r(n)&&r(t)?(t.forEach((function(t,r){if(i.call(n,r)){var o=n[r];o&&"object"==typeof o&&t&&"object"==typeof t?n[r]=e(o,t,a):n.push(t)}else n[r]=t})),n):Object.keys(t).reduce((function(n,r){var o=t[r];return i.call(n,r)?n[r]=e(n[r],o,a):n[r]=o,n}),o)}}},function(e,n){e.exports=function(e){return e instanceof Date}},function(e,n,t){var a=t(12),i=t(55);e.exports=function(e){var n=a(e),t=n.getFullYear(),r=new Date(0);r.setFullYear(t+1,0,4),r.setHours(0,0,0,0);var o=i(r),s=new Date(0);s.setFullYear(t,0,4),s.setHours(0,0,0,0);var c=i(s);return n.getTime()>=o.getTime()?t+1:n.getTime()>=c.getTime()?t:t-1}},function(e,n,t){"use strict";e.exports=function(e,n){return function(){for(var t=new Array(arguments.length),a=0;a<t.length;a++)t[a]=arguments[a];return e.apply(n,t)}}},function(e,n,t){"use strict";var a=t(1);function i(e){return encodeURIComponent(e).replace(/%3A/gi,":").replace(/%24/g,"$").replace(/%2C/gi,",").replace(/%20/g,"+").replace(/%5B/gi,"[").replace(/%5D/gi,"]")}e.exports=function(e,n,t){if(!n)return e;var r;if(t)r=t(n);else if(a.isURLSearchParams(n))r=n.toString();else{var o=[];a.forEach(n,(function(e,n){null!=e&&(a.isArray(e)?n+="[]":e=[e],a.forEach(e,(function(e){a.isDate(e)?e=e.toISOString():a.isObject(e)&&(e=JSON.stringify(e)),o.push(i(n)+"="+i(e))})))})),r=o.join("&")}if(r){var s=e.indexOf("#");-1!==s&&(e=e.slice(0,s)),e+=(-1===e.indexOf("?")?"?":"&")+r}return e}},function(e,n,t){"use strict";e.exports=function(e,n,t,a,i){return e.config=n,t&&(e.code=t),e.request=a,e.response=i,e.isAxiosError=!0,e.toJSON=function(){return{message:this.message,name:this.name,description:this.description,number:this.number,fileName:this.fileName,lineNumber:this.lineNumber,columnNumber:this.columnNumber,stack:this.stack,config:this.config,code:this.code}},e}},function(e,n,t){"use strict";var a=t(1),i=t(310),r=t(311),o=t(106),s=t(312),c=t(315),l=t(316),d=t(109);e.exports=function(e){return new Promise((function(n,t){var h=e.data,u=e.headers,p=e.responseType;a.isFormData(h)&&delete u["Content-Type"];var m=new XMLHttpRequest;if(e.auth){var f=e.auth.username||"",g=e.auth.password?unescape(encodeURIComponent(e.auth.password)):"";u.Authorization="Basic "+btoa(f+":"+g)}var y=s(e.baseURL,e.url);function b(){if(m){var a="getAllResponseHeaders"in m?c(m.getAllResponseHeaders()):null,r={data:p&&"text"!==p&&"json"!==p?m.response:m.responseText,status:m.status,statusText:m.statusText,headers:a,config:e,request:m};i(n,t,r),m=null}}if(m.open(e.method.toUpperCase(),o(y,e.params,e.paramsSerializer),!0),m.timeout=e.timeout,"onloadend"in m?m.onloadend=b:m.onreadystatechange=function(){m&&4===m.readyState&&(0!==m.status||m.responseURL&&0===m.responseURL.indexOf("file:"))&&setTimeout(b)},m.onabort=function(){m&&(t(d("Request aborted",e,"ECONNABORTED",m)),m=null)},m.onerror=function(){t(d("Network Error",e,null,m)),m=null},m.ontimeout=function(){var n="timeout of "+e.timeout+"ms exceeded";e.timeoutErrorMessage&&(n=e.timeoutErrorMessage),t(d(n,e,e.transitional&&e.transitional.clarifyTimeoutError?"ETIMEDOUT":"ECONNABORTED",m)),m=null},a.isStandardBrowserEnv()){var v=(e.withCredentials||l(y))&&e.xsrfCookieName?r.read(e.xsrfCookieName):void 0;v&&(u[e.xsrfHeaderName]=v)}"setRequestHeader"in m&&a.forEach(u,(function(e,n){void 0===h&&"content-type"===n.toLowerCase()?delete u[n]:m.setRequestHeader(n,e)})),a.isUndefined(e.withCredentials)||(m.withCredentials=!!e.withCredentials),p&&"json"!==p&&(m.responseType=e.responseType),"function"==typeof e.onDownloadProgress&&m.addEventListener("progress",e.onDownloadProgress),"function"==typeof e.onUploadProgress&&m.upload&&m.upload.addEventListener("progress",e.onUploadProgress),e.cancelToken&&e.cancelToken.promise.then((function(e){m&&(m.abort(),t(e),m=null)})),h||(h=null),m.send(h)}))}},function(e,n,t){"use strict";var a=t(107);e.exports=function(e,n,t,i,r){var o=new Error(e);return a(o,n,t,i,r)}},function(e,n,t){"use strict";e.exports=function(e){return!(!e||!e.__CANCEL__)}},function(e,n,t){"use strict";var a=t(1);e.exports=function(e,n){n=n||{};var t={},i=["url","method","data"],r=["headers","auth","proxy","params"],o=["baseURL","transformRequest","transformResponse","paramsSerializer","timeout","timeoutMessage","withCredentials","adapter","responseType","xsrfCookieName","xsrfHeaderName","onUploadProgress","onDownloadProgress","decompress","maxContentLength","maxBodyLength","maxRedirects","transport","httpAgent","httpsAgent","cancelToken","socketPath","responseEncoding"],s=["validateStatus"];function c(e,n){return a.isPlainObject(e)&&a.isPlainObject(n)?a.merge(e,n):a.isPlainObject(n)?a.merge({},n):a.isArray(n)?n.slice():n}function l(i){a.isUndefined(n[i])?a.isUndefined(e[i])||(t[i]=c(void 0,e[i])):t[i]=c(e[i],n[i])}a.forEach(i,(function(e){a.isUndefined(n[e])||(t[e]=c(void 0,n[e]))})),a.forEach(r,l),a.forEach(o,(function(i){a.isUndefined(n[i])?a.isUndefined(e[i])||(t[i]=c(void 0,e[i])):t[i]=c(void 0,n[i])})),a.forEach(s,(function(a){a in n?t[a]=c(e[a],n[a]):a in e&&(t[a]=c(void 0,e[a]))}));var d=i.concat(r).concat(o).concat(s),h=Object.keys(e).concat(Object.keys(n)).filter((function(e){return-1===d.indexOf(e)}));return a.forEach(h,l),t}},function(e,n,t){"use strict";function a(e){this.message=e}a.prototype.toString=function(){return"Cancel"+(this.message?": "+this.message:"")},a.prototype.__CANCEL__=!0,e.exports=a},function(e,n,t){},function(e,n,t){},function(e,n,t){var a=t(171),i=t(176),r=t(247),o=t(255),s=t(264),c=t(129),l=r((function(e){var n=c(e);return s(n)&&(n=void 0),o(a(e,1,s,!0),i(n,2))}));e.exports=l},function(e,n,t){var a=t(291),i=t(296),r=t(104),o=t(12),s=t(299),c=t(300);var l={M:function(e){return e.getMonth()+1},MM:function(e){return u(e.getMonth()+1,2)},Q:function(e){return Math.ceil((e.getMonth()+1)/3)},D:function(e){return e.getDate()},DD:function(e){return u(e.getDate(),2)},DDD:function(e){return a(e)},DDDD:function(e){return u(a(e),3)},d:function(e){return e.getDay()},E:function(e){return e.getDay()||7},W:function(e){return i(e)},WW:function(e){return u(i(e),2)},YY:function(e){return u(e.getFullYear(),4).substr(2)},YYYY:function(e){return u(e.getFullYear(),4)},GG:function(e){return String(r(e)).substr(2)},GGGG:function(e){return r(e)},H:function(e){return e.getHours()},HH:function(e){return u(e.getHours(),2)},h:function(e){var n=e.getHours();return 0===n?12:n>12?n%12:n},hh:function(e){return u(l.h(e),2)},m:function(e){return e.getMinutes()},mm:function(e){return u(e.getMinutes(),2)},s:function(e){return e.getSeconds()},ss:function(e){return u(e.getSeconds(),2)},S:function(e){return Math.floor(e.getMilliseconds()/100)},SS:function(e){return u(Math.floor(e.getMilliseconds()/10),2)},SSS:function(e){return u(e.getMilliseconds(),3)},Z:function(e){return h(e.getTimezoneOffset(),":")},ZZ:function(e){return h(e.getTimezoneOffset())},X:function(e){return Math.floor(e.getTime()/1e3)},x:function(e){return e.getTime()}};function d(e){return e.match(/\[[\s\S]/)?e.replace(/^\[|]$/g,""):e.replace(/\\/g,"")}function h(e,n){n=n||"";var t=e>0?"-":"+",a=Math.abs(e),i=a%60;return t+u(Math.floor(a/60),2)+n+u(i,2)}function u(e,n){for(var t=Math.abs(e).toString();t.length<n;)t="0"+t;return t}e.exports=function(e,n,t){var a=n?String(n):"YYYY-MM-DDTHH:mm:ss.SSSZ",i=(t||{}).locale,r=c.format.formatters,h=c.format.formattingTokensRegExp;i&&i.format&&i.format.formatters&&(r=i.format.formatters,i.format.formattingTokensRegExp&&(h=i.format.formattingTokensRegExp));var u=o(e);return s(u)?function(e,n,t){var a,i,r=e.match(t),o=r.length;for(a=0;a<o;a++)i=n[r[a]]||l[r[a]],r[a]=i||d(r[a]);return function(e){for(var n="",t=0;t<o;t++)r[t]instanceof Function?n+=r[t](e,l):n+=r[t];return n}}(a,r,h)(u):"Invalid Date"}},function(e,n,t){e.exports=t(304)},function(e,n,t){"use strict";
/**
 * @file Embedded JavaScript templating engine. {@link http://ejs.co}
 * @author Matthew Eernisse <mde@fleegix.org>
 * @author Tiancheng "Timothy" Gu <timothygu99@gmail.com>
 * @project EJS
 * @license {@link http://www.apache.org/licenses/LICENSE-2.0 Apache License, Version 2.0}
 */var a=t(322),i=t(323),r=t(324),o=!1,s=t(325).version,c=["delimiter","scope","context","debug","compileDebug","client","_with","rmWhitespace","strict","filename","async"],l=c.concat("cache"),d=/^\uFEFF/,h=/^[a-zA-Z_$][0-9a-zA-Z_$]*$/;function u(e,t){var i;if(t.some((function(t){return i=n.resolveInclude(e,t,!0),a.existsSync(i)})))return i}function p(e,t){var a,i=e.filename,r=arguments.length>1;if(e.cache){if(!i)throw new Error("cache option requires a filename");if(a=n.cache.get(i))return a;r||(t=f(i).toString().replace(d,""))}else if(!r){if(!i)throw new Error("Internal EJS error: no file name or template provided");t=f(i).toString().replace(d,"")}return a=n.compile(t,e),e.cache&&n.cache.set(i,a),a}function m(e,t,a){var i;if(!a){if("function"==typeof n.promiseImpl)return new n.promiseImpl((function(n,a){try{n(i=p(e)(t))}catch(e){a(e)}}));throw new Error("Please provide a callback function")}try{i=p(e)(t)}catch(e){return a(e)}a(null,i)}function f(e){return n.fileLoader(e)}function g(e,t){var i=r.shallowCopy(r.createNullProtoObjWherePossible(),t);if(i.filename=function(e,t){var i,r,o=t.views,s=/^[A-Za-z]+:\\|^\//.exec(e);if(s&&s.length)e=e.replace(/^\/*/,""),i=Array.isArray(t.root)?u(e,t.root):n.resolveInclude(e,t.root||"/",!0);else if(t.filename&&(r=n.resolveInclude(e,t.filename),a.existsSync(r)&&(i=r)),!i&&Array.isArray(o)&&(i=u(e,o)),!i&&"function"!=typeof t.includer)throw new Error('Could not find the include file "'+t.escapeFunction(e)+'"');return i}(e,i),"function"==typeof t.includer){var o=t.includer(e,i.filename);if(o&&(o.filename&&(i.filename=o.filename),o.template))return p(i,o.template)}return p(i)}function y(e,n,t,a,i){var r=n.split("\n"),o=Math.max(a-3,0),s=Math.min(r.length,a+3),c=i(t),l=r.slice(o,s).map((function(e,n){var t=n+o+1;return(t==a?" >> ":"    ")+t+"| "+e})).join("\n");throw e.path=c,e.message=(c||"ejs")+":"+a+"\n"+l+"\n\n"+e.message,e}function b(e){return e.replace(/;(\s*$)/,"$1")}function v(e,t){var a=r.hasOwnOnlyObject(t),i=r.createNullProtoObjWherePossible();this.templateText=e,this.mode=null,this.truncate=!1,this.currentLine=1,this.source="",i.client=a.client||!1,i.escapeFunction=a.escape||a.escapeFunction||r.escapeXML,i.compileDebug=!1!==a.compileDebug,i.debug=!!a.debug,i.filename=a.filename,i.openDelimiter=a.openDelimiter||n.openDelimiter||"<",i.closeDelimiter=a.closeDelimiter||n.closeDelimiter||">",i.delimiter=a.delimiter||n.delimiter||"%",i.strict=a.strict||!1,i.context=a.context,i.cache=a.cache||!1,i.rmWhitespace=a.rmWhitespace,i.root=a.root,i.includer=a.includer,i.outputFunctionName=a.outputFunctionName,i.localsName=a.localsName||n.localsName||"locals",i.views=a.views,i.async=a.async,i.destructuredLocals=a.destructuredLocals,i.legacyInclude=void 0===a.legacyInclude||!!a.legacyInclude,i.strict?i._with=!1:i._with=void 0===a._with||a._with,this.opts=i,this.regex=this.createRegex()}n.cache=r.cache,n.fileLoader=a.readFileSync,n.localsName="locals",n.promiseImpl=new Function("return this;")().Promise,n.resolveInclude=function(e,n,t){var a=i.dirname,r=i.extname,o=(0,i.resolve)(t?n:a(n),e);return r(e)||(o+=".ejs"),o},n.compile=function(e,n){return n&&n.scope&&(o||(console.warn("`scope` option is deprecated and will be removed in EJS 3"),o=!0),n.context||(n.context=n.scope),delete n.scope),new v(e,n).compile()},n.render=function(e,n,t){var a=n||r.createNullProtoObjWherePossible(),i=t||r.createNullProtoObjWherePossible();return 2==arguments.length&&r.shallowCopyFromList(i,a,c),p(i,e)(a)},n.renderFile=function(){var e,n,t,a=Array.prototype.slice.call(arguments),i=a.shift(),o={filename:i};return"function"==typeof arguments[arguments.length-1]&&(e=a.pop()),a.length?(n=a.shift(),a.length?r.shallowCopy(o,a.pop()):(n.settings&&(n.settings.views&&(o.views=n.settings.views),n.settings["view cache"]&&(o.cache=!0),(t=n.settings["view options"])&&r.shallowCopy(o,t)),r.shallowCopyFromList(o,n,l)),o.filename=i):n=r.createNullProtoObjWherePossible(),m(o,n,e)},n.Template=v,n.clearCache=function(){n.cache.reset()},v.modes={EVAL:"eval",ESCAPED:"escaped",RAW:"raw",COMMENT:"comment",LITERAL:"literal"},v.prototype={createRegex:function(){var e="(<%%|%%>|<%=|<%-|<%_|<%#|<%|%>|-%>|_%>)",n=r.escapeRegExpChars(this.opts.delimiter),t=r.escapeRegExpChars(this.opts.openDelimiter),a=r.escapeRegExpChars(this.opts.closeDelimiter);return e=e.replace(/%/g,n).replace(/</g,t).replace(/>/g,a),new RegExp(e)},compile:function(){var e,n,t,a=this.opts,o="",s="",c=a.escapeFunction,l=a.filename?JSON.stringify(a.filename):"undefined";if(!this.source){if(this.generateSource(),o+='  var __output = "";\n  function __append(s) { if (s !== undefined && s !== null) __output += s }\n',a.outputFunctionName){if(!h.test(a.outputFunctionName))throw new Error("outputFunctionName is not a valid JS identifier.");o+="  var "+a.outputFunctionName+" = __append;\n"}if(a.localsName&&!h.test(a.localsName))throw new Error("localsName is not a valid JS identifier.");if(a.destructuredLocals&&a.destructuredLocals.length){for(var d="  var __locals = ("+a.localsName+" || {}),\n",u=0;u<a.destructuredLocals.length;u++){var p=a.destructuredLocals[u];if(!h.test(p))throw new Error("destructuredLocals["+u+"] is not a valid JS identifier.");u>0&&(d+=",\n  "),d+=p+" = __locals."+p}o+=d+";\n"}!1!==a._with&&(o+="  with ("+a.localsName+" || {}) {\n",s+="  }\n"),s+="  return __output;\n",this.source=o+this.source+s}e=a.compileDebug?"var __line = 1\n  , __lines = "+JSON.stringify(this.templateText)+"\n  , __filename = "+l+";\ntry {\n"+this.source+"} catch (e) {\n  rethrow(e, __lines, __filename, __line, escapeFn);\n}\n":this.source,a.client&&(e="escapeFn = escapeFn || "+c.toString()+";\n"+e,a.compileDebug&&(e="rethrow = rethrow || "+y.toString()+";\n"+e)),a.strict&&(e='"use strict";\n'+e),a.debug&&console.log(e),a.compileDebug&&a.filename&&(e=e+"\n//# sourceURL="+l+"\n");try{if(a.async)try{t=new Function("return (async function(){}).constructor;")()}catch(e){throw e instanceof SyntaxError?new Error("This environment does not support async/await"):e}else t=Function;n=new t(a.localsName+", escapeFn, include, rethrow",e)}catch(e){throw e instanceof SyntaxError&&(a.filename&&(e.message+=" in "+a.filename),e.message+=" while compiling ejs\n\n",e.message+="If the above error is not helpful, you may want to try EJS-Lint:\n",e.message+="https://github.com/RyanZim/EJS-Lint",a.async||(e.message+="\n",e.message+="Or, if you meant to create an async function, pass `async: true` as an option.")),e}var m=a.client?n:function(e){return n.apply(a.context,[e||r.createNullProtoObjWherePossible(),c,function(n,t){var i=r.shallowCopy(r.createNullProtoObjWherePossible(),e);return t&&(i=r.shallowCopy(i,t)),g(n,a)(i)},y])};if(a.filename&&"function"==typeof Object.defineProperty){var f=a.filename,b=i.basename(f,i.extname(f));try{Object.defineProperty(m,"name",{value:b,writable:!1,enumerable:!1,configurable:!0})}catch(e){}}return m},generateSource:function(){this.opts.rmWhitespace&&(this.templateText=this.templateText.replace(/[\r\n]+/g,"\n").replace(/^\s+|\s+$/gm,"")),this.templateText=this.templateText.replace(/[ \t]*<%_/gm,"<%_").replace(/_%>[ \t]*/gm,"_%>");var e=this,n=this.parseTemplateText(),t=this.opts.delimiter,a=this.opts.openDelimiter,i=this.opts.closeDelimiter;n&&n.length&&n.forEach((function(r,o){var s;if(0===r.indexOf(a+t)&&0!==r.indexOf(a+t+t)&&(s=n[o+2])!=t+i&&s!="-"+t+i&&s!="_"+t+i)throw new Error('Could not find matching close tag for "'+r+'".');e.scanLine(r)}))},parseTemplateText:function(){for(var e,n=this.templateText,t=this.regex,a=t.exec(n),i=[];a;)0!==(e=a.index)&&(i.push(n.substring(0,e)),n=n.slice(e)),i.push(a[0]),n=n.slice(a[0].length),a=t.exec(n);return n&&i.push(n),i},_addOutput:function(e){if(this.truncate&&(e=e.replace(/^(?:\r\n|\r|\n)/,""),this.truncate=!1),!e)return e;e=(e=(e=(e=e.replace(/\\/g,"\\\\")).replace(/\n/g,"\\n")).replace(/\r/g,"\\r")).replace(/"/g,'\\"'),this.source+='    ; __append("'+e+'")\n'},scanLine:function(e){var n,t=this.opts.delimiter,a=this.opts.openDelimiter,i=this.opts.closeDelimiter;switch(n=e.split("\n").length-1,e){case a+t:case a+t+"_":this.mode=v.modes.EVAL;break;case a+t+"=":this.mode=v.modes.ESCAPED;break;case a+t+"-":this.mode=v.modes.RAW;break;case a+t+"#":this.mode=v.modes.COMMENT;break;case a+t+t:this.mode=v.modes.LITERAL,this.source+='    ; __append("'+e.replace(a+t+t,a+t)+'")\n';break;case t+t+i:this.mode=v.modes.LITERAL,this.source+='    ; __append("'+e.replace(t+t+i,t+i)+'")\n';break;case t+i:case"-"+t+i:case"_"+t+i:this.mode==v.modes.LITERAL&&this._addOutput(e),this.mode=null,this.truncate=0===e.indexOf("-")||0===e.indexOf("_");break;default:if(this.mode){switch(this.mode){case v.modes.EVAL:case v.modes.ESCAPED:case v.modes.RAW:e.lastIndexOf("//")>e.lastIndexOf("\n")&&(e+="\n")}switch(this.mode){case v.modes.EVAL:this.source+="    ; "+e+"\n";break;case v.modes.ESCAPED:this.source+="    ; __append(escapeFn("+b(e)+"))\n";break;case v.modes.RAW:this.source+="    ; __append("+b(e)+")\n";break;case v.modes.COMMENT:break;case v.modes.LITERAL:this._addOutput(e)}}else this._addOutput(e)}this.opts.compileDebug&&n&&(this.currentLine+=n,this.source+="    ; __line = "+this.currentLine+"\n")}},n.escapeXML=r.escapeXML,n.__express=n.renderFile,n.VERSION=s,n.name="ejs","undefined"!=typeof window&&(window.ejs=n)},function(e,n,t){"use strict";t.r(n);var a={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},i=(t(267),t(5)),r=Object(i.a)(a,(function(){return(0,this._self._c)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);n.default=r.exports},function(e,n,t){"use strict";t.r(n);var a={name:"CodeGroup",data:()=>({codeTabs:[],activeCodeTabIndex:-1}),watch:{activeCodeTabIndex(e){this.codeTabs.forEach(e=>{e.elm.classList.remove("theme-code-block__active")}),this.codeTabs[e].elm.classList.add("theme-code-block__active")}},mounted(){this.codeTabs=(this.$slots.default||[]).filter(e=>Boolean(e.componentOptions)).map((e,n)=>(""===e.componentOptions.propsData.active&&(this.activeCodeTabIndex=n),{title:e.componentOptions.propsData.title,elm:e.elm})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab(e){this.activeCodeTabIndex=e}}},i=(t(268),t(5)),r=Object(i.a)(a,(function(){var e=this,n=e._self._c;return n("div",{staticClass:"theme-code-group"},[n("div",{staticClass:"theme-code-group__nav"},[n("ul",{staticClass:"theme-code-group__ul"},e._l(e.codeTabs,(function(t,a){return n("li",{key:t.title,staticClass:"theme-code-group__li"},[n("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":a===e.activeCodeTabIndex},on:{click:function(n){return e.changeCodeTab(a)}}},[e._v("\n            "+e._s(t.title)+"\n          ")])])})),0)]),e._v(" "),e._t("default"),e._v(" "),e.codeTabs.length<1?n("pre",{staticClass:"pre-blank"},[e._v("// Make sure to add code blocks to your code group")]):e._e()],2)}),[],!1,null,"2f5f1757",null);n.default=r.exports},function(e,n,t){"use strict";var a=t(2),i=t(21),r=t(126),o=t(42);e.exports=function(e,n,t,s){s||(s={});var c=s.enumerable,l=void 0!==s.name?s.name:n;if(a(t)&&r(t,l,s),s.global)c?e[n]=t:o(n,t);else{try{s.unsafe?e[n]&&(c=!0):delete e[n]}catch(e){}c?e[n]=t:i.f(e,n,{value:t,enumerable:!1,configurable:!s.nonConfigurable,writable:!s.nonWritable})}return e}},function(e,n,t){"use strict";e.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(e,n,t){"use strict";var a=t(162),i=String;e.exports=function(e){if("Symbol"===a(e))throw new TypeError("Cannot convert a Symbol value to a string");return i(e)}},function(e,n,t){"use strict";var a=t(0),i=t(8),r=a.document,o=i(r)&&i(r.createElement);e.exports=function(e){return o?r.createElement(e):{}}},function(e,n,t){"use strict";var a=t(6),i=t(4);e.exports=a&&i((function(){return 42!==Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(e,n,t){"use strict";var a=t(3),i=t(4),r=t(2),o=t(10),s=t(6),c=t(137).CONFIGURABLE,l=t(138),d=t(139),h=d.enforce,u=d.get,p=String,m=Object.defineProperty,f=a("".slice),g=a("".replace),y=a([].join),b=s&&!i((function(){return 8!==m((function(){}),"length",{value:8}).length})),v=String(String).split("String"),w=e.exports=function(e,n,t){"Symbol("===f(p(n),0,7)&&(n="["+g(p(n),/^Symbol\(([^)]*)\).*$/,"$1")+"]"),t&&t.getter&&(n="get "+n),t&&t.setter&&(n="set "+n),(!o(e,"name")||c&&e.name!==n)&&(s?m(e,"name",{value:n,configurable:!0}):e.name=n),b&&t&&o(t,"arity")&&e.length!==t.arity&&m(e,"length",{value:t.arity});try{t&&o(t,"constructor")&&t.constructor?s&&m(e,"prototype",{writable:!1}):e.prototype&&(e.prototype=void 0)}catch(e){}var a=h(e);return o(a,"source")||(a.source=y(v,"string"==typeof n?n:"")),e};Function.prototype.toString=w((function(){return r(this)&&u(this).source||l(this)}),"toString")},function(e,n,t){"use strict";var a=t(71),i=t(73),r=a("keys");e.exports=function(e){return r[e]||(r[e]=i(e))}},function(e,n,t){"use strict";var a=t(3),i=t(10),r=t(32),o=t(143).indexOf,s=t(59),c=a([].push);e.exports=function(e,n){var t,a=r(e),l=0,d=[];for(t in a)!i(s,t)&&i(a,t)&&c(d,t);for(;n.length>l;)i(a,t=n[l++])&&(~o(d,t)||c(d,t));return d}},function(e,n){e.exports=function(e){var n=null==e?0:e.length;return n?e[n-1]:void 0}},function(e,n,t){e.exports=t(329)},function(e,n,t){"use strict";var a=t(30),i=t(149).left,r=t(150),o=t(69);a({target:"Array",proto:!0,forced:!t(151)&&o>79&&o<83||!r("reduce")},{reduce:function(e){var n=arguments.length;return i(this,e,n,n>1?arguments[1]:void 0)}})},function(e,n,t){"use strict";var a={}.propertyIsEnumerable,i=Object.getOwnPropertyDescriptor,r=i&&!a.call({1:2},1);n.f=r?function(e){var n=i(this,e);return!!n&&n.enumerable}:a},function(e,n,t){"use strict";var a=t(38),i=t(8),r=t(65),o=t(134),s=t(136),c=t(35),l=TypeError,d=c("toPrimitive");e.exports=function(e,n){if(!i(e)||r(e))return e;var t,c=o(e,d);if(c){if(void 0===n&&(n="default"),t=a(c,e,n),!i(t)||r(t))return t;throw new l("Can't convert object to primitive value")}return void 0===n&&(n="number"),s(e,n)}},function(e,n,t){"use strict";var a=t(34),i=t(63);e.exports=function(e,n){var t=e[n];return i(t)?void 0:a(t)}},function(e,n,t){"use strict";var a=String;e.exports=function(e){try{return a(e)}catch(e){return"Object"}}},function(e,n,t){"use strict";var a=t(38),i=t(2),r=t(8),o=TypeError;e.exports=function(e,n){var t,s;if("string"===n&&i(t=e.toString)&&!r(s=a(t,e)))return s;if(i(t=e.valueOf)&&!r(s=a(t,e)))return s;if("string"!==n&&i(t=e.toString)&&!r(s=a(t,e)))return s;throw new o("Can't convert object to primitive value")}},function(e,n,t){"use strict";var a=t(6),i=t(10),r=Function.prototype,o=a&&Object.getOwnPropertyDescriptor,s=i(r,"name"),c=s&&"something"===function(){}.name,l=s&&(!a||a&&o(r,"name").configurable);e.exports={EXISTS:s,PROPER:c,CONFIGURABLE:l}},function(e,n,t){"use strict";var a=t(3),i=t(2),r=t(41),o=a(Function.toString);i(r.inspectSource)||(r.inspectSource=function(e){return o(e)}),e.exports=r.inspectSource},function(e,n,t){"use strict";var a,i,r,o=t(140),s=t(0),c=t(8),l=t(18),d=t(10),h=t(41),u=t(127),p=t(59),m=s.TypeError,f=s.WeakMap;if(o||h.state){var g=h.state||(h.state=new f);g.get=g.get,g.has=g.has,g.set=g.set,a=function(e,n){if(g.has(e))throw new m("Object already initialized");return n.facade=e,g.set(e,n),n},i=function(e){return g.get(e)||{}},r=function(e){return g.has(e)}}else{var y=u("state");p[y]=!0,a=function(e,n){if(d(e,y))throw new m("Object already initialized");return n.facade=e,l(e,y,n),n},i=function(e){return d(e,y)?e[y]:{}},r=function(e){return d(e,y)}}e.exports={set:a,get:i,has:r,enforce:function(e){return r(e)?i(e):a(e,{})},getterFor:function(e){return function(n){var t;if(!c(n)||(t=i(n)).type!==e)throw new m("Incompatible receiver, "+e+" required");return t}}}},function(e,n,t){"use strict";var a=t(0),i=t(2),r=a.WeakMap;e.exports=i(r)&&/native code/.test(String(r))},function(e,n,t){"use strict";var a=t(33),i=t(3),r=t(142),o=t(147),s=t(57),c=i([].concat);e.exports=a("Reflect","ownKeys")||function(e){var n=r.f(s(e)),t=o.f;return t?c(n,t(e)):n}},function(e,n,t){"use strict";var a=t(128),i=t(122).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(e){return a(e,i)}},function(e,n,t){"use strict";var a=t(32),i=t(144),r=t(37),o=function(e){return function(n,t,o){var s=a(n),c=r(s);if(0===c)return!e&&-1;var l,d=i(o,c);if(e&&t!=t){for(;c>d;)if((l=s[d++])!=l)return!0}else for(;c>d;d++)if((e||d in s)&&s[d]===t)return e||d||0;return!e&&-1}};e.exports={includes:o(!0),indexOf:o(!1)}},function(e,n,t){"use strict";var a=t(76),i=Math.max,r=Math.min;e.exports=function(e,n){var t=a(e);return t<0?i(t+n,0):r(t,n)}},function(e,n,t){"use strict";var a=Math.ceil,i=Math.floor;e.exports=Math.trunc||function(e){var n=+e;return(n>0?i:a)(n)}},function(e,n,t){"use strict";var a=t(76),i=Math.min;e.exports=function(e){var n=a(e);return n>0?i(n,9007199254740991):0}},function(e,n,t){"use strict";n.f=Object.getOwnPropertySymbols},function(e,n,t){"use strict";var a=t(4),i=t(2),r=/#|\.prototype\./,o=function(e,n){var t=c[s(e)];return t===d||t!==l&&(i(n)?a(n):!!n)},s=o.normalize=function(e){return String(e).replace(r,".").toLowerCase()},c=o.data={},l=o.NATIVE="N",d=o.POLYFILL="P";e.exports=o},function(e,n,t){"use strict";var a=t(34),i=t(36),r=t(58),o=t(37),s=TypeError,c="Reduce of empty array with no initial value",l=function(e){return function(n,t,l,d){var h=i(n),u=r(h),p=o(h);if(a(t),0===p&&l<2)throw new s(c);var m=e?p-1:0,f=e?-1:1;if(l<2)for(;;){if(m in u){d=u[m],m+=f;break}if(m+=f,e?m<0:p<=m)throw new s(c)}for(;e?m>=0:p>m;m+=f)m in u&&(d=t(d,u[m],m,h));return d}};e.exports={left:l(!1),right:l(!0)}},function(e,n,t){"use strict";var a=t(4);e.exports=function(e,n){var t=[][e];return!!t&&a((function(){t.call(null,n||function(){return 1},1)}))}},function(e,n,t){"use strict";var a=t(152);e.exports="NODE"===a},function(e,n,t){"use strict";var a=t(0),i=t(70),r=t(22),o=function(e){return i.slice(0,e.length)===e};e.exports=o("Bun/")?"BUN":o("Cloudflare-Workers")?"CLOUDFLARE":o("Deno/")?"DENO":o("Node.js/")?"NODE":a.Bun&&"string"==typeof Bun.version?"BUN":a.Deno&&"object"==typeof Deno.version?"DENO":"process"===r(a.process)?"NODE":a.window&&a.document?"BROWSER":"REST"},function(e,n,t){"use strict";var a=t(30),i=t(0),r=t(154),o=t(155),s=i.WebAssembly,c=7!==new Error("e",{cause:7}).cause,l=function(e,n){var t={};t[e]=o(e,n,c),a({global:!0,constructor:!0,arity:1,forced:c},t)},d=function(e,n){if(s&&s[e]){var t={};t[e]=o("WebAssembly."+e,n,c),a({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:c},t)}};l("Error",(function(e){return function(n){return r(e,this,arguments)}})),l("EvalError",(function(e){return function(n){return r(e,this,arguments)}})),l("RangeError",(function(e){return function(n){return r(e,this,arguments)}})),l("ReferenceError",(function(e){return function(n){return r(e,this,arguments)}})),l("SyntaxError",(function(e){return function(n){return r(e,this,arguments)}})),l("TypeError",(function(e){return function(n){return r(e,this,arguments)}})),l("URIError",(function(e){return function(n){return r(e,this,arguments)}})),d("CompileError",(function(e){return function(n){return r(e,this,arguments)}})),d("LinkError",(function(e){return function(n){return r(e,this,arguments)}})),d("RuntimeError",(function(e){return function(n){return r(e,this,arguments)}}))},function(e,n,t){"use strict";var a=t(31),i=Function.prototype,r=i.apply,o=i.call;e.exports="object"==typeof Reflect&&Reflect.apply||(a?o.bind(r):function(){return o.apply(r,arguments)})},function(e,n,t){"use strict";var a=t(33),i=t(10),r=t(18),o=t(66),s=t(77),c=t(75),l=t(159),d=t(160),h=t(161),u=t(164),p=t(165),m=t(6),f=t(72);e.exports=function(e,n,t,g){var y=g?2:1,b=e.split("."),v=b[b.length-1],w=a.apply(null,b);if(w){var _=w.prototype;if(!f&&i(_,"cause")&&delete _.cause,!t)return w;var k=a("Error"),x=n((function(e,n){var t=h(g?n:e,void 0),a=g?new w(e):new w;return void 0!==t&&r(a,"message",t),p(a,x,a.stack,2),this&&o(_,this)&&d(a,this,x),arguments.length>y&&u(a,arguments[y]),a}));if(x.prototype=_,"Error"!==v?s?s(x,k):c(x,k,{name:!0}):m&&"stackTraceLimit"in w&&(l(x,w,"stackTraceLimit"),l(x,w,"prepareStackTrace")),c(x,w),!f)try{_.name!==v&&r(_,"name",v),_.constructor=x}catch(e){}return x}}},function(e,n,t){"use strict";var a=t(3),i=t(34);e.exports=function(e,n,t){try{return a(i(Object.getOwnPropertyDescriptor(e,n)[t]))}catch(e){}}},function(e,n,t){"use strict";var a=t(158),i=String,r=TypeError;e.exports=function(e){if(a(e))return e;throw new r("Can't set "+i(e)+" as a prototype")}},function(e,n,t){"use strict";var a=t(8);e.exports=function(e){return a(e)||null===e}},function(e,n,t){"use strict";var a=t(21).f;e.exports=function(e,n,t){t in e||a(e,t,{configurable:!0,get:function(){return n[t]},set:function(e){n[t]=e}})}},function(e,n,t){"use strict";var a=t(2),i=t(8),r=t(77);e.exports=function(e,n,t){var o,s;return r&&a(o=n.constructor)&&o!==t&&i(s=o.prototype)&&s!==t.prototype&&r(e,s),e}},function(e,n,t){"use strict";var a=t(123);e.exports=function(e,n){return void 0===e?arguments.length<2?"":n:a(e)}},function(e,n,t){"use strict";var a=t(163),i=t(2),r=t(22),o=t(35)("toStringTag"),s=Object,c="Arguments"===r(function(){return arguments}());e.exports=a?r:function(e){var n,t,a;return void 0===e?"Undefined":null===e?"Null":"string"==typeof(t=function(e,n){try{return e[n]}catch(e){}}(n=s(e),o))?t:c?r(n):"Object"===(a=r(n))&&i(n.callee)?"Arguments":a}},function(e,n,t){"use strict";var a={};a[t(35)("toStringTag")]="z",e.exports="[object z]"===String(a)},function(e,n,t){"use strict";var a=t(8),i=t(18);e.exports=function(e,n){a(n)&&"cause"in n&&i(e,"cause",n.cause)}},function(e,n,t){"use strict";var a=t(18),i=t(166),r=t(167),o=Error.captureStackTrace;e.exports=function(e,n,t,s){r&&(o?o(e,n):a(e,"stack",i(t,s)))}},function(e,n,t){"use strict";var a=t(3),i=Error,r=a("".replace),o=String(new i("zxcasd").stack),s=/\n\s*at [^:]*:[^\n]*/,c=s.test(o);e.exports=function(e,n){if(c&&"string"==typeof e&&!i.prepareStackTrace)for(;n--;)e=r(e,s,"");return e}},function(e,n,t){"use strict";var a=t(4),i=t(39);e.exports=!a((function(){var e=new Error("a");return!("stack"in e)||(Object.defineProperty(e,"stack",i(1,7)),7!==e.stack)}))},function(e,n,t){"use strict";var a=t(6),i=t(169),r=TypeError,o=Object.getOwnPropertyDescriptor,s=a&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(e){return e instanceof TypeError}}();e.exports=s?function(e,n){if(i(e)&&!o(e,"length").writable)throw new r("Cannot set read only .length");return e.length=n}:function(e,n){return e.length=n}},function(e,n,t){"use strict";var a=t(22);e.exports=Array.isArray||function(e){return"Array"===a(e)}},function(e,n,t){"use strict";var a=TypeError;e.exports=function(e){if(e>9007199254740991)throw a("Maximum allowed index exceeded");return e}},function(e,n,t){var a=t(78),i=t(172);e.exports=function e(n,t,r,o,s){var c=-1,l=n.length;for(r||(r=i),s||(s=[]);++c<l;){var d=n[c];t>0&&r(d)?t>1?e(d,t-1,r,o,s):a(s,d):o||(s[s.length]=d)}return s}},function(e,n,t){var a=t(19),i=t(43),r=t(7),o=a?a.isConcatSpreadable:void 0;e.exports=function(e){return r(e)||i(e)||!!(o&&e&&e[o])}},function(e,n,t){var a=t(17),i=t(14);e.exports=function(e){return i(e)&&"[object Arguments]"==a(e)}},function(e,n,t){var a=t(19),i=Object.prototype,r=i.hasOwnProperty,o=i.toString,s=a?a.toStringTag:void 0;e.exports=function(e){var n=r.call(e,s),t=e[s];try{e[s]=void 0;var a=!0}catch(e){}var i=o.call(e);return a&&(n?e[s]=t:delete e[s]),i}},function(e,n){var t=Object.prototype.toString;e.exports=function(e){return t.call(e)}},function(e,n,t){var a=t(177),i=t(233),r=t(51),o=t(7),s=t(244);e.exports=function(e){return"function"==typeof e?e:null==e?r:"object"==typeof e?o(e)?i(e[0],e[1]):a(e):s(e)}},function(e,n,t){var a=t(178),i=t(232),r=t(95);e.exports=function(e){var n=i(e);return 1==n.length&&n[0][2]?r(n[0][0],n[0][1]):function(t){return t===e||a(t,e,n)}}},function(e,n,t){var a=t(80),i=t(84);e.exports=function(e,n,t,r){var o=t.length,s=o,c=!r;if(null==e)return!s;for(e=Object(e);o--;){var l=t[o];if(c&&l[2]?l[1]!==e[l[0]]:!(l[0]in e))return!1}for(;++o<s;){var d=(l=t[o])[0],h=e[d],u=l[1];if(c&&l[2]){if(void 0===h&&!(d in e))return!1}else{var p=new a;if(r)var m=r(h,u,d,e,n,p);if(!(void 0===m?i(u,h,3,r,p):m))return!1}}return!0}},function(e,n){e.exports=function(){this.__data__=[],this.size=0}},function(e,n,t){var a=t(24),i=Array.prototype.splice;e.exports=function(e){var n=this.__data__,t=a(n,e);return!(t<0)&&(t==n.length-1?n.pop():i.call(n,t,1),--this.size,!0)}},function(e,n,t){var a=t(24);e.exports=function(e){var n=this.__data__,t=a(n,e);return t<0?void 0:n[t][1]}},function(e,n,t){var a=t(24);e.exports=function(e){return a(this.__data__,e)>-1}},function(e,n,t){var a=t(24);e.exports=function(e,n){var t=this.__data__,i=a(t,e);return i<0?(++this.size,t.push([e,n])):t[i][1]=n,this}},function(e,n,t){var a=t(23);e.exports=function(){this.__data__=new a,this.size=0}},function(e,n){e.exports=function(e){var n=this.__data__,t=n.delete(e);return this.size=n.size,t}},function(e,n){e.exports=function(e){return this.__data__.get(e)}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n,t){var a=t(23),i=t(44),r=t(46);e.exports=function(e,n){var t=this.__data__;if(t instanceof a){var o=t.__data__;if(!i||o.length<199)return o.push([e,n]),this.size=++t.size,this;t=this.__data__=new r(o)}return t.set(e,n),this.size=t.size,this}},function(e,n,t){var a=t(82),i=t(190),r=t(45),o=t(83),s=/^\[object .+?Constructor\]$/,c=Function.prototype,l=Object.prototype,d=c.toString,h=l.hasOwnProperty,u=RegExp("^"+d.call(h).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");e.exports=function(e){return!(!r(e)||i(e))&&(a(e)?u:s).test(o(e))}},function(e,n,t){var a,i=t(191),r=(a=/[^.]+$/.exec(i&&i.keys&&i.keys.IE_PROTO||""))?"Symbol(src)_1."+a:"";e.exports=function(e){return!!r&&r in e}},function(e,n,t){var a=t(9)["__core-js_shared__"];e.exports=a},function(e,n){e.exports=function(e,n){return null==e?void 0:e[n]}},function(e,n,t){var a=t(194),i=t(23),r=t(44);e.exports=function(){this.size=0,this.__data__={hash:new a,map:new(r||i),string:new a}}},function(e,n,t){var a=t(195),i=t(196),r=t(197),o=t(198),s=t(199);function c(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var a=e[n];this.set(a[0],a[1])}}c.prototype.clear=a,c.prototype.delete=i,c.prototype.get=r,c.prototype.has=o,c.prototype.set=s,e.exports=c},function(e,n,t){var a=t(25);e.exports=function(){this.__data__=a?a(null):{},this.size=0}},function(e,n){e.exports=function(e){var n=this.has(e)&&delete this.__data__[e];return this.size-=n?1:0,n}},function(e,n,t){var a=t(25),i=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;if(a){var t=n[e];return"__lodash_hash_undefined__"===t?void 0:t}return i.call(n,e)?n[e]:void 0}},function(e,n,t){var a=t(25),i=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;return a?void 0!==n[e]:i.call(n,e)}},function(e,n,t){var a=t(25);e.exports=function(e,n){var t=this.__data__;return this.size+=this.has(e)?0:1,t[e]=a&&void 0===n?"__lodash_hash_undefined__":n,this}},function(e,n,t){var a=t(26);e.exports=function(e){var n=a(this,e).delete(e);return this.size-=n?1:0,n}},function(e,n){e.exports=function(e){var n=typeof e;return"string"==n||"number"==n||"symbol"==n||"boolean"==n?"__proto__"!==e:null===e}},function(e,n,t){var a=t(26);e.exports=function(e){return a(this,e).get(e)}},function(e,n,t){var a=t(26);e.exports=function(e){return a(this,e).has(e)}},function(e,n,t){var a=t(26);e.exports=function(e,n){var t=a(this,e),i=t.size;return t.set(e,n),this.size+=t.size==i?0:1,this}},function(e,n,t){var a=t(80),i=t(85),r=t(209),o=t(212),s=t(228),c=t(7),l=t(89),d=t(91),h="[object Object]",u=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,p,m,f){var g=c(e),y=c(n),b=g?"[object Array]":s(e),v=y?"[object Array]":s(n),w=(b="[object Arguments]"==b?h:b)==h,_=(v="[object Arguments]"==v?h:v)==h,k=b==v;if(k&&l(e)){if(!l(n))return!1;g=!0,w=!1}if(k&&!w)return f||(f=new a),g||d(e)?i(e,n,t,p,m,f):r(e,n,b,t,p,m,f);if(!(1&t)){var x=w&&u.call(e,"__wrapped__"),C=_&&u.call(n,"__wrapped__");if(x||C){var P=x?e.value():e,T=C?n.value():n;return f||(f=new a),m(P,T,t,p,f)}}return!!k&&(f||(f=new a),o(e,n,t,p,m,f))}},function(e,n){e.exports=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length;++t<a;)if(n(e[t],t,e))return!0;return!1}},function(e,n,t){var a=t(19),i=t(210),r=t(81),o=t(85),s=t(211),c=t(47),l=a?a.prototype:void 0,d=l?l.valueOf:void 0;e.exports=function(e,n,t,a,l,h,u){switch(t){case"[object DataView]":if(e.byteLength!=n.byteLength||e.byteOffset!=n.byteOffset)return!1;e=e.buffer,n=n.buffer;case"[object ArrayBuffer]":return!(e.byteLength!=n.byteLength||!h(new i(e),new i(n)));case"[object Boolean]":case"[object Date]":case"[object Number]":return r(+e,+n);case"[object Error]":return e.name==n.name&&e.message==n.message;case"[object RegExp]":case"[object String]":return e==n+"";case"[object Map]":var p=s;case"[object Set]":var m=1&a;if(p||(p=c),e.size!=n.size&&!m)return!1;var f=u.get(e);if(f)return f==n;a|=2,u.set(e,n);var g=o(p(e),p(n),a,l,h,u);return u.delete(e),g;case"[object Symbol]":if(d)return d.call(e)==d.call(n)}return!1}},function(e,n,t){var a=t(9).Uint8Array;e.exports=a},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e,a){t[++n]=[a,e]})),t}},function(e,n,t){var a=t(213),i=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,r,o,s){var c=1&t,l=a(e),d=l.length;if(d!=a(n).length&&!c)return!1;for(var h=d;h--;){var u=l[h];if(!(c?u in n:i.call(n,u)))return!1}var p=s.get(e),m=s.get(n);if(p&&m)return p==n&&m==e;var f=!0;s.set(e,n),s.set(n,e);for(var g=c;++h<d;){var y=e[u=l[h]],b=n[u];if(r)var v=c?r(b,y,u,n,e,s):r(y,b,u,e,n,s);if(!(void 0===v?y===b||o(y,b,t,r,s):v)){f=!1;break}g||(g="constructor"==u)}if(f&&!g){var w=e.constructor,_=n.constructor;w==_||!("constructor"in e)||!("constructor"in n)||"function"==typeof w&&w instanceof w&&"function"==typeof _&&_ instanceof _||(f=!1)}return s.delete(e),s.delete(n),f}},function(e,n,t){var a=t(214),i=t(215),r=t(88);e.exports=function(e){return a(e,r,i)}},function(e,n,t){var a=t(78),i=t(7);e.exports=function(e,n,t){var r=n(e);return i(e)?r:a(r,t(e))}},function(e,n,t){var a=t(216),i=t(217),r=Object.prototype.propertyIsEnumerable,o=Object.getOwnPropertySymbols,s=o?function(e){return null==e?[]:(e=Object(e),a(o(e),(function(n){return r.call(e,n)})))}:i;e.exports=s},function(e,n){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length,i=0,r=[];++t<a;){var o=e[t];n(o,t,e)&&(r[i++]=o)}return r}},function(e,n){e.exports=function(){return[]}},function(e,n,t){var a=t(219),i=t(43),r=t(7),o=t(89),s=t(90),c=t(91),l=Object.prototype.hasOwnProperty;e.exports=function(e,n){var t=r(e),d=!t&&i(e),h=!t&&!d&&o(e),u=!t&&!d&&!h&&c(e),p=t||d||h||u,m=p?a(e.length,String):[],f=m.length;for(var g in e)!n&&!l.call(e,g)||p&&("length"==g||h&&("offset"==g||"parent"==g)||u&&("buffer"==g||"byteLength"==g||"byteOffset"==g)||s(g,f))||m.push(g);return m}},function(e,n){e.exports=function(e,n){for(var t=-1,a=Array(e);++t<e;)a[t]=n(t);return a}},function(e,n){e.exports=function(){return!1}},function(e,n,t){var a=t(17),i=t(48),r=t(14),o={};o["[object Float32Array]"]=o["[object Float64Array]"]=o["[object Int8Array]"]=o["[object Int16Array]"]=o["[object Int32Array]"]=o["[object Uint8Array]"]=o["[object Uint8ClampedArray]"]=o["[object Uint16Array]"]=o["[object Uint32Array]"]=!0,o["[object Arguments]"]=o["[object Array]"]=o["[object ArrayBuffer]"]=o["[object Boolean]"]=o["[object DataView]"]=o["[object Date]"]=o["[object Error]"]=o["[object Function]"]=o["[object Map]"]=o["[object Number]"]=o["[object Object]"]=o["[object RegExp]"]=o["[object Set]"]=o["[object String]"]=o["[object WeakMap]"]=!1,e.exports=function(e){return r(e)&&i(e.length)&&!!o[a(e)]}},function(e,n){e.exports=function(e){return function(n){return e(n)}}},function(e,n,t){(function(e){var a=t(79),i=n&&!n.nodeType&&n,r=i&&"object"==typeof e&&e&&!e.nodeType&&e,o=r&&r.exports===i&&a.process,s=function(){try{var e=r&&r.require&&r.require("util").types;return e||o&&o.binding&&o.binding("util")}catch(e){}}();e.exports=s}).call(this,t(60)(e))},function(e,n,t){var a=t(225),i=t(226),r=Object.prototype.hasOwnProperty;e.exports=function(e){if(!a(e))return i(e);var n=[];for(var t in Object(e))r.call(e,t)&&"constructor"!=t&&n.push(t);return n}},function(e,n){var t=Object.prototype;e.exports=function(e){var n=e&&e.constructor;return e===("function"==typeof n&&n.prototype||t)}},function(e,n,t){var a=t(227)(Object.keys,Object);e.exports=a},function(e,n){e.exports=function(e,n){return function(t){return e(n(t))}}},function(e,n,t){var a=t(229),i=t(44),r=t(230),o=t(93),s=t(231),c=t(17),l=t(83),d=l(a),h=l(i),u=l(r),p=l(o),m=l(s),f=c;(a&&"[object DataView]"!=f(new a(new ArrayBuffer(1)))||i&&"[object Map]"!=f(new i)||r&&"[object Promise]"!=f(r.resolve())||o&&"[object Set]"!=f(new o)||s&&"[object WeakMap]"!=f(new s))&&(f=function(e){var n=c(e),t="[object Object]"==n?e.constructor:void 0,a=t?l(t):"";if(a)switch(a){case d:return"[object DataView]";case h:return"[object Map]";case u:return"[object Promise]";case p:return"[object Set]";case m:return"[object WeakMap]"}return n}),e.exports=f},function(e,n,t){var a=t(11)(t(9),"DataView");e.exports=a},function(e,n,t){var a=t(11)(t(9),"Promise");e.exports=a},function(e,n,t){var a=t(11)(t(9),"WeakMap");e.exports=a},function(e,n,t){var a=t(94),i=t(88);e.exports=function(e){for(var n=i(e),t=n.length;t--;){var r=n[t],o=e[r];n[t]=[r,o,a(o)]}return n}},function(e,n,t){var a=t(84),i=t(234),r=t(241),o=t(49),s=t(94),c=t(95),l=t(27);e.exports=function(e,n){return o(e)&&s(n)?c(l(e),n):function(t){var o=i(t,e);return void 0===o&&o===n?r(t,e):a(n,o,3)}}},function(e,n,t){var a=t(96);e.exports=function(e,n,t){var i=null==e?void 0:a(e,n);return void 0===i?t:i}},function(e,n,t){var a=t(236),i=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,r=/\\(\\)?/g,o=a((function(e){var n=[];return 46===e.charCodeAt(0)&&n.push(""),e.replace(i,(function(e,t,a,i){n.push(a?i.replace(r,"$1"):t||e)})),n}));e.exports=o},function(e,n,t){var a=t(237);e.exports=function(e){var n=a(e,(function(e){return 500===t.size&&t.clear(),e})),t=n.cache;return n}},function(e,n,t){var a=t(46);function i(e,n){if("function"!=typeof e||null!=n&&"function"!=typeof n)throw new TypeError("Expected a function");var t=function(){var a=arguments,i=n?n.apply(this,a):a[0],r=t.cache;if(r.has(i))return r.get(i);var o=e.apply(this,a);return t.cache=r.set(i,o)||r,o};return t.cache=new(i.Cache||a),t}i.Cache=a,e.exports=i},function(e,n,t){var a=t(239);e.exports=function(e){return null==e?"":a(e)}},function(e,n,t){var a=t(19),i=t(240),r=t(7),o=t(50),s=a?a.prototype:void 0,c=s?s.toString:void 0;e.exports=function e(n){if("string"==typeof n)return n;if(r(n))return i(n,e)+"";if(o(n))return c?c.call(n):"";var t=n+"";return"0"==t&&1/n==-1/0?"-0":t}},function(e,n){e.exports=function(e,n){for(var t=-1,a=null==e?0:e.length,i=Array(a);++t<a;)i[t]=n(e[t],t,e);return i}},function(e,n,t){var a=t(242),i=t(243);e.exports=function(e,n){return null!=e&&i(e,n,a)}},function(e,n){e.exports=function(e,n){return null!=e&&n in Object(e)}},function(e,n,t){var a=t(97),i=t(43),r=t(7),o=t(90),s=t(48),c=t(27);e.exports=function(e,n,t){for(var l=-1,d=(n=a(n,e)).length,h=!1;++l<d;){var u=c(n[l]);if(!(h=null!=e&&t(e,u)))break;e=e[u]}return h||++l!=d?h:!!(d=null==e?0:e.length)&&s(d)&&o(u,d)&&(r(e)||i(e))}},function(e,n,t){var a=t(245),i=t(246),r=t(49),o=t(27);e.exports=function(e){return r(e)?a(o(e)):i(e)}},function(e,n){e.exports=function(e){return function(n){return null==n?void 0:n[e]}}},function(e,n,t){var a=t(96);e.exports=function(e){return function(n){return a(n,e)}}},function(e,n,t){var a=t(51),i=t(248),r=t(250);e.exports=function(e,n){return r(i(e,n,a),e+"")}},function(e,n,t){var a=t(249),i=Math.max;e.exports=function(e,n,t){return n=i(void 0===n?e.length-1:n,0),function(){for(var r=arguments,o=-1,s=i(r.length-n,0),c=Array(s);++o<s;)c[o]=r[n+o];o=-1;for(var l=Array(n+1);++o<n;)l[o]=r[o];return l[n]=t(c),a(e,this,l)}}},function(e,n){e.exports=function(e,n,t){switch(t.length){case 0:return e.call(n);case 1:return e.call(n,t[0]);case 2:return e.call(n,t[0],t[1]);case 3:return e.call(n,t[0],t[1],t[2])}return e.apply(n,t)}},function(e,n,t){var a=t(251),i=t(254)(a);e.exports=i},function(e,n,t){var a=t(252),i=t(253),r=t(51),o=i?function(e,n){return i(e,"toString",{configurable:!0,enumerable:!1,value:a(n),writable:!0})}:r;e.exports=o},function(e,n){e.exports=function(e){return function(){return e}}},function(e,n,t){var a=t(11),i=function(){try{var e=a(Object,"defineProperty");return e({},"",{}),e}catch(e){}}();e.exports=i},function(e,n){var t=Date.now;e.exports=function(e){var n=0,a=0;return function(){var i=t(),r=16-(i-a);if(a=i,r>0){if(++n>=800)return arguments[0]}else n=0;return e.apply(void 0,arguments)}}},function(e,n,t){var a=t(86),i=t(256),r=t(261),o=t(87),s=t(262),c=t(47);e.exports=function(e,n,t){var l=-1,d=i,h=e.length,u=!0,p=[],m=p;if(t)u=!1,d=r;else if(h>=200){var f=n?null:s(e);if(f)return c(f);u=!1,d=o,m=new a}else m=n?[]:p;e:for(;++l<h;){var g=e[l],y=n?n(g):g;if(g=t||0!==g?g:0,u&&y==y){for(var b=m.length;b--;)if(m[b]===y)continue e;n&&m.push(y),p.push(g)}else d(m,y,t)||(m!==p&&m.push(y),p.push(g))}return p}},function(e,n,t){var a=t(257);e.exports=function(e,n){return!!(null==e?0:e.length)&&a(e,n,0)>-1}},function(e,n,t){var a=t(258),i=t(259),r=t(260);e.exports=function(e,n,t){return n==n?r(e,n,t):a(e,i,t)}},function(e,n){e.exports=function(e,n,t,a){for(var i=e.length,r=t+(a?1:-1);a?r--:++r<i;)if(n(e[r],r,e))return r;return-1}},function(e,n){e.exports=function(e){return e!=e}},function(e,n){e.exports=function(e,n,t){for(var a=t-1,i=e.length;++a<i;)if(e[a]===n)return a;return-1}},function(e,n){e.exports=function(e,n,t){for(var a=-1,i=null==e?0:e.length;++a<i;)if(t(n,e[a]))return!0;return!1}},function(e,n,t){var a=t(93),i=t(263),r=t(47),o=a&&1/r(new a([,-0]))[1]==1/0?function(e){return new a(e)}:i;e.exports=o},function(e,n){e.exports=function(){}},function(e,n,t){var a=t(92),i=t(14);e.exports=function(e){return i(e)&&a(e)}},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";t(98)},function(e,n,t){"use strict";t(99)},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";var a=t(272),i=t(102),r=t(54),o=Object.prototype.hasOwnProperty,s={brackets:function(e){return e+"[]"},comma:"comma",indices:function(e,n){return e+"["+n+"]"},repeat:function(e){return e}},c=Array.isArray,l=Array.prototype.push,d=function(e,n){l.apply(e,c(n)?n:[n])},h=Date.prototype.toISOString,u=r.default,p={addQueryPrefix:!1,allowDots:!1,allowEmptyArrays:!1,arrayFormat:"indices",charset:"utf-8",charsetSentinel:!1,delimiter:"&",encode:!0,encodeDotInKeys:!1,encoder:i.encode,encodeValuesOnly:!1,format:u,formatter:r.formatters[u],indices:!1,serializeDate:function(e){return h.call(e)},skipNulls:!1,strictNullHandling:!1},m={},f=function e(n,t,r,o,s,l,h,u,f,g,y,b,v,w,_,k,x,C){for(var P,T=n,M=C,A=0,S=!1;void 0!==(M=M.get(m))&&!S;){var I=M.get(n);if(A+=1,void 0!==I){if(I===A)throw new RangeError("Cyclic object value");S=!0}void 0===M.get(m)&&(A=0)}if("function"==typeof g?T=g(t,T):T instanceof Date?T=v(T):"comma"===r&&c(T)&&(T=i.maybeMap(T,(function(e){return e instanceof Date?v(e):e}))),null===T){if(l)return f&&!k?f(t,p.encoder,x,"key",w):t;T=""}if("string"==typeof(P=T)||"number"==typeof P||"boolean"==typeof P||"symbol"==typeof P||"bigint"==typeof P||i.isBuffer(T))return f?[_(k?t:f(t,p.encoder,x,"key",w))+"="+_(f(T,p.encoder,x,"value",w))]:[_(t)+"="+_(String(T))];var L,D=[];if(void 0===T)return D;if("comma"===r&&c(T))k&&f&&(T=i.maybeMap(T,f)),L=[{value:T.length>0?T.join(",")||null:void 0}];else if(c(g))L=g;else{var z=Object.keys(T);L=y?z.sort(y):z}var U=u?t.replace(/\./g,"%2E"):t,O=o&&c(T)&&1===T.length?U+"[]":U;if(s&&c(T)&&0===T.length)return O+"[]";for(var R=0;R<L.length;++R){var E=L[R],G="object"==typeof E&&void 0!==E.value?E.value:T[E];if(!h||null!==G){var q=b&&u?E.replace(/\./g,"%2E"):E,B=c(T)?"function"==typeof r?r(O,q):O:O+(b?"."+q:"["+q+"]");C.set(n,A);var j=a();j.set(m,C),d(D,e(G,B,r,o,s,l,h,u,"comma"===r&&k&&c(T)?null:f,g,y,b,v,w,_,k,x,j))}}return D};e.exports=function(e,n){var t,i=e,l=function(e){if(!e)return p;if(void 0!==e.allowEmptyArrays&&"boolean"!=typeof e.allowEmptyArrays)throw new TypeError("`allowEmptyArrays` option can only be `true` or `false`, when provided");if(void 0!==e.encodeDotInKeys&&"boolean"!=typeof e.encodeDotInKeys)throw new TypeError("`encodeDotInKeys` option can only be `true` or `false`, when provided");if(null!==e.encoder&&void 0!==e.encoder&&"function"!=typeof e.encoder)throw new TypeError("Encoder has to be a function.");var n=e.charset||p.charset;if(void 0!==e.charset&&"utf-8"!==e.charset&&"iso-8859-1"!==e.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");var t=r.default;if(void 0!==e.format){if(!o.call(r.formatters,e.format))throw new TypeError("Unknown format option provided.");t=e.format}var a,i=r.formatters[t],l=p.filter;if(("function"==typeof e.filter||c(e.filter))&&(l=e.filter),a=e.arrayFormat in s?e.arrayFormat:"indices"in e?e.indices?"indices":"repeat":p.arrayFormat,"commaRoundTrip"in e&&"boolean"!=typeof e.commaRoundTrip)throw new TypeError("`commaRoundTrip` must be a boolean, or absent");var d=void 0===e.allowDots?!0===e.encodeDotInKeys||p.allowDots:!!e.allowDots;return{addQueryPrefix:"boolean"==typeof e.addQueryPrefix?e.addQueryPrefix:p.addQueryPrefix,allowDots:d,allowEmptyArrays:"boolean"==typeof e.allowEmptyArrays?!!e.allowEmptyArrays:p.allowEmptyArrays,arrayFormat:a,charset:n,charsetSentinel:"boolean"==typeof e.charsetSentinel?e.charsetSentinel:p.charsetSentinel,commaRoundTrip:e.commaRoundTrip,delimiter:void 0===e.delimiter?p.delimiter:e.delimiter,encode:"boolean"==typeof e.encode?e.encode:p.encode,encodeDotInKeys:"boolean"==typeof e.encodeDotInKeys?e.encodeDotInKeys:p.encodeDotInKeys,encoder:"function"==typeof e.encoder?e.encoder:p.encoder,encodeValuesOnly:"boolean"==typeof e.encodeValuesOnly?e.encodeValuesOnly:p.encodeValuesOnly,filter:l,format:t,formatter:i,serializeDate:"function"==typeof e.serializeDate?e.serializeDate:p.serializeDate,skipNulls:"boolean"==typeof e.skipNulls?e.skipNulls:p.skipNulls,sort:"function"==typeof e.sort?e.sort:null,strictNullHandling:"boolean"==typeof e.strictNullHandling?e.strictNullHandling:p.strictNullHandling}}(n);"function"==typeof l.filter?i=(0,l.filter)("",i):c(l.filter)&&(t=l.filter);var h=[];if("object"!=typeof i||null===i)return"";var u=s[l.arrayFormat],m="comma"===u&&l.commaRoundTrip;t||(t=Object.keys(i)),l.sort&&t.sort(l.sort);for(var g=a(),y=0;y<t.length;++y){var b=t[y];l.skipNulls&&null===i[b]||d(h,f(i[b],b,u,m,l.allowEmptyArrays,l.strictNullHandling,l.skipNulls,l.encodeDotInKeys,l.encode?l.encoder:null,l.filter,l.sort,l.allowDots,l.serializeDate,l.format,l.formatter,l.encodeValuesOnly,l.charset,g))}var v=h.join(l.delimiter),w=!0===l.addQueryPrefix?"?":"";return l.charsetSentinel&&("iso-8859-1"===l.charset?w+="utf8=%26%2310003%3B&":w+="utf8=%E2%9C%93&"),v.length>0?w+v:""}},function(e,n,t){"use strict";var a=t(15),i=t(283),r=t(288),o=t(20),s=a("%WeakMap%",!0),c=a("%Map%",!0),l=i("WeakMap.prototype.get",!0),d=i("WeakMap.prototype.set",!0),h=i("WeakMap.prototype.has",!0),u=i("Map.prototype.get",!0),p=i("Map.prototype.set",!0),m=i("Map.prototype.has",!0),f=function(e,n){for(var t,a=e;null!==(t=a.next);a=t)if(t.key===n)return a.next=t.next,t.next=e.next,e.next=t,t};e.exports=function(){var e,n,t,a={assert:function(e){if(!a.has(e))throw new o("Side channel does not contain "+r(e))},get:function(a){if(s&&a&&("object"==typeof a||"function"==typeof a)){if(e)return l(e,a)}else if(c){if(n)return u(n,a)}else if(t)return function(e,n){var t=f(e,n);return t&&t.value}(t,a)},has:function(a){if(s&&a&&("object"==typeof a||"function"==typeof a)){if(e)return h(e,a)}else if(c){if(n)return m(n,a)}else if(t)return function(e,n){return!!f(e,n)}(t,a);return!1},set:function(a,i){s&&a&&("object"==typeof a||"function"==typeof a)?(e||(e=new s),d(e,a,i)):c?(n||(n=new c),p(n,a,i)):(t||(t={key:{},next:null}),function(e,n,t){var a=f(e,n);a?a.value=t:e.next={key:n,next:e.next,value:t}}(t,a,i))}};return a}},function(e,n,t){"use strict";e.exports=Error},function(e,n,t){"use strict";e.exports=EvalError},function(e,n,t){"use strict";e.exports=RangeError},function(e,n,t){"use strict";e.exports=ReferenceError},function(e,n,t){"use strict";e.exports=URIError},function(e,n,t){"use strict";var a="undefined"!=typeof Symbol&&Symbol,i=t(279);e.exports=function(){return"function"==typeof a&&("function"==typeof Symbol&&("symbol"==typeof a("foo")&&("symbol"==typeof Symbol("bar")&&i())))}},function(e,n,t){"use strict";e.exports=function(){if("function"!=typeof Symbol||"function"!=typeof Object.getOwnPropertySymbols)return!1;if("symbol"==typeof Symbol.iterator)return!0;var e={},n=Symbol("test"),t=Object(n);if("string"==typeof n)return!1;if("[object Symbol]"!==Object.prototype.toString.call(n))return!1;if("[object Symbol]"!==Object.prototype.toString.call(t))return!1;for(n in e[n]=42,e)return!1;if("function"==typeof Object.keys&&0!==Object.keys(e).length)return!1;if("function"==typeof Object.getOwnPropertyNames&&0!==Object.getOwnPropertyNames(e).length)return!1;var a=Object.getOwnPropertySymbols(e);if(1!==a.length||a[0]!==n)return!1;if(!Object.prototype.propertyIsEnumerable.call(e,n))return!1;if("function"==typeof Object.getOwnPropertyDescriptor){var i=Object.getOwnPropertyDescriptor(e,n);if(42!==i.value||!0!==i.enumerable)return!1}return!0}},function(e,n,t){"use strict";var a={__proto__:null,foo:{}},i=Object;e.exports=function(){return{__proto__:a}.foo===a.foo&&!(a instanceof i)}},function(e,n,t){"use strict";var a="Function.prototype.bind called on incompatible ",i=Object.prototype.toString,r=Math.max,o=function(e,n){for(var t=[],a=0;a<e.length;a+=1)t[a]=e[a];for(var i=0;i<n.length;i+=1)t[i+e.length]=n[i];return t},s=function(e,n){for(var t=[],a=n||0,i=0;a<e.length;a+=1,i+=1)t[i]=e[a];return t},c=function(e,n){for(var t="",a=0;a<e.length;a+=1)t+=e[a],a+1<e.length&&(t+=n);return t};e.exports=function(e){var n=this;if("function"!=typeof n||"[object Function]"!==i.apply(n))throw new TypeError(a+n);for(var t,l=s(arguments,1),d=function(){if(this instanceof t){var a=n.apply(this,o(l,arguments));return Object(a)===a?a:this}return n.apply(e,o(l,arguments))},h=r(0,n.length-l.length),u=[],p=0;p<h;p++)u[p]="$"+p;if(t=Function("binder","return function ("+c(u,",")+"){ return binder.apply(this,arguments); }")(d),n.prototype){var m=function(){};m.prototype=n.prototype,t.prototype=new m,m.prototype=null}return t}},function(e,n,t){"use strict";var a=Function.prototype.call,i=Object.prototype.hasOwnProperty,r=t(52);e.exports=r.call(a,i)},function(e,n,t){"use strict";var a=t(15),i=t(284),r=i(a("String.prototype.indexOf"));e.exports=function(e,n){var t=a(e,!!n);return"function"==typeof t&&r(e,".prototype.")>-1?i(t):t}},function(e,n,t){"use strict";var a=t(52),i=t(15),r=t(285),o=t(20),s=i("%Function.prototype.apply%"),c=i("%Function.prototype.call%"),l=i("%Reflect.apply%",!0)||a.call(c,s),d=t(53),h=i("%Math.max%");e.exports=function(e){if("function"!=typeof e)throw new o("a function is required");var n=l(a,c,arguments);return r(n,1+h(0,e.length-(arguments.length-1)),!0)};var u=function(){return l(a,s,arguments)};d?d(e.exports,"apply",{value:u}):e.exports.apply=u},function(e,n,t){"use strict";var a=t(15),i=t(286),r=t(287)(),o=t(101),s=t(20),c=a("%Math.floor%");e.exports=function(e,n){if("function"!=typeof e)throw new s("`fn` is not a function");if("number"!=typeof n||n<0||n>4294967295||c(n)!==n)throw new s("`length` must be a positive 32-bit integer");var t=arguments.length>2&&!!arguments[2],a=!0,l=!0;if("length"in e&&o){var d=o(e,"length");d&&!d.configurable&&(a=!1),d&&!d.writable&&(l=!1)}return(a||l||!t)&&(r?i(e,"length",n,!0,!0):i(e,"length",n)),e}},function(e,n,t){"use strict";var a=t(53),i=t(100),r=t(20),o=t(101);e.exports=function(e,n,t){if(!e||"object"!=typeof e&&"function"!=typeof e)throw new r("`obj` must be an object or a function`");if("string"!=typeof n&&"symbol"!=typeof n)throw new r("`property` must be a string or a symbol`");if(arguments.length>3&&"boolean"!=typeof arguments[3]&&null!==arguments[3])throw new r("`nonEnumerable`, if provided, must be a boolean or null");if(arguments.length>4&&"boolean"!=typeof arguments[4]&&null!==arguments[4])throw new r("`nonWritable`, if provided, must be a boolean or null");if(arguments.length>5&&"boolean"!=typeof arguments[5]&&null!==arguments[5])throw new r("`nonConfigurable`, if provided, must be a boolean or null");if(arguments.length>6&&"boolean"!=typeof arguments[6])throw new r("`loose`, if provided, must be a boolean");var s=arguments.length>3?arguments[3]:null,c=arguments.length>4?arguments[4]:null,l=arguments.length>5?arguments[5]:null,d=arguments.length>6&&arguments[6],h=!!o&&o(e,n);if(a)a(e,n,{configurable:null===l&&h?h.configurable:!l,enumerable:null===s&&h?h.enumerable:!s,value:t,writable:null===c&&h?h.writable:!c});else{if(!d&&(s||c||l))throw new i("This environment does not support defining a property as non-configurable, non-writable, or non-enumerable.");e[n]=t}}},function(e,n,t){"use strict";var a=t(53),i=function(){return!!a};i.hasArrayLengthDefineBug=function(){if(!a)return null;try{return 1!==a([],"length",{value:1}).length}catch(e){return!0}},e.exports=i},function(e,n,t){var a="function"==typeof Map&&Map.prototype,i=Object.getOwnPropertyDescriptor&&a?Object.getOwnPropertyDescriptor(Map.prototype,"size"):null,r=a&&i&&"function"==typeof i.get?i.get:null,o=a&&Map.prototype.forEach,s="function"==typeof Set&&Set.prototype,c=Object.getOwnPropertyDescriptor&&s?Object.getOwnPropertyDescriptor(Set.prototype,"size"):null,l=s&&c&&"function"==typeof c.get?c.get:null,d=s&&Set.prototype.forEach,h="function"==typeof WeakMap&&WeakMap.prototype?WeakMap.prototype.has:null,u="function"==typeof WeakSet&&WeakSet.prototype?WeakSet.prototype.has:null,p="function"==typeof WeakRef&&WeakRef.prototype?WeakRef.prototype.deref:null,m=Boolean.prototype.valueOf,f=Object.prototype.toString,g=Function.prototype.toString,y=String.prototype.match,b=String.prototype.slice,v=String.prototype.replace,w=String.prototype.toUpperCase,_=String.prototype.toLowerCase,k=RegExp.prototype.test,x=Array.prototype.concat,C=Array.prototype.join,P=Array.prototype.slice,T=Math.floor,M="function"==typeof BigInt?BigInt.prototype.valueOf:null,A=Object.getOwnPropertySymbols,S="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?Symbol.prototype.toString:null,I="function"==typeof Symbol&&"object"==typeof Symbol.iterator,L="function"==typeof Symbol&&Symbol.toStringTag&&(typeof Symbol.toStringTag===I||"symbol")?Symbol.toStringTag:null,D=Object.prototype.propertyIsEnumerable,z=("function"==typeof Reflect?Reflect.getPrototypeOf:Object.getPrototypeOf)||([].__proto__===Array.prototype?function(e){return e.__proto__}:null);function U(e,n){if(e===1/0||e===-1/0||e!=e||e&&e>-1e3&&e<1e3||k.call(/e/,n))return n;var t=/[0-9](?=(?:[0-9]{3})+(?![0-9]))/g;if("number"==typeof e){var a=e<0?-T(-e):T(e);if(a!==e){var i=String(a),r=b.call(n,i.length+1);return v.call(i,t,"$&_")+"."+v.call(v.call(r,/([0-9]{3})/g,"$&_"),/_$/,"")}}return v.call(n,t,"$&_")}var O=t(289),R=O.custom,E=N(R)?R:null;function G(e,n,t){var a="double"===(t.quoteStyle||n)?'"':"'";return a+e+a}function q(e){return v.call(String(e),/"/g,"&quot;")}function B(e){return!("[object Array]"!==V(e)||L&&"object"==typeof e&&L in e)}function j(e){return!("[object RegExp]"!==V(e)||L&&"object"==typeof e&&L in e)}function N(e){if(I)return e&&"object"==typeof e&&e instanceof Symbol;if("symbol"==typeof e)return!0;if(!e||"object"!=typeof e||!S)return!1;try{return S.call(e),!0}catch(e){}return!1}e.exports=function e(n,t,a,i){var s=t||{};if(F(s,"quoteStyle")&&"single"!==s.quoteStyle&&"double"!==s.quoteStyle)throw new TypeError('option "quoteStyle" must be "single" or "double"');if(F(s,"maxStringLength")&&("number"==typeof s.maxStringLength?s.maxStringLength<0&&s.maxStringLength!==1/0:null!==s.maxStringLength))throw new TypeError('option "maxStringLength", if provided, must be a positive integer, Infinity, or `null`');var c=!F(s,"customInspect")||s.customInspect;if("boolean"!=typeof c&&"symbol"!==c)throw new TypeError("option \"customInspect\", if provided, must be `true`, `false`, or `'symbol'`");if(F(s,"indent")&&null!==s.indent&&"\t"!==s.indent&&!(parseInt(s.indent,10)===s.indent&&s.indent>0))throw new TypeError('option "indent" must be "\\t", an integer > 0, or `null`');if(F(s,"numericSeparator")&&"boolean"!=typeof s.numericSeparator)throw new TypeError('option "numericSeparator", if provided, must be `true` or `false`');var f=s.numericSeparator;if(void 0===n)return"undefined";if(null===n)return"null";if("boolean"==typeof n)return n?"true":"false";if("string"==typeof n)return function e(n,t){if(n.length>t.maxStringLength){var a=n.length-t.maxStringLength,i="... "+a+" more character"+(a>1?"s":"");return e(b.call(n,0,t.maxStringLength),t)+i}return G(v.call(v.call(n,/(['\\])/g,"\\$1"),/[\x00-\x1f]/g,W),"single",t)}(n,s);if("number"==typeof n){if(0===n)return 1/0/n>0?"0":"-0";var w=String(n);return f?U(n,w):w}if("bigint"==typeof n){var k=String(n)+"n";return f?U(n,k):k}var T=void 0===s.depth?5:s.depth;if(void 0===a&&(a=0),a>=T&&T>0&&"object"==typeof n)return B(n)?"[Array]":"[Object]";var A=function(e,n){var t;if("\t"===e.indent)t="\t";else{if(!("number"==typeof e.indent&&e.indent>0))return null;t=C.call(Array(e.indent+1)," ")}return{base:t,prev:C.call(Array(n+1),t)}}(s,a);if(void 0===i)i=[];else if(H(i,n)>=0)return"[Circular]";function R(n,t,r){if(t&&(i=P.call(i)).push(t),r){var o={depth:s.depth};return F(s,"quoteStyle")&&(o.quoteStyle=s.quoteStyle),e(n,o,a+1,i)}return e(n,s,a+1,i)}if("function"==typeof n&&!j(n)){var $=function(e){if(e.name)return e.name;var n=y.call(g.call(e),/^function\s*([\w$]+)/);if(n)return n[1];return null}(n),J=Q(n,R);return"[Function"+($?": "+$:" (anonymous)")+"]"+(J.length>0?" { "+C.call(J,", ")+" }":"")}if(N(n)){var ee=I?v.call(String(n),/^(Symbol\(.*\))_[^)]*$/,"$1"):S.call(n);return"object"!=typeof n||I?ee:K(ee)}if(function(e){if(!e||"object"!=typeof e)return!1;if("undefined"!=typeof HTMLElement&&e instanceof HTMLElement)return!0;return"string"==typeof e.nodeName&&"function"==typeof e.getAttribute}(n)){for(var ne="<"+_.call(String(n.nodeName)),te=n.attributes||[],ae=0;ae<te.length;ae++)ne+=" "+te[ae].name+"="+G(q(te[ae].value),"double",s);return ne+=">",n.childNodes&&n.childNodes.length&&(ne+="..."),ne+="</"+_.call(String(n.nodeName))+">"}if(B(n)){if(0===n.length)return"[]";var ie=Q(n,R);return A&&!function(e){for(var n=0;n<e.length;n++)if(H(e[n],"\n")>=0)return!1;return!0}(ie)?"["+X(ie,A)+"]":"[ "+C.call(ie,", ")+" ]"}if(function(e){return!("[object Error]"!==V(e)||L&&"object"==typeof e&&L in e)}(n)){var re=Q(n,R);return"cause"in Error.prototype||!("cause"in n)||D.call(n,"cause")?0===re.length?"["+String(n)+"]":"{ ["+String(n)+"] "+C.call(re,", ")+" }":"{ ["+String(n)+"] "+C.call(x.call("[cause]: "+R(n.cause),re),", ")+" }"}if("object"==typeof n&&c){if(E&&"function"==typeof n[E]&&O)return O(n,{depth:T-a});if("symbol"!==c&&"function"==typeof n.inspect)return n.inspect()}if(function(e){if(!r||!e||"object"!=typeof e)return!1;try{r.call(e);try{l.call(e)}catch(e){return!0}return e instanceof Map}catch(e){}return!1}(n)){var oe=[];return o&&o.call(n,(function(e,t){oe.push(R(t,n,!0)+" => "+R(e,n))})),Y("Map",r.call(n),oe,A)}if(function(e){if(!l||!e||"object"!=typeof e)return!1;try{l.call(e);try{r.call(e)}catch(e){return!0}return e instanceof Set}catch(e){}return!1}(n)){var se=[];return d&&d.call(n,(function(e){se.push(R(e,n))})),Y("Set",l.call(n),se,A)}if(function(e){if(!h||!e||"object"!=typeof e)return!1;try{h.call(e,h);try{u.call(e,u)}catch(e){return!0}return e instanceof WeakMap}catch(e){}return!1}(n))return Z("WeakMap");if(function(e){if(!u||!e||"object"!=typeof e)return!1;try{u.call(e,u);try{h.call(e,h)}catch(e){return!0}return e instanceof WeakSet}catch(e){}return!1}(n))return Z("WeakSet");if(function(e){if(!p||!e||"object"!=typeof e)return!1;try{return p.call(e),!0}catch(e){}return!1}(n))return Z("WeakRef");if(function(e){return!("[object Number]"!==V(e)||L&&"object"==typeof e&&L in e)}(n))return K(R(Number(n)));if(function(e){if(!e||"object"!=typeof e||!M)return!1;try{return M.call(e),!0}catch(e){}return!1}(n))return K(R(M.call(n)));if(function(e){return!("[object Boolean]"!==V(e)||L&&"object"==typeof e&&L in e)}(n))return K(m.call(n));if(function(e){return!("[object String]"!==V(e)||L&&"object"==typeof e&&L in e)}(n))return K(R(String(n)));if("undefined"!=typeof window&&n===window)return"{ [object Window] }";if("undefined"!=typeof globalThis&&n===globalThis||"undefined"!=typeof global&&n===global)return"{ [object globalThis] }";if(!function(e){return!("[object Date]"!==V(e)||L&&"object"==typeof e&&L in e)}(n)&&!j(n)){var ce=Q(n,R),le=z?z(n)===Object.prototype:n instanceof Object||n.constructor===Object,de=n instanceof Object?"":"null prototype",he=!le&&L&&Object(n)===n&&L in n?b.call(V(n),8,-1):de?"Object":"",ue=(le||"function"!=typeof n.constructor?"":n.constructor.name?n.constructor.name+" ":"")+(he||de?"["+C.call(x.call([],he||[],de||[]),": ")+"] ":"");return 0===ce.length?ue+"{}":A?ue+"{"+X(ce,A)+"}":ue+"{ "+C.call(ce,", ")+" }"}return String(n)};var $=Object.prototype.hasOwnProperty||function(e){return e in this};function F(e,n){return $.call(e,n)}function V(e){return f.call(e)}function H(e,n){if(e.indexOf)return e.indexOf(n);for(var t=0,a=e.length;t<a;t++)if(e[t]===n)return t;return-1}function W(e){var n=e.charCodeAt(0),t={8:"b",9:"t",10:"n",12:"f",13:"r"}[n];return t?"\\"+t:"\\x"+(n<16?"0":"")+w.call(n.toString(16))}function K(e){return"Object("+e+")"}function Z(e){return e+" { ? }"}function Y(e,n,t,a){return e+" ("+n+") {"+(a?X(t,a):C.call(t,", "))+"}"}function X(e,n){if(0===e.length)return"";var t="\n"+n.prev+n.base;return t+C.call(e,","+t)+"\n"+n.prev}function Q(e,n){var t=B(e),a=[];if(t){a.length=e.length;for(var i=0;i<e.length;i++)a[i]=F(e,i)?n(e[i],e):""}var r,o="function"==typeof A?A(e):[];if(I){r={};for(var s=0;s<o.length;s++)r["$"+o[s]]=o[s]}for(var c in e)F(e,c)&&(t&&String(Number(c))===c&&c<e.length||I&&r["$"+c]instanceof Symbol||(k.call(/[^\w$]/,c)?a.push(n(c,e)+": "+n(e[c],e)):a.push(c+": "+n(e[c],e))));if("function"==typeof A)for(var l=0;l<o.length;l++)D.call(e,o[l])&&a.push("["+n(o[l])+"]: "+n(e[o[l]],e));return a}},function(e,n){},function(e,n,t){"use strict";var a=t(102),i=Object.prototype.hasOwnProperty,r=Array.isArray,o={allowDots:!1,allowEmptyArrays:!1,allowPrototypes:!1,allowSparse:!1,arrayLimit:20,charset:"utf-8",charsetSentinel:!1,comma:!1,decodeDotInKeys:!1,decoder:a.decode,delimiter:"&",depth:5,duplicates:"combine",ignoreQueryPrefix:!1,interpretNumericEntities:!1,parameterLimit:1e3,parseArrays:!0,plainObjects:!1,strictDepth:!1,strictNullHandling:!1},s=function(e){return e.replace(/&#(\d+);/g,(function(e,n){return String.fromCharCode(parseInt(n,10))}))},c=function(e,n){return e&&"string"==typeof e&&n.comma&&e.indexOf(",")>-1?e.split(","):e},l=function(e,n,t,a){if(e){var r=t.allowDots?e.replace(/\.([^.[]+)/g,"[$1]"):e,o=/(\[[^[\]]*])/g,s=t.depth>0&&/(\[[^[\]]*])/.exec(r),l=s?r.slice(0,s.index):r,d=[];if(l){if(!t.plainObjects&&i.call(Object.prototype,l)&&!t.allowPrototypes)return;d.push(l)}for(var h=0;t.depth>0&&null!==(s=o.exec(r))&&h<t.depth;){if(h+=1,!t.plainObjects&&i.call(Object.prototype,s[1].slice(1,-1))&&!t.allowPrototypes)return;d.push(s[1])}if(s){if(!0===t.strictDepth)throw new RangeError("Input depth exceeded depth option of "+t.depth+" and strictDepth is true");d.push("["+r.slice(s.index)+"]")}return function(e,n,t,a){for(var i=a?n:c(n,t),r=e.length-1;r>=0;--r){var o,s=e[r];if("[]"===s&&t.parseArrays)o=t.allowEmptyArrays&&(""===i||t.strictNullHandling&&null===i)?[]:[].concat(i);else{o=t.plainObjects?Object.create(null):{};var l="["===s.charAt(0)&&"]"===s.charAt(s.length-1)?s.slice(1,-1):s,d=t.decodeDotInKeys?l.replace(/%2E/g,"."):l,h=parseInt(d,10);t.parseArrays||""!==d?!isNaN(h)&&s!==d&&String(h)===d&&h>=0&&t.parseArrays&&h<=t.arrayLimit?(o=[])[h]=i:"__proto__"!==d&&(o[d]=i):o={0:i}}i=o}return i}(d,n,t,a)}};e.exports=function(e,n){var t=function(e){if(!e)return o;if(void 0!==e.allowEmptyArrays&&"boolean"!=typeof e.allowEmptyArrays)throw new TypeError("`allowEmptyArrays` option can only be `true` or `false`, when provided");if(void 0!==e.decodeDotInKeys&&"boolean"!=typeof e.decodeDotInKeys)throw new TypeError("`decodeDotInKeys` option can only be `true` or `false`, when provided");if(null!==e.decoder&&void 0!==e.decoder&&"function"!=typeof e.decoder)throw new TypeError("Decoder has to be a function.");if(void 0!==e.charset&&"utf-8"!==e.charset&&"iso-8859-1"!==e.charset)throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");var n=void 0===e.charset?o.charset:e.charset,t=void 0===e.duplicates?o.duplicates:e.duplicates;if("combine"!==t&&"first"!==t&&"last"!==t)throw new TypeError("The duplicates option must be either combine, first, or last");return{allowDots:void 0===e.allowDots?!0===e.decodeDotInKeys||o.allowDots:!!e.allowDots,allowEmptyArrays:"boolean"==typeof e.allowEmptyArrays?!!e.allowEmptyArrays:o.allowEmptyArrays,allowPrototypes:"boolean"==typeof e.allowPrototypes?e.allowPrototypes:o.allowPrototypes,allowSparse:"boolean"==typeof e.allowSparse?e.allowSparse:o.allowSparse,arrayLimit:"number"==typeof e.arrayLimit?e.arrayLimit:o.arrayLimit,charset:n,charsetSentinel:"boolean"==typeof e.charsetSentinel?e.charsetSentinel:o.charsetSentinel,comma:"boolean"==typeof e.comma?e.comma:o.comma,decodeDotInKeys:"boolean"==typeof e.decodeDotInKeys?e.decodeDotInKeys:o.decodeDotInKeys,decoder:"function"==typeof e.decoder?e.decoder:o.decoder,delimiter:"string"==typeof e.delimiter||a.isRegExp(e.delimiter)?e.delimiter:o.delimiter,depth:"number"==typeof e.depth||!1===e.depth?+e.depth:o.depth,duplicates:t,ignoreQueryPrefix:!0===e.ignoreQueryPrefix,interpretNumericEntities:"boolean"==typeof e.interpretNumericEntities?e.interpretNumericEntities:o.interpretNumericEntities,parameterLimit:"number"==typeof e.parameterLimit?e.parameterLimit:o.parameterLimit,parseArrays:!1!==e.parseArrays,plainObjects:"boolean"==typeof e.plainObjects?e.plainObjects:o.plainObjects,strictDepth:"boolean"==typeof e.strictDepth?!!e.strictDepth:o.strictDepth,strictNullHandling:"boolean"==typeof e.strictNullHandling?e.strictNullHandling:o.strictNullHandling}}(n);if(""===e||null==e)return t.plainObjects?Object.create(null):{};for(var d="string"==typeof e?function(e,n){var t={__proto__:null},l=n.ignoreQueryPrefix?e.replace(/^\?/,""):e;l=l.replace(/%5B/gi,"[").replace(/%5D/gi,"]");var d,h=n.parameterLimit===1/0?void 0:n.parameterLimit,u=l.split(n.delimiter,h),p=-1,m=n.charset;if(n.charsetSentinel)for(d=0;d<u.length;++d)0===u[d].indexOf("utf8=")&&("utf8=%E2%9C%93"===u[d]?m="utf-8":"utf8=%26%2310003%3B"===u[d]&&(m="iso-8859-1"),p=d,d=u.length);for(d=0;d<u.length;++d)if(d!==p){var f,g,y=u[d],b=y.indexOf("]="),v=-1===b?y.indexOf("="):b+1;-1===v?(f=n.decoder(y,o.decoder,m,"key"),g=n.strictNullHandling?null:""):(f=n.decoder(y.slice(0,v),o.decoder,m,"key"),g=a.maybeMap(c(y.slice(v+1),n),(function(e){return n.decoder(e,o.decoder,m,"value")}))),g&&n.interpretNumericEntities&&"iso-8859-1"===m&&(g=s(g)),y.indexOf("[]=")>-1&&(g=r(g)?[g]:g);var w=i.call(t,f);w&&"combine"===n.duplicates?t[f]=a.combine(t[f],g):w&&"last"!==n.duplicates||(t[f]=g)}return t}(e,t):e,h=t.plainObjects?Object.create(null):{},u=Object.keys(d),p=0;p<u.length;++p){var m=u[p],f=l(m,d[m],t,"string"==typeof e);h=a.merge(h,f,t)}return!0===t.allowSparse?h:a.compact(h)}},function(e,n,t){var a=t(12),i=t(293),r=t(294);e.exports=function(e){var n=a(e);return r(n,i(n))+1}},function(e,n){e.exports=function(e){var n=new Date(e.getTime()),t=n.getTimezoneOffset();return n.setSeconds(0,0),6e4*t+n.getTime()%6e4}},function(e,n,t){var a=t(12);e.exports=function(e){var n=a(e),t=new Date(0);return t.setFullYear(n.getFullYear(),0,1),t.setHours(0,0,0,0),t}},function(e,n,t){var a=t(295);e.exports=function(e,n){var t=a(e),i=a(n),r=t.getTime()-6e4*t.getTimezoneOffset(),o=i.getTime()-6e4*i.getTimezoneOffset();return Math.round((r-o)/864e5)}},function(e,n,t){var a=t(12);e.exports=function(e){var n=a(e);return n.setHours(0,0,0,0),n}},function(e,n,t){var a=t(12),i=t(55),r=t(298);e.exports=function(e){var n=a(e),t=i(n).getTime()-r(n).getTime();return Math.round(t/6048e5)+1}},function(e,n,t){var a=t(12);e.exports=function(e,n){var t=n&&Number(n.weekStartsOn)||0,i=a(e),r=i.getDay(),o=(r<t?7:0)+r-t;return i.setDate(i.getDate()-o),i.setHours(0,0,0,0),i}},function(e,n,t){var a=t(104),i=t(55);e.exports=function(e){var n=a(e),t=new Date(0);return t.setFullYear(n,0,4),t.setHours(0,0,0,0),i(t)}},function(e,n,t){var a=t(103);e.exports=function(e){if(a(e))return!isNaN(e);throw new TypeError(toString.call(e)+" is not an instance of Date")}},function(e,n,t){var a=t(301),i=t(302);e.exports={distanceInWords:a(),format:i()}},function(e,n){e.exports=function(){var e={lessThanXSeconds:{one:"less than a second",other:"less than {{count}} seconds"},xSeconds:{one:"1 second",other:"{{count}} seconds"},halfAMinute:"half a minute",lessThanXMinutes:{one:"less than a minute",other:"less than {{count}} minutes"},xMinutes:{one:"1 minute",other:"{{count}} minutes"},aboutXHours:{one:"about 1 hour",other:"about {{count}} hours"},xHours:{one:"1 hour",other:"{{count}} hours"},xDays:{one:"1 day",other:"{{count}} days"},aboutXMonths:{one:"about 1 month",other:"about {{count}} months"},xMonths:{one:"1 month",other:"{{count}} months"},aboutXYears:{one:"about 1 year",other:"about {{count}} years"},xYears:{one:"1 year",other:"{{count}} years"},overXYears:{one:"over 1 year",other:"over {{count}} years"},almostXYears:{one:"almost 1 year",other:"almost {{count}} years"}};return{localize:function(n,t,a){var i;return a=a||{},i="string"==typeof e[n]?e[n]:1===t?e[n].one:e[n].other.replace("{{count}}",t),a.addSuffix?a.comparison>0?"in "+i:i+" ago":i}}}},function(e,n,t){var a=t(303);e.exports=function(){var e=["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],n=["January","February","March","April","May","June","July","August","September","October","November","December"],t=["Su","Mo","Tu","We","Th","Fr","Sa"],i=["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],r=["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],o=["AM","PM"],s=["am","pm"],c=["a.m.","p.m."],l={MMM:function(n){return e[n.getMonth()]},MMMM:function(e){return n[e.getMonth()]},dd:function(e){return t[e.getDay()]},ddd:function(e){return i[e.getDay()]},dddd:function(e){return r[e.getDay()]},A:function(e){return e.getHours()/12>=1?o[1]:o[0]},a:function(e){return e.getHours()/12>=1?s[1]:s[0]},aa:function(e){return e.getHours()/12>=1?c[1]:c[0]}};return["M","D","DDD","d","Q","W"].forEach((function(e){l[e+"o"]=function(n,t){return function(e){var n=e%100;if(n>20||n<10)switch(n%10){case 1:return e+"st";case 2:return e+"nd";case 3:return e+"rd"}return e+"th"}(t[e](n))}})),{formatters:l,formattingTokensRegExp:a(l)}}},function(e,n){var t=["M","MM","Q","D","DD","DDD","DDDD","d","E","W","WW","YY","YYYY","GG","GGGG","H","HH","h","hh","m","mm","s","ss","S","SS","SSS","Z","ZZ","X","x"];e.exports=function(e){var n=[];for(var a in e)e.hasOwnProperty(a)&&n.push(a);var i=t.concat(n).sort().reverse();return new RegExp("(\\[[^\\[]*\\])|(\\\\)?("+i.join("|")+"|.)","g")}},function(e,n,t){"use strict";var a=t(1),i=t(105),r=t(305),o=t(111);function s(e){var n=new r(e),t=i(r.prototype.request,n);return a.extend(t,r.prototype,n),a.extend(t,n),t}var c=s(t(56));c.Axios=r,c.create=function(e){return s(o(c.defaults,e))},c.Cancel=t(112),c.CancelToken=t(319),c.isCancel=t(110),c.all=function(e){return Promise.all(e)},c.spread=t(320),c.isAxiosError=t(321),e.exports=c,e.exports.default=c},function(e,n,t){"use strict";var a=t(1),i=t(106),r=t(306),o=t(307),s=t(111),c=t(317),l=c.validators;function d(e){this.defaults=e,this.interceptors={request:new r,response:new r}}d.prototype.request=function(e){"string"==typeof e?(e=arguments[1]||{}).url=arguments[0]:e=e||{},(e=s(this.defaults,e)).method?e.method=e.method.toLowerCase():this.defaults.method?e.method=this.defaults.method.toLowerCase():e.method="get";var n=e.transitional;void 0!==n&&c.assertOptions(n,{silentJSONParsing:l.transitional(l.boolean,"1.0.0"),forcedJSONParsing:l.transitional(l.boolean,"1.0.0"),clarifyTimeoutError:l.transitional(l.boolean,"1.0.0")},!1);var t=[],a=!0;this.interceptors.request.forEach((function(n){"function"==typeof n.runWhen&&!1===n.runWhen(e)||(a=a&&n.synchronous,t.unshift(n.fulfilled,n.rejected))}));var i,r=[];if(this.interceptors.response.forEach((function(e){r.push(e.fulfilled,e.rejected)})),!a){var d=[o,void 0];for(Array.prototype.unshift.apply(d,t),d=d.concat(r),i=Promise.resolve(e);d.length;)i=i.then(d.shift(),d.shift());return i}for(var h=e;t.length;){var u=t.shift(),p=t.shift();try{h=u(h)}catch(e){p(e);break}}try{i=o(h)}catch(e){return Promise.reject(e)}for(;r.length;)i=i.then(r.shift(),r.shift());return i},d.prototype.getUri=function(e){return e=s(this.defaults,e),i(e.url,e.params,e.paramsSerializer).replace(/^\?/,"")},a.forEach(["delete","get","head","options"],(function(e){d.prototype[e]=function(n,t){return this.request(s(t||{},{method:e,url:n,data:(t||{}).data}))}})),a.forEach(["post","put","patch"],(function(e){d.prototype[e]=function(n,t,a){return this.request(s(a||{},{method:e,url:n,data:t}))}})),e.exports=d},function(e,n,t){"use strict";var a=t(1);function i(){this.handlers=[]}i.prototype.use=function(e,n,t){return this.handlers.push({fulfilled:e,rejected:n,synchronous:!!t&&t.synchronous,runWhen:t?t.runWhen:null}),this.handlers.length-1},i.prototype.eject=function(e){this.handlers[e]&&(this.handlers[e]=null)},i.prototype.forEach=function(e){a.forEach(this.handlers,(function(n){null!==n&&e(n)}))},e.exports=i},function(e,n,t){"use strict";var a=t(1),i=t(308),r=t(110),o=t(56);function s(e){e.cancelToken&&e.cancelToken.throwIfRequested()}e.exports=function(e){return s(e),e.headers=e.headers||{},e.data=i.call(e,e.data,e.headers,e.transformRequest),e.headers=a.merge(e.headers.common||{},e.headers[e.method]||{},e.headers),a.forEach(["delete","get","head","post","put","patch","common"],(function(n){delete e.headers[n]})),(e.adapter||o.adapter)(e).then((function(n){return s(e),n.data=i.call(e,n.data,n.headers,e.transformResponse),n}),(function(n){return r(n)||(s(e),n&&n.response&&(n.response.data=i.call(e,n.response.data,n.response.headers,e.transformResponse))),Promise.reject(n)}))}},function(e,n,t){"use strict";var a=t(1),i=t(56);e.exports=function(e,n,t){var r=this||i;return a.forEach(t,(function(t){e=t.call(r,e,n)})),e}},function(e,n,t){"use strict";var a=t(1);e.exports=function(e,n){a.forEach(e,(function(t,a){a!==n&&a.toUpperCase()===n.toUpperCase()&&(e[n]=t,delete e[a])}))}},function(e,n,t){"use strict";var a=t(109);e.exports=function(e,n,t){var i=t.config.validateStatus;t.status&&i&&!i(t.status)?n(a("Request failed with status code "+t.status,t.config,null,t.request,t)):e(t)}},function(e,n,t){"use strict";var a=t(1);e.exports=a.isStandardBrowserEnv()?{write:function(e,n,t,i,r,o){var s=[];s.push(e+"="+encodeURIComponent(n)),a.isNumber(t)&&s.push("expires="+new Date(t).toGMTString()),a.isString(i)&&s.push("path="+i),a.isString(r)&&s.push("domain="+r),!0===o&&s.push("secure"),document.cookie=s.join("; ")},read:function(e){var n=document.cookie.match(new RegExp("(^|;\\s*)("+e+")=([^;]*)"));return n?decodeURIComponent(n[3]):null},remove:function(e){this.write(e,"",Date.now()-864e5)}}:{write:function(){},read:function(){return null},remove:function(){}}},function(e,n,t){"use strict";var a=t(313),i=t(314);e.exports=function(e,n){return e&&!a(n)?i(e,n):n}},function(e,n,t){"use strict";e.exports=function(e){return/^([a-z][a-z\d\+\-\.]*:)?\/\//i.test(e)}},function(e,n,t){"use strict";e.exports=function(e,n){return n?e.replace(/\/+$/,"")+"/"+n.replace(/^\/+/,""):e}},function(e,n,t){"use strict";var a=t(1),i=["age","authorization","content-length","content-type","etag","expires","from","host","if-modified-since","if-unmodified-since","last-modified","location","max-forwards","proxy-authorization","referer","retry-after","user-agent"];e.exports=function(e){var n,t,r,o={};return e?(a.forEach(e.split("\n"),(function(e){if(r=e.indexOf(":"),n=a.trim(e.substr(0,r)).toLowerCase(),t=a.trim(e.substr(r+1)),n){if(o[n]&&i.indexOf(n)>=0)return;o[n]="set-cookie"===n?(o[n]?o[n]:[]).concat([t]):o[n]?o[n]+", "+t:t}})),o):o}},function(e,n,t){"use strict";var a=t(1);e.exports=a.isStandardBrowserEnv()?function(){var e,n=/(msie|trident)/i.test(navigator.userAgent),t=document.createElement("a");function i(e){var a=e;return n&&(t.setAttribute("href",a),a=t.href),t.setAttribute("href",a),{href:t.href,protocol:t.protocol?t.protocol.replace(/:$/,""):"",host:t.host,search:t.search?t.search.replace(/^\?/,""):"",hash:t.hash?t.hash.replace(/^#/,""):"",hostname:t.hostname,port:t.port,pathname:"/"===t.pathname.charAt(0)?t.pathname:"/"+t.pathname}}return e=i(window.location.href),function(n){var t=a.isString(n)?i(n):n;return t.protocol===e.protocol&&t.host===e.host}}():function(){return!0}},function(e,n,t){"use strict";var a=t(318),i={};["object","boolean","number","function","string","symbol"].forEach((function(e,n){i[e]=function(t){return typeof t===e||"a"+(n<1?"n ":" ")+e}}));var r={},o=a.version.split(".");function s(e,n){for(var t=n?n.split("."):o,a=e.split("."),i=0;i<3;i++){if(t[i]>a[i])return!0;if(t[i]<a[i])return!1}return!1}i.transitional=function(e,n,t){var i=n&&s(n);function o(e,n){return"[Axios v"+a.version+"] Transitional option '"+e+"'"+n+(t?". "+t:"")}return function(t,a,s){if(!1===e)throw new Error(o(a," has been removed in "+n));return i&&!r[a]&&(r[a]=!0,console.warn(o(a," has been deprecated since v"+n+" and will be removed in the near future"))),!e||e(t,a,s)}},e.exports={isOlderVersion:s,assertOptions:function(e,n,t){if("object"!=typeof e)throw new TypeError("options must be an object");for(var a=Object.keys(e),i=a.length;i-- >0;){var r=a[i],o=n[r];if(o){var s=e[r],c=void 0===s||o(s,r,e);if(!0!==c)throw new TypeError("option "+r+" must be "+c)}else if(!0!==t)throw Error("Unknown option "+r)}},validators:i}},function(e){e.exports=JSON.parse('{"name":"axios","version":"0.21.4","description":"Promise based HTTP client for the browser and node.js","main":"index.js","scripts":{"test":"grunt test","start":"node ./sandbox/server.js","build":"NODE_ENV=production grunt build","preversion":"npm test","version":"npm run build && grunt version && git add -A dist && git add CHANGELOG.md bower.json package.json","postversion":"git push && git push --tags","examples":"node ./examples/server.js","coveralls":"cat coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js","fix":"eslint --fix lib/**/*.js"},"repository":{"type":"git","url":"https://github.com/axios/axios.git"},"keywords":["xhr","http","ajax","promise","node"],"author":"Matt Zabriskie","license":"MIT","bugs":{"url":"https://github.com/axios/axios/issues"},"homepage":"https://axios-http.com","devDependencies":{"coveralls":"^3.0.0","es6-promise":"^4.2.4","grunt":"^1.3.0","grunt-banner":"^0.6.0","grunt-cli":"^1.2.0","grunt-contrib-clean":"^1.1.0","grunt-contrib-watch":"^1.0.0","grunt-eslint":"^23.0.0","grunt-karma":"^4.0.0","grunt-mocha-test":"^0.13.3","grunt-ts":"^6.0.0-beta.19","grunt-webpack":"^4.0.2","istanbul-instrumenter-loader":"^1.0.0","jasmine-core":"^2.4.1","karma":"^6.3.2","karma-chrome-launcher":"^3.1.0","karma-firefox-launcher":"^2.1.0","karma-jasmine":"^1.1.1","karma-jasmine-ajax":"^0.1.13","karma-safari-launcher":"^1.0.0","karma-sauce-launcher":"^4.3.6","karma-sinon":"^1.0.5","karma-sourcemap-loader":"^0.3.8","karma-webpack":"^4.0.2","load-grunt-tasks":"^3.5.2","minimist":"^1.2.0","mocha":"^8.2.1","sinon":"^4.5.0","terser-webpack-plugin":"^4.2.3","typescript":"^4.0.5","url-search-params":"^0.10.0","webpack":"^4.44.2","webpack-dev-server":"^3.11.0"},"browser":{"./lib/adapters/http.js":"./lib/adapters/xhr.js"},"jsdelivr":"dist/axios.min.js","unpkg":"dist/axios.min.js","typings":"./index.d.ts","dependencies":{"follow-redirects":"^1.14.0"},"bundlesize":[{"path":"./dist/axios.min.js","threshold":"5kB"}]}')},function(e,n,t){"use strict";var a=t(112);function i(e){if("function"!=typeof e)throw new TypeError("executor must be a function.");var n;this.promise=new Promise((function(e){n=e}));var t=this;e((function(e){t.reason||(t.reason=new a(e),n(t.reason))}))}i.prototype.throwIfRequested=function(){if(this.reason)throw this.reason},i.source=function(){var e;return{token:new i((function(n){e=n})),cancel:e}},e.exports=i},function(e,n,t){"use strict";e.exports=function(e){return function(n){return e.apply(null,n)}}},function(e,n,t){"use strict";e.exports=function(e){return"object"==typeof e&&!0===e.isAxiosError}},function(e,n){},function(e,n){function t(e,n){for(var t=0,a=e.length-1;a>=0;a--){var i=e[a];"."===i?e.splice(a,1):".."===i?(e.splice(a,1),t++):t&&(e.splice(a,1),t--)}if(n)for(;t--;t)e.unshift("..");return e}function a(e,n){if(e.filter)return e.filter(n);for(var t=[],a=0;a<e.length;a++)n(e[a],a,e)&&t.push(e[a]);return t}n.resolve=function(){for(var e="",n=!1,i=arguments.length-1;i>=-1&&!n;i--){var r=i>=0?arguments[i]:process.cwd();if("string"!=typeof r)throw new TypeError("Arguments to path.resolve must be strings");r&&(e=r+"/"+e,n="/"===r.charAt(0))}return(n?"/":"")+(e=t(a(e.split("/"),(function(e){return!!e})),!n).join("/"))||"."},n.normalize=function(e){var r=n.isAbsolute(e),o="/"===i(e,-1);return(e=t(a(e.split("/"),(function(e){return!!e})),!r).join("/"))||r||(e="."),e&&o&&(e+="/"),(r?"/":"")+e},n.isAbsolute=function(e){return"/"===e.charAt(0)},n.join=function(){var e=Array.prototype.slice.call(arguments,0);return n.normalize(a(e,(function(e,n){if("string"!=typeof e)throw new TypeError("Arguments to path.join must be strings");return e})).join("/"))},n.relative=function(e,t){function a(e){for(var n=0;n<e.length&&""===e[n];n++);for(var t=e.length-1;t>=0&&""===e[t];t--);return n>t?[]:e.slice(n,t-n+1)}e=n.resolve(e).substr(1),t=n.resolve(t).substr(1);for(var i=a(e.split("/")),r=a(t.split("/")),o=Math.min(i.length,r.length),s=o,c=0;c<o;c++)if(i[c]!==r[c]){s=c;break}var l=[];for(c=s;c<i.length;c++)l.push("..");return(l=l.concat(r.slice(s))).join("/")},n.sep="/",n.delimiter=":",n.dirname=function(e){if("string"!=typeof e&&(e+=""),0===e.length)return".";for(var n=e.charCodeAt(0),t=47===n,a=-1,i=!0,r=e.length-1;r>=1;--r)if(47===(n=e.charCodeAt(r))){if(!i){a=r;break}}else i=!1;return-1===a?t?"/":".":t&&1===a?"/":e.slice(0,a)},n.basename=function(e,n){var t=function(e){"string"!=typeof e&&(e+="");var n,t=0,a=-1,i=!0;for(n=e.length-1;n>=0;--n)if(47===e.charCodeAt(n)){if(!i){t=n+1;break}}else-1===a&&(i=!1,a=n+1);return-1===a?"":e.slice(t,a)}(e);return n&&t.substr(-1*n.length)===n&&(t=t.substr(0,t.length-n.length)),t},n.extname=function(e){"string"!=typeof e&&(e+="");for(var n=-1,t=0,a=-1,i=!0,r=0,o=e.length-1;o>=0;--o){var s=e.charCodeAt(o);if(47!==s)-1===a&&(i=!1,a=o+1),46===s?-1===n?n=o:1!==r&&(r=1):-1!==n&&(r=-1);else if(!i){t=o+1;break}}return-1===n||-1===a||0===r||1===r&&n===a-1&&n===t+1?"":e.slice(n,a)};var i="b"==="ab".substr(-1)?function(e,n,t){return e.substr(n,t)}:function(e,n,t){return n<0&&(n=e.length+n),e.substr(n,t)}},function(e,n,t){"use strict";var a=/[|\\{}()[\]^$+*?.]/g,i=Object.prototype.hasOwnProperty,r=function(e,n){return i.apply(e,[n])};n.escapeRegExpChars=function(e){return e?String(e).replace(a,"\\$&"):""};var o={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&#34;","'":"&#39;"},s=/[&<>'"]/g;function c(e){return o[e]||e}function l(){return Function.prototype.toString.call(this)+';\nvar _ENCODE_HTML_RULES = {\n      "&": "&amp;"\n    , "<": "&lt;"\n    , ">": "&gt;"\n    , \'"\': "&#34;"\n    , "\'": "&#39;"\n    }\n  , _MATCH_HTML = /[&<>\'"]/g;\nfunction encode_char(c) {\n  return _ENCODE_HTML_RULES[c] || c;\n};\n'}n.escapeXML=function(e){return null==e?"":String(e).replace(s,c)};try{"function"==typeof Object.defineProperty?Object.defineProperty(n.escapeXML,"toString",{value:l}):n.escapeXML.toString=l}catch(e){console.warn("Unable to set escapeXML.toString (is the Function prototype frozen?)")}n.shallowCopy=function(e,n){if(n=n||{},null!=e)for(var t in n)r(n,t)&&"__proto__"!==t&&"constructor"!==t&&(e[t]=n[t]);return e},n.shallowCopyFromList=function(e,n,t){if(t=t||[],n=n||{},null!=e)for(var a=0;a<t.length;a++){var i=t[a];if(void 0!==n[i]){if(!r(n,i))continue;if("__proto__"===i||"constructor"===i)continue;e[i]=n[i]}}return e},n.cache={_data:{},set:function(e,n){this._data[e]=n},get:function(e){return this._data[e]},remove:function(e){delete this._data[e]},reset:function(){this._data={}}},n.hyphenToCamel=function(e){return e.replace(/-[a-z]/g,(function(e){return e[1].toUpperCase()}))},n.createNullProtoObjWherePossible="function"==typeof Object.create?function(){return Object.create(null)}:{__proto__:null}instanceof Object?function(){return{}}:function(){return{__proto__:null}},n.hasOwnOnlyObject=function(e){var t=n.createNullProtoObjWherePossible();for(var a in e)r(e,a)&&(t[a]=e[a]);return t}},function(e){e.exports=JSON.parse('{"name":"ejs","description":"Embedded JavaScript templates","keywords":["template","engine","ejs"],"version":"3.1.10","author":"Matthew Eernisse <mde@fleegix.org> (http://fleegix.org)","license":"Apache-2.0","bin":{"ejs":"./bin/cli.js"},"main":"./lib/ejs.js","jsdelivr":"ejs.min.js","unpkg":"ejs.min.js","repository":{"type":"git","url":"git://github.com/mde/ejs.git"},"bugs":"https://github.com/mde/ejs/issues","homepage":"https://github.com/mde/ejs","dependencies":{"jake":"^10.8.5"},"devDependencies":{"browserify":"^16.5.1","eslint":"^6.8.0","git-directory-deploy":"^1.5.1","jsdoc":"^4.0.2","lru-cache":"^4.0.1","mocha":"^10.2.0","uglify-js":"^3.3.16"},"engines":{"node":">=0.10.0"},"scripts":{"test":"npx jake test"}}')},function(e,n,t){},function(e,n,t){"use strict";t(113)},function(e,n,t){"use strict";t(114)},function(e,n,t){"use strict";t.r(n);
/*!
 * Vue.js v2.7.16
 * (c) 2014-2023 Evan You
 * Released under the MIT License.
 */
var a=Object.freeze({}),i=Array.isArray;function r(e){return null==e}function o(e){return null!=e}function s(e){return!0===e}function c(e){return"string"==typeof e||"number"==typeof e||"symbol"==typeof e||"boolean"==typeof e}function l(e){return"function"==typeof e}function d(e){return null!==e&&"object"==typeof e}var h=Object.prototype.toString;function u(e){return"[object Object]"===h.call(e)}function p(e){return"[object RegExp]"===h.call(e)}function m(e){var n=parseFloat(String(e));return n>=0&&Math.floor(n)===n&&isFinite(e)}function f(e){return o(e)&&"function"==typeof e.then&&"function"==typeof e.catch}function g(e){return null==e?"":Array.isArray(e)||u(e)&&e.toString===h?JSON.stringify(e,y,2):String(e)}function y(e,n){return n&&n.__v_isRef?n.value:n}function b(e){var n=parseFloat(e);return isNaN(n)?e:n}function v(e,n){for(var t=Object.create(null),a=e.split(","),i=0;i<a.length;i++)t[a[i]]=!0;return n?function(e){return t[e.toLowerCase()]}:function(e){return t[e]}}v("slot,component",!0);var w=v("key,ref,slot,slot-scope,is");function _(e,n){var t=e.length;if(t){if(n===e[t-1])return void(e.length=t-1);var a=e.indexOf(n);if(a>-1)return e.splice(a,1)}}var k=Object.prototype.hasOwnProperty;function x(e,n){return k.call(e,n)}function C(e){var n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}var P=/-(\w)/g,T=C((function(e){return e.replace(P,(function(e,n){return n?n.toUpperCase():""}))})),M=C((function(e){return e.charAt(0).toUpperCase()+e.slice(1)})),A=/\B([A-Z])/g,S=C((function(e){return e.replace(A,"-$1").toLowerCase()}));var I=Function.prototype.bind?function(e,n){return e.bind(n)}:function(e,n){function t(t){var a=arguments.length;return a?a>1?e.apply(n,arguments):e.call(n,t):e.call(n)}return t._length=e.length,t};function L(e,n){n=n||0;for(var t=e.length-n,a=new Array(t);t--;)a[t]=e[t+n];return a}function D(e,n){for(var t in n)e[t]=n[t];return e}function z(e){for(var n={},t=0;t<e.length;t++)e[t]&&D(n,e[t]);return n}function U(e,n,t){}var O=function(e,n,t){return!1},R=function(e){return e};function E(e,n){if(e===n)return!0;var t=d(e),a=d(n);if(!t||!a)return!t&&!a&&String(e)===String(n);try{var i=Array.isArray(e),r=Array.isArray(n);if(i&&r)return e.length===n.length&&e.every((function(e,t){return E(e,n[t])}));if(e instanceof Date&&n instanceof Date)return e.getTime()===n.getTime();if(i||r)return!1;var o=Object.keys(e),s=Object.keys(n);return o.length===s.length&&o.every((function(t){return E(e[t],n[t])}))}catch(e){return!1}}function G(e,n){for(var t=0;t<e.length;t++)if(E(e[t],n))return t;return-1}function q(e){var n=!1;return function(){n||(n=!0,e.apply(this,arguments))}}function B(e,n){return e===n?0===e&&1/e!=1/n:e==e||n==n}var j=["component","directive","filter"],N=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],$={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:O,isReservedAttr:O,isUnknownElement:O,getTagNamespace:U,parsePlatformTagName:R,mustUseProp:O,async:!0,_lifecycleHooks:N},F=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function V(e){var n=(e+"").charCodeAt(0);return 36===n||95===n}function H(e,n,t,a){Object.defineProperty(e,n,{value:t,enumerable:!!a,writable:!0,configurable:!0})}var W=new RegExp("[^".concat(F.source,".$_\\d]"));var K="__proto__"in{},Z="undefined"!=typeof window,Y=Z&&window.navigator.userAgent.toLowerCase(),X=Y&&/msie|trident/.test(Y),Q=Y&&Y.indexOf("msie 9.0")>0,J=Y&&Y.indexOf("edge/")>0;Y&&Y.indexOf("android");var ee=Y&&/iphone|ipad|ipod|ios/.test(Y);Y&&/chrome\/\d+/.test(Y),Y&&/phantomjs/.test(Y);var ne,te=Y&&Y.match(/firefox\/(\d+)/),ae={}.watch,ie=!1;if(Z)try{var re={};Object.defineProperty(re,"passive",{get:function(){ie=!0}}),window.addEventListener("test-passive",null,re)}catch(e){}var oe=function(){return void 0===ne&&(ne=!Z&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),ne},se=Z&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ce(e){return"function"==typeof e&&/native code/.test(e.toString())}var le,de="undefined"!=typeof Symbol&&ce(Symbol)&&"undefined"!=typeof Reflect&&ce(Reflect.ownKeys);le="undefined"!=typeof Set&&ce(Set)?Set:function(){function e(){this.set=Object.create(null)}return e.prototype.has=function(e){return!0===this.set[e]},e.prototype.add=function(e){this.set[e]=!0},e.prototype.clear=function(){this.set=Object.create(null)},e}();var he=null;function ue(e){void 0===e&&(e=null),e||he&&he._scope.off(),he=e,e&&e._scope.on()}var pe=function(){function e(e,n,t,a,i,r,o,s){this.tag=e,this.data=n,this.children=t,this.text=a,this.elm=i,this.ns=void 0,this.context=r,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=n&&n.key,this.componentOptions=o,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(e.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),e}(),me=function(e){void 0===e&&(e="");var n=new pe;return n.text=e,n.isComment=!0,n};function fe(e){return new pe(void 0,void 0,void 0,String(e))}function ge(e){var n=new pe(e.tag,e.data,e.children&&e.children.slice(),e.text,e.elm,e.context,e.componentOptions,e.asyncFactory);return n.ns=e.ns,n.isStatic=e.isStatic,n.key=e.key,n.isComment=e.isComment,n.fnContext=e.fnContext,n.fnOptions=e.fnOptions,n.fnScopeId=e.fnScopeId,n.asyncMeta=e.asyncMeta,n.isCloned=!0,n}"function"==typeof SuppressedError&&SuppressedError;var ye=0,be=[],ve=function(){function e(){this._pending=!1,this.id=ye++,this.subs=[]}return e.prototype.addSub=function(e){this.subs.push(e)},e.prototype.removeSub=function(e){this.subs[this.subs.indexOf(e)]=null,this._pending||(this._pending=!0,be.push(this))},e.prototype.depend=function(n){e.target&&e.target.addDep(this)},e.prototype.notify=function(e){var n=this.subs.filter((function(e){return e}));for(var t=0,a=n.length;t<a;t++){0,n[t].update()}},e}();ve.target=null;var we=[];function _e(e){we.push(e),ve.target=e}function ke(){we.pop(),ve.target=we[we.length-1]}var xe=Array.prototype,Ce=Object.create(xe);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(e){var n=xe[e];H(Ce,e,(function(){for(var t=[],a=0;a<arguments.length;a++)t[a]=arguments[a];var i,r=n.apply(this,t),o=this.__ob__;switch(e){case"push":case"unshift":i=t;break;case"splice":i=t.slice(2)}return i&&o.observeArray(i),o.dep.notify(),r}))}));var Pe=Object.getOwnPropertyNames(Ce),Te={},Me=!0;function Ae(e){Me=e}var Se={notify:U,depend:U,addSub:U,removeSub:U},Ie=function(){function e(e,n,t){if(void 0===n&&(n=!1),void 0===t&&(t=!1),this.value=e,this.shallow=n,this.mock=t,this.dep=t?Se:new ve,this.vmCount=0,H(e,"__ob__",this),i(e)){if(!t)if(K)e.__proto__=Ce;else for(var a=0,r=Pe.length;a<r;a++){H(e,s=Pe[a],Ce[s])}n||this.observeArray(e)}else{var o=Object.keys(e);for(a=0;a<o.length;a++){var s;De(e,s=o[a],Te,void 0,n,t)}}}return e.prototype.observeArray=function(e){for(var n=0,t=e.length;n<t;n++)Le(e[n],!1,this.mock)},e}();function Le(e,n,t){return e&&x(e,"__ob__")&&e.__ob__ instanceof Ie?e.__ob__:!Me||!t&&oe()||!i(e)&&!u(e)||!Object.isExtensible(e)||e.__v_skip||qe(e)||e instanceof pe?void 0:new Ie(e,n,t)}function De(e,n,t,a,r,o,s){void 0===s&&(s=!1);var c=new ve,l=Object.getOwnPropertyDescriptor(e,n);if(!l||!1!==l.configurable){var d=l&&l.get,h=l&&l.set;d&&!h||t!==Te&&2!==arguments.length||(t=e[n]);var u=r?t&&t.__ob__:Le(t,!1,o);return Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){var n=d?d.call(e):t;return ve.target&&(c.depend(),u&&(u.dep.depend(),i(n)&&Oe(n))),qe(n)&&!r?n.value:n},set:function(n){var a=d?d.call(e):t;if(B(a,n)){if(h)h.call(e,n);else{if(d)return;if(!r&&qe(a)&&!qe(n))return void(a.value=n);t=n}u=r?n&&n.__ob__:Le(n,!1,o),c.notify()}}}),c}}function ze(e,n,t){if(!Ge(e)){var a=e.__ob__;return i(e)&&m(n)?(e.length=Math.max(e.length,n),e.splice(n,1,t),a&&!a.shallow&&a.mock&&Le(t,!1,!0),t):n in e&&!(n in Object.prototype)?(e[n]=t,t):e._isVue||a&&a.vmCount?t:a?(De(a.value,n,t,void 0,a.shallow,a.mock),a.dep.notify(),t):(e[n]=t,t)}}function Ue(e,n){if(i(e)&&m(n))e.splice(n,1);else{var t=e.__ob__;e._isVue||t&&t.vmCount||Ge(e)||x(e,n)&&(delete e[n],t&&t.dep.notify())}}function Oe(e){for(var n=void 0,t=0,a=e.length;t<a;t++)(n=e[t])&&n.__ob__&&n.__ob__.dep.depend(),i(n)&&Oe(n)}function Re(e){return Ee(e,!0),H(e,"__v_isShallow",!0),e}function Ee(e,n){if(!Ge(e)){Le(e,n,oe());0}}function Ge(e){return!(!e||!e.__v_isReadonly)}function qe(e){return!(!e||!0!==e.__v_isRef)}function Be(e,n,t){Object.defineProperty(e,t,{enumerable:!0,configurable:!0,get:function(){var e=n[t];if(qe(e))return e.value;var a=e&&e.__ob__;return a&&a.dep.depend(),e},set:function(e){var a=n[t];qe(a)&&!qe(e)?a.value=e:n[t]=e}})}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var je;var Ne=function(){function e(e){void 0===e&&(e=!1),this.detached=e,this.active=!0,this.effects=[],this.cleanups=[],this.parent=je,!e&&je&&(this.index=(je.scopes||(je.scopes=[])).push(this)-1)}return e.prototype.run=function(e){if(this.active){var n=je;try{return je=this,e()}finally{je=n}}else 0},e.prototype.on=function(){je=this},e.prototype.off=function(){je=this.parent},e.prototype.stop=function(e){if(this.active){var n=void 0,t=void 0;for(n=0,t=this.effects.length;n<t;n++)this.effects[n].teardown();for(n=0,t=this.cleanups.length;n<t;n++)this.cleanups[n]();if(this.scopes)for(n=0,t=this.scopes.length;n<t;n++)this.scopes[n].stop(!0);if(!this.detached&&this.parent&&!e){var a=this.parent.scopes.pop();a&&a!==this&&(this.parent.scopes[this.index]=a,a.index=this.index)}this.parent=void 0,this.active=!1}},e}();function $e(e){var n=e._provided,t=e.$parent&&e.$parent._provided;return t===n?e._provided=Object.create(t):n}var Fe=C((function(e){var n="&"===e.charAt(0),t="~"===(e=n?e.slice(1):e).charAt(0),a="!"===(e=t?e.slice(1):e).charAt(0);return{name:e=a?e.slice(1):e,once:t,capture:a,passive:n}}));function Ve(e,n){function t(){var e=t.fns;if(!i(e))return An(e,null,arguments,n,"v-on handler");for(var a=e.slice(),r=0;r<a.length;r++)An(a[r],null,arguments,n,"v-on handler")}return t.fns=e,t}function He(e,n,t,a,i,o){var c,l,d,h;for(c in e)l=e[c],d=n[c],h=Fe(c),r(l)||(r(d)?(r(l.fns)&&(l=e[c]=Ve(l,o)),s(h.once)&&(l=e[c]=i(h.name,l,h.capture)),t(h.name,l,h.capture,h.passive,h.params)):l!==d&&(d.fns=l,e[c]=d));for(c in n)r(e[c])&&a((h=Fe(c)).name,n[c],h.capture)}function We(e,n,t){var a;e instanceof pe&&(e=e.data.hook||(e.data.hook={}));var i=e[n];function c(){t.apply(this,arguments),_(a.fns,c)}r(i)?a=Ve([c]):o(i.fns)&&s(i.merged)?(a=i).fns.push(c):a=Ve([i,c]),a.merged=!0,e[n]=a}function Ke(e,n,t,a,i){if(o(n)){if(x(n,t))return e[t]=n[t],i||delete n[t],!0;if(x(n,a))return e[t]=n[a],i||delete n[a],!0}return!1}function Ze(e){return c(e)?[fe(e)]:i(e)?function e(n,t){var a,l,d,h,u=[];for(a=0;a<n.length;a++)r(l=n[a])||"boolean"==typeof l||(d=u.length-1,h=u[d],i(l)?l.length>0&&(Ye((l=e(l,"".concat(t||"","_").concat(a)))[0])&&Ye(h)&&(u[d]=fe(h.text+l[0].text),l.shift()),u.push.apply(u,l)):c(l)?Ye(h)?u[d]=fe(h.text+l):""!==l&&u.push(fe(l)):Ye(l)&&Ye(h)?u[d]=fe(h.text+l.text):(s(n._isVList)&&o(l.tag)&&r(l.key)&&o(t)&&(l.key="__vlist".concat(t,"_").concat(a,"__")),u.push(l)));return u}(e):void 0}function Ye(e){return o(e)&&o(e.text)&&!1===e.isComment}function Xe(e,n){var t,a,r,s,c=null;if(i(e)||"string"==typeof e)for(c=new Array(e.length),t=0,a=e.length;t<a;t++)c[t]=n(e[t],t);else if("number"==typeof e)for(c=new Array(e),t=0;t<e;t++)c[t]=n(t+1,t);else if(d(e))if(de&&e[Symbol.iterator]){c=[];for(var l=e[Symbol.iterator](),h=l.next();!h.done;)c.push(n(h.value,c.length)),h=l.next()}else for(r=Object.keys(e),c=new Array(r.length),t=0,a=r.length;t<a;t++)s=r[t],c[t]=n(e[s],s,t);return o(c)||(c=[]),c._isVList=!0,c}function Qe(e,n,t,a){var i,r=this.$scopedSlots[e];r?(t=t||{},a&&(t=D(D({},a),t)),i=r(t)||(l(n)?n():n)):i=this.$slots[e]||(l(n)?n():n);var o=t&&t.slot;return o?this.$createElement("template",{slot:o},i):i}function Je(e){return Lt(this.$options,"filters",e,!0)||R}function en(e,n){return i(e)?-1===e.indexOf(n):e!==n}function nn(e,n,t,a,i){var r=$.keyCodes[n]||t;return i&&a&&!$.keyCodes[n]?en(i,a):r?en(r,e):a?S(a)!==n:void 0===e}function tn(e,n,t,a,r){if(t)if(d(t)){i(t)&&(t=z(t));var o=void 0,s=function(i){if("class"===i||"style"===i||w(i))o=e;else{var s=e.attrs&&e.attrs.type;o=a||$.mustUseProp(n,s,i)?e.domProps||(e.domProps={}):e.attrs||(e.attrs={})}var c=T(i),l=S(i);c in o||l in o||(o[i]=t[i],r&&((e.on||(e.on={}))["update:".concat(i)]=function(e){t[i]=e}))};for(var c in t)s(c)}else;return e}function an(e,n){var t=this._staticTrees||(this._staticTrees=[]),a=t[e];return a&&!n||on(a=t[e]=this.$options.staticRenderFns[e].call(this._renderProxy,this._c,this),"__static__".concat(e),!1),a}function rn(e,n,t){return on(e,"__once__".concat(n).concat(t?"_".concat(t):""),!0),e}function on(e,n,t){if(i(e))for(var a=0;a<e.length;a++)e[a]&&"string"!=typeof e[a]&&sn(e[a],"".concat(n,"_").concat(a),t);else sn(e,n,t)}function sn(e,n,t){e.isStatic=!0,e.key=n,e.isOnce=t}function cn(e,n){if(n)if(u(n)){var t=e.on=e.on?D({},e.on):{};for(var a in n){var i=t[a],r=n[a];t[a]=i?[].concat(i,r):r}}else;return e}function ln(e,n,t,a){n=n||{$stable:!t};for(var r=0;r<e.length;r++){var o=e[r];i(o)?ln(o,n,t):o&&(o.proxy&&(o.fn.proxy=!0),n[o.key]=o.fn)}return a&&(n.$key=a),n}function dn(e,n){for(var t=0;t<n.length;t+=2){var a=n[t];"string"==typeof a&&a&&(e[n[t]]=n[t+1])}return e}function hn(e,n){return"string"==typeof e?n+e:e}function un(e){e._o=rn,e._n=b,e._s=g,e._l=Xe,e._t=Qe,e._q=E,e._i=G,e._m=an,e._f=Je,e._k=nn,e._b=tn,e._v=fe,e._e=me,e._u=ln,e._g=cn,e._d=dn,e._p=hn}function pn(e,n){if(!e||!e.length)return{};for(var t={},a=0,i=e.length;a<i;a++){var r=e[a],o=r.data;if(o&&o.attrs&&o.attrs.slot&&delete o.attrs.slot,r.context!==n&&r.fnContext!==n||!o||null==o.slot)(t.default||(t.default=[])).push(r);else{var s=o.slot,c=t[s]||(t[s]=[]);"template"===r.tag?c.push.apply(c,r.children||[]):c.push(r)}}for(var l in t)t[l].every(mn)&&delete t[l];return t}function mn(e){return e.isComment&&!e.asyncFactory||" "===e.text}function fn(e){return e.isComment&&e.asyncFactory}function gn(e,n,t,i){var r,o=Object.keys(t).length>0,s=n?!!n.$stable:!o,c=n&&n.$key;if(n){if(n._normalized)return n._normalized;if(s&&i&&i!==a&&c===i.$key&&!o&&!i.$hasNormal)return i;for(var l in r={},n)n[l]&&"$"!==l[0]&&(r[l]=yn(e,t,l,n[l]))}else r={};for(var d in t)d in r||(r[d]=bn(t,d));return n&&Object.isExtensible(n)&&(n._normalized=r),H(r,"$stable",s),H(r,"$key",c),H(r,"$hasNormal",o),r}function yn(e,n,t,a){var r=function(){var n=he;ue(e);var t=arguments.length?a.apply(null,arguments):a({}),r=(t=t&&"object"==typeof t&&!i(t)?[t]:Ze(t))&&t[0];return ue(n),t&&(!r||1===t.length&&r.isComment&&!fn(r))?void 0:t};return a.proxy&&Object.defineProperty(n,t,{get:r,enumerable:!0,configurable:!0}),r}function bn(e,n){return function(){return e[n]}}function vn(e){return{get attrs(){if(!e._attrsProxy){var n=e._attrsProxy={};H(n,"_v_attr_proxy",!0),wn(n,e.$attrs,a,e,"$attrs")}return e._attrsProxy},get listeners(){e._listenersProxy||wn(e._listenersProxy={},e.$listeners,a,e,"$listeners");return e._listenersProxy},get slots(){return function(e){e._slotsProxy||kn(e._slotsProxy={},e.$scopedSlots);return e._slotsProxy}(e)},emit:I(e.$emit,e),expose:function(n){n&&Object.keys(n).forEach((function(t){return Be(e,n,t)}))}}}function wn(e,n,t,a,i){var r=!1;for(var o in n)o in e?n[o]!==t[o]&&(r=!0):(r=!0,_n(e,o,a,i));for(var o in e)o in n||(r=!0,delete e[o]);return r}function _n(e,n,t,a){Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){return t[a][n]}})}function kn(e,n){for(var t in n)e[t]=n[t];for(var t in e)t in n||delete e[t]}var xn=null;function Cn(e,n){return(e.__esModule||de&&"Module"===e[Symbol.toStringTag])&&(e=e.default),d(e)?n.extend(e):e}function Pn(e){if(i(e))for(var n=0;n<e.length;n++){var t=e[n];if(o(t)&&(o(t.componentOptions)||fn(t)))return t}}function Tn(e,n,t,a,h,u){return(i(t)||c(t))&&(h=a,a=t,t=void 0),s(u)&&(h=2),function(e,n,t,a,c){if(o(t)&&o(t.__ob__))return me();o(t)&&o(t.is)&&(n=t.is);if(!n)return me();0;i(a)&&l(a[0])&&((t=t||{}).scopedSlots={default:a[0]},a.length=0);2===c?a=Ze(a):1===c&&(a=function(e){for(var n=0;n<e.length;n++)if(i(e[n]))return Array.prototype.concat.apply([],e);return e}(a));var h,u;if("string"==typeof n){var p=void 0;u=e.$vnode&&e.$vnode.ns||$.getTagNamespace(n),h=$.isReservedTag(n)?new pe($.parsePlatformTagName(n),t,a,void 0,void 0,e):t&&t.pre||!o(p=Lt(e.$options,"components",n))?new pe(n,t,a,void 0,void 0,e):_t(p,t,e,a,n)}else h=_t(n,t,e,a);return i(h)?h:o(h)?(o(u)&&function e(n,t,a){n.ns=t,"foreignObject"===n.tag&&(t=void 0,a=!0);if(o(n.children))for(var i=0,c=n.children.length;i<c;i++){var l=n.children[i];o(l.tag)&&(r(l.ns)||s(a)&&"svg"!==l.tag)&&e(l,t,a)}}(h,u),o(t)&&function(e){d(e.style)&&$n(e.style);d(e.class)&&$n(e.class)}(t),h):me()}(e,n,t,a,h)}function Mn(e,n,t){_e();try{if(n)for(var a=n;a=a.$parent;){var i=a.$options.errorCaptured;if(i)for(var r=0;r<i.length;r++)try{if(!1===i[r].call(a,e,n,t))return}catch(e){Sn(e,a,"errorCaptured hook")}}Sn(e,n,t)}finally{ke()}}function An(e,n,t,a,i){var r;try{(r=t?e.apply(n,t):e.call(n))&&!r._isVue&&f(r)&&!r._handled&&(r.catch((function(e){return Mn(e,a,i+" (Promise/async)")})),r._handled=!0)}catch(e){Mn(e,a,i)}return r}function Sn(e,n,t){if($.errorHandler)try{return $.errorHandler.call(null,e,n,t)}catch(n){n!==e&&In(n,null,"config.errorHandler")}In(e,n,t)}function In(e,n,t){if(!Z||"undefined"==typeof console)throw e;console.error(e)}var Ln,Dn=!1,zn=[],Un=!1;function On(){Un=!1;var e=zn.slice(0);zn.length=0;for(var n=0;n<e.length;n++)e[n]()}if("undefined"!=typeof Promise&&ce(Promise)){var Rn=Promise.resolve();Ln=function(){Rn.then(On),ee&&setTimeout(U)},Dn=!0}else if(X||"undefined"==typeof MutationObserver||!ce(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Ln="undefined"!=typeof setImmediate&&ce(setImmediate)?function(){setImmediate(On)}:function(){setTimeout(On,0)};else{var En=1,Gn=new MutationObserver(On),qn=document.createTextNode(String(En));Gn.observe(qn,{characterData:!0}),Ln=function(){En=(En+1)%2,qn.data=String(En)},Dn=!0}function Bn(e,n){var t;if(zn.push((function(){if(e)try{e.call(n)}catch(e){Mn(e,n,"nextTick")}else t&&t(n)})),Un||(Un=!0,Ln()),!e&&"undefined"!=typeof Promise)return new Promise((function(e){t=e}))}function jn(e){return function(n,t){if(void 0===t&&(t=he),t)return function(e,n,t){var a=e.$options;a[n]=Mt(a[n],t)}(t,e,n)}}jn("beforeMount"),jn("mounted"),jn("beforeUpdate"),jn("updated"),jn("beforeDestroy"),jn("destroyed"),jn("activated"),jn("deactivated"),jn("serverPrefetch"),jn("renderTracked"),jn("renderTriggered"),jn("errorCaptured");var Nn=new le;function $n(e){return function e(n,t){var a,r,o=i(n);if(!o&&!d(n)||n.__v_skip||Object.isFrozen(n)||n instanceof pe)return;if(n.__ob__){var s=n.__ob__.dep.id;if(t.has(s))return;t.add(s)}if(o)for(a=n.length;a--;)e(n[a],t);else if(qe(n))e(n.value,t);else for(r=Object.keys(n),a=r.length;a--;)e(n[r[a]],t)}(e,Nn),Nn.clear(),e}var Fn,Vn=0,Hn=function(){function e(e,n,t,a,i){var r,o;r=this,void 0===(o=je&&!je._vm?je:e?e._scope:void 0)&&(o=je),o&&o.active&&o.effects.push(r),(this.vm=e)&&i&&(e._watcher=this),a?(this.deep=!!a.deep,this.user=!!a.user,this.lazy=!!a.lazy,this.sync=!!a.sync,this.before=a.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++Vn,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new le,this.newDepIds=new le,this.expression="",l(n)?this.getter=n:(this.getter=function(e){if(!W.test(e)){var n=e.split(".");return function(e){for(var t=0;t<n.length;t++){if(!e)return;e=e[n[t]]}return e}}}(n),this.getter||(this.getter=U)),this.value=this.lazy?void 0:this.get()}return e.prototype.get=function(){var e;_e(this);var n=this.vm;try{e=this.getter.call(n,n)}catch(e){if(!this.user)throw e;Mn(e,n,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&$n(e),ke(),this.cleanupDeps()}return e},e.prototype.addDep=function(e){var n=e.id;this.newDepIds.has(n)||(this.newDepIds.add(n),this.newDeps.push(e),this.depIds.has(n)||e.addSub(this))},e.prototype.cleanupDeps=function(){for(var e=this.deps.length;e--;){var n=this.deps[e];this.newDepIds.has(n.id)||n.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},e.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():pt(this)},e.prototype.run=function(){if(this.active){var e=this.get();if(e!==this.value||d(e)||this.deep){var n=this.value;if(this.value=e,this.user){var t='callback for watcher "'.concat(this.expression,'"');An(this.cb,this.vm,[e,n],this.vm,t)}else this.cb.call(this.vm,e,n)}}},e.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},e.prototype.depend=function(){for(var e=this.deps.length;e--;)this.deps[e].depend()},e.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&_(this.vm._scope.effects,this),this.active){for(var e=this.deps.length;e--;)this.deps[e].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},e}();function Wn(e,n){Fn.$on(e,n)}function Kn(e,n){Fn.$off(e,n)}function Zn(e,n){var t=Fn;return function a(){var i=n.apply(null,arguments);null!==i&&t.$off(e,a)}}function Yn(e,n,t){Fn=e,He(n,t||{},Wn,Kn,Zn,e),Fn=void 0}var Xn=null;function Qn(e){var n=Xn;return Xn=e,function(){Xn=n}}function Jn(e){for(;e&&(e=e.$parent);)if(e._inactive)return!0;return!1}function et(e,n){if(n){if(e._directInactive=!1,Jn(e))return}else if(e._directInactive)return;if(e._inactive||null===e._inactive){e._inactive=!1;for(var t=0;t<e.$children.length;t++)et(e.$children[t]);nt(e,"activated")}}function nt(e,n,t,a){void 0===a&&(a=!0),_e();var i=he,r=je;a&&ue(e);var o=e.$options[n],s="".concat(n," hook");if(o)for(var c=0,l=o.length;c<l;c++)An(o[c],e,t||null,e,s);e._hasHookEvent&&e.$emit("hook:"+n),a&&(ue(i),r&&r.on()),ke()}var tt=[],at=[],it={},rt=!1,ot=!1,st=0;var ct=0,lt=Date.now;if(Z&&!X){var dt=window.performance;dt&&"function"==typeof dt.now&&lt()>document.createEvent("Event").timeStamp&&(lt=function(){return dt.now()})}var ht=function(e,n){if(e.post){if(!n.post)return 1}else if(n.post)return-1;return e.id-n.id};function ut(){var e,n;for(ct=lt(),ot=!0,tt.sort(ht),st=0;st<tt.length;st++)(e=tt[st]).before&&e.before(),n=e.id,it[n]=null,e.run();var t=at.slice(),a=tt.slice();st=tt.length=at.length=0,it={},rt=ot=!1,function(e){for(var n=0;n<e.length;n++)e[n]._inactive=!0,et(e[n],!0)}(t),function(e){var n=e.length;for(;n--;){var t=e[n],a=t.vm;a&&a._watcher===t&&a._isMounted&&!a._isDestroyed&&nt(a,"updated")}}(a),function(){for(var e=0;e<be.length;e++){var n=be[e];n.subs=n.subs.filter((function(e){return e})),n._pending=!1}be.length=0}(),se&&$.devtools&&se.emit("flush")}function pt(e){var n=e.id;if(null==it[n]&&(e!==ve.target||!e.noRecurse)){if(it[n]=!0,ot){for(var t=tt.length-1;t>st&&tt[t].id>e.id;)t--;tt.splice(t+1,0,e)}else tt.push(e);rt||(rt=!0,Bn(ut))}}function mt(e,n){if(e){for(var t=Object.create(null),a=de?Reflect.ownKeys(e):Object.keys(e),i=0;i<a.length;i++){var r=a[i];if("__ob__"!==r){var o=e[r].from;if(o in n._provided)t[r]=n._provided[o];else if("default"in e[r]){var s=e[r].default;t[r]=l(s)?s.call(n):s}else 0}}return t}}function ft(e,n,t,r,o){var c,l=this,d=o.options;x(r,"_uid")?(c=Object.create(r))._original=r:(c=r,r=r._original);var h=s(d._compiled),u=!h;this.data=e,this.props=n,this.children=t,this.parent=r,this.listeners=e.on||a,this.injections=mt(d.inject,r),this.slots=function(){return l.$slots||gn(r,e.scopedSlots,l.$slots=pn(t,r)),l.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return gn(r,e.scopedSlots,this.slots())}}),h&&(this.$options=d,this.$slots=this.slots(),this.$scopedSlots=gn(r,e.scopedSlots,this.$slots)),d._scopeId?this._c=function(e,n,t,a){var o=Tn(c,e,n,t,a,u);return o&&!i(o)&&(o.fnScopeId=d._scopeId,o.fnContext=r),o}:this._c=function(e,n,t,a){return Tn(c,e,n,t,a,u)}}function gt(e,n,t,a,i){var r=ge(e);return r.fnContext=t,r.fnOptions=a,n.slot&&((r.data||(r.data={})).slot=n.slot),r}function yt(e,n){for(var t in n)e[T(t)]=n[t]}function bt(e){return e.name||e.__name||e._componentTag}un(ft.prototype);var vt={init:function(e,n){if(e.componentInstance&&!e.componentInstance._isDestroyed&&e.data.keepAlive){var t=e;vt.prepatch(t,t)}else{(e.componentInstance=function(e,n){var t={_isComponent:!0,_parentVnode:e,parent:n},a=e.data.inlineTemplate;o(a)&&(t.render=a.render,t.staticRenderFns=a.staticRenderFns);return new e.componentOptions.Ctor(t)}(e,Xn)).$mount(n?e.elm:void 0,n)}},prepatch:function(e,n){var t=n.componentOptions;!function(e,n,t,i,r){var o=i.data.scopedSlots,s=e.$scopedSlots,c=!!(o&&!o.$stable||s!==a&&!s.$stable||o&&e.$scopedSlots.$key!==o.$key||!o&&e.$scopedSlots.$key),l=!!(r||e.$options._renderChildren||c),d=e.$vnode;e.$options._parentVnode=i,e.$vnode=i,e._vnode&&(e._vnode.parent=i),e.$options._renderChildren=r;var h=i.data.attrs||a;e._attrsProxy&&wn(e._attrsProxy,h,d.data&&d.data.attrs||a,e,"$attrs")&&(l=!0),e.$attrs=h,t=t||a;var u=e.$options._parentListeners;if(e._listenersProxy&&wn(e._listenersProxy,t,u||a,e,"$listeners"),e.$listeners=e.$options._parentListeners=t,Yn(e,t,u),n&&e.$options.props){Ae(!1);for(var p=e._props,m=e.$options._propKeys||[],f=0;f<m.length;f++){var g=m[f],y=e.$options.props;p[g]=Dt(g,y,n,e)}Ae(!0),e.$options.propsData=n}l&&(e.$slots=pn(r,i.context),e.$forceUpdate())}(n.componentInstance=e.componentInstance,t.propsData,t.listeners,n,t.children)},insert:function(e){var n,t=e.context,a=e.componentInstance;a._isMounted||(a._isMounted=!0,nt(a,"mounted")),e.data.keepAlive&&(t._isMounted?((n=a)._inactive=!1,at.push(n)):et(a,!0))},destroy:function(e){var n=e.componentInstance;n._isDestroyed||(e.data.keepAlive?function e(n,t){if(!(t&&(n._directInactive=!0,Jn(n))||n._inactive)){n._inactive=!0;for(var a=0;a<n.$children.length;a++)e(n.$children[a]);nt(n,"deactivated")}}(n,!0):n.$destroy())}},wt=Object.keys(vt);function _t(e,n,t,c,l){if(!r(e)){var h=t.$options._base;if(d(e)&&(e=h.extend(e)),"function"==typeof e){var u;if(r(e.cid)&&void 0===(e=function(e,n){if(s(e.error)&&o(e.errorComp))return e.errorComp;if(o(e.resolved))return e.resolved;var t=xn;if(t&&o(e.owners)&&-1===e.owners.indexOf(t)&&e.owners.push(t),s(e.loading)&&o(e.loadingComp))return e.loadingComp;if(t&&!o(e.owners)){var a=e.owners=[t],i=!0,c=null,l=null;t.$on("hook:destroyed",(function(){return _(a,t)}));var h=function(e){for(var n=0,t=a.length;n<t;n++)a[n].$forceUpdate();e&&(a.length=0,null!==c&&(clearTimeout(c),c=null),null!==l&&(clearTimeout(l),l=null))},u=q((function(t){e.resolved=Cn(t,n),i?a.length=0:h(!0)})),p=q((function(n){o(e.errorComp)&&(e.error=!0,h(!0))})),m=e(u,p);return d(m)&&(f(m)?r(e.resolved)&&m.then(u,p):f(m.component)&&(m.component.then(u,p),o(m.error)&&(e.errorComp=Cn(m.error,n)),o(m.loading)&&(e.loadingComp=Cn(m.loading,n),0===m.delay?e.loading=!0:c=setTimeout((function(){c=null,r(e.resolved)&&r(e.error)&&(e.loading=!0,h(!1))}),m.delay||200)),o(m.timeout)&&(l=setTimeout((function(){l=null,r(e.resolved)&&p(null)}),m.timeout)))),i=!1,e.loading?e.loadingComp:e.resolved}}(u=e,h)))return function(e,n,t,a,i){var r=me();return r.asyncFactory=e,r.asyncMeta={data:n,context:t,children:a,tag:i},r}(u,n,t,c,l);n=n||{},Ht(e),o(n.model)&&function(e,n){var t=e.model&&e.model.prop||"value",a=e.model&&e.model.event||"input";(n.attrs||(n.attrs={}))[t]=n.model.value;var r=n.on||(n.on={}),s=r[a],c=n.model.callback;o(s)?(i(s)?-1===s.indexOf(c):s!==c)&&(r[a]=[c].concat(s)):r[a]=c}(e.options,n);var p=function(e,n,t){var a=n.options.props;if(!r(a)){var i={},s=e.attrs,c=e.props;if(o(s)||o(c))for(var l in a){var d=S(l);Ke(i,c,l,d,!0)||Ke(i,s,l,d,!1)}return i}}(n,e);if(s(e.options.functional))return function(e,n,t,r,s){var c=e.options,l={},d=c.props;if(o(d))for(var h in d)l[h]=Dt(h,d,n||a);else o(t.attrs)&&yt(l,t.attrs),o(t.props)&&yt(l,t.props);var u=new ft(t,l,s,r,e),p=c.render.call(null,u._c,u);if(p instanceof pe)return gt(p,t,u.parent,c,u);if(i(p)){for(var m=Ze(p)||[],f=new Array(m.length),g=0;g<m.length;g++)f[g]=gt(m[g],t,u.parent,c,u);return f}}(e,p,n,t,c);var m=n.on;if(n.on=n.nativeOn,s(e.options.abstract)){var g=n.slot;n={},g&&(n.slot=g)}!function(e){for(var n=e.hook||(e.hook={}),t=0;t<wt.length;t++){var a=wt[t],i=n[a],r=vt[a];i===r||i&&i._merged||(n[a]=i?kt(r,i):r)}}(n);var y=bt(e.options)||l;return new pe("vue-component-".concat(e.cid).concat(y?"-".concat(y):""),n,void 0,void 0,void 0,t,{Ctor:e,propsData:p,listeners:m,tag:l,children:c},u)}}}function kt(e,n){var t=function(t,a){e(t,a),n(t,a)};return t._merged=!0,t}var xt=U,Ct=$.optionMergeStrategies;function Pt(e,n,t){if(void 0===t&&(t=!0),!n)return e;for(var a,i,r,o=de?Reflect.ownKeys(n):Object.keys(n),s=0;s<o.length;s++)"__ob__"!==(a=o[s])&&(i=e[a],r=n[a],t&&x(e,a)?i!==r&&u(i)&&u(r)&&Pt(i,r):ze(e,a,r));return e}function Tt(e,n,t){return t?function(){var a=l(n)?n.call(t,t):n,i=l(e)?e.call(t,t):e;return a?Pt(a,i):i}:n?e?function(){return Pt(l(n)?n.call(this,this):n,l(e)?e.call(this,this):e)}:n:e}function Mt(e,n){var t=n?e?e.concat(n):i(n)?n:[n]:e;return t?function(e){for(var n=[],t=0;t<e.length;t++)-1===n.indexOf(e[t])&&n.push(e[t]);return n}(t):t}function At(e,n,t,a){var i=Object.create(e||null);return n?D(i,n):i}Ct.data=function(e,n,t){return t?Tt(e,n,t):n&&"function"!=typeof n?e:Tt(e,n)},N.forEach((function(e){Ct[e]=Mt})),j.forEach((function(e){Ct[e+"s"]=At})),Ct.watch=function(e,n,t,a){if(e===ae&&(e=void 0),n===ae&&(n=void 0),!n)return Object.create(e||null);if(!e)return n;var r={};for(var o in D(r,e),n){var s=r[o],c=n[o];s&&!i(s)&&(s=[s]),r[o]=s?s.concat(c):i(c)?c:[c]}return r},Ct.props=Ct.methods=Ct.inject=Ct.computed=function(e,n,t,a){if(!e)return n;var i=Object.create(null);return D(i,e),n&&D(i,n),i},Ct.provide=function(e,n){return e?function(){var t=Object.create(null);return Pt(t,l(e)?e.call(this):e),n&&Pt(t,l(n)?n.call(this):n,!1),t}:n};var St=function(e,n){return void 0===n?e:n};function It(e,n,t){if(l(n)&&(n=n.options),function(e,n){var t=e.props;if(t){var a,r,o={};if(i(t))for(a=t.length;a--;)"string"==typeof(r=t[a])&&(o[T(r)]={type:null});else if(u(t))for(var s in t)r=t[s],o[T(s)]=u(r)?r:{type:r};else 0;e.props=o}}(n),function(e,n){var t=e.inject;if(t){var a=e.inject={};if(i(t))for(var r=0;r<t.length;r++)a[t[r]]={from:t[r]};else if(u(t))for(var o in t){var s=t[o];a[o]=u(s)?D({from:o},s):{from:s}}else 0}}(n),function(e){var n=e.directives;if(n)for(var t in n){var a=n[t];l(a)&&(n[t]={bind:a,update:a})}}(n),!n._base&&(n.extends&&(e=It(e,n.extends,t)),n.mixins))for(var a=0,r=n.mixins.length;a<r;a++)e=It(e,n.mixins[a],t);var o,s={};for(o in e)c(o);for(o in n)x(e,o)||c(o);function c(a){var i=Ct[a]||St;s[a]=i(e[a],n[a],t,a)}return s}function Lt(e,n,t,a){if("string"==typeof t){var i=e[n];if(x(i,t))return i[t];var r=T(t);if(x(i,r))return i[r];var o=M(r);return x(i,o)?i[o]:i[t]||i[r]||i[o]}}function Dt(e,n,t,a){var i=n[e],r=!x(t,e),o=t[e],s=Rt(Boolean,i.type);if(s>-1)if(r&&!x(i,"default"))o=!1;else if(""===o||o===S(e)){var c=Rt(String,i.type);(c<0||s<c)&&(o=!0)}if(void 0===o){o=function(e,n,t){if(!x(n,"default"))return;var a=n.default;0;if(e&&e.$options.propsData&&void 0===e.$options.propsData[t]&&void 0!==e._props[t])return e._props[t];return l(a)&&"Function"!==Ut(n.type)?a.call(e):a}(a,i,e);var d=Me;Ae(!0),Le(o),Ae(d)}return o}var zt=/^\s*function (\w+)/;function Ut(e){var n=e&&e.toString().match(zt);return n?n[1]:""}function Ot(e,n){return Ut(e)===Ut(n)}function Rt(e,n){if(!i(n))return Ot(n,e)?0:-1;for(var t=0,a=n.length;t<a;t++)if(Ot(n[t],e))return t;return-1}var Et={enumerable:!0,configurable:!0,get:U,set:U};function Gt(e,n,t){Et.get=function(){return this[n][t]},Et.set=function(e){this[n][t]=e},Object.defineProperty(e,t,Et)}function qt(e){var n=e.$options;if(n.props&&function(e,n){var t=e.$options.propsData||{},a=e._props=Re({}),i=e.$options._propKeys=[];e.$parent&&Ae(!1);var r=function(r){i.push(r);var o=Dt(r,n,t,e);De(a,r,o,void 0,!0),r in e||Gt(e,"_props",r)};for(var o in n)r(o);Ae(!0)}(e,n.props),function(e){var n=e.$options,t=n.setup;if(t){var a=e._setupContext=vn(e);ue(e),_e();var i=An(t,null,[e._props||Re({}),a],e,"setup");if(ke(),ue(),l(i))n.render=i;else if(d(i))if(e._setupState=i,i.__sfc){var r=e._setupProxy={};for(var o in i)"__sfc"!==o&&Be(r,i,o)}else for(var o in i)V(o)||Be(e,i,o);else 0}}(e),n.methods&&function(e,n){e.$options.props;for(var t in n)e[t]="function"!=typeof n[t]?U:I(n[t],e)}(e,n.methods),n.data)!function(e){var n=e.$options.data;u(n=e._data=l(n)?function(e,n){_e();try{return e.call(n,n)}catch(e){return Mn(e,n,"data()"),{}}finally{ke()}}(n,e):n||{})||(n={});var t=Object.keys(n),a=e.$options.props,i=(e.$options.methods,t.length);for(;i--;){var r=t[i];0,a&&x(a,r)||V(r)||Gt(e,"_data",r)}var o=Le(n);o&&o.vmCount++}(e);else{var t=Le(e._data={});t&&t.vmCount++}n.computed&&function(e,n){var t=e._computedWatchers=Object.create(null),a=oe();for(var i in n){var r=n[i],o=l(r)?r:r.get;0,a||(t[i]=new Hn(e,o||U,U,Bt)),i in e||jt(e,i,r)}}(e,n.computed),n.watch&&n.watch!==ae&&function(e,n){for(var t in n){var a=n[t];if(i(a))for(var r=0;r<a.length;r++)Ft(e,t,a[r]);else Ft(e,t,a)}}(e,n.watch)}var Bt={lazy:!0};function jt(e,n,t){var a=!oe();l(t)?(Et.get=a?Nt(n):$t(t),Et.set=U):(Et.get=t.get?a&&!1!==t.cache?Nt(n):$t(t.get):U,Et.set=t.set||U),Object.defineProperty(e,n,Et)}function Nt(e){return function(){var n=this._computedWatchers&&this._computedWatchers[e];if(n)return n.dirty&&n.evaluate(),ve.target&&n.depend(),n.value}}function $t(e){return function(){return e.call(this,this)}}function Ft(e,n,t,a){return u(t)&&(a=t,t=t.handler),"string"==typeof t&&(t=e[t]),e.$watch(n,t,a)}var Vt=0;function Ht(e){var n=e.options;if(e.super){var t=Ht(e.super);if(t!==e.superOptions){e.superOptions=t;var a=function(e){var n,t=e.options,a=e.sealedOptions;for(var i in t)t[i]!==a[i]&&(n||(n={}),n[i]=t[i]);return n}(e);a&&D(e.extendOptions,a),(n=e.options=It(t,e.extendOptions)).name&&(n.components[n.name]=e)}}return n}function Wt(e){this._init(e)}function Kt(e){e.cid=0;var n=1;e.extend=function(e){e=e||{};var t=this,a=t.cid,i=e._Ctor||(e._Ctor={});if(i[a])return i[a];var r=bt(e)||bt(t.options);var o=function(e){this._init(e)};return(o.prototype=Object.create(t.prototype)).constructor=o,o.cid=n++,o.options=It(t.options,e),o.super=t,o.options.props&&function(e){var n=e.options.props;for(var t in n)Gt(e.prototype,"_props",t)}(o),o.options.computed&&function(e){var n=e.options.computed;for(var t in n)jt(e.prototype,t,n[t])}(o),o.extend=t.extend,o.mixin=t.mixin,o.use=t.use,j.forEach((function(e){o[e]=t[e]})),r&&(o.options.components[r]=o),o.superOptions=t.options,o.extendOptions=e,o.sealedOptions=D({},o.options),i[a]=o,o}}function Zt(e){return e&&(bt(e.Ctor.options)||e.tag)}function Yt(e,n){return i(e)?e.indexOf(n)>-1:"string"==typeof e?e.split(",").indexOf(n)>-1:!!p(e)&&e.test(n)}function Xt(e,n){var t=e.cache,a=e.keys,i=e._vnode,r=e.$vnode;for(var o in t){var s=t[o];if(s){var c=s.name;c&&!n(c)&&Qt(t,o,a,i)}}r.componentOptions.children=void 0}function Qt(e,n,t,a){var i=e[n];!i||a&&i.tag===a.tag||i.componentInstance.$destroy(),e[n]=null,_(t,n)}Wt.prototype._init=function(e){var n=this;n._uid=Vt++,n._isVue=!0,n.__v_skip=!0,n._scope=new Ne(!0),n._scope.parent=void 0,n._scope._vm=!0,e&&e._isComponent?function(e,n){var t=e.$options=Object.create(e.constructor.options),a=n._parentVnode;t.parent=n.parent,t._parentVnode=a;var i=a.componentOptions;t.propsData=i.propsData,t._parentListeners=i.listeners,t._renderChildren=i.children,t._componentTag=i.tag,n.render&&(t.render=n.render,t.staticRenderFns=n.staticRenderFns)}(n,e):n.$options=It(Ht(n.constructor),e||{},n),n._renderProxy=n,n._self=n,function(e){var n=e.$options,t=n.parent;if(t&&!n.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(e)}e.$parent=t,e.$root=t?t.$root:e,e.$children=[],e.$refs={},e._provided=t?t._provided:Object.create(null),e._watcher=null,e._inactive=null,e._directInactive=!1,e._isMounted=!1,e._isDestroyed=!1,e._isBeingDestroyed=!1}(n),function(e){e._events=Object.create(null),e._hasHookEvent=!1;var n=e.$options._parentListeners;n&&Yn(e,n)}(n),function(e){e._vnode=null,e._staticTrees=null;var n=e.$options,t=e.$vnode=n._parentVnode,i=t&&t.context;e.$slots=pn(n._renderChildren,i),e.$scopedSlots=t?gn(e.$parent,t.data.scopedSlots,e.$slots):a,e._c=function(n,t,a,i){return Tn(e,n,t,a,i,!1)},e.$createElement=function(n,t,a,i){return Tn(e,n,t,a,i,!0)};var r=t&&t.data;De(e,"$attrs",r&&r.attrs||a,null,!0),De(e,"$listeners",n._parentListeners||a,null,!0)}(n),nt(n,"beforeCreate",void 0,!1),function(e){var n=mt(e.$options.inject,e);n&&(Ae(!1),Object.keys(n).forEach((function(t){De(e,t,n[t])})),Ae(!0))}(n),qt(n),function(e){var n=e.$options.provide;if(n){var t=l(n)?n.call(e):n;if(!d(t))return;for(var a=$e(e),i=de?Reflect.ownKeys(t):Object.keys(t),r=0;r<i.length;r++){var o=i[r];Object.defineProperty(a,o,Object.getOwnPropertyDescriptor(t,o))}}}(n),nt(n,"created"),n.$options.el&&n.$mount(n.$options.el)},function(e){var n={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(e.prototype,"$data",n),Object.defineProperty(e.prototype,"$props",t),e.prototype.$set=ze,e.prototype.$delete=Ue,e.prototype.$watch=function(e,n,t){if(u(n))return Ft(this,e,n,t);(t=t||{}).user=!0;var a=new Hn(this,e,n,t);if(t.immediate){var i='callback for immediate watcher "'.concat(a.expression,'"');_e(),An(n,this,[a.value],this,i),ke()}return function(){a.teardown()}}}(Wt),function(e){var n=/^hook:/;e.prototype.$on=function(e,t){var a=this;if(i(e))for(var r=0,o=e.length;r<o;r++)a.$on(e[r],t);else(a._events[e]||(a._events[e]=[])).push(t),n.test(e)&&(a._hasHookEvent=!0);return a},e.prototype.$once=function(e,n){var t=this;function a(){t.$off(e,a),n.apply(t,arguments)}return a.fn=n,t.$on(e,a),t},e.prototype.$off=function(e,n){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(i(e)){for(var a=0,r=e.length;a<r;a++)t.$off(e[a],n);return t}var o,s=t._events[e];if(!s)return t;if(!n)return t._events[e]=null,t;for(var c=s.length;c--;)if((o=s[c])===n||o.fn===n){s.splice(c,1);break}return t},e.prototype.$emit=function(e){var n=this,t=n._events[e];if(t){t=t.length>1?L(t):t;for(var a=L(arguments,1),i='event handler for "'.concat(e,'"'),r=0,o=t.length;r<o;r++)An(t[r],n,a,n,i)}return n}}(Wt),function(e){e.prototype._update=function(e,n){var t=this,a=t.$el,i=t._vnode,r=Qn(t);t._vnode=e,t.$el=i?t.__patch__(i,e):t.__patch__(t.$el,e,n,!1),r(),a&&(a.__vue__=null),t.$el&&(t.$el.__vue__=t);for(var o=t;o&&o.$vnode&&o.$parent&&o.$vnode===o.$parent._vnode;)o.$parent.$el=o.$el,o=o.$parent},e.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},e.prototype.$destroy=function(){var e=this;if(!e._isBeingDestroyed){nt(e,"beforeDestroy"),e._isBeingDestroyed=!0;var n=e.$parent;!n||n._isBeingDestroyed||e.$options.abstract||_(n.$children,e),e._scope.stop(),e._data.__ob__&&e._data.__ob__.vmCount--,e._isDestroyed=!0,e.__patch__(e._vnode,null),nt(e,"destroyed"),e.$off(),e.$el&&(e.$el.__vue__=null),e.$vnode&&(e.$vnode.parent=null)}}}(Wt),function(e){un(e.prototype),e.prototype.$nextTick=function(e){return Bn(e,this)},e.prototype._render=function(){var e=this,n=e.$options,t=n.render,a=n._parentVnode;a&&e._isMounted&&(e.$scopedSlots=gn(e.$parent,a.data.scopedSlots,e.$slots,e.$scopedSlots),e._slotsProxy&&kn(e._slotsProxy,e.$scopedSlots)),e.$vnode=a;var r,o=he,s=xn;try{ue(e),xn=e,r=t.call(e._renderProxy,e.$createElement)}catch(n){Mn(n,e,"render"),r=e._vnode}finally{xn=s,ue(o)}return i(r)&&1===r.length&&(r=r[0]),r instanceof pe||(r=me()),r.parent=a,r}}(Wt);var Jt=[String,RegExp,Array],ea={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Jt,exclude:Jt,max:[String,Number]},methods:{cacheVNode:function(){var e=this.cache,n=this.keys,t=this.vnodeToCache,a=this.keyToCache;if(t){var i=t.tag,r=t.componentInstance,o=t.componentOptions;e[a]={name:Zt(o),tag:i,componentInstance:r},n.push(a),this.max&&n.length>parseInt(this.max)&&Qt(e,n[0],n,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var e in this.cache)Qt(this.cache,e,this.keys)},mounted:function(){var e=this;this.cacheVNode(),this.$watch("include",(function(n){Xt(e,(function(e){return Yt(n,e)}))})),this.$watch("exclude",(function(n){Xt(e,(function(e){return!Yt(n,e)}))}))},updated:function(){this.cacheVNode()},render:function(){var e=this.$slots.default,n=Pn(e),t=n&&n.componentOptions;if(t){var a=Zt(t),i=this.include,r=this.exclude;if(i&&(!a||!Yt(i,a))||r&&a&&Yt(r,a))return n;var o=this.cache,s=this.keys,c=null==n.key?t.Ctor.cid+(t.tag?"::".concat(t.tag):""):n.key;o[c]?(n.componentInstance=o[c].componentInstance,_(s,c),s.push(c)):(this.vnodeToCache=n,this.keyToCache=c),n.data.keepAlive=!0}return n||e&&e[0]}}};!function(e){var n={get:function(){return $}};Object.defineProperty(e,"config",n),e.util={warn:xt,extend:D,mergeOptions:It,defineReactive:De},e.set=ze,e.delete=Ue,e.nextTick=Bn,e.observable=function(e){return Le(e),e},e.options=Object.create(null),j.forEach((function(n){e.options[n+"s"]=Object.create(null)})),e.options._base=e,D(e.options.components,ea),function(e){e.use=function(e){var n=this._installedPlugins||(this._installedPlugins=[]);if(n.indexOf(e)>-1)return this;var t=L(arguments,1);return t.unshift(this),l(e.install)?e.install.apply(e,t):l(e)&&e.apply(null,t),n.push(e),this}}(e),function(e){e.mixin=function(e){return this.options=It(this.options,e),this}}(e),Kt(e),function(e){j.forEach((function(n){e[n]=function(e,t){return t?("component"===n&&u(t)&&(t.name=t.name||e,t=this.options._base.extend(t)),"directive"===n&&l(t)&&(t={bind:t,update:t}),this.options[n+"s"][e]=t,t):this.options[n+"s"][e]}}))}(e)}(Wt),Object.defineProperty(Wt.prototype,"$isServer",{get:oe}),Object.defineProperty(Wt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Wt,"FunctionalRenderContext",{value:ft}),Wt.version="2.7.16";var na=v("style,class"),ta=v("input,textarea,option,select,progress"),aa=v("contenteditable,draggable,spellcheck"),ia=v("events,caret,typing,plaintext-only"),ra=v("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),oa="http://www.w3.org/1999/xlink",sa=function(e){return":"===e.charAt(5)&&"xlink"===e.slice(0,5)},ca=function(e){return sa(e)?e.slice(6,e.length):""},la=function(e){return null==e||!1===e};function da(e){for(var n=e.data,t=e,a=e;o(a.componentInstance);)(a=a.componentInstance._vnode)&&a.data&&(n=ha(a.data,n));for(;o(t=t.parent);)t&&t.data&&(n=ha(n,t.data));return function(e,n){if(o(e)||o(n))return ua(e,pa(n));return""}(n.staticClass,n.class)}function ha(e,n){return{staticClass:ua(e.staticClass,n.staticClass),class:o(e.class)?[e.class,n.class]:n.class}}function ua(e,n){return e?n?e+" "+n:e:n||""}function pa(e){return Array.isArray(e)?function(e){for(var n,t="",a=0,i=e.length;a<i;a++)o(n=pa(e[a]))&&""!==n&&(t&&(t+=" "),t+=n);return t}(e):d(e)?function(e){var n="";for(var t in e)e[t]&&(n&&(n+=" "),n+=t);return n}(e):"string"==typeof e?e:""}var ma={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},fa=v("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),ga=v("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),ya=function(e){return fa(e)||ga(e)};var ba=Object.create(null);var va=v("text,number,password,search,email,tel,url");var wa=Object.freeze({__proto__:null,createElement:function(e,n){var t=document.createElement(e);return"select"!==e||n.data&&n.data.attrs&&void 0!==n.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(e,n){return document.createElementNS(ma[e],n)},createTextNode:function(e){return document.createTextNode(e)},createComment:function(e){return document.createComment(e)},insertBefore:function(e,n,t){e.insertBefore(n,t)},removeChild:function(e,n){e.removeChild(n)},appendChild:function(e,n){e.appendChild(n)},parentNode:function(e){return e.parentNode},nextSibling:function(e){return e.nextSibling},tagName:function(e){return e.tagName},setTextContent:function(e,n){e.textContent=n},setStyleScope:function(e,n){e.setAttribute(n,"")}}),_a={create:function(e,n){ka(n)},update:function(e,n){e.data.ref!==n.data.ref&&(ka(e,!0),ka(n))},destroy:function(e){ka(e,!0)}};function ka(e,n){var t=e.data.ref;if(o(t)){var a=e.context,r=e.componentInstance||e.elm,s=n?null:r,c=n?void 0:r;if(l(t))An(t,a,[s],a,"template ref function");else{var d=e.data.refInFor,h="string"==typeof t||"number"==typeof t,u=qe(t),p=a.$refs;if(h||u)if(d){var m=h?p[t]:t.value;n?i(m)&&_(m,r):i(m)?m.includes(r)||m.push(r):h?(p[t]=[r],xa(a,t,p[t])):t.value=[r]}else if(h){if(n&&p[t]!==r)return;p[t]=c,xa(a,t,s)}else if(u){if(n&&t.value!==r)return;t.value=s}else 0}}}function xa(e,n,t){var a=e._setupState;a&&x(a,n)&&(qe(a[n])?a[n].value=t:a[n]=t)}var Ca=new pe("",{},[]),Pa=["create","activate","update","remove","destroy"];function Ta(e,n){return e.key===n.key&&e.asyncFactory===n.asyncFactory&&(e.tag===n.tag&&e.isComment===n.isComment&&o(e.data)===o(n.data)&&function(e,n){if("input"!==e.tag)return!0;var t,a=o(t=e.data)&&o(t=t.attrs)&&t.type,i=o(t=n.data)&&o(t=t.attrs)&&t.type;return a===i||va(a)&&va(i)}(e,n)||s(e.isAsyncPlaceholder)&&r(n.asyncFactory.error))}function Ma(e,n,t){var a,i,r={};for(a=n;a<=t;++a)o(i=e[a].key)&&(r[i]=a);return r}var Aa={create:Sa,update:Sa,destroy:function(e){Sa(e,Ca)}};function Sa(e,n){(e.data.directives||n.data.directives)&&function(e,n){var t,a,i,r=e===Ca,o=n===Ca,s=La(e.data.directives,e.context),c=La(n.data.directives,n.context),l=[],d=[];for(t in c)a=s[t],i=c[t],a?(i.oldValue=a.value,i.oldArg=a.arg,za(i,"update",n,e),i.def&&i.def.componentUpdated&&d.push(i)):(za(i,"bind",n,e),i.def&&i.def.inserted&&l.push(i));if(l.length){var h=function(){for(var t=0;t<l.length;t++)za(l[t],"inserted",n,e)};r?We(n,"insert",h):h()}d.length&&We(n,"postpatch",(function(){for(var t=0;t<d.length;t++)za(d[t],"componentUpdated",n,e)}));if(!r)for(t in s)c[t]||za(s[t],"unbind",e,e,o)}(e,n)}var Ia=Object.create(null);function La(e,n){var t,a,i=Object.create(null);if(!e)return i;for(t=0;t<e.length;t++){if((a=e[t]).modifiers||(a.modifiers=Ia),i[Da(a)]=a,n._setupState&&n._setupState.__sfc){var r=a.def||Lt(n,"_setupState","v-"+a.name);a.def="function"==typeof r?{bind:r,update:r}:r}a.def=a.def||Lt(n.$options,"directives",a.name)}return i}function Da(e){return e.rawName||"".concat(e.name,".").concat(Object.keys(e.modifiers||{}).join("."))}function za(e,n,t,a,i){var r=e.def&&e.def[n];if(r)try{r(t.elm,e,t,a,i)}catch(a){Mn(a,t.context,"directive ".concat(e.name," ").concat(n," hook"))}}var Ua=[_a,Aa];function Oa(e,n){var t=n.componentOptions;if(!(o(t)&&!1===t.Ctor.options.inheritAttrs||r(e.data.attrs)&&r(n.data.attrs))){var a,i,c=n.elm,l=e.data.attrs||{},d=n.data.attrs||{};for(a in(o(d.__ob__)||s(d._v_attr_proxy))&&(d=n.data.attrs=D({},d)),d)i=d[a],l[a]!==i&&Ra(c,a,i,n.data.pre);for(a in(X||J)&&d.value!==l.value&&Ra(c,"value",d.value),l)r(d[a])&&(sa(a)?c.removeAttributeNS(oa,ca(a)):aa(a)||c.removeAttribute(a))}}function Ra(e,n,t,a){a||e.tagName.indexOf("-")>-1?Ea(e,n,t):ra(n)?la(t)?e.removeAttribute(n):(t="allowfullscreen"===n&&"EMBED"===e.tagName?"true":n,e.setAttribute(n,t)):aa(n)?e.setAttribute(n,function(e,n){return la(n)||"false"===n?"false":"contenteditable"===e&&ia(n)?n:"true"}(n,t)):sa(n)?la(t)?e.removeAttributeNS(oa,ca(n)):e.setAttributeNS(oa,n,t):Ea(e,n,t)}function Ea(e,n,t){if(la(t))e.removeAttribute(n);else{if(X&&!Q&&"TEXTAREA"===e.tagName&&"placeholder"===n&&""!==t&&!e.__ieph){var a=function(n){n.stopImmediatePropagation(),e.removeEventListener("input",a)};e.addEventListener("input",a),e.__ieph=!0}e.setAttribute(n,t)}}var Ga={create:Oa,update:Oa};function qa(e,n){var t=n.elm,a=n.data,i=e.data;if(!(r(a.staticClass)&&r(a.class)&&(r(i)||r(i.staticClass)&&r(i.class)))){var s=da(n),c=t._transitionClasses;o(c)&&(s=ua(s,pa(c))),s!==t._prevClass&&(t.setAttribute("class",s),t._prevClass=s)}}var Ba,ja={create:qa,update:qa};function Na(e,n,t){var a=Ba;return function i(){var r=n.apply(null,arguments);null!==r&&Va(e,i,t,a)}}var $a=Dn&&!(te&&Number(te[1])<=53);function Fa(e,n,t,a){if($a){var i=ct,r=n;n=r._wrapper=function(e){if(e.target===e.currentTarget||e.timeStamp>=i||e.timeStamp<=0||e.target.ownerDocument!==document)return r.apply(this,arguments)}}Ba.addEventListener(e,n,ie?{capture:t,passive:a}:t)}function Va(e,n,t,a){(a||Ba).removeEventListener(e,n._wrapper||n,t)}function Ha(e,n){if(!r(e.data.on)||!r(n.data.on)){var t=n.data.on||{},a=e.data.on||{};Ba=n.elm||e.elm,function(e){if(o(e.__r)){var n=X?"change":"input";e[n]=[].concat(e.__r,e[n]||[]),delete e.__r}o(e.__c)&&(e.change=[].concat(e.__c,e.change||[]),delete e.__c)}(t),He(t,a,Fa,Va,Na,n.context),Ba=void 0}}var Wa,Ka={create:Ha,update:Ha,destroy:function(e){return Ha(e,Ca)}};function Za(e,n){if(!r(e.data.domProps)||!r(n.data.domProps)){var t,a,i=n.elm,c=e.data.domProps||{},l=n.data.domProps||{};for(t in(o(l.__ob__)||s(l._v_attr_proxy))&&(l=n.data.domProps=D({},l)),c)t in l||(i[t]="");for(t in l){if(a=l[t],"textContent"===t||"innerHTML"===t){if(n.children&&(n.children.length=0),a===c[t])continue;1===i.childNodes.length&&i.removeChild(i.childNodes[0])}if("value"===t&&"PROGRESS"!==i.tagName){i._value=a;var d=r(a)?"":String(a);Ya(i,d)&&(i.value=d)}else if("innerHTML"===t&&ga(i.tagName)&&r(i.innerHTML)){(Wa=Wa||document.createElement("div")).innerHTML="<svg>".concat(a,"</svg>");for(var h=Wa.firstChild;i.firstChild;)i.removeChild(i.firstChild);for(;h.firstChild;)i.appendChild(h.firstChild)}else if(a!==c[t])try{i[t]=a}catch(e){}}}}function Ya(e,n){return!e.composing&&("OPTION"===e.tagName||function(e,n){var t=!0;try{t=document.activeElement!==e}catch(e){}return t&&e.value!==n}(e,n)||function(e,n){var t=e.value,a=e._vModifiers;if(o(a)){if(a.number)return b(t)!==b(n);if(a.trim)return t.trim()!==n.trim()}return t!==n}(e,n))}var Xa={create:Za,update:Za},Qa=C((function(e){var n={},t=/:(.+)/;return e.split(/;(?![^(]*\))/g).forEach((function(e){if(e){var a=e.split(t);a.length>1&&(n[a[0].trim()]=a[1].trim())}})),n}));function Ja(e){var n=ei(e.style);return e.staticStyle?D(e.staticStyle,n):n}function ei(e){return Array.isArray(e)?z(e):"string"==typeof e?Qa(e):e}var ni,ti=/^--/,ai=/\s*!important$/,ii=function(e,n,t){if(ti.test(n))e.style.setProperty(n,t);else if(ai.test(t))e.style.setProperty(S(n),t.replace(ai,""),"important");else{var a=oi(n);if(Array.isArray(t))for(var i=0,r=t.length;i<r;i++)e.style[a]=t[i];else e.style[a]=t}},ri=["Webkit","Moz","ms"],oi=C((function(e){if(ni=ni||document.createElement("div").style,"filter"!==(e=T(e))&&e in ni)return e;for(var n=e.charAt(0).toUpperCase()+e.slice(1),t=0;t<ri.length;t++){var a=ri[t]+n;if(a in ni)return a}}));function si(e,n){var t=n.data,a=e.data;if(!(r(t.staticStyle)&&r(t.style)&&r(a.staticStyle)&&r(a.style))){var i,s,c=n.elm,l=a.staticStyle,d=a.normalizedStyle||a.style||{},h=l||d,u=ei(n.data.style)||{};n.data.normalizedStyle=o(u.__ob__)?D({},u):u;var p=function(e,n){var t,a={};if(n)for(var i=e;i.componentInstance;)(i=i.componentInstance._vnode)&&i.data&&(t=Ja(i.data))&&D(a,t);(t=Ja(e.data))&&D(a,t);for(var r=e;r=r.parent;)r.data&&(t=Ja(r.data))&&D(a,t);return a}(n,!0);for(s in h)r(p[s])&&ii(c,s,"");for(s in p)i=p[s],ii(c,s,null==i?"":i)}}var ci={create:si,update:si},li=/\s+/;function di(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(li).forEach((function(n){return e.classList.add(n)})):e.classList.add(n);else{var t=" ".concat(e.getAttribute("class")||""," ");t.indexOf(" "+n+" ")<0&&e.setAttribute("class",(t+n).trim())}}function hi(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(li).forEach((function(n){return e.classList.remove(n)})):e.classList.remove(n),e.classList.length||e.removeAttribute("class");else{for(var t=" ".concat(e.getAttribute("class")||""," "),a=" "+n+" ";t.indexOf(a)>=0;)t=t.replace(a," ");(t=t.trim())?e.setAttribute("class",t):e.removeAttribute("class")}}function ui(e){if(e){if("object"==typeof e){var n={};return!1!==e.css&&D(n,pi(e.name||"v")),D(n,e),n}return"string"==typeof e?pi(e):void 0}}var pi=C((function(e){return{enterClass:"".concat(e,"-enter"),enterToClass:"".concat(e,"-enter-to"),enterActiveClass:"".concat(e,"-enter-active"),leaveClass:"".concat(e,"-leave"),leaveToClass:"".concat(e,"-leave-to"),leaveActiveClass:"".concat(e,"-leave-active")}})),mi=Z&&!Q,fi="transition",gi="transitionend",yi="animation",bi="animationend";mi&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(fi="WebkitTransition",gi="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(yi="WebkitAnimation",bi="webkitAnimationEnd"));var vi=Z?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(e){return e()};function wi(e){vi((function(){vi(e)}))}function _i(e,n){var t=e._transitionClasses||(e._transitionClasses=[]);t.indexOf(n)<0&&(t.push(n),di(e,n))}function ki(e,n){e._transitionClasses&&_(e._transitionClasses,n),hi(e,n)}function xi(e,n,t){var a=Pi(e,n),i=a.type,r=a.timeout,o=a.propCount;if(!i)return t();var s="transition"===i?gi:bi,c=0,l=function(){e.removeEventListener(s,d),t()},d=function(n){n.target===e&&++c>=o&&l()};setTimeout((function(){c<o&&l()}),r+1),e.addEventListener(s,d)}var Ci=/\b(transform|all)(,|$)/;function Pi(e,n){var t,a=window.getComputedStyle(e),i=(a[fi+"Delay"]||"").split(", "),r=(a[fi+"Duration"]||"").split(", "),o=Ti(i,r),s=(a[yi+"Delay"]||"").split(", "),c=(a[yi+"Duration"]||"").split(", "),l=Ti(s,c),d=0,h=0;return"transition"===n?o>0&&(t="transition",d=o,h=r.length):"animation"===n?l>0&&(t="animation",d=l,h=c.length):h=(t=(d=Math.max(o,l))>0?o>l?"transition":"animation":null)?"transition"===t?r.length:c.length:0,{type:t,timeout:d,propCount:h,hasTransform:"transition"===t&&Ci.test(a[fi+"Property"])}}function Ti(e,n){for(;e.length<n.length;)e=e.concat(e);return Math.max.apply(null,n.map((function(n,t){return Mi(n)+Mi(e[t])})))}function Mi(e){return 1e3*Number(e.slice(0,-1).replace(",","."))}function Ai(e,n){var t=e.elm;o(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var a=ui(e.data.transition);if(!r(a)&&!o(t._enterCb)&&1===t.nodeType){for(var i=a.css,s=a.type,c=a.enterClass,h=a.enterToClass,u=a.enterActiveClass,p=a.appearClass,m=a.appearToClass,f=a.appearActiveClass,g=a.beforeEnter,y=a.enter,v=a.afterEnter,w=a.enterCancelled,_=a.beforeAppear,k=a.appear,x=a.afterAppear,C=a.appearCancelled,P=a.duration,T=Xn,M=Xn.$vnode;M&&M.parent;)T=M.context,M=M.parent;var A=!T._isMounted||!e.isRootInsert;if(!A||k||""===k){var S=A&&p?p:c,I=A&&f?f:u,L=A&&m?m:h,D=A&&_||g,z=A&&l(k)?k:y,U=A&&x||v,O=A&&C||w,R=b(d(P)?P.enter:P);0;var E=!1!==i&&!Q,G=Li(z),B=t._enterCb=q((function(){E&&(ki(t,L),ki(t,I)),B.cancelled?(E&&ki(t,S),O&&O(t)):U&&U(t),t._enterCb=null}));e.data.show||We(e,"insert",(function(){var n=t.parentNode,a=n&&n._pending&&n._pending[e.key];a&&a.tag===e.tag&&a.elm._leaveCb&&a.elm._leaveCb(),z&&z(t,B)})),D&&D(t),E&&(_i(t,S),_i(t,I),wi((function(){ki(t,S),B.cancelled||(_i(t,L),G||(Ii(R)?setTimeout(B,R):xi(t,s,B)))}))),e.data.show&&(n&&n(),z&&z(t,B)),E||G||B()}}}function Si(e,n){var t=e.elm;o(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var a=ui(e.data.transition);if(r(a)||1!==t.nodeType)return n();if(!o(t._leaveCb)){var i=a.css,s=a.type,c=a.leaveClass,l=a.leaveToClass,h=a.leaveActiveClass,u=a.beforeLeave,p=a.leave,m=a.afterLeave,f=a.leaveCancelled,g=a.delayLeave,y=a.duration,v=!1!==i&&!Q,w=Li(p),_=b(d(y)?y.leave:y);0;var k=t._leaveCb=q((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[e.key]=null),v&&(ki(t,l),ki(t,h)),k.cancelled?(v&&ki(t,c),f&&f(t)):(n(),m&&m(t)),t._leaveCb=null}));g?g(x):x()}function x(){k.cancelled||(!e.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[e.key]=e),u&&u(t),v&&(_i(t,c),_i(t,h),wi((function(){ki(t,c),k.cancelled||(_i(t,l),w||(Ii(_)?setTimeout(k,_):xi(t,s,k)))}))),p&&p(t,k),v||w||k())}}function Ii(e){return"number"==typeof e&&!isNaN(e)}function Li(e){if(r(e))return!1;var n=e.fns;return o(n)?Li(Array.isArray(n)?n[0]:n):(e._length||e.length)>1}function Di(e,n){!0!==n.data.show&&Ai(n)}var zi=function(e){var n,t,a={},l=e.modules,d=e.nodeOps;for(n=0;n<Pa.length;++n)for(a[Pa[n]]=[],t=0;t<l.length;++t)o(l[t][Pa[n]])&&a[Pa[n]].push(l[t][Pa[n]]);function h(e){var n=d.parentNode(e);o(n)&&d.removeChild(n,e)}function u(e,n,t,i,r,c,l){if(o(e.elm)&&o(c)&&(e=c[l]=ge(e)),e.isRootInsert=!r,!function(e,n,t,i){var r=e.data;if(o(r)){var c=o(e.componentInstance)&&r.keepAlive;if(o(r=r.hook)&&o(r=r.init)&&r(e,!1),o(e.componentInstance))return p(e,n),m(t,e.elm,i),s(c)&&function(e,n,t,i){var r,s=e;for(;s.componentInstance;)if(s=s.componentInstance._vnode,o(r=s.data)&&o(r=r.transition)){for(r=0;r<a.activate.length;++r)a.activate[r](Ca,s);n.push(s);break}m(t,e.elm,i)}(e,n,t,i),!0}}(e,n,t,i)){var h=e.data,u=e.children,g=e.tag;o(g)?(e.elm=e.ns?d.createElementNS(e.ns,g):d.createElement(g,e),b(e),f(e,u,n),o(h)&&y(e,n),m(t,e.elm,i)):s(e.isComment)?(e.elm=d.createComment(e.text),m(t,e.elm,i)):(e.elm=d.createTextNode(e.text),m(t,e.elm,i))}}function p(e,n){o(e.data.pendingInsert)&&(n.push.apply(n,e.data.pendingInsert),e.data.pendingInsert=null),e.elm=e.componentInstance.$el,g(e)?(y(e,n),b(e)):(ka(e),n.push(e))}function m(e,n,t){o(e)&&(o(t)?d.parentNode(t)===e&&d.insertBefore(e,n,t):d.appendChild(e,n))}function f(e,n,t){if(i(n)){0;for(var a=0;a<n.length;++a)u(n[a],t,e.elm,null,!0,n,a)}else c(e.text)&&d.appendChild(e.elm,d.createTextNode(String(e.text)))}function g(e){for(;e.componentInstance;)e=e.componentInstance._vnode;return o(e.tag)}function y(e,t){for(var i=0;i<a.create.length;++i)a.create[i](Ca,e);o(n=e.data.hook)&&(o(n.create)&&n.create(Ca,e),o(n.insert)&&t.push(e))}function b(e){var n;if(o(n=e.fnScopeId))d.setStyleScope(e.elm,n);else for(var t=e;t;)o(n=t.context)&&o(n=n.$options._scopeId)&&d.setStyleScope(e.elm,n),t=t.parent;o(n=Xn)&&n!==e.context&&n!==e.fnContext&&o(n=n.$options._scopeId)&&d.setStyleScope(e.elm,n)}function w(e,n,t,a,i,r){for(;a<=i;++a)u(t[a],r,e,n,!1,t,a)}function _(e){var n,t,i=e.data;if(o(i))for(o(n=i.hook)&&o(n=n.destroy)&&n(e),n=0;n<a.destroy.length;++n)a.destroy[n](e);if(o(n=e.children))for(t=0;t<e.children.length;++t)_(e.children[t])}function k(e,n,t){for(;n<=t;++n){var a=e[n];o(a)&&(o(a.tag)?(x(a),_(a)):h(a.elm))}}function x(e,n){if(o(n)||o(e.data)){var t,i=a.remove.length+1;for(o(n)?n.listeners+=i:n=function(e,n){function t(){0==--t.listeners&&h(e)}return t.listeners=n,t}(e.elm,i),o(t=e.componentInstance)&&o(t=t._vnode)&&o(t.data)&&x(t,n),t=0;t<a.remove.length;++t)a.remove[t](e,n);o(t=e.data.hook)&&o(t=t.remove)?t(e,n):n()}else h(e.elm)}function C(e,n,t,a){for(var i=t;i<a;i++){var r=n[i];if(o(r)&&Ta(e,r))return i}}function P(e,n,t,i,c,l){if(e!==n){o(n.elm)&&o(i)&&(n=i[c]=ge(n));var h=n.elm=e.elm;if(s(e.isAsyncPlaceholder))o(n.asyncFactory.resolved)?A(e.elm,n,t):n.isAsyncPlaceholder=!0;else if(s(n.isStatic)&&s(e.isStatic)&&n.key===e.key&&(s(n.isCloned)||s(n.isOnce)))n.componentInstance=e.componentInstance;else{var p,m=n.data;o(m)&&o(p=m.hook)&&o(p=p.prepatch)&&p(e,n);var f=e.children,y=n.children;if(o(m)&&g(n)){for(p=0;p<a.update.length;++p)a.update[p](e,n);o(p=m.hook)&&o(p=p.update)&&p(e,n)}r(n.text)?o(f)&&o(y)?f!==y&&function(e,n,t,a,i){var s,c,l,h=0,p=0,m=n.length-1,f=n[0],g=n[m],y=t.length-1,b=t[0],v=t[y],_=!i;for(0;h<=m&&p<=y;)r(f)?f=n[++h]:r(g)?g=n[--m]:Ta(f,b)?(P(f,b,a,t,p),f=n[++h],b=t[++p]):Ta(g,v)?(P(g,v,a,t,y),g=n[--m],v=t[--y]):Ta(f,v)?(P(f,v,a,t,y),_&&d.insertBefore(e,f.elm,d.nextSibling(g.elm)),f=n[++h],v=t[--y]):Ta(g,b)?(P(g,b,a,t,p),_&&d.insertBefore(e,g.elm,f.elm),g=n[--m],b=t[++p]):(r(s)&&(s=Ma(n,h,m)),r(c=o(b.key)?s[b.key]:C(b,n,h,m))?u(b,a,e,f.elm,!1,t,p):Ta(l=n[c],b)?(P(l,b,a,t,p),n[c]=void 0,_&&d.insertBefore(e,l.elm,f.elm)):u(b,a,e,f.elm,!1,t,p),b=t[++p]);h>m?w(e,r(t[y+1])?null:t[y+1].elm,t,p,y,a):p>y&&k(n,h,m)}(h,f,y,t,l):o(y)?(o(e.text)&&d.setTextContent(h,""),w(h,null,y,0,y.length-1,t)):o(f)?k(f,0,f.length-1):o(e.text)&&d.setTextContent(h,""):e.text!==n.text&&d.setTextContent(h,n.text),o(m)&&o(p=m.hook)&&o(p=p.postpatch)&&p(e,n)}}}function T(e,n,t){if(s(t)&&o(e.parent))e.parent.data.pendingInsert=n;else for(var a=0;a<n.length;++a)n[a].data.hook.insert(n[a])}var M=v("attrs,class,staticClass,staticStyle,key");function A(e,n,t,a){var i,r=n.tag,c=n.data,l=n.children;if(a=a||c&&c.pre,n.elm=e,s(n.isComment)&&o(n.asyncFactory))return n.isAsyncPlaceholder=!0,!0;if(o(c)&&(o(i=c.hook)&&o(i=i.init)&&i(n,!0),o(i=n.componentInstance)))return p(n,t),!0;if(o(r)){if(o(l))if(e.hasChildNodes())if(o(i=c)&&o(i=i.domProps)&&o(i=i.innerHTML)){if(i!==e.innerHTML)return!1}else{for(var d=!0,h=e.firstChild,u=0;u<l.length;u++){if(!h||!A(h,l[u],t,a)){d=!1;break}h=h.nextSibling}if(!d||h)return!1}else f(n,l,t);if(o(c)){var m=!1;for(var g in c)if(!M(g)){m=!0,y(n,t);break}!m&&c.class&&$n(c.class)}}else e.data!==n.text&&(e.data=n.text);return!0}return function(e,n,t,i){if(!r(n)){var c,l=!1,h=[];if(r(e))l=!0,u(n,h);else{var p=o(e.nodeType);if(!p&&Ta(e,n))P(e,n,h,null,null,i);else{if(p){if(1===e.nodeType&&e.hasAttribute("data-server-rendered")&&(e.removeAttribute("data-server-rendered"),t=!0),s(t)&&A(e,n,h))return T(n,h,!0),e;c=e,e=new pe(d.tagName(c).toLowerCase(),{},[],void 0,c)}var m=e.elm,f=d.parentNode(m);if(u(n,h,m._leaveCb?null:f,d.nextSibling(m)),o(n.parent))for(var y=n.parent,b=g(n);y;){for(var v=0;v<a.destroy.length;++v)a.destroy[v](y);if(y.elm=n.elm,b){for(var w=0;w<a.create.length;++w)a.create[w](Ca,y);var x=y.data.hook.insert;if(x.merged)for(var C=x.fns.slice(1),M=0;M<C.length;M++)C[M]()}else ka(y);y=y.parent}o(f)?k([e],0,0):o(e.tag)&&_(e)}}return T(n,h,l),n.elm}o(e)&&_(e)}}({nodeOps:wa,modules:[Ga,ja,Ka,Xa,ci,Z?{create:Di,activate:Di,remove:function(e,n){!0!==e.data.show?Si(e,n):n()}}:{}].concat(Ua)});Q&&document.addEventListener("selectionchange",(function(){var e=document.activeElement;e&&e.vmodel&&ji(e,"input")}));var Ui={inserted:function(e,n,t,a){"select"===t.tag?(a.elm&&!a.elm._vOptions?We(t,"postpatch",(function(){Ui.componentUpdated(e,n,t)})):Oi(e,n,t.context),e._vOptions=[].map.call(e.options,Gi)):("textarea"===t.tag||va(e.type))&&(e._vModifiers=n.modifiers,n.modifiers.lazy||(e.addEventListener("compositionstart",qi),e.addEventListener("compositionend",Bi),e.addEventListener("change",Bi),Q&&(e.vmodel=!0)))},componentUpdated:function(e,n,t){if("select"===t.tag){Oi(e,n,t.context);var a=e._vOptions,i=e._vOptions=[].map.call(e.options,Gi);if(i.some((function(e,n){return!E(e,a[n])})))(e.multiple?n.value.some((function(e){return Ei(e,i)})):n.value!==n.oldValue&&Ei(n.value,i))&&ji(e,"change")}}};function Oi(e,n,t){Ri(e,n,t),(X||J)&&setTimeout((function(){Ri(e,n,t)}),0)}function Ri(e,n,t){var a=n.value,i=e.multiple;if(!i||Array.isArray(a)){for(var r,o,s=0,c=e.options.length;s<c;s++)if(o=e.options[s],i)r=G(a,Gi(o))>-1,o.selected!==r&&(o.selected=r);else if(E(Gi(o),a))return void(e.selectedIndex!==s&&(e.selectedIndex=s));i||(e.selectedIndex=-1)}}function Ei(e,n){return n.every((function(n){return!E(n,e)}))}function Gi(e){return"_value"in e?e._value:e.value}function qi(e){e.target.composing=!0}function Bi(e){e.target.composing&&(e.target.composing=!1,ji(e.target,"input"))}function ji(e,n){var t=document.createEvent("HTMLEvents");t.initEvent(n,!0,!0),e.dispatchEvent(t)}function Ni(e){return!e.componentInstance||e.data&&e.data.transition?e:Ni(e.componentInstance._vnode)}var $i={model:Ui,show:{bind:function(e,n,t){var a=n.value,i=(t=Ni(t)).data&&t.data.transition,r=e.__vOriginalDisplay="none"===e.style.display?"":e.style.display;a&&i?(t.data.show=!0,Ai(t,(function(){e.style.display=r}))):e.style.display=a?r:"none"},update:function(e,n,t){var a=n.value;!a!=!n.oldValue&&((t=Ni(t)).data&&t.data.transition?(t.data.show=!0,a?Ai(t,(function(){e.style.display=e.__vOriginalDisplay})):Si(t,(function(){e.style.display="none"}))):e.style.display=a?e.__vOriginalDisplay:"none")},unbind:function(e,n,t,a,i){i||(e.style.display=e.__vOriginalDisplay)}}},Fi={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function Vi(e){var n=e&&e.componentOptions;return n&&n.Ctor.options.abstract?Vi(Pn(n.children)):e}function Hi(e){var n={},t=e.$options;for(var a in t.propsData)n[a]=e[a];var i=t._parentListeners;for(var a in i)n[T(a)]=i[a];return n}function Wi(e,n){if(/\d-keep-alive$/.test(n.tag))return e("keep-alive",{props:n.componentOptions.propsData})}var Ki=function(e){return e.tag||fn(e)},Zi=function(e){return"show"===e.name},Yi={name:"transition",props:Fi,abstract:!0,render:function(e){var n=this,t=this.$slots.default;if(t&&(t=t.filter(Ki)).length){0;var a=this.mode;0;var i=t[0];if(function(e){for(;e=e.parent;)if(e.data.transition)return!0}(this.$vnode))return i;var r=Vi(i);if(!r)return i;if(this._leaving)return Wi(e,i);var o="__transition-".concat(this._uid,"-");r.key=null==r.key?r.isComment?o+"comment":o+r.tag:c(r.key)?0===String(r.key).indexOf(o)?r.key:o+r.key:r.key;var s=(r.data||(r.data={})).transition=Hi(this),l=this._vnode,d=Vi(l);if(r.data.directives&&r.data.directives.some(Zi)&&(r.data.show=!0),d&&d.data&&!function(e,n){return n.key===e.key&&n.tag===e.tag}(r,d)&&!fn(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var h=d.data.transition=D({},s);if("out-in"===a)return this._leaving=!0,We(h,"afterLeave",(function(){n._leaving=!1,n.$forceUpdate()})),Wi(e,i);if("in-out"===a){if(fn(r))return l;var u,p=function(){u()};We(s,"afterEnter",p),We(s,"enterCancelled",p),We(h,"delayLeave",(function(e){u=e}))}}return i}}},Xi=D({tag:String,moveClass:String},Fi);function Qi(e){e.elm._moveCb&&e.elm._moveCb(),e.elm._enterCb&&e.elm._enterCb()}function Ji(e){e.data.newPos=e.elm.getBoundingClientRect()}function er(e){var n=e.data.pos,t=e.data.newPos,a=n.left-t.left,i=n.top-t.top;if(a||i){e.data.moved=!0;var r=e.elm.style;r.transform=r.WebkitTransform="translate(".concat(a,"px,").concat(i,"px)"),r.transitionDuration="0s"}}delete Xi.mode;var nr={Transition:Yi,TransitionGroup:{props:Xi,beforeMount:function(){var e=this,n=this._update;this._update=function(t,a){var i=Qn(e);e.__patch__(e._vnode,e.kept,!1,!0),e._vnode=e.kept,i(),n.call(e,t,a)}},render:function(e){for(var n=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),a=this.prevChildren=this.children,i=this.$slots.default||[],r=this.children=[],o=Hi(this),s=0;s<i.length;s++){if((d=i[s]).tag)if(null!=d.key&&0!==String(d.key).indexOf("__vlist"))r.push(d),t[d.key]=d,(d.data||(d.data={})).transition=o;else;}if(a){var c=[],l=[];for(s=0;s<a.length;s++){var d;(d=a[s]).data.transition=o,d.data.pos=d.elm.getBoundingClientRect(),t[d.key]?c.push(d):l.push(d)}this.kept=e(n,null,c),this.removed=l}return e(n,null,r)},updated:function(){var e=this.prevChildren,n=this.moveClass||(this.name||"v")+"-move";e.length&&this.hasMove(e[0].elm,n)&&(e.forEach(Qi),e.forEach(Ji),e.forEach(er),this._reflow=document.body.offsetHeight,e.forEach((function(e){if(e.data.moved){var t=e.elm,a=t.style;_i(t,n),a.transform=a.WebkitTransform=a.transitionDuration="",t.addEventListener(gi,t._moveCb=function e(a){a&&a.target!==t||a&&!/transform$/.test(a.propertyName)||(t.removeEventListener(gi,e),t._moveCb=null,ki(t,n))})}})))},methods:{hasMove:function(e,n){if(!mi)return!1;if(this._hasMove)return this._hasMove;var t=e.cloneNode();e._transitionClasses&&e._transitionClasses.forEach((function(e){hi(t,e)})),di(t,n),t.style.display="none",this.$el.appendChild(t);var a=Pi(t);return this.$el.removeChild(t),this._hasMove=a.hasTransform}}}};function tr(e,n){for(var t in n)e[t]=n[t];return e}Wt.config.mustUseProp=function(e,n,t){return"value"===t&&ta(e)&&"button"!==n||"selected"===t&&"option"===e||"checked"===t&&"input"===e||"muted"===t&&"video"===e},Wt.config.isReservedTag=ya,Wt.config.isReservedAttr=na,Wt.config.getTagNamespace=function(e){return ga(e)?"svg":"math"===e?"math":void 0},Wt.config.isUnknownElement=function(e){if(!Z)return!0;if(ya(e))return!1;if(e=e.toLowerCase(),null!=ba[e])return ba[e];var n=document.createElement(e);return e.indexOf("-")>-1?ba[e]=n.constructor===window.HTMLUnknownElement||n.constructor===window.HTMLElement:ba[e]=/HTMLUnknownElement/.test(n.toString())},D(Wt.options.directives,$i),D(Wt.options.components,nr),Wt.prototype.__patch__=Z?zi:U,Wt.prototype.$mount=function(e,n){return function(e,n,t){var a;e.$el=n,e.$options.render||(e.$options.render=me),nt(e,"beforeMount"),a=function(){e._update(e._render(),t)},new Hn(e,a,U,{before:function(){e._isMounted&&!e._isDestroyed&&nt(e,"beforeUpdate")}},!0),t=!1;var i=e._preWatchers;if(i)for(var r=0;r<i.length;r++)i[r].run();return null==e.$vnode&&(e._isMounted=!0,nt(e,"mounted")),e}(this,e=e&&Z?function(e){if("string"==typeof e){var n=document.querySelector(e);return n||document.createElement("div")}return e}(e):void 0,n)},Z&&setTimeout((function(){$.devtools&&se&&se.emit("init",Wt)}),0);var ar=/[!'()*]/g,ir=function(e){return"%"+e.charCodeAt(0).toString(16)},rr=/%2C/g,or=function(e){return encodeURIComponent(e).replace(ar,ir).replace(rr,",")};function sr(e){try{return decodeURIComponent(e)}catch(e){0}return e}var cr=function(e){return null==e||"object"==typeof e?e:String(e)};function lr(e){var n={};return(e=e.trim().replace(/^(\?|#|&)/,""))?(e.split("&").forEach((function(e){var t=e.replace(/\+/g," ").split("="),a=sr(t.shift()),i=t.length>0?sr(t.join("=")):null;void 0===n[a]?n[a]=i:Array.isArray(n[a])?n[a].push(i):n[a]=[n[a],i]})),n):n}function dr(e){var n=e?Object.keys(e).map((function(n){var t=e[n];if(void 0===t)return"";if(null===t)return or(n);if(Array.isArray(t)){var a=[];return t.forEach((function(e){void 0!==e&&(null===e?a.push(or(n)):a.push(or(n)+"="+or(e)))})),a.join("&")}return or(n)+"="+or(t)})).filter((function(e){return e.length>0})).join("&"):null;return n?"?"+n:""}var hr=/\/?$/;function ur(e,n,t,a){var i=a&&a.options.stringifyQuery,r=n.query||{};try{r=pr(r)}catch(e){}var o={name:n.name||e&&e.name,meta:e&&e.meta||{},path:n.path||"/",hash:n.hash||"",query:r,params:n.params||{},fullPath:gr(n,i),matched:e?fr(e):[]};return t&&(o.redirectedFrom=gr(t,i)),Object.freeze(o)}function pr(e){if(Array.isArray(e))return e.map(pr);if(e&&"object"==typeof e){var n={};for(var t in e)n[t]=pr(e[t]);return n}return e}var mr=ur(null,{path:"/"});function fr(e){for(var n=[];e;)n.unshift(e),e=e.parent;return n}function gr(e,n){var t=e.path,a=e.query;void 0===a&&(a={});var i=e.hash;return void 0===i&&(i=""),(t||"/")+(n||dr)(a)+i}function yr(e,n,t){return n===mr?e===n:!!n&&(e.path&&n.path?e.path.replace(hr,"")===n.path.replace(hr,"")&&(t||e.hash===n.hash&&br(e.query,n.query)):!(!e.name||!n.name)&&(e.name===n.name&&(t||e.hash===n.hash&&br(e.query,n.query)&&br(e.params,n.params))))}function br(e,n){if(void 0===e&&(e={}),void 0===n&&(n={}),!e||!n)return e===n;var t=Object.keys(e).sort(),a=Object.keys(n).sort();return t.length===a.length&&t.every((function(t,i){var r=e[t];if(a[i]!==t)return!1;var o=n[t];return null==r||null==o?r===o:"object"==typeof r&&"object"==typeof o?br(r,o):String(r)===String(o)}))}function vr(e){for(var n=0;n<e.matched.length;n++){var t=e.matched[n];for(var a in t.instances){var i=t.instances[a],r=t.enteredCbs[a];if(i&&r){delete t.enteredCbs[a];for(var o=0;o<r.length;o++)i._isBeingDestroyed||r[o](i)}}}}var wr={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(e,n){var t=n.props,a=n.children,i=n.parent,r=n.data;r.routerView=!0;for(var o=i.$createElement,s=t.name,c=i.$route,l=i._routerViewCache||(i._routerViewCache={}),d=0,h=!1;i&&i._routerRoot!==i;){var u=i.$vnode?i.$vnode.data:{};u.routerView&&d++,u.keepAlive&&i._directInactive&&i._inactive&&(h=!0),i=i.$parent}if(r.routerViewDepth=d,h){var p=l[s],m=p&&p.component;return m?(p.configProps&&_r(m,r,p.route,p.configProps),o(m,r,a)):o()}var f=c.matched[d],g=f&&f.components[s];if(!f||!g)return l[s]=null,o();l[s]={component:g},r.registerRouteInstance=function(e,n){var t=f.instances[s];(n&&t!==e||!n&&t===e)&&(f.instances[s]=n)},(r.hook||(r.hook={})).prepatch=function(e,n){f.instances[s]=n.componentInstance},r.hook.init=function(e){e.data.keepAlive&&e.componentInstance&&e.componentInstance!==f.instances[s]&&(f.instances[s]=e.componentInstance),vr(c)};var y=f.props&&f.props[s];return y&&(tr(l[s],{route:c,configProps:y}),_r(g,r,c,y)),o(g,r,a)}};function _r(e,n,t,a){var i=n.props=function(e,n){switch(typeof n){case"undefined":return;case"object":return n;case"function":return n(e);case"boolean":return n?e.params:void 0;default:0}}(t,a);if(i){i=n.props=tr({},i);var r=n.attrs=n.attrs||{};for(var o in i)e.props&&o in e.props||(r[o]=i[o],delete i[o])}}function kr(e,n,t){var a=e.charAt(0);if("/"===a)return e;if("?"===a||"#"===a)return n+e;var i=n.split("/");t&&i[i.length-1]||i.pop();for(var r=e.replace(/^\//,"").split("/"),o=0;o<r.length;o++){var s=r[o];".."===s?i.pop():"."!==s&&i.push(s)}return""!==i[0]&&i.unshift(""),i.join("/")}function xr(e){return e.replace(/\/(?:\s*\/)+/g,"/")}var Cr=Array.isArray||function(e){return"[object Array]"==Object.prototype.toString.call(e)},Pr=qr,Tr=Lr,Mr=function(e,n){return zr(Lr(e,n),n)},Ar=zr,Sr=Gr,Ir=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function Lr(e,n){for(var t,a=[],i=0,r=0,o="",s=n&&n.delimiter||"/";null!=(t=Ir.exec(e));){var c=t[0],l=t[1],d=t.index;if(o+=e.slice(r,d),r=d+c.length,l)o+=l[1];else{var h=e[r],u=t[2],p=t[3],m=t[4],f=t[5],g=t[6],y=t[7];o&&(a.push(o),o="");var b=null!=u&&null!=h&&h!==u,v="+"===g||"*"===g,w="?"===g||"*"===g,_=t[2]||s,k=m||f;a.push({name:p||i++,prefix:u||"",delimiter:_,optional:w,repeat:v,partial:b,asterisk:!!y,pattern:k?Or(k):y?".*":"[^"+Ur(_)+"]+?"})}}return r<e.length&&(o+=e.substr(r)),o&&a.push(o),a}function Dr(e){return encodeURI(e).replace(/[\/?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()}))}function zr(e,n){for(var t=new Array(e.length),a=0;a<e.length;a++)"object"==typeof e[a]&&(t[a]=new RegExp("^(?:"+e[a].pattern+")$",Er(n)));return function(n,a){for(var i="",r=n||{},o=(a||{}).pretty?Dr:encodeURIComponent,s=0;s<e.length;s++){var c=e[s];if("string"!=typeof c){var l,d=r[c.name];if(null==d){if(c.optional){c.partial&&(i+=c.prefix);continue}throw new TypeError('Expected "'+c.name+'" to be defined')}if(Cr(d)){if(!c.repeat)throw new TypeError('Expected "'+c.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(c.optional)continue;throw new TypeError('Expected "'+c.name+'" to not be empty')}for(var h=0;h<d.length;h++){if(l=o(d[h]),!t[s].test(l))throw new TypeError('Expected all "'+c.name+'" to match "'+c.pattern+'", but received `'+JSON.stringify(l)+"`");i+=(0===h?c.prefix:c.delimiter)+l}}else{if(l=c.asterisk?encodeURI(d).replace(/[?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()})):o(d),!t[s].test(l))throw new TypeError('Expected "'+c.name+'" to match "'+c.pattern+'", but received "'+l+'"');i+=c.prefix+l}}else i+=c}return i}}function Ur(e){return e.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function Or(e){return e.replace(/([=!:$\/()])/g,"\\$1")}function Rr(e,n){return e.keys=n,e}function Er(e){return e&&e.sensitive?"":"i"}function Gr(e,n,t){Cr(n)||(t=n||t,n=[]);for(var a=(t=t||{}).strict,i=!1!==t.end,r="",o=0;o<e.length;o++){var s=e[o];if("string"==typeof s)r+=Ur(s);else{var c=Ur(s.prefix),l="(?:"+s.pattern+")";n.push(s),s.repeat&&(l+="(?:"+c+l+")*"),r+=l=s.optional?s.partial?c+"("+l+")?":"(?:"+c+"("+l+"))?":c+"("+l+")"}}var d=Ur(t.delimiter||"/"),h=r.slice(-d.length)===d;return a||(r=(h?r.slice(0,-d.length):r)+"(?:"+d+"(?=$))?"),r+=i?"$":a&&h?"":"(?="+d+"|$)",Rr(new RegExp("^"+r,Er(t)),n)}function qr(e,n,t){return Cr(n)||(t=n||t,n=[]),t=t||{},e instanceof RegExp?function(e,n){var t=e.source.match(/\((?!\?)/g);if(t)for(var a=0;a<t.length;a++)n.push({name:a,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return Rr(e,n)}(e,n):Cr(e)?function(e,n,t){for(var a=[],i=0;i<e.length;i++)a.push(qr(e[i],n,t).source);return Rr(new RegExp("(?:"+a.join("|")+")",Er(t)),n)}(e,n,t):function(e,n,t){return Gr(Lr(e,t),n,t)}(e,n,t)}Pr.parse=Tr,Pr.compile=Mr,Pr.tokensToFunction=Ar,Pr.tokensToRegExp=Sr;var Br=Object.create(null);function jr(e,n,t){n=n||{};try{var a=Br[e]||(Br[e]=Pr.compile(e));return"string"==typeof n.pathMatch&&(n[0]=n.pathMatch),a(n,{pretty:!0})}catch(e){return""}finally{delete n[0]}}function Nr(e,n,t,a){var i="string"==typeof e?{path:e}:e;if(i._normalized)return i;if(i.name){var r=(i=tr({},e)).params;return r&&"object"==typeof r&&(i.params=tr({},r)),i}if(!i.path&&i.params&&n){(i=tr({},i))._normalized=!0;var o=tr(tr({},n.params),i.params);if(n.name)i.name=n.name,i.params=o;else if(n.matched.length){var s=n.matched[n.matched.length-1].path;i.path=jr(s,o,n.path)}else 0;return i}var c=function(e){var n="",t="",a=e.indexOf("#");a>=0&&(n=e.slice(a),e=e.slice(0,a));var i=e.indexOf("?");return i>=0&&(t=e.slice(i+1),e=e.slice(0,i)),{path:e,query:t,hash:n}}(i.path||""),l=n&&n.path||"/",d=c.path?kr(c.path,l,t||i.append):l,h=function(e,n,t){void 0===n&&(n={});var a,i=t||lr;try{a=i(e||"")}catch(e){a={}}for(var r in n){var o=n[r];a[r]=Array.isArray(o)?o.map(cr):cr(o)}return a}(c.query,i.query,a&&a.options.parseQuery),u=i.hash||c.hash;return u&&"#"!==u.charAt(0)&&(u="#"+u),{_normalized:!0,path:d,query:h,hash:u}}var $r,Fr=function(){},Vr={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(e){var n=this,t=this.$router,a=this.$route,i=t.resolve(this.to,a,this.append),r=i.location,o=i.route,s=i.href,c={},l=t.options.linkActiveClass,d=t.options.linkExactActiveClass,h=null==l?"router-link-active":l,u=null==d?"router-link-exact-active":d,p=null==this.activeClass?h:this.activeClass,m=null==this.exactActiveClass?u:this.exactActiveClass,f=o.redirectedFrom?ur(null,Nr(o.redirectedFrom),null,t):o;c[m]=yr(a,f,this.exactPath),c[p]=this.exact||this.exactPath?c[m]:function(e,n){return 0===e.path.replace(hr,"/").indexOf(n.path.replace(hr,"/"))&&(!n.hash||e.hash===n.hash)&&function(e,n){for(var t in n)if(!(t in e))return!1;return!0}(e.query,n.query)}(a,f);var g=c[m]?this.ariaCurrentValue:null,y=function(e){Hr(e)&&(n.replace?t.replace(r,Fr):t.push(r,Fr))},b={click:Hr};Array.isArray(this.event)?this.event.forEach((function(e){b[e]=y})):b[this.event]=y;var v={class:c},w=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:o,navigate:y,isActive:c[p],isExactActive:c[m]});if(w){if(1===w.length)return w[0];if(w.length>1||!w.length)return 0===w.length?e():e("span",{},w)}if("a"===this.tag)v.on=b,v.attrs={href:s,"aria-current":g};else{var _=function e(n){var t;if(n)for(var a=0;a<n.length;a++){if("a"===(t=n[a]).tag)return t;if(t.children&&(t=e(t.children)))return t}}(this.$slots.default);if(_){_.isStatic=!1;var k=_.data=tr({},_.data);for(var x in k.on=k.on||{},k.on){var C=k.on[x];x in b&&(k.on[x]=Array.isArray(C)?C:[C])}for(var P in b)P in k.on?k.on[P].push(b[P]):k.on[P]=y;var T=_.data.attrs=tr({},_.data.attrs);T.href=s,T["aria-current"]=g}else v.on=b}return e(this.tag,v,this.$slots.default)}};function Hr(e){if(!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey||e.defaultPrevented||void 0!==e.button&&0!==e.button)){if(e.currentTarget&&e.currentTarget.getAttribute){var n=e.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(n))return}return e.preventDefault&&e.preventDefault(),!0}}var Wr="undefined"!=typeof window;function Kr(e,n,t,a,i){var r=n||[],o=t||Object.create(null),s=a||Object.create(null);e.forEach((function(e){!function e(n,t,a,i,r,o){var s=i.path,c=i.name;0;var l=i.pathToRegexpOptions||{},d=function(e,n,t){t||(e=e.replace(/\/$/,""));if("/"===e[0])return e;if(null==n)return e;return xr(n.path+"/"+e)}(s,r,l.strict);"boolean"==typeof i.caseSensitive&&(l.sensitive=i.caseSensitive);var h={path:d,regex:Zr(d,l),components:i.components||{default:i.component},alias:i.alias?"string"==typeof i.alias?[i.alias]:i.alias:[],instances:{},enteredCbs:{},name:c,parent:r,matchAs:o,redirect:i.redirect,beforeEnter:i.beforeEnter,meta:i.meta||{},props:null==i.props?{}:i.components?i.props:{default:i.props}};i.children&&i.children.forEach((function(i){var r=o?xr(o+"/"+i.path):void 0;e(n,t,a,i,h,r)}));t[h.path]||(n.push(h.path),t[h.path]=h);if(void 0!==i.alias)for(var u=Array.isArray(i.alias)?i.alias:[i.alias],p=0;p<u.length;++p){0;var m={path:u[p],children:i.children};e(n,t,a,m,r,h.path||"/")}c&&(a[c]||(a[c]=h))}(r,o,s,e,i)}));for(var c=0,l=r.length;c<l;c++)"*"===r[c]&&(r.push(r.splice(c,1)[0]),l--,c--);return{pathList:r,pathMap:o,nameMap:s}}function Zr(e,n){return Pr(e,[],n)}function Yr(e,n){var t=Kr(e),a=t.pathList,i=t.pathMap,r=t.nameMap;function o(e,t,o){var s=Nr(e,t,!1,n),l=s.name;if(l){var d=r[l];if(!d)return c(null,s);var h=d.regex.keys.filter((function(e){return!e.optional})).map((function(e){return e.name}));if("object"!=typeof s.params&&(s.params={}),t&&"object"==typeof t.params)for(var u in t.params)!(u in s.params)&&h.indexOf(u)>-1&&(s.params[u]=t.params[u]);return s.path=jr(d.path,s.params),c(d,s,o)}if(s.path){s.params={};for(var p=0;p<a.length;p++){var m=a[p],f=i[m];if(Xr(f.regex,s.path,s.params))return c(f,s,o)}}return c(null,s)}function s(e,t){var a=e.redirect,i="function"==typeof a?a(ur(e,t,null,n)):a;if("string"==typeof i&&(i={path:i}),!i||"object"!=typeof i)return c(null,t);var s=i,l=s.name,d=s.path,h=t.query,u=t.hash,p=t.params;if(h=s.hasOwnProperty("query")?s.query:h,u=s.hasOwnProperty("hash")?s.hash:u,p=s.hasOwnProperty("params")?s.params:p,l){r[l];return o({_normalized:!0,name:l,query:h,hash:u,params:p},void 0,t)}if(d){var m=function(e,n){return kr(e,n.parent?n.parent.path:"/",!0)}(d,e);return o({_normalized:!0,path:jr(m,p),query:h,hash:u},void 0,t)}return c(null,t)}function c(e,t,a){return e&&e.redirect?s(e,a||t):e&&e.matchAs?function(e,n,t){var a=o({_normalized:!0,path:jr(t,n.params)});if(a){var i=a.matched,r=i[i.length-1];return n.params=a.params,c(r,n)}return c(null,n)}(0,t,e.matchAs):ur(e,t,a,n)}return{match:o,addRoute:function(e,n){var t="object"!=typeof e?r[e]:void 0;Kr([n||e],a,i,r,t),t&&t.alias.length&&Kr(t.alias.map((function(e){return{path:e,children:[n]}})),a,i,r,t)},getRoutes:function(){return a.map((function(e){return i[e]}))},addRoutes:function(e){Kr(e,a,i,r)}}}function Xr(e,n,t){var a=n.match(e);if(!a)return!1;if(!t)return!0;for(var i=1,r=a.length;i<r;++i){var o=e.keys[i-1];o&&(t[o.name||"pathMatch"]="string"==typeof a[i]?sr(a[i]):a[i])}return!0}var Qr=Wr&&window.performance&&window.performance.now?window.performance:Date;function Jr(){return Qr.now().toFixed(3)}var eo=Jr();function no(){return eo}function to(e){return eo=e}var ao=Object.create(null);function io(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var e=window.location.protocol+"//"+window.location.host,n=window.location.href.replace(e,""),t=tr({},window.history.state);return t.key=no(),window.history.replaceState(t,"",n),window.addEventListener("popstate",so),function(){window.removeEventListener("popstate",so)}}function ro(e,n,t,a){if(e.app){var i=e.options.scrollBehavior;i&&e.app.$nextTick((function(){var r=function(){var e=no();if(e)return ao[e]}(),o=i.call(e,n,t,a?r:null);o&&("function"==typeof o.then?o.then((function(e){po(e,r)})).catch((function(e){0})):po(o,r))}))}}function oo(){var e=no();e&&(ao[e]={x:window.pageXOffset,y:window.pageYOffset})}function so(e){oo(),e.state&&e.state.key&&to(e.state.key)}function co(e){return ho(e.x)||ho(e.y)}function lo(e){return{x:ho(e.x)?e.x:window.pageXOffset,y:ho(e.y)?e.y:window.pageYOffset}}function ho(e){return"number"==typeof e}var uo=/^#\d/;function po(e,n){var t,a="object"==typeof e;if(a&&"string"==typeof e.selector){var i=uo.test(e.selector)?document.getElementById(e.selector.slice(1)):document.querySelector(e.selector);if(i){var r=e.offset&&"object"==typeof e.offset?e.offset:{};n=function(e,n){var t=document.documentElement.getBoundingClientRect(),a=e.getBoundingClientRect();return{x:a.left-t.left-n.x,y:a.top-t.top-n.y}}(i,r={x:ho((t=r).x)?t.x:0,y:ho(t.y)?t.y:0})}else co(e)&&(n=lo(e))}else a&&co(e)&&(n=lo(e));n&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:n.x,top:n.y,behavior:e.behavior}):window.scrollTo(n.x,n.y))}var mo,fo=Wr&&((-1===(mo=window.navigator.userAgent).indexOf("Android 2.")&&-1===mo.indexOf("Android 4.0")||-1===mo.indexOf("Mobile Safari")||-1!==mo.indexOf("Chrome")||-1!==mo.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function go(e,n){oo();var t=window.history;try{if(n){var a=tr({},t.state);a.key=no(),t.replaceState(a,"",e)}else t.pushState({key:to(Jr())},"",e)}catch(t){window.location[n?"replace":"assign"](e)}}function yo(e){go(e,!0)}var bo={redirected:2,aborted:4,cancelled:8,duplicated:16};function vo(e,n){return _o(e,n,bo.redirected,'Redirected when going from "'+e.fullPath+'" to "'+function(e){if("string"==typeof e)return e;if("path"in e)return e.path;var n={};return ko.forEach((function(t){t in e&&(n[t]=e[t])})),JSON.stringify(n,null,2)}(n)+'" via a navigation guard.')}function wo(e,n){return _o(e,n,bo.cancelled,'Navigation cancelled from "'+e.fullPath+'" to "'+n.fullPath+'" with a new navigation.')}function _o(e,n,t,a){var i=new Error(a);return i._isRouter=!0,i.from=e,i.to=n,i.type=t,i}var ko=["params","query","hash"];function xo(e){return Object.prototype.toString.call(e).indexOf("Error")>-1}function Co(e,n){return xo(e)&&e._isRouter&&(null==n||e.type===n)}function Po(e,n,t){var a=function(i){i>=e.length?t():e[i]?n(e[i],(function(){a(i+1)})):a(i+1)};a(0)}function To(e){return function(n,t,a){var i=!1,r=0,o=null;Mo(e,(function(e,n,t,s){if("function"==typeof e&&void 0===e.cid){i=!0,r++;var c,l=Io((function(n){var i;((i=n).__esModule||So&&"Module"===i[Symbol.toStringTag])&&(n=n.default),e.resolved="function"==typeof n?n:$r.extend(n),t.components[s]=n,--r<=0&&a()})),d=Io((function(e){var n="Failed to resolve async component "+s+": "+e;o||(o=xo(e)?e:new Error(n),a(o))}));try{c=e(l,d)}catch(e){d(e)}if(c)if("function"==typeof c.then)c.then(l,d);else{var h=c.component;h&&"function"==typeof h.then&&h.then(l,d)}}})),i||a()}}function Mo(e,n){return Ao(e.map((function(e){return Object.keys(e.components).map((function(t){return n(e.components[t],e.instances[t],e,t)}))})))}function Ao(e){return Array.prototype.concat.apply([],e)}var So="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function Io(e){var n=!1;return function(){for(var t=[],a=arguments.length;a--;)t[a]=arguments[a];if(!n)return n=!0,e.apply(this,t)}}var Lo=function(e,n){this.router=e,this.base=function(e){if(!e)if(Wr){var n=document.querySelector("base");e=(e=n&&n.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else e="/";"/"!==e.charAt(0)&&(e="/"+e);return e.replace(/\/$/,"")}(n),this.current=mr,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function Do(e,n,t,a){var i=Mo(e,(function(e,a,i,r){var o=function(e,n){"function"!=typeof e&&(e=$r.extend(e));return e.options[n]}(e,n);if(o)return Array.isArray(o)?o.map((function(e){return t(e,a,i,r)})):t(o,a,i,r)}));return Ao(a?i.reverse():i)}function zo(e,n){if(n)return function(){return e.apply(n,arguments)}}Lo.prototype.listen=function(e){this.cb=e},Lo.prototype.onReady=function(e,n){this.ready?e():(this.readyCbs.push(e),n&&this.readyErrorCbs.push(n))},Lo.prototype.onError=function(e){this.errorCbs.push(e)},Lo.prototype.transitionTo=function(e,n,t){var a,i=this;try{a=this.router.match(e,this.current)}catch(e){throw this.errorCbs.forEach((function(n){n(e)})),e}var r=this.current;this.confirmTransition(a,(function(){i.updateRoute(a),n&&n(a),i.ensureURL(),i.router.afterHooks.forEach((function(e){e&&e(a,r)})),i.ready||(i.ready=!0,i.readyCbs.forEach((function(e){e(a)})))}),(function(e){t&&t(e),e&&!i.ready&&(Co(e,bo.redirected)&&r===mr||(i.ready=!0,i.readyErrorCbs.forEach((function(n){n(e)}))))}))},Lo.prototype.confirmTransition=function(e,n,t){var a=this,i=this.current;this.pending=e;var r=function(e){!Co(e)&&xo(e)&&(a.errorCbs.length?a.errorCbs.forEach((function(n){n(e)})):console.error(e)),t&&t(e)},o=e.matched.length-1,s=i.matched.length-1;if(yr(e,i)&&o===s&&e.matched[o]===i.matched[s])return this.ensureURL(),e.hash&&ro(this.router,i,e,!1),r(function(e,n){var t=_o(e,n,bo.duplicated,'Avoided redundant navigation to current location: "'+e.fullPath+'".');return t.name="NavigationDuplicated",t}(i,e));var c=function(e,n){var t,a=Math.max(e.length,n.length);for(t=0;t<a&&e[t]===n[t];t++);return{updated:n.slice(0,t),activated:n.slice(t),deactivated:e.slice(t)}}(this.current.matched,e.matched),l=c.updated,d=c.deactivated,h=c.activated,u=[].concat(function(e){return Do(e,"beforeRouteLeave",zo,!0)}(d),this.router.beforeHooks,function(e){return Do(e,"beforeRouteUpdate",zo)}(l),h.map((function(e){return e.beforeEnter})),To(h)),p=function(n,t){if(a.pending!==e)return r(wo(i,e));try{n(e,i,(function(n){!1===n?(a.ensureURL(!0),r(function(e,n){return _o(e,n,bo.aborted,'Navigation aborted from "'+e.fullPath+'" to "'+n.fullPath+'" via a navigation guard.')}(i,e))):xo(n)?(a.ensureURL(!0),r(n)):"string"==typeof n||"object"==typeof n&&("string"==typeof n.path||"string"==typeof n.name)?(r(vo(i,e)),"object"==typeof n&&n.replace?a.replace(n):a.push(n)):t(n)}))}catch(e){r(e)}};Po(u,p,(function(){Po(function(e){return Do(e,"beforeRouteEnter",(function(e,n,t,a){return function(e,n,t){return function(a,i,r){return e(a,i,(function(e){"function"==typeof e&&(n.enteredCbs[t]||(n.enteredCbs[t]=[]),n.enteredCbs[t].push(e)),r(e)}))}}(e,t,a)}))}(h).concat(a.router.resolveHooks),p,(function(){if(a.pending!==e)return r(wo(i,e));a.pending=null,n(e),a.router.app&&a.router.app.$nextTick((function(){vr(e)}))}))}))},Lo.prototype.updateRoute=function(e){this.current=e,this.cb&&this.cb(e)},Lo.prototype.setupListeners=function(){},Lo.prototype.teardown=function(){this.listeners.forEach((function(e){e()})),this.listeners=[],this.current=mr,this.pending=null};var Uo=function(e){function n(n,t){e.call(this,n,t),this._startLocation=Oo(this.base)}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router,t=n.options.scrollBehavior,a=fo&&t;a&&this.listeners.push(io());var i=function(){var t=e.current,i=Oo(e.base);e.current===mr&&i===e._startLocation||e.transitionTo(i,(function(e){a&&ro(n,e,t,!0)}))};window.addEventListener("popstate",i),this.listeners.push((function(){window.removeEventListener("popstate",i)}))}},n.prototype.go=function(e){window.history.go(e)},n.prototype.push=function(e,n,t){var a=this,i=this.current;this.transitionTo(e,(function(e){go(xr(a.base+e.fullPath)),ro(a.router,e,i,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var a=this,i=this.current;this.transitionTo(e,(function(e){yo(xr(a.base+e.fullPath)),ro(a.router,e,i,!1),n&&n(e)}),t)},n.prototype.ensureURL=function(e){if(Oo(this.base)!==this.current.fullPath){var n=xr(this.base+this.current.fullPath);e?go(n):yo(n)}},n.prototype.getCurrentLocation=function(){return Oo(this.base)},n}(Lo);function Oo(e){var n=window.location.pathname,t=n.toLowerCase(),a=e.toLowerCase();return!e||t!==a&&0!==t.indexOf(xr(a+"/"))||(n=n.slice(e.length)),(n||"/")+window.location.search+window.location.hash}var Ro=function(e){function n(n,t,a){e.call(this,n,t),a&&function(e){var n=Oo(e);if(!/^\/#/.test(n))return window.location.replace(xr(e+"/#"+n)),!0}(this.base)||Eo()}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router.options.scrollBehavior,t=fo&&n;t&&this.listeners.push(io());var a=function(){var n=e.current;Eo()&&e.transitionTo(Go(),(function(a){t&&ro(e.router,a,n,!0),fo||jo(a.fullPath)}))},i=fo?"popstate":"hashchange";window.addEventListener(i,a),this.listeners.push((function(){window.removeEventListener(i,a)}))}},n.prototype.push=function(e,n,t){var a=this,i=this.current;this.transitionTo(e,(function(e){Bo(e.fullPath),ro(a.router,e,i,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var a=this,i=this.current;this.transitionTo(e,(function(e){jo(e.fullPath),ro(a.router,e,i,!1),n&&n(e)}),t)},n.prototype.go=function(e){window.history.go(e)},n.prototype.ensureURL=function(e){var n=this.current.fullPath;Go()!==n&&(e?Bo(n):jo(n))},n.prototype.getCurrentLocation=function(){return Go()},n}(Lo);function Eo(){var e=Go();return"/"===e.charAt(0)||(jo("/"+e),!1)}function Go(){var e=window.location.href,n=e.indexOf("#");return n<0?"":e=e.slice(n+1)}function qo(e){var n=window.location.href,t=n.indexOf("#");return(t>=0?n.slice(0,t):n)+"#"+e}function Bo(e){fo?go(qo(e)):window.location.hash=e}function jo(e){fo?yo(qo(e)):window.location.replace(qo(e))}var No=function(e){function n(n,t){e.call(this,n,t),this.stack=[],this.index=-1}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.push=function(e,n,t){var a=this;this.transitionTo(e,(function(e){a.stack=a.stack.slice(0,a.index+1).concat(e),a.index++,n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var a=this;this.transitionTo(e,(function(e){a.stack=a.stack.slice(0,a.index).concat(e),n&&n(e)}),t)},n.prototype.go=function(e){var n=this,t=this.index+e;if(!(t<0||t>=this.stack.length)){var a=this.stack[t];this.confirmTransition(a,(function(){var e=n.current;n.index=t,n.updateRoute(a),n.router.afterHooks.forEach((function(n){n&&n(a,e)}))}),(function(e){Co(e,bo.duplicated)&&(n.index=t)}))}},n.prototype.getCurrentLocation=function(){var e=this.stack[this.stack.length-1];return e?e.fullPath:"/"},n.prototype.ensureURL=function(){},n}(Lo),$o=function(e){void 0===e&&(e={}),this.app=null,this.apps=[],this.options=e,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Yr(e.routes||[],this);var n=e.mode||"hash";switch(this.fallback="history"===n&&!fo&&!1!==e.fallback,this.fallback&&(n="hash"),Wr||(n="abstract"),this.mode=n,n){case"history":this.history=new Uo(this,e.base);break;case"hash":this.history=new Ro(this,e.base,this.fallback);break;case"abstract":this.history=new No(this,e.base);break;default:0}},Fo={currentRoute:{configurable:!0}};$o.prototype.match=function(e,n,t){return this.matcher.match(e,n,t)},Fo.currentRoute.get=function(){return this.history&&this.history.current},$o.prototype.init=function(e){var n=this;if(this.apps.push(e),e.$once("hook:destroyed",(function(){var t=n.apps.indexOf(e);t>-1&&n.apps.splice(t,1),n.app===e&&(n.app=n.apps[0]||null),n.app||n.history.teardown()})),!this.app){this.app=e;var t=this.history;if(t instanceof Uo||t instanceof Ro){var a=function(e){t.setupListeners(),function(e){var a=t.current,i=n.options.scrollBehavior;fo&&i&&"fullPath"in e&&ro(n,e,a,!1)}(e)};t.transitionTo(t.getCurrentLocation(),a,a)}t.listen((function(e){n.apps.forEach((function(n){n._route=e}))}))}},$o.prototype.beforeEach=function(e){return Ho(this.beforeHooks,e)},$o.prototype.beforeResolve=function(e){return Ho(this.resolveHooks,e)},$o.prototype.afterEach=function(e){return Ho(this.afterHooks,e)},$o.prototype.onReady=function(e,n){this.history.onReady(e,n)},$o.prototype.onError=function(e){this.history.onError(e)},$o.prototype.push=function(e,n,t){var a=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){a.history.push(e,n,t)}));this.history.push(e,n,t)},$o.prototype.replace=function(e,n,t){var a=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){a.history.replace(e,n,t)}));this.history.replace(e,n,t)},$o.prototype.go=function(e){this.history.go(e)},$o.prototype.back=function(){this.go(-1)},$o.prototype.forward=function(){this.go(1)},$o.prototype.getMatchedComponents=function(e){var n=e?e.matched?e:this.resolve(e).route:this.currentRoute;return n?[].concat.apply([],n.matched.map((function(e){return Object.keys(e.components).map((function(n){return e.components[n]}))}))):[]},$o.prototype.resolve=function(e,n,t){var a=Nr(e,n=n||this.history.current,t,this),i=this.match(a,n),r=i.redirectedFrom||i.fullPath;return{location:a,route:i,href:function(e,n,t){var a="hash"===t?"#"+n:n;return e?xr(e+"/"+a):a}(this.history.base,r,this.mode),normalizedTo:a,resolved:i}},$o.prototype.getRoutes=function(){return this.matcher.getRoutes()},$o.prototype.addRoute=function(e,n){this.matcher.addRoute(e,n),this.history.current!==mr&&this.history.transitionTo(this.history.getCurrentLocation())},$o.prototype.addRoutes=function(e){this.matcher.addRoutes(e),this.history.current!==mr&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties($o.prototype,Fo);var Vo=$o;function Ho(e,n){return e.push(n),function(){var t=e.indexOf(n);t>-1&&e.splice(t,1)}}$o.install=function e(n){if(!e.installed||$r!==n){e.installed=!0,$r=n;var t=function(e){return void 0!==e},a=function(e,n){var a=e.$options._parentVnode;t(a)&&t(a=a.data)&&t(a=a.registerRouteInstance)&&a(e,n)};n.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),n.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,a(this,this)},destroyed:function(){a(this)}}),Object.defineProperty(n.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(n.prototype,"$route",{get:function(){return this._routerRoot._route}}),n.component("RouterView",wr),n.component("RouterLink",Vr);var i=n.config.optionMergeStrategies;i.beforeRouteEnter=i.beforeRouteLeave=i.beforeRouteUpdate=i.created}},$o.version="3.6.5",$o.isNavigationFailure=Co,$o.NavigationFailureType=bo,$o.START_LOCATION=mr,Wr&&window.Vue&&window.Vue.use($o);t(131);t(153),t(29);var Wo={NotFound:()=>Promise.all([t.e(0),t.e(4)]).then(t.bind(null,415)),Layout:()=>Promise.all([t.e(0),t.e(2)]).then(t.bind(null,414))},Ko={"v-0fdda314":()=>t.e(5).then(t.bind(null,416)),"v-37c5bc96":()=>t.e(6).then(t.bind(null,417)),"v-07282236":()=>t.e(8).then(t.bind(null,418)),"v-4553c3a8":()=>t.e(7).then(t.bind(null,419)),"v-bd5cd4d4":()=>t.e(9).then(t.bind(null,420)),"v-07abb6d6":()=>t.e(10).then(t.bind(null,421)),"v-577baade":()=>t.e(11).then(t.bind(null,422)),"v-abb0f8a2":()=>t.e(12).then(t.bind(null,423)),"v-d7471e16":()=>t.e(13).then(t.bind(null,424)),"v-b2fd417c":()=>t.e(14).then(t.bind(null,425)),"v-7f8db936":()=>t.e(16).then(t.bind(null,426)),"v-9842a21a":()=>t.e(15).then(t.bind(null,427)),"v-171f66aa":()=>t.e(17).then(t.bind(null,428)),"v-788071c5":()=>t.e(18).then(t.bind(null,429)),"v-662ff4fd":()=>t.e(20).then(t.bind(null,430)),"v-25995465":()=>t.e(19).then(t.bind(null,431)),"v-065eff05":()=>t.e(21).then(t.bind(null,432)),"v-f3b213e2":()=>t.e(22).then(t.bind(null,433)),"v-0b217b45":()=>t.e(23).then(t.bind(null,434)),"v-2d0923f6":()=>t.e(24).then(t.bind(null,435)),"v-4d042c96":()=>t.e(25).then(t.bind(null,436)),"v-49e4dae0":()=>t.e(26).then(t.bind(null,437)),"v-c73e5956":()=>t.e(27).then(t.bind(null,438)),"v-89581d5c":()=>t.e(28).then(t.bind(null,439)),"v-6ffd4505":()=>t.e(29).then(t.bind(null,440)),"v-f96426b0":()=>t.e(31).then(t.bind(null,441)),"v-b25c491a":()=>t.e(30).then(t.bind(null,442)),"v-5849f946":()=>t.e(32).then(t.bind(null,443)),"v-2861ec8b":()=>t.e(33).then(t.bind(null,444)),"v-04292a45":()=>t.e(34).then(t.bind(null,445)),"v-65bd31e5":()=>t.e(36).then(t.bind(null,446)),"v-d22a70f6":()=>t.e(35).then(t.bind(null,447)),"v-e33712b6":()=>t.e(37).then(t.bind(null,448)),"v-a0731602":()=>t.e(38).then(t.bind(null,449)),"v-75a07a01":()=>t.e(39).then(t.bind(null,450)),"v-5474106a":()=>t.e(40).then(t.bind(null,451)),"v-4349358d":()=>t.e(42).then(t.bind(null,452)),"v-02aef12e":()=>t.e(41).then(t.bind(null,453)),"v-9e671962":()=>t.e(43).then(t.bind(null,454)),"v-1e4fb111":()=>t.e(44).then(t.bind(null,455)),"v-e85a225a":()=>t.e(45).then(t.bind(null,456)),"v-c25059fe":()=>t.e(46).then(t.bind(null,457)),"v-53bdf925":()=>t.e(48).then(t.bind(null,458)),"v-798f0c72":()=>t.e(47).then(t.bind(null,459)),"v-c2e69736":()=>t.e(49).then(t.bind(null,460)),"v-121d90a9":()=>t.e(50).then(t.bind(null,461)),"v-3e3c1a05":()=>t.e(51).then(t.bind(null,462)),"v-891a4db6":()=>t.e(52).then(t.bind(null,463)),"v-64b7ef30":()=>t.e(53).then(t.bind(null,464)),"v-90037b60":()=>t.e(54).then(t.bind(null,465)),"v-7ddc0f42":()=>t.e(55).then(t.bind(null,466)),"v-c1b2e1c6":()=>t.e(56).then(t.bind(null,467)),"v-26a9c736":()=>t.e(57).then(t.bind(null,468)),"v-b267e276":()=>t.e(58).then(t.bind(null,469)),"v-0ee7ecc5":()=>t.e(60).then(t.bind(null,470)),"v-259f44f9":()=>t.e(59).then(t.bind(null,471)),"v-2e715932":()=>t.e(62).then(t.bind(null,472)),"v-bb1ed876":()=>t.e(63).then(t.bind(null,473)),"v-e114a476":()=>t.e(61).then(t.bind(null,474)),"v-23bbae21":()=>t.e(64).then(t.bind(null,475)),"v-6ed8ffdb":()=>t.e(65).then(t.bind(null,476)),"v-f8e02736":()=>t.e(66).then(t.bind(null,477)),"v-01216c79":()=>t.e(68).then(t.bind(null,478)),"v-2f8a0a7e":()=>t.e(67).then(t.bind(null,479))};function Zo(e){const n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}const Yo=/-(\w)/g,Xo=Zo(e=>e.replace(Yo,(e,n)=>n?n.toUpperCase():"")),Qo=/\B([A-Z])/g,Jo=Zo(e=>e.replace(Qo,"-$1").toLowerCase()),es=Zo(e=>e.charAt(0).toUpperCase()+e.slice(1));function ns(e,n){if(!n)return;if(e(n))return e(n);return n.includes("-")?e(es(Xo(n))):e(es(n))||e(Jo(n))}const ts=Object.assign({},Wo,Ko),as=e=>ts[e],is=e=>Ko[e],rs=e=>Wo[e],os=e=>Wt.component(e);function ss(e){return ns(is,e)}function cs(e){return ns(rs,e)}function ls(e){return ns(as,e)}function ds(e){return ns(os,e)}function hs(...e){return Promise.all(e.filter(e=>e).map(async e=>{if(!ds(e)&&ls(e)){const n=await ls(e)();Wt.component(e,n.default)}}))}function us(e,n){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[e]=n)}var ps=t(115),ms=t.n(ps),fs={created(){if(this.siteMeta=this.$site.headTags.filter(([e])=>"meta"===e).map(([e,n])=>n),this.$ssrContext){const n=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(e=n)?e.map(e=>{let n="<meta";return Object.keys(e).forEach(t=>{n+=` ${t}="${e[t]}"`}),n+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=ys(this.$canonicalUrl)}var e},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const e=this.getMergedMetaTags();this.currentMetaTags=bs(e,this.currentMetaTags)},getMergedMetaTags(){const e=this.$page.frontmatter.meta||[];return ms()([{name:"description",content:this.$description}],e,this.siteMeta,vs)},updateCanonicalLink(){gs(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",ys(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){bs(null,this.currentMetaTags),gs()}};function gs(){const e=document.querySelector("link[rel='canonical']");e&&e.remove()}function ys(e=""){return e?`<link href="${e}" rel="canonical" />`:""}function bs(e,n){if(n&&[...n].filter(e=>e.parentNode===document.head).forEach(e=>document.head.removeChild(e)),e)return e.map(e=>{const n=document.createElement("meta");return Object.keys(e).forEach(t=>{n.setAttribute(t,e[t])}),document.head.appendChild(n),n})}function vs(e){for(const n of["name","property","itemprop"])if(e.hasOwnProperty(n))return e[n]+n;return JSON.stringify(e)}var ws=t(61),_s={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(ws)()((function(){this.setActiveHash()}),300),setActiveHash(){const e=[].slice.call(document.querySelectorAll(".sidebar-link")),n=[].slice.call(document.querySelectorAll(".header-anchor")).filter(n=>e.some(e=>e.hash===n.hash)),t=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),a=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),i=window.innerHeight+t;for(let e=0;e<n.length;e++){const r=n[e],o=n[e+1],s=0===e&&0===t||t>=r.parentElement.offsetTop+10&&(!o||t<o.parentElement.offsetTop-10),c=decodeURIComponent(this.$route.hash);if(s&&c!==decodeURIComponent(r.hash)){const t=r;if(i===a)for(let t=e+1;t<n.length;t++)if(c===decodeURIComponent(n[t].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(t.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},ks=t(28),xs=t.n(ks),Cs={mounted(){xs.a.configure({showSpinner:!1}),this.$router.beforeEach((e,n,t)=>{e.path===n.path||Wt.component(e.name)||xs.a.start(),t()}),this.$router.afterEach(()=>{xs.a.done(),this.isSidebarOpen=!1})}};t(265),t(266);class Ps{constructor(){this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}show({text:e="",duration:n=3e3}){let t=document.createElement("div");t.className="message move-in",t.innerHTML=`\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">${e}</div>\n    `,this.containerEl.appendChild(t),n>0&&setTimeout(()=>{this.close(t)},n)}close(e){e.className=e.className.replace("move-in",""),e.className+="move-out",e.addEventListener("animationend",()=>{e.remove()})}}var Ts={mounted(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy(){setTimeout(()=>{(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach(e=>{document.querySelectorAll(e).forEach(this.generateCopyButton)})},1e3)},generateCopyButton(e){if(e.classList.contains("codecopy-enabled"))return;const n=document.createElement("i");n.className="code-copy",n.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',n.title="Copy to clipboard",n.addEventListener("click",()=>{this.copyToClipboard(e.innerText)}),e.appendChild(n),e.classList.add("codecopy-enabled")},copyToClipboard(e){const n=document.createElement("textarea");n.value=e,n.setAttribute("readonly",""),n.style.position="absolute",n.style.left="-9999px",document.body.appendChild(n);const t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);n.select(),document.execCommand("copy");(new Ps).show({text:"复制成功",duration:1e3}),document.body.removeChild(n),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}},Ms="auto",As="zoom-in",Ss="zoom-out",Is="grab",Ls="move";function Ds(e,n,t){var a=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],i={passive:!1};a?e.addEventListener(n,t,i):e.removeEventListener(n,t,i)}function zs(e,n){if(e){var t=new Image;t.onload=function(){n&&n(t)},t.src=e}}function Us(e){return e.dataset.original?e.dataset.original:"A"===e.parentNode.tagName?e.parentNode.getAttribute("href"):null}function Os(e,n,t){!function(e){var n=Rs,t=Es;if(e.transition){var a=e.transition;delete e.transition,e[n]=a}if(e.transform){var i=e.transform;delete e.transform,e[t]=i}}(n);var a=e.style,i={};for(var r in n)t&&(i[r]=a[r]||""),a[r]=n[r];return i}var Rs="transition",Es="transform",Gs="transform",qs="transitionend";var Bs=function(){},js={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Bs,onClose:Bs,onGrab:Bs,onMove:Bs,onRelease:Bs,onBeforeOpen:Bs,onBeforeClose:Bs,onBeforeGrab:Bs,onBeforeRelease:Bs,onImageLoading:Bs,onImageLoaded:Bs},Ns={init:function(e){var n,t;n=this,t=e,Object.getOwnPropertyNames(Object.getPrototypeOf(n)).forEach((function(e){n[e]=n[e].bind(t)}))},click:function(e){if(e.preventDefault(),Fs(e))return window.open(this.target.srcOriginal||e.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(e.currentTarget)},scroll:function(){var e=document.documentElement||document.body.parentNode||document.body,n=window.pageXOffset||e.scrollLeft,t=window.pageYOffset||e.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:n,y:t});var a=this.lastScrollPosition.x-n,i=this.lastScrollPosition.y-t,r=this.options.scrollThreshold;(Math.abs(i)>=r||Math.abs(a)>=r)&&(this.lastScrollPosition=null,this.close())},keydown:function(e){(function(e){return"Escape"===(e.key||e.code)||27===e.keyCode})(e)&&(this.released?this.close():this.release(this.close))},mousedown:function(e){if($s(e)&&!Fs(e)){e.preventDefault();var n=e.clientX,t=e.clientY;this.pressTimer=setTimeout(function(){this.grab(n,t)}.bind(this),200)}},mousemove:function(e){this.released||this.move(e.clientX,e.clientY)},mouseup:function(e){$s(e)&&!Fs(e)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(e){e.preventDefault();var n=e.touches[0],t=n.clientX,a=n.clientY;this.pressTimer=setTimeout(function(){this.grab(t,a)}.bind(this),200)},touchmove:function(e){if(!this.released){var n=e.touches[0],t=n.clientX,a=n.clientY;this.move(t,a)}},touchend:function(e){(function(e){e.targetTouches.length})(e)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function $s(e){return 0===e.button}function Fs(e){return e.metaKey||e.ctrlKey}var Vs={init:function(e){this.el=document.createElement("div"),this.instance=e,this.parent=document.body,Os(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(e.options),Ds(this.el,"click",e.handler.clickOverlay.bind(e))},updateStyle:function(e){Os(this.el,{zIndex:e.zIndex,backgroundColor:e.bgColor,transition:"opacity\n        "+e.transitionDuration+"s\n        "+e.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},Hs="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},Ws=function(){function e(e,n){for(var t=0;t<n.length;t++){var a=n[t];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),Object.defineProperty(e,a.key,a)}}return function(n,t,a){return t&&e(n.prototype,t),a&&e(n,a),n}}(),Ks=Object.assign||function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)Object.prototype.hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},Zs={init:function(e,n){this.el=e,this.instance=n,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=Us(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var e=this.instance.options,n=e.zIndex,t=e.enableGrab,a=e.transitionDuration,i=e.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:n+1,cursor:t?Is:Ss,transition:Gs+"\n        "+a+"s\n        "+i,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=Os(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,Os(this.el,{transform:"none"})},grab:function(e,n,t){var a=Ys(),i=a.x-e,r=a.y-n;Os(this.el,{cursor:Ls,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+r)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(e,n,t){var a=Ys(),i=a.x-e,r=a.y-n;Os(this.el,{transition:Gs,transform:"translate3d(\n        "+(this.translate.x+i)+"px, "+(this.translate.y+r)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){Os(this.el,this.styleClose)},restoreOpenStyle:function(){Os(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var e=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var n=this.el.cloneNode(!1);n.setAttribute("src",this.srcOriginal),n.style.position="fixed",n.style.visibility="hidden",e.appendChild(n),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),e.removeChild(n)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var e=Ys(),n=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:e.x-n,y:e.y-t}},calculateScale:function(){var e=this.el.dataset,n=e.zoomingHeight,t=e.zoomingWidth,a=this.instance.options,i=a.customSize,r=a.scaleBase;if(!i&&n&&t)return{x:t/this.rect.width,y:n/this.rect.height};if(i&&"object"===(void 0===i?"undefined":Hs(i)))return{x:i.width/this.rect.width,y:i.height/this.rect.height};var o=this.rect.width/2,s=this.rect.height/2,c=Ys(),l={x:c.x-o,y:c.y-s},d=l.x/o,h=l.y/s,u=r+Math.min(d,h);if(i&&"string"==typeof i){var p=t||this.el.naturalWidth,m=n||this.el.naturalHeight,f=parseFloat(i)*p/(100*this.rect.width),g=parseFloat(i)*m/(100*this.rect.height);if(u>f||u>g)return{x:f,y:g}}return{x:u,y:u}}};function Ys(){var e=document.documentElement;return{x:Math.min(e.clientWidth,window.innerWidth)/2,y:Math.min(e.clientHeight,window.innerHeight)/2}}function Xs(e,n,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(a){Ds(e,a,n[a],t)}))}var Qs=function(){function e(n){!function(e,n){if(!(e instanceof n))throw new TypeError("Cannot call a class as a function")}(this,e),this.target=Object.create(Zs),this.overlay=Object.create(Vs),this.handler=Object.create(Ns),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=Ks({},js,n),this.overlay.init(this),this.handler.init(this)}return Ws(e,[{key:"listen",value:function(e){if("string"==typeof e)for(var n=document.querySelectorAll(e),t=n.length;t--;)this.listen(n[t]);else"IMG"===e.tagName&&(e.style.cursor=As,Ds(e,"click",this.handler.click),this.options.preloadImage&&zs(Us(e)));return this}},{key:"config",value:function(e){return e?(Ks(this.options,e),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(e){var n=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var a="string"==typeof e?document.querySelector(e):e;if("IMG"===a.tagName){if(this.options.onBeforeOpen(a),this.target.init(a,this),!this.options.preloadImage){var i=this.target.srcOriginal;null!=i&&(this.options.onImageLoading(a),zs(i,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),Ds(document,"scroll",this.handler.scroll),Ds(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&Ds(window,"resize",this.handler.resizeWindow);var r=function e(){Ds(a,qs,e,!1),n.lock=!1,n.target.upgradeSource(),n.options.enableGrab&&Xs(document,n.handler,!0),t(a)};return Ds(a,qs,r),this}}}},{key:"close",value:function(){var e=this,n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=Ms,this.overlay.fadeOut(),this.target.zoomOut(),Ds(document,"scroll",this.handler.scroll,!1),Ds(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&Ds(window,"resize",this.handler.resizeWindow,!1);var a=function a(){Ds(t,qs,a,!1),e.shown=!1,e.lock=!1,e.target.downgradeSource(),e.options.enableGrab&&Xs(document,e.handler,!1),e.target.restoreCloseStyle(),e.overlay.remove(),n(t)};return Ds(t,qs,a),this}}},{key:"grab",value:function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var i=this.target.el;this.options.onBeforeGrab(i),this.released=!1,this.target.grab(e,n,t);var r=function e(){Ds(i,qs,e,!1),a(i)};return Ds(i,qs,r),this}}},{key:"move",value:function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Ls,this.target.move(e,n,t);var i=this.target.el,r=function e(){Ds(i,qs,e,!1),a(i)};return Ds(i,qs,r),this}}},{key:"release",value:function(){var e=this,n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=Ms,this.target.restoreOpenStyle();var a=function a(){Ds(t,qs,a,!1),e.lock=!1,e.released=!0,n(t)};return Ds(t,qs,a),this}}}]),e}();const Js=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),ec=Number("500");class nc{constructor(){this.instance=new Qs(Js)}update(e=".theme-vdoing-content img:not(.no-zoom)"){"undefined"!=typeof window&&this.instance.listen(e)}updateDelay(e=".theme-vdoing-content img:not(.no-zoom)",n=ec){setTimeout(()=>this.update(e),n)}}var tc=[fs,_s,Cs,Ts,{watch:{"$page.path"(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted(){this.$vuepress.zooming=new nc,this.$vuepress.zooming.updateDelay()}}],ac={name:"GlobalLayout",computed:{layout(){const e=this.getLayout();return us("layout",e),Wt.component(e)}},methods:{getLayout(){if(this.$page.path){const e=this.$page.frontmatter.layout;return e&&(this.$vuepress.getLayoutAsyncComponent(e)||this.$vuepress.getVueComponent(e))?e:"Layout"}return"NotFound"}}},ic=t(5),rc=Object(ic.a)(ac,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(e,n,t){switch(n){case"components":e[n]||(e[n]={}),Object.assign(e[n],t);break;case"mixins":e[n]||(e[n]=[]),e[n].push(...t);break;default:throw new Error("Unknown option name.")}}(rc,"mixins",tc);const oc=[{name:"v-0fdda314",path:"/pages/f27694/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-0fdda314").then(t)}},{path:"/pages/f27694/index.html",redirect:"/pages/f27694/"},{path:"/00.目录页/00.Content.html",redirect:"/pages/f27694/"},{name:"v-37c5bc96",path:"/hbm/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-37c5bc96").then(t)}},{path:"/hbm/index.html",redirect:"/hbm/"},{path:"/00.目录页/01.hbm.html",redirect:"/hbm/"},{name:"v-07282236",path:"/gpu/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-07282236").then(t)}},{path:"/gpu/index.html",redirect:"/gpu/"},{path:"/00.目录页/03.gpu.html",redirect:"/gpu/"},{name:"v-4553c3a8",path:"/compiler/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-4553c3a8").then(t)}},{path:"/compiler/index.html",redirect:"/compiler/"},{path:"/00.目录页/02.compiler.html",redirect:"/compiler/"},{name:"v-bd5cd4d4",path:"/cpu/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-bd5cd4d4").then(t)}},{path:"/cpu/index.html",redirect:"/cpu/"},{path:"/00.目录页/04.cpu.html",redirect:"/cpu/"},{name:"v-07abb6d6",path:"/llm/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-07abb6d6").then(t)}},{path:"/llm/index.html",redirect:"/llm/"},{path:"/00.目录页/05.llm.html",redirect:"/llm/"},{name:"v-577baade",path:"/unix/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-577baade").then(t)}},{path:"/unix/index.html",redirect:"/unix/"},{path:"/00.目录页/06.unix.html",redirect:"/unix/"},{name:"v-abb0f8a2",path:"/pages/24769e/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-abb0f8a2").then(t)}},{path:"/pages/24769e/index.html",redirect:"/pages/24769e/"},{path:"/01.hbm/01.HBM_Paper_List.html",redirect:"/pages/24769e/"},{name:"v-d7471e16",path:"/pages/2476af/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-d7471e16").then(t)}},{path:"/pages/2476af/index.html",redirect:"/pages/2476af/"},{path:"/01.hbm/02.hbm_dead_block_predictor.html",redirect:"/pages/2476af/"},{name:"v-b2fd417c",path:"/pages/24769f/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-b2fd417c").then(t)}},{path:"/pages/24769f/index.html",redirect:"/pages/24769f/"},{path:"/01.hbm/03.Dynamically_Adapting _Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.html",redirect:"/pages/24769f/"},{name:"v-7f8db936",path:"/pages/2476bf/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-7f8db936").then(t)}},{path:"/pages/2476bf/index.html",redirect:"/pages/2476bf/"},{path:"/01.hbm/05.cache_mem_compression.html",redirect:"/pages/2476bf/"},{name:"v-9842a21a",path:"/pages/24760e/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-9842a21a").then(t)}},{path:"/pages/24760e/index.html",redirect:"/pages/24760e/"},{path:"/01.hbm/04.DRAM_PCM_NVM_Cache.html",redirect:"/pages/24760e/"},{name:"v-171f66aa",path:"/pages/f07695/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-171f66aa").then(t)}},{path:"/pages/f07695/index.html",redirect:"/pages/f07695/"},{path:"/01.hbm/06.memory ecc.html",redirect:"/pages/f07695/"},{name:"v-788071c5",path:"/pages/f07696/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-788071c5").then(t)}},{path:"/pages/f07696/index.html",redirect:"/pages/f07696/"},{path:"/01.hbm/07.hbm-latency.html",redirect:"/pages/f07696/"},{name:"v-662ff4fd",path:"/pages/f07699/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-662ff4fd").then(t)}},{path:"/pages/f07699/index.html",redirect:"/pages/f07699/"},{path:"/01.hbm/09.compressibility_prediction.html",redirect:"/pages/f07699/"},{name:"v-25995465",path:"/pages/f07698/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-25995465").then(t)}},{path:"/pages/f07698/index.html",redirect:"/pages/f07698/"},{path:"/01.hbm/08.compression.html",redirect:"/pages/f07698/"},{name:"v-065eff05",path:"/pages/f07692/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-065eff05").then(t)}},{path:"/pages/f07692/index.html",redirect:"/pages/f07692/"},{path:"/01.hbm/10.software_memory_paper.html",redirect:"/pages/f07692/"},{name:"v-f3b213e2",path:"/pages/000001/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-f3b213e2").then(t)}},{path:"/pages/000001/index.html",redirect:"/pages/000001/"},{path:"/02.compiler/01.llvm_frontend.html",redirect:"/pages/000001/"},{name:"v-0b217b45",path:"/pages/000002/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-0b217b45").then(t)}},{path:"/pages/000002/index.html",redirect:"/pages/000002/"},{path:"/02.compiler/02.GetStartedLLVMChap5Notes.html",redirect:"/pages/000002/"},{name:"v-2d0923f6",path:"/pages/000003/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-2d0923f6").then(t)}},{path:"/pages/000003/index.html",redirect:"/pages/000003/"},{path:"/02.compiler/03.GetStartedLLVMChap6Notes.html",redirect:"/pages/000003/"},{name:"v-4d042c96",path:"/pages/000004/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-4d042c96").then(t)}},{path:"/pages/000004/index.html",redirect:"/pages/000004/"},{path:"/02.compiler/04. LearningLLVMDiary0.html",redirect:"/pages/000004/"},{name:"v-49e4dae0",path:"/pages/000005/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-49e4dae0").then(t)}},{path:"/pages/000005/index.html",redirect:"/pages/000005/"},{path:"/02.compiler/05. addInstACE.html",redirect:"/pages/000005/"},{name:"v-c73e5956",path:"/pages/000006/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-c73e5956").then(t)}},{path:"/pages/000006/index.html",redirect:"/pages/000006/"},{path:"/02.compiler/06.Value&Use.html",redirect:"/pages/000006/"},{name:"v-89581d5c",path:"/pages/000007/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-89581d5c").then(t)}},{path:"/pages/000007/index.html",redirect:"/pages/000007/"},{path:"/02.compiler/07. UnderstaningLLVMwithSourceCode.html",redirect:"/pages/000007/"},{name:"v-6ffd4505",path:"/pages/cc7034/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-6ffd4505").then(t)}},{path:"/pages/cc7034/index.html",redirect:"/pages/cc7034/"},{path:"/03.gpu/01.operand_collector.html",redirect:"/pages/cc7034/"},{name:"v-f96426b0",path:"/pages/14769f/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-f96426b0").then(t)}},{path:"/pages/14769f/index.html",redirect:"/pages/14769f/"},{path:"/03.gpu/03.Precise Exception.html",redirect:"/pages/14769f/"},{name:"v-b25c491a",path:"/pages/2476ae/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-b25c491a").then(t)}},{path:"/pages/2476ae/index.html",redirect:"/pages/2476ae/"},{path:"/03.gpu/02.warp_execution.html",redirect:"/pages/2476ae/"},{name:"v-5849f946",path:"/pages/44771e/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-5849f946").then(t)}},{path:"/pages/44771e/index.html",redirect:"/pages/44771e/"},{path:"/03.gpu/04.Unified_Memory.html",redirect:"/pages/44771e/"},{name:"v-2861ec8b",path:"/pages/44871e/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-2861ec8b").then(t)}},{path:"/pages/44871e/index.html",redirect:"/pages/44871e/"},{path:"/03.gpu/05.TensorCore.html",redirect:"/pages/44871e/"},{name:"v-04292a45",path:"/pages/45871e/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-04292a45").then(t)}},{path:"/pages/45871e/index.html",redirect:"/pages/45871e/"},{path:"/03.gpu/06.MemoryBehaviour.html",redirect:"/pages/45871e/"},{name:"v-65bd31e5",path:"/pages/458720/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-65bd31e5").then(t)}},{path:"/pages/458720/index.html",redirect:"/pages/458720/"},{path:"/03.gpu/08.LLM.html",redirect:"/pages/458720/"},{name:"v-d22a70f6",path:"/pages/45871f/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-d22a70f6").then(t)}},{path:"/pages/45871f/index.html",redirect:"/pages/45871f/"},{path:"/03.gpu/07.GPUVirtualization.html",redirect:"/pages/45871f/"},{name:"v-e33712b6",path:"/pages/458721/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-e33712b6").then(t)}},{path:"/pages/458721/index.html",redirect:"/pages/458721/"},{path:"/03.gpu/09.Simulator.html",redirect:"/pages/458721/"},{name:"v-a0731602",path:"/pages/458722/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-a0731602").then(t)}},{path:"/pages/458722/index.html",redirect:"/pages/458722/"},{path:"/03.gpu/10. Architectural Survey.html",redirect:"/pages/458722/"},{name:"v-75a07a01",path:"/pages/458724/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-75a07a01").then(t)}},{path:"/pages/458724/index.html",redirect:"/pages/458724/"},{path:"/03.gpu/11.IntegratedCPUGPUMemory.html",redirect:"/pages/458724/"},{name:"v-5474106a",path:"/pages/458725/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-5474106a").then(t)}},{path:"/pages/458725/index.html",redirect:"/pages/458725/"},{path:"/03.gpu/12.gpgpusim.html",redirect:"/pages/458725/"},{name:"v-4349358d",path:"/pages/458726/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-4349358d").then(t)}},{path:"/pages/458726/index.html",redirect:"/pages/458726/"},{path:"/03.gpu/13.gpgpusim.html",redirect:"/pages/458726/"},{name:"v-02aef12e",path:"/pages/47871e/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-02aef12e").then(t)}},{path:"/pages/47871e/index.html",redirect:"/pages/47871e/"},{path:"/03.gpu/1234.TODO.html",redirect:"/pages/47871e/"},{name:"v-9e671962",path:"/pages/458727/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-9e671962").then(t)}},{path:"/pages/458727/index.html",redirect:"/pages/458727/"},{path:"/03.gpu/14.gpgpusim.html",redirect:"/pages/458727/"},{name:"v-1e4fb111",path:"/pages/45872/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-1e4fb111").then(t)}},{path:"/pages/45872/index.html",redirect:"/pages/45872/"},{path:"/03.gpu/15.gpgpusim.html",redirect:"/pages/45872/"},{name:"v-e85a225a",path:"/pages/45874/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-e85a225a").then(t)}},{path:"/pages/45874/index.html",redirect:"/pages/45874/"},{path:"/03.gpu/16.gpgpusim.html",redirect:"/pages/45874/"},{name:"v-c25059fe",path:"/pages/45873/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-c25059fe").then(t)}},{path:"/pages/45873/index.html",redirect:"/pages/45873/"},{path:"/03.gpu/17.warp_mem.html",redirect:"/pages/45873/"},{name:"v-53bdf925",path:"/pages/45876/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-53bdf925").then(t)}},{path:"/pages/45876/index.html",redirect:"/pages/45876/"},{path:"/03.gpu/19.gpu_cache_mem.html",redirect:"/pages/45876/"},{name:"v-798f0c72",path:"/pages/45875/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-798f0c72").then(t)}},{path:"/pages/45875/index.html",redirect:"/pages/45875/"},{path:"/03.gpu/18.gpucoherency.html",redirect:"/pages/45875/"},{name:"v-c2e69736",path:"/pages/45877/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-c2e69736").then(t)}},{path:"/pages/45877/index.html",redirect:"/pages/45877/"},{path:"/03.gpu/20.gpu_tlb_ptw.html",redirect:"/pages/45877/"},{name:"v-121d90a9",path:"/pages/cc7035/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-121d90a9").then(t)}},{path:"/pages/cc7035/index.html",redirect:"/pages/cc7035/"},{path:"/04.cpu/01.checkpoint.html",redirect:"/pages/cc7035/"},{name:"v-3e3c1a05",path:"/pages/cc7036/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-3e3c1a05").then(t)}},{path:"/pages/cc7036/index.html",redirect:"/pages/cc7036/"},{path:"/04.cpu/02.topdown.html",redirect:"/pages/cc7036/"},{name:"v-891a4db6",path:"/pages/cc7037/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-891a4db6").then(t)}},{path:"/pages/cc7037/index.html",redirect:"/pages/cc7037/"},{path:"/04.cpu/03.loadstore.html",redirect:"/pages/cc7037/"},{name:"v-64b7ef30",path:"/pages/cc7038/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-64b7ef30").then(t)}},{path:"/pages/cc7038/index.html",redirect:"/pages/cc7038/"},{path:"/04.cpu/05.cache structure.html",redirect:"/pages/cc7038/"},{name:"v-90037b60",path:"/pages/cc7039/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-90037b60").then(t)}},{path:"/pages/cc7039/index.html",redirect:"/pages/cc7039/"},{path:"/04.cpu/06.cache timing.html",redirect:"/pages/cc7039/"},{name:"v-7ddc0f42",path:"/pages/cc7040/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-7ddc0f42").then(t)}},{path:"/pages/cc7040/index.html",redirect:"/pages/cc7040/"},{path:"/04.cpu/07.register file.html",redirect:"/pages/cc7040/"},{name:"v-c1b2e1c6",path:"/pages/f07697/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-c1b2e1c6").then(t)}},{path:"/pages/f07697/index.html",redirect:"/pages/f07697/"},{path:"/04.cpu/1234.markdown.html",redirect:"/pages/f07697/"},{name:"v-26a9c736",path:"/pages/dc7035/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-26a9c736").then(t)}},{path:"/pages/dc7035/index.html",redirect:"/pages/dc7035/"},{path:"/05.llm/01.How_LLM_Works.html",redirect:"/pages/dc7035/"},{name:"v-b267e276",path:"/pages/dc7036/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-b267e276").then(t)}},{path:"/pages/dc7036/index.html",redirect:"/pages/dc7036/"},{path:"/05.llm/02.LLM_HW_Opt.html",redirect:"/pages/dc7036/"},{name:"v-0ee7ecc5",path:"/pages/dc7038/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-0ee7ecc5").then(t)}},{path:"/pages/dc7038/index.html",redirect:"/pages/dc7038/"},{path:"/05.llm/04.mem_usage_llm.html",redirect:"/pages/dc7038/"},{name:"v-259f44f9",path:"/pages/dc7037/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-259f44f9").then(t)}},{path:"/pages/dc7037/index.html",redirect:"/pages/dc7037/"},{path:"/05.llm/03.gem5_LLAMA.html",redirect:"/pages/dc7037/"},{name:"v-2e715932",path:"/message-board/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-2e715932").then(t)}},{path:"/message-board/index.html",redirect:"/message-board/"},{path:"/09.nine/01.留言板.html",redirect:"/message-board/"},{name:"v-bb1ed876",path:"/pages/c45600/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-bb1ed876").then(t)}},{path:"/pages/c45600/index.html",redirect:"/pages/c45600/"},{path:"/09.nine/02.template.html",redirect:"/pages/c45600/"},{name:"v-e114a476",path:"/pages/ec7035/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-e114a476").then(t)}},{path:"/pages/ec7035/index.html",redirect:"/pages/ec7035/"},{path:"/06.unix/01.malloc.html",redirect:"/pages/ec7035/"},{name:"v-23bbae21",path:"/pages/f00000/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-23bbae21").then(t)}},{path:"/pages/f00000/index.html",redirect:"/pages/f00000/"},{path:"/10.mix/01.leakagecurrent.html",redirect:"/pages/f00000/"},{name:"v-6ed8ffdb",path:"/pages/f6e80a/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-6ed8ffdb").then(t)}},{path:"/pages/f6e80a/index.html",redirect:"/pages/f6e80a/"},{path:"/10.mix/assignment_qishao.html",redirect:"/pages/f6e80a/"},{name:"v-f8e02736",path:"/archives/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-f8e02736").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{name:"v-01216c79",path:"/pages/904dad/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-01216c79").then(t)}},{path:"/pages/904dad/index.html",redirect:"/pages/904dad/"},{path:"/pictures/addPictures.html",redirect:"/pages/904dad/"},{name:"v-2f8a0a7e",path:"/",component:rc,beforeEnter:(e,n,t)=>{hs("Layout","v-2f8a0a7e").then(t)}},{path:"/index.html",redirect:"/"},{path:"*",component:rc}],sc={title:"CPU & GPU Microarch. Qi Shao",description:"Computer System",base:"/qishao-notes/",headTags:[["link",{rel:"stylesheet",href:"custom.css"}],["meta",{name:"google-site-verification",content:"66w5U9NY5gJWu7iBtHKMbhpXkV94jy31L_RHbvrZZzY"}],["meta",{name:"keywords",content:"Hitqishao,golang,vue,go-web,go-admin,go-ldap-admin"}],["meta",{name:"theme-color",content:"#11a8cd"}],["meta",{name:"referrer",content:"no-referrer-when-downgrade"}],["script",{language:"javascript",type:"text/javascript",src:"/qishao-notes/js/pgmanor-self.js"}]],pages:[{title:"Content",frontmatter:{title:"Content",date:"2022-07-18T17:23:23.000Z",permalink:"/pages/f27694/"},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/00.Content.html",relativePath:"00.目录页/00.Content.md",key:"v-0fdda314",path:"/pages/f27694/",headersStr:null,content:" 1. GPU\n 2. CPU\n 3. HBM\n 4. Compiler\n 5. LLM\n 6. unix\n 7. misc",normalizedContent:" 1. gpu\n 2. cpu\n 3. hbm\n 4. compiler\n 5. llm\n 6. unix\n 7. misc",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"HBM",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"01.hbm"}},title:"HBM",date:"2022-07-20T11:05:42.000Z",permalink:"/hbm/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/01.hbm.html",relativePath:"00.目录页/01.hbm.md",key:"v-37c5bc96",path:"/hbm/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"gpu",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"03.gpu"}},title:"gpu",date:"2022-07-20T11:05:54.000Z",permalink:"/gpu/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/03.gpu.html",relativePath:"00.目录页/03.gpu.md",key:"v-07282236",path:"/gpu/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"llvm & mlir",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"02.compiler"}},title:"llvm & mlir",date:"2023-11-21T11:05:54.000Z",permalink:"/compiler/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/02.compiler.html",relativePath:"00.目录页/02.compiler.md",key:"v-4553c3a8",path:"/compiler/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"cpu",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"04.cpu"}},title:"cpu",date:"2023-11-09T15:54:15.000Z",permalink:"/cpu/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/04.cpu.html",relativePath:"00.目录页/04.cpu.md",key:"v-bd5cd4d4",path:"/cpu/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"llm",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"05.llm"}},title:"llm",date:"2024-01-02T15:54:15.000Z",permalink:"/llm/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/05.llm.html",relativePath:"00.目录页/05.llm.md",key:"v-07abb6d6",path:"/llm/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"unix",frontmatter:{pageComponent:{name:"Catalogue",data:{key:"06.unix"}},title:"unix",date:"2024-03-03T15:54:15.000Z",permalink:"/unix/",sidebar:!1,article:!1,comment:!1,editLink:!1},regularPath:"/00.%E7%9B%AE%E5%BD%95%E9%A1%B5/06.unix.html",relativePath:"00.目录页/06.unix.md",key:"v-577baade",path:"/unix/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"HBM Paper List",frontmatter:{title:"HBM Paper List",date:"2023-05-08T00:00:00.000Z",permalink:"/pages/24769e/"},regularPath:"/01.hbm/01.HBM_Paper_List.html",relativePath:"01.hbm/01.HBM_Paper_List.md",key:"v-abb0f8a2",path:"/pages/24769e/",headers:[{level:3,title:"2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache",slug:"_2-cameo-a-two-level-memory-organization-with-capacity-of-main-memory-and-flexibility-of-hardware-managed-cache",normalizedTitle:"2. cameo:a two-level memory organization with capacity of main memory and flexibility of hardware-managed cache",charIndex:1920},{level:3,title:"5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories",slug:"_5-mempod-a-clustered-architecture-for-efficient-and-scalable-migration-in-flat-address-space-multi-level-memories",normalizedTitle:"5. mempod: a clustered architecture for efficient and scalable migration in flat address space multi-level memories",charIndex:2752},{level:3,title:"6. Transparent Hardware Management of Stacked DRAM as Part of Memory",slug:"_6-transparent-hardware-management-of-stacked-dram-as-part-of-memory",normalizedTitle:"6. transparent hardware management of stacked dram as part of memory",charIndex:4917},{level:3,title:"8.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM",slug:"_8-batman-techniques-for-maximizing-system-bandwidth-of-memory-systems-with-stacked-dram",normalizedTitle:"8.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram",charIndex:7541},{level:3,title:"9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories",slug:"_9-heterogeneous-memory-architectures-a-hw-sw-approach-for-mixing-die-stacked-and-off-package-memories",normalizedTitle:"9. heterogeneous memory architectures: a hw/sw approach for mixing die-stacked and off-package memories",charIndex:8728},{level:3,title:"10.Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems",slug:"_10-challenges-in-heterogeneous-die-stacked-and-off-chip-memory-systems",normalizedTitle:"10.challenges in heterogeneous die-stacked and off-chip memory systems",charIndex:10868},{level:3,title:"11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation",slug:"_11-banshee-bandwidth-efficient-dram-caching-via-software-hardware-cooperation",normalizedTitle:"11. banshee: bandwidth-efficient dram caching via software/hardware cooperation",charIndex:874},{level:3,title:"13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache",slug:"_13-unison-cache-a-scalable-and-effective-die-stacked-dram-cache",normalizedTitle:"13. unison cache: a scalable and effective die-stacked dram cache",charIndex:1006},{level:3,title:"14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors",slug:"_14-dynamically-adapting-page-migration-policies-based-on-applications-memory-access-behaviors",normalizedTitle:"14. dynamically adapting page migration policies based on applications memory access behaviors",charIndex:1073},{level:3,title:"15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems",slug:"_15-on-the-fly-page-migration-and-address-reconciliation-for-heterogeneous-memory-systems",normalizedTitle:"15. on-the-fly page migration and address reconciliation for heterogeneous memory systems",charIndex:1169}],headersStr:"2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache 5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories 6. Transparent Hardware Management of Stacked DRAM as Part of Memory 8.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM 9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories 10.Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems 11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems",content:" 1.  Baryon: Efficient Hybrid Memory Management with Compression and Sub-Blocking\n 2.  CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache\n 3.  Hybrid2: Combining Caching and Migration in Hybrid Memory Systems\n 4.  SILC-FM: Subblocked InterLeaved Cache-Like Flat Memory Organization\n 5.  MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories\n 6.  Transparent Hardware Management of Stacked DRAM as Part of Memory\n 7.  CHAMELEON: A Dynamically Reconfigurable Heterogeneous Memory System\n 8.  BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n 9.  Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories\n 10. Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems\n 11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation\n 12. Die-Stacked DRAM: Memory, Cache, or MemCache?\n 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache\n 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors\n 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems\n 16. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems [DAC]\n 17. TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems International Conference on Computer Design (ICCD)\n\nDRAM NVM\n\n 1. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture\n 2. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures\n 3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method\n 4. Page Placement in Hybrid Memory Systems\n\n----------------------------------------\n\n\n# 2. CAMEO:A Two-Level Memory Organization with Capacity of Main Memory and Flexibility of Hardware-Managed Cache\n\n# MemPod:\n\nCAMEO [13] proposes a cache-like flat address space memory management scheme in an attempt to close the gap between cache and flat memory organizations. CAMEO operates similarly to THM, however it does so at the granularity of cache lines (64B). Migrations are restricted within segments with one fast line location per segment. Its bookkeeping structures are entirely stored in memory, while a “Line Location Predictor” attempts to save some bookkeeping-related accesses by predicting the location of a line.\n\nCAMEO initiates a line migration upon every access to slow memory.\n\nCAMEO can incur high migration traffic as every access could induce a migration.\n\n\n\n----------------------------------------\n\n\n# 5. MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories\n\nYear: 2017\n\nMemPod uses MEA counters to track page access activity and identify hot pages. They are dramatically smaller than prior tracking mechanisms while capturing activity counts and temporal recency in a way that provides more effective prediction of future page access.\n\nWhat makes MEA most useful, though, is its failure mode – when it fails to find the most-accessed pages, it does so by favoring recency over quantity. That is, a page accessed several times near the end of an interval can easily knock out a page accessed many more times early in the interval. As a result, it combines both access counting and temporal locality, at a fraction of the cost of access counting alone.\n\n\n\nThree triggers are most commonly used whenever state must be updated based on tracking information (MC scheduling, migrations, dynamic voltage and frequency scaling etc.). Interval-based (or epoch-based) triggers occur with a set frequency, while threshold-based solutions trigger whenever a predetermined criterion is met. Finally, event-based triggers react to predefined events. Both interval-based and threshold-based approaches face the same challenge of identifying the optimal interval or threshold value.\n\nMemPod achieves the best performance (lower AMMAT) with 50us intervals and 64 counters per Pod. MemPod’s lightweight operation allows for such small intervals. For comparison purposes, HMA [14] identi-fied the best epoch length to be 100ms (2000x larger) in order to support all the lengthy processes that take place during a migration event for that method.\n\nBased on these results, we use 64 MEA 2-bit counters over 50us intervals for subsequent results in this paper. Each one of the 64 MEA entries needs 21 bits for addressing the 1.1M pages per Pod and 2 bits for its counter, leading to an area cost of only 184B per Pod and 736B total. Compared to the state of the art, MemPod’s activity tracking requirement is ∼712x smaller than THM’s (512KB) and ∼12800x smaller than HMA’s (9MB).\n\n\n\n----------------------------------------\n\n\n# 6. Transparent Hardware Management of Stacked DRAM as Part of Memory\n\n# MemPod:\n\nSim, et al. proposed a technique for transparent hardware management of a hybrid memory system [17],which we will refer to as “THM”. THM does not require OS intervention while managing migrations. In order to keep bookkeeping costs manageable, THM allows migrations only within sets of pages (called segments). Each segment includes one fast memory page and a set of slow memory pages. The slow pages of each segment can only migrate to the one fast page location, and any such migration results in the eviction of the currently-residing page. THM monitors memory accesses with one “competing counter” per segment resulting in a low cost profiling solution. Finally, THM supports caching part of its structures on chip while the rest is stored in memory.\n\nTHM’s competing counters can lead to false positives, allowing a cold page to migrate to fast memory.\n\nTHM offers significantly limited flexibility by restricting migrations withing segments, however this decision reduces bookkeeping costs significantly. Competing counters in each segment are used for activity tracking, occasionally leading to false (threshold-based) migration triggering if a cold page gets accessed at the right time. Identifying migration candidates incurs very little overhead since there is exactly one fast memory location for each slow memory page that triggers migration.\n\nCite from paper\n\n\n\nSimilar to set dueling, they adopted sample region. The locations in fast memory are grouped into 32 distinct regions in an interleaving fashion, and four regions are dedicated to sampling, while other 28 regions follow the threshold decision from sampling.\n\n• Nstatic: # of memory requests serviced from fast memory with static mapping • Ndynamic: # of memory requests expected to be serviced from fast memory when swapping with a given threshold • Nswap: # of expected swaps for a given threshold.\n\n\n\nK differs depending on the relative latency of fast and slow memory. The cost of a single fast swap is about 1200 cycles, and the difference in access latency between fast and slow memory is 72 cycles.Thus, in general, the swapped-in segment needs to get at least 17 more (future) hits than the swapped-out segment for swapping to be valuable. K is computed in hardware at boot time.\n\nMe Competing counter needs a threshold to invoke swap.\n\nSample region will have different threshold. Based on different threshold, Nswap is different. Thus they choose the the best threshold with max Bexpected as candidate threshold.\n\n----------------------------------------\n\n\n# 8.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n\nInsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nAs the NM simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. We observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nWe leverage our key insight on controlling data movement and propose Bandwidth-Aware Tiered-Memory Management (BATMAN), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the NM and the FM. We define the desired access rate of the NM as the target access rate (TAR). TAR is the fraction of memory accesses serviced by the NM when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2X 2/3 4X 4/5 8X 8/9\n\nMe Bandwidth-Aware Tired-Memory Management tries to distritube memory according to HBM and DRAM bandwidth ratio. And also treat it as a threshold to refuse page migration.\n\n----------------------------------------\n\n\n# 9. Heterogeneous Memory Architectures: A HW/SW Approach for Mixing Die-stacked and Off-package Memories\n\n# MemPod:\n\nHMA [14] is a HW/SW mechanism that attempts to predict frequently accessed pages in memory and, at predefined intervals, migrate those pages to fast memory. HW support is required for profiling memory accesses using counters for each memory page, while the migration is handled by the OS. Due to the costly OS involvement, HMA’s intervals are kept large. Additionally, the hardware cost of its profiling counters is high. However, HMA is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages as the OS can update page tables and TLBs to reflect migrations.\n\nHMA does not require a remap table due to the OS updating the existing system’s structures. For activity tracking it uses Full Counters. The costly OS involvement and the high penalty for sorting all its counters force HMA to operate at very large intervals, weakening its adaptability to phase changes. However, HMA offers full flexibility for migrations.\n\nInterrupt and TLB shoot down assumption A fixed 5us time penalty is charged for each page fault [27] to cover the basic interrupt costs, and then another 3uspenalty is applied whenever a TLB shootdown [33] is required.\n\nCite from org paper This first-touch hot-page (FTHP) policy is effectively a generalization of both the history-based and first-touch algorithms.\n\nWe propose a dynamic feedback-directed HMA policy that can dynamically adjust the hotness threshold θ to achieve a best-of-both-worlds approach between history-based and first-touch policies.\n\n * At the start of each epoch, the size of the hot set is compared to the size of the die-stacked DRAM (N).\n * If the hot set is too small to fill the fast memory, then θ is lowered which causes more pages to be classified as hot.\n * Likewise, if the hot set is too large, θ is increased which causes fewer pages to be put in the hot set.\n * If the feedback mechanism works well, then the size of the hot set should converge to N.\n\n----------------------------------------\n\n\n# 10.Challenges in Heterogeneous Die-Stacked and Off-Chip Memory Systems\n\nYear: 2012 Software OS management To prevent the mapping of pages with insufficient miss traffic, we employ a threshold θ such that any page with fewer than θ LLC misses is not considered for mapping into stacked DRAM. The application of a threshold may result in cases when the list of most frequently missed pages has only k < P items. In this case, we simply keep a random set of P − k of the existing pages from the previous epoch already in stacked DRAM to avoid consuming bandwidth to swap out the page back to the off-chip memory.\n\n----------------------------------------\n\n\n# 11. Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware Cooperation\n\nYear: 2017 Software aovid tag look up by storing DRAM Cache presence information in the page table and tlbs.\n\nSpecifically, Banshee uses a hardwaremanaged frequency-based replacement (FBR) policy that only caches hot pages to reduce unnecessary data replacement traffic. To reduce the cost of accessing/updating frequency counters (which are stored in in-package DRAM), Banshee uses a new sampling approach to only read/write counters for a fraction of memory accesses.\n\nBackground\n\n * Using tags Alloy Cache | Unison Cache\n * Using address remapping Heterogeneous Memory Architecture (HMA) | Tagless DRAM Cache (TDC)\n\n\n\nThey reuse reverse maping mechanism.\n\nBanshee tracks each page’s access frequency with a counter, stored in the metadata. We store counters not only for the pages in the DRAM cache, but also for some pages not in cache, which are candidates to bring into the cache.\n\nInstead, an access in Banshee only updates a page’s frequency counter with a certain sample rate. For a sample rate of 10%, for example, the frequency counters are accessed/updated only once for every 10 DRAM accesses.\n\nFrequency-based replacement may lead to thrashing problem.\n\nBanshee solves this problem by only replacing a page when the candidate’s counter is greater than the victim’s counter by a certain threshold. This ensures that a page just evicted from the DRAM cache must be accessed for at least 2·threshold/sampling rate times before it can enter the cache again, thus preventing a page from entering and leaving frequently.\n\nBy default, the threshold is the product of the number of cachelines in a page and the sampling coefficient divided by two (threshold = page_size x sampling_coeff / 2). Intuitively, this means replacement can happen only if the benefit of swapping the pages outweighs the cost of the replacement operation.\n\nIf a counter saturates after being incremented, all counters in the metadata will be reduced by half using a shift operation in hardware.\n\n\n\n----------------------------------------\n\n\n# 13. Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache\n\nThe state-of-the-art block-based design, called Alloy Cache, colocates a tag with each data block (e.g., 64B) in the stacked DRAM to provide fast access to data in a single DRAM access. However, such a design suffers from low hit rates due to poor temporal locality in the DRAM cache. In contrast, the state-of-the-art page-based design, called Footprint Cache, organizes the DRAM cache at page granularity (e.g., 4KB), but fetches only the blocks that will likely be touched within a page. In doing so, the Footprint Cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. However, multi-gigabyte stacked DRAM caches will soon be practical and needed by server applications, thereby mandating tens of MBs of tag storage even for page-based DRAM caches.\n\nWe introduce a novel stacked-DRAM cache design, Unison Cache. Similar to Alloy Cache’s approach, Unison Cache incorporates the tag metadata directly into the stacked DRAM to enable scalability to arbitrary stacked-DRAM capacities. Then, leveraging the insights from the Footprint Cache design, Unison Cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. Our evaluation using server workloads and caches of up to 8GB reveals that Unison cache improves performance by 14% compared to Alloy Cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical SRAM-based tags of around 50MB.\n\nMe Block-based high miss ratio, page-based high migration penalty when fetching useless data. If footprint meta data is adopted to collect data trace, FootCache meta table cannot scale with increasing capactiy of HBM.\n\n\n\n----------------------------------------\n\n\n# 14. Dynamically Adapting Page Migration Policies Based on Applications Memory Access Behaviors\n\nLink to notes for Dynamically Adapting...\n\n\n# 15. On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems\n\n“on-the-fly” migration performs better than epoch-based page migration techniques, since we migrate recent hot pages.\n\nInstead of relying completely on OS to perform AR for the evicted entries, as done in [Ramoset al. 2011], we propose a hardware-based AR, where the MigC hardware initiates TLB shootdown and cache flushing without explicitly stopping the user program.\n\nLike previous studies [Meswani et al. 2015; Prodromou et al. 2017; Su et al. 2015], we observe that not all applications benefit from page migration, since page migration incurs performance overheads due to extra data movement.\n\nThe model works with the principle that, to get performance benefit, one should migrate the smallest set of pages from slow to fast memory that yields in the largest increase in memory accesses to fast memory to amortize the migration overhead.\n\nMe Try to use 80% principle. In our study, we look at the 80-percentile accesses and identify the “set of top-accessed pages” that contribute to more than 80% of all memory accesses.\n\nIn our proposal, we migrate a page immediately when it receives sufficient number of memory accesses, unlike any epoch-based schemes. We allow full flexibility in page relocation like HMAHS [Meswani et al. 2015] and keep a remap table for address redirection. We keep this table small by periodically evicting entries and it is placed on-chip.\n\nThe particular access count, which can separate such top-accessed pages from other pages, is referred to as “filter count.” Note that, filter count indicates an upper bound for hotness threshold\n\nMe This paper explain the address reconciliation AR in detail.\n\nFlow of AR -First, all cache lines from these pages, which are currently residing in the cache hierarchies and tagged with OS-visible PA, must be invalidated (and dirty lines written back), since the current OS-visible PA will be replaced with the new PA. All future accesses to these pages will only have access to the new PA. -Next,corresponding page table entries (PTEs) for A and B need to be updated with new PAs. -The TLB entries in all cores using the old PA must also be invalidated (as well as any other OS structures that contain the physical page addresses).\n\n(i) flush_cache_page() (ii) change PTE, (iii) flush_tlb_page()\n\nWe use a Migration Benefit Quotient (MBQ), by calculating the difference between the total number of accesses to any page and the filter count used in classifying applications’ memory locality, as described. MBQ indicates how many of the future accesses of a page may go to fast memory if the page were migrated from slow to fast memory using the filter count as a hotness threshold.\n\nSacturation account the sum of access counts of all pages with 32,909 accesses or less (the bars corresponding to x-axis 0 to 3,290) accounts for 98% of all accesses. For our purposes, we use this access count as a saturation count (or assume that the maximum number of accesses any page can receive).\n\nFor each workload, we use the difference between the saturation count and the filter count to determine MBQ.\n\n\n\n 1. Low MBQ, difference is less than 1K. For example, in Figure 5(b) xalanc, pages with memory access count 609 or less (the bars corresponding to x-axis 0 to 60) provide 98% of the memory accesses. Hence, the difference between saturation count (609) and filter count (70) is 539. These workloads may not achieve significant increase in accesses to fast memory after page migration.\n 2. Medium MBQ, difference is in between 1K to K (e.g., Figure 5(c) omnetpp). These workloads may receive moderate benefits, depending the migration overheads.\n 3. High MBQ, difference is more than 8K (e.g., Figure 5(a) mcf). These workloads are likely to receive higher hits in faster memory as a result of page migration.\n\nMe In short\n\n * sactuartion account. The sum of access counts fo all pages with sactuartion account accounts for 98% of all accesses.\n * filter account. The “set of top-accessed pages” that contribute to more than 80% of all memory accesses. If they are close, it means that this is a uniform benchmark. If they have a large difference, this means that there is a subset of pages that has far more memory accesses.\n\n----------------------------------------\n\n# 17. TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems International Conference on Computer Design (ICCD)\n\nYear: 2019",normalizedContent:" 1.  baryon: efficient hybrid memory management with compression and sub-blocking\n 2.  cameo:a two-level memory organization with capacity of main memory and flexibility of hardware-managed cache\n 3.  hybrid2: combining caching and migration in hybrid memory systems\n 4.  silc-fm: subblocked interleaved cache-like flat memory organization\n 5.  mempod: a clustered architecture for efficient and scalable migration in flat address space multi-level memories\n 6.  transparent hardware management of stacked dram as part of memory\n 7.  chameleon: a dynamically reconfigurable heterogeneous memory system\n 8.  batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n 9.  heterogeneous memory architectures: a hw/sw approach for mixing die-stacked and off-package memories\n 10. challenges in heterogeneous die-stacked and off-chip memory systems\n 11. banshee: bandwidth-efficient dram caching via software/hardware cooperation\n 12. die-stacked dram: memory, cache, or memcache?\n 13. unison cache: a scalable and effective die-stacked dram cache\n 14. dynamically adapting page migration policies based on applications memory access behaviors\n 15. on-the-fly page migration and address reconciliation for heterogeneous memory systems\n 16. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems [dac]\n 17. tictoc: enabling bandwidth-efficient dram caching for both hits and misses in hybrid memory systems international conference on computer design (iccd)\n\ndram nvm\n\n 1. an operating system level data migration scheme in hybrid dram-nvm memory architecture\n 2. clock-dwf: a write-history-aware page replacement algorithm for hybrid pcm and dram memory architectures\n 3. apmigration: improving performance of hybrid memory performance via an adaptive page migration method\n 4. page placement in hybrid memory systems\n\n----------------------------------------\n\n\n# 2. cameo:a two-level memory organization with capacity of main memory and flexibility of hardware-managed cache\n\n# mempod:\n\ncameo [13] proposes a cache-like flat address space memory management scheme in an attempt to close the gap between cache and flat memory organizations. cameo operates similarly to thm, however it does so at the granularity of cache lines (64b). migrations are restricted within segments with one fast line location per segment. its bookkeeping structures are entirely stored in memory, while a “line location predictor” attempts to save some bookkeeping-related accesses by predicting the location of a line.\n\ncameo initiates a line migration upon every access to slow memory.\n\ncameo can incur high migration traffic as every access could induce a migration.\n\n\n\n----------------------------------------\n\n\n# 5. mempod: a clustered architecture for efficient and scalable migration in flat address space multi-level memories\n\nyear: 2017\n\nmempod uses mea counters to track page access activity and identify hot pages. they are dramatically smaller than prior tracking mechanisms while capturing activity counts and temporal recency in a way that provides more effective prediction of future page access.\n\nwhat makes mea most useful, though, is its failure mode – when it fails to find the most-accessed pages, it does so by favoring recency over quantity. that is, a page accessed several times near the end of an interval can easily knock out a page accessed many more times early in the interval. as a result, it combines both access counting and temporal locality, at a fraction of the cost of access counting alone.\n\n\n\nthree triggers are most commonly used whenever state must be updated based on tracking information (mc scheduling, migrations, dynamic voltage and frequency scaling etc.). interval-based (or epoch-based) triggers occur with a set frequency, while threshold-based solutions trigger whenever a predetermined criterion is met. finally, event-based triggers react to predefined events. both interval-based and threshold-based approaches face the same challenge of identifying the optimal interval or threshold value.\n\nmempod achieves the best performance (lower ammat) with 50us intervals and 64 counters per pod. mempod’s lightweight operation allows for such small intervals. for comparison purposes, hma [14] identi-fied the best epoch length to be 100ms (2000x larger) in order to support all the lengthy processes that take place during a migration event for that method.\n\nbased on these results, we use 64 mea 2-bit counters over 50us intervals for subsequent results in this paper. each one of the 64 mea entries needs 21 bits for addressing the 1.1m pages per pod and 2 bits for its counter, leading to an area cost of only 184b per pod and 736b total. compared to the state of the art, mempod’s activity tracking requirement is ∼712x smaller than thm’s (512kb) and ∼12800x smaller than hma’s (9mb).\n\n\n\n----------------------------------------\n\n\n# 6. transparent hardware management of stacked dram as part of memory\n\n# mempod:\n\nsim, et al. proposed a technique for transparent hardware management of a hybrid memory system [17],which we will refer to as “thm”. thm does not require os intervention while managing migrations. in order to keep bookkeeping costs manageable, thm allows migrations only within sets of pages (called segments). each segment includes one fast memory page and a set of slow memory pages. the slow pages of each segment can only migrate to the one fast page location, and any such migration results in the eviction of the currently-residing page. thm monitors memory accesses with one “competing counter” per segment resulting in a low cost profiling solution. finally, thm supports caching part of its structures on chip while the rest is stored in memory.\n\nthm’s competing counters can lead to false positives, allowing a cold page to migrate to fast memory.\n\nthm offers significantly limited flexibility by restricting migrations withing segments, however this decision reduces bookkeeping costs significantly. competing counters in each segment are used for activity tracking, occasionally leading to false (threshold-based) migration triggering if a cold page gets accessed at the right time. identifying migration candidates incurs very little overhead since there is exactly one fast memory location for each slow memory page that triggers migration.\n\ncite from paper\n\n\n\nsimilar to set dueling, they adopted sample region. the locations in fast memory are grouped into 32 distinct regions in an interleaving fashion, and four regions are dedicated to sampling, while other 28 regions follow the threshold decision from sampling.\n\n• nstatic: # of memory requests serviced from fast memory with static mapping • ndynamic: # of memory requests expected to be serviced from fast memory when swapping with a given threshold • nswap: # of expected swaps for a given threshold.\n\n\n\nk differs depending on the relative latency of fast and slow memory. the cost of a single fast swap is about 1200 cycles, and the difference in access latency between fast and slow memory is 72 cycles.thus, in general, the swapped-in segment needs to get at least 17 more (future) hits than the swapped-out segment for swapping to be valuable. k is computed in hardware at boot time.\n\nme competing counter needs a threshold to invoke swap.\n\nsample region will have different threshold. based on different threshold, nswap is different. thus they choose the the best threshold with max bexpected as candidate threshold.\n\n----------------------------------------\n\n\n# 8.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n\ninsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nas the nm simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. we observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nwe leverage our key insight on controlling data movement and propose bandwidth-aware tiered-memory management (batman), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the nm and the fm. we define the desired access rate of the nm as the target access rate (tar). tar is the fraction of memory accesses serviced by the nm when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2x 2/3 4x 4/5 8x 8/9\n\nme bandwidth-aware tired-memory management tries to distritube memory according to hbm and dram bandwidth ratio. and also treat it as a threshold to refuse page migration.\n\n----------------------------------------\n\n\n# 9. heterogeneous memory architectures: a hw/sw approach for mixing die-stacked and off-package memories\n\n# mempod:\n\nhma [14] is a hw/sw mechanism that attempts to predict frequently accessed pages in memory and, at predefined intervals, migrate those pages to fast memory. hw support is required for profiling memory accesses using counters for each memory page, while the migration is handled by the os. due to the costly os involvement, hma’s intervals are kept large. additionally, the hardware cost of its profiling counters is high. however, hma is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages as the os can update page tables and tlbs to reflect migrations.\n\nhma does not require a remap table due to the os updating the existing system’s structures. for activity tracking it uses full counters. the costly os involvement and the high penalty for sorting all its counters force hma to operate at very large intervals, weakening its adaptability to phase changes. however, hma offers full flexibility for migrations.\n\ninterrupt and tlb shoot down assumption a fixed 5us time penalty is charged for each page fault [27] to cover the basic interrupt costs, and then another 3uspenalty is applied whenever a tlb shootdown [33] is required.\n\ncite from org paper this first-touch hot-page (fthp) policy is effectively a generalization of both the history-based and first-touch algorithms.\n\nwe propose a dynamic feedback-directed hma policy that can dynamically adjust the hotness threshold θ to achieve a best-of-both-worlds approach between history-based and first-touch policies.\n\n * at the start of each epoch, the size of the hot set is compared to the size of the die-stacked dram (n).\n * if the hot set is too small to fill the fast memory, then θ is lowered which causes more pages to be classified as hot.\n * likewise, if the hot set is too large, θ is increased which causes fewer pages to be put in the hot set.\n * if the feedback mechanism works well, then the size of the hot set should converge to n.\n\n----------------------------------------\n\n\n# 10.challenges in heterogeneous die-stacked and off-chip memory systems\n\nyear: 2012 software os management to prevent the mapping of pages with insufficient miss traffic, we employ a threshold θ such that any page with fewer than θ llc misses is not considered for mapping into stacked dram. the application of a threshold may result in cases when the list of most frequently missed pages has only k < p items. in this case, we simply keep a random set of p − k of the existing pages from the previous epoch already in stacked dram to avoid consuming bandwidth to swap out the page back to the off-chip memory.\n\n----------------------------------------\n\n\n# 11. banshee: bandwidth-efficient dram caching via software/hardware cooperation\n\nyear: 2017 software aovid tag look up by storing dram cache presence information in the page table and tlbs.\n\nspecifically, banshee uses a hardwaremanaged frequency-based replacement (fbr) policy that only caches hot pages to reduce unnecessary data replacement traffic. to reduce the cost of accessing/updating frequency counters (which are stored in in-package dram), banshee uses a new sampling approach to only read/write counters for a fraction of memory accesses.\n\nbackground\n\n * using tags alloy cache | unison cache\n * using address remapping heterogeneous memory architecture (hma) | tagless dram cache (tdc)\n\n\n\nthey reuse reverse maping mechanism.\n\nbanshee tracks each page’s access frequency with a counter, stored in the metadata. we store counters not only for the pages in the dram cache, but also for some pages not in cache, which are candidates to bring into the cache.\n\ninstead, an access in banshee only updates a page’s frequency counter with a certain sample rate. for a sample rate of 10%, for example, the frequency counters are accessed/updated only once for every 10 dram accesses.\n\nfrequency-based replacement may lead to thrashing problem.\n\nbanshee solves this problem by only replacing a page when the candidate’s counter is greater than the victim’s counter by a certain threshold. this ensures that a page just evicted from the dram cache must be accessed for at least 2·threshold/sampling rate times before it can enter the cache again, thus preventing a page from entering and leaving frequently.\n\nby default, the threshold is the product of the number of cachelines in a page and the sampling coefficient divided by two (threshold = page_size x sampling_coeff / 2). intuitively, this means replacement can happen only if the benefit of swapping the pages outweighs the cost of the replacement operation.\n\nif a counter saturates after being incremented, all counters in the metadata will be reduced by half using a shift operation in hardware.\n\n\n\n----------------------------------------\n\n\n# 13. unison cache: a scalable and effective die-stacked dram cache\n\nthe state-of-the-art block-based design, called alloy cache, colocates a tag with each data block (e.g., 64b) in the stacked dram to provide fast access to data in a single dram access. however, such a design suffers from low hit rates due to poor temporal locality in the dram cache. in contrast, the state-of-the-art page-based design, called footprint cache, organizes the dram cache at page granularity (e.g., 4kb), but fetches only the blocks that will likely be touched within a page. in doing so, the footprint cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. however, multi-gigabyte stacked dram caches will soon be practical and needed by server applications, thereby mandating tens of mbs of tag storage even for page-based dram caches.\n\nwe introduce a novel stacked-dram cache design, unison cache. similar to alloy cache’s approach, unison cache incorporates the tag metadata directly into the stacked dram to enable scalability to arbitrary stacked-dram capacities. then, leveraging the insights from the footprint cache design, unison cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. our evaluation using server workloads and caches of up to 8gb reveals that unison cache improves performance by 14% compared to alloy cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical sram-based tags of around 50mb.\n\nme block-based high miss ratio, page-based high migration penalty when fetching useless data. if footprint meta data is adopted to collect data trace, footcache meta table cannot scale with increasing capactiy of hbm.\n\n\n\n----------------------------------------\n\n\n# 14. dynamically adapting page migration policies based on applications memory access behaviors\n\nlink to notes for dynamically adapting...\n\n\n# 15. on-the-fly page migration and address reconciliation for heterogeneous memory systems\n\n“on-the-fly” migration performs better than epoch-based page migration techniques, since we migrate recent hot pages.\n\ninstead of relying completely on os to perform ar for the evicted entries, as done in [ramoset al. 2011], we propose a hardware-based ar, where the migc hardware initiates tlb shootdown and cache flushing without explicitly stopping the user program.\n\nlike previous studies [meswani et al. 2015; prodromou et al. 2017; su et al. 2015], we observe that not all applications benefit from page migration, since page migration incurs performance overheads due to extra data movement.\n\nthe model works with the principle that, to get performance benefit, one should migrate the smallest set of pages from slow to fast memory that yields in the largest increase in memory accesses to fast memory to amortize the migration overhead.\n\nme try to use 80% principle. in our study, we look at the 80-percentile accesses and identify the “set of top-accessed pages” that contribute to more than 80% of all memory accesses.\n\nin our proposal, we migrate a page immediately when it receives sufficient number of memory accesses, unlike any epoch-based schemes. we allow full flexibility in page relocation like hmahs [meswani et al. 2015] and keep a remap table for address redirection. we keep this table small by periodically evicting entries and it is placed on-chip.\n\nthe particular access count, which can separate such top-accessed pages from other pages, is referred to as “filter count.” note that, filter count indicates an upper bound for hotness threshold\n\nme this paper explain the address reconciliation ar in detail.\n\nflow of ar -first, all cache lines from these pages, which are currently residing in the cache hierarchies and tagged with os-visible pa, must be invalidated (and dirty lines written back), since the current os-visible pa will be replaced with the new pa. all future accesses to these pages will only have access to the new pa. -next,corresponding page table entries (ptes) for a and b need to be updated with new pas. -the tlb entries in all cores using the old pa must also be invalidated (as well as any other os structures that contain the physical page addresses).\n\n(i) flush_cache_page() (ii) change pte, (iii) flush_tlb_page()\n\nwe use a migration benefit quotient (mbq), by calculating the difference between the total number of accesses to any page and the filter count used in classifying applications’ memory locality, as described. mbq indicates how many of the future accesses of a page may go to fast memory if the page were migrated from slow to fast memory using the filter count as a hotness threshold.\n\nsacturation account the sum of access counts of all pages with 32,909 accesses or less (the bars corresponding to x-axis 0 to 3,290) accounts for 98% of all accesses. for our purposes, we use this access count as a saturation count (or assume that the maximum number of accesses any page can receive).\n\nfor each workload, we use the difference between the saturation count and the filter count to determine mbq.\n\n\n\n 1. low mbq, difference is less than 1k. for example, in figure 5(b) xalanc, pages with memory access count 609 or less (the bars corresponding to x-axis 0 to 60) provide 98% of the memory accesses. hence, the difference between saturation count (609) and filter count (70) is 539. these workloads may not achieve significant increase in accesses to fast memory after page migration.\n 2. medium mbq, difference is in between 1k to k (e.g., figure 5(c) omnetpp). these workloads may receive moderate benefits, depending the migration overheads.\n 3. high mbq, difference is more than 8k (e.g., figure 5(a) mcf). these workloads are likely to receive higher hits in faster memory as a result of page migration.\n\nme in short\n\n * sactuartion account. the sum of access counts fo all pages with sactuartion account accounts for 98% of all accesses.\n * filter account. the “set of top-accessed pages” that contribute to more than 80% of all memory accesses. if they are close, it means that this is a uniform benchmark. if they have a large difference, this means that there is a subset of pages that has far more memory accesses.\n\n----------------------------------------\n\n# 17. tictoc: enabling bandwidth-efficient dram caching for both hits and misses in hybrid memory systems international conference on computer design (iccd)\n\nyear: 2019",charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"HBM Dead Block Predictor",frontmatter:{title:"HBM Dead Block Predictor",date:"2023-05-15T00:00:00.000Z",permalink:"/pages/2476af/"},regularPath:"/01.hbm/02.hbm_dead_block_predictor.html",relativePath:"01.hbm/02.hbm_dead_block_predictor.md",key:"v-d7471e16",path:"/pages/2476af/",headers:[{level:3,title:"1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory",slug:"_1-data-placement-in-hpc-architectures-with-heterogeneous-off-chip-memory",normalizedTitle:"1. data placement in hpc architectures with heterogeneous off-chip memory",charIndex:1},{level:3,title:"2. Die-Stacked DRAM: Memory, Cache, or MemCache?",slug:"_2-die-stacked-dram-memory-cache-or-memcache",normalizedTitle:"2. die-stacked dram: memory, cache, or memcache?",charIndex:76},{level:3,title:"4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)",slug:"_4-bumblebee-a-memcache-design-for-die-stacked-and-off-chip-heterogeneous-memory-systems-2023",normalizedTitle:"4. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems (2023)",charIndex:182},{level:3,title:"5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM",slug:"_5-batman-techniques-for-maximizing-system-bandwidth-of-memory-systems-with-stacked-dram",normalizedTitle:"5.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram",charIndex:7054},{level:3,title:"6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches",slug:"_6-bear-techniques-for-mitigating-bandwidth-bloat-in-gigascale-dram-caches",normalizedTitle:"6. bear: techniques for mitigating bandwidth bloat in gigascale dram caches",charIndex:371},{level:3,title:"7.To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches",slug:"_7-to-update-or-not-to-update-bandwidth-efficient-intelligent-replacement-policies-for-dram-caches",normalizedTitle:"7.to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches",charIndex:10703},{level:3,title:"8.ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction",slug:"_8-accord-enabling-associativity-for-gigascale-dram-caches-by-coordinating-way-install-and-way-prediction",normalizedTitle:"8.accord: enabling associativity for gigascale dram caches by coordinating way-install and way-prediction",charIndex:13307},{level:3,title:"9. A Survey of Cache Bypassing Techniques",slug:"_9-a-survey-of-cache-bypassing-techniques",normalizedTitle:"9. a survey of cache bypassing techniques",charIndex:13505},{level:3,title:"10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel",slug:"_10-the-evicted-address-filter-a-unified-mechanism-to-address-both-cache-pollution-and-thrashing-not-read-yet-intel",normalizedTitle:"10. the evicted-address filter: a unified mechanism to address both cache pollution and thrashing - not read yet intel",charIndex:745},{level:3,title:"11. Bypass and Insertion Algorithms for Exclusive Last-level Caches",slug:"_11-bypass-and-insertion-algorithms-for-exclusive-last-level-caches",normalizedTitle:"11. bypass and insertion algorithms for exclusive last-level caches",charIndex:865},{level:3,title:"12. Counter-Based Cache Replacement and Bypassing Algorithms",slug:"_12-counter-based-cache-replacement-and-bypassing-algorithms",normalizedTitle:"12. counter-based cache replacement and bypassing algorithms",charIndex:934},{level:3,title:"13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)",slug:"_13-techniques-for-bandwidth-efficient-prefetching-of-linked-data-structures-in-hybrid-prefetching-systems-lds-prefetch",normalizedTitle:"13. techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems (lds prefetch)",charIndex:996}],headersStr:"1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory 2. Die-Stacked DRAM: Memory, Cache, or MemCache? 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023) 5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM 6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches 7.To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches 8.ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction 9. A Survey of Cache Bypassing Techniques 10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel 11. Bypass and Insertion Algorithms for Exclusive Last-level Caches 12. Counter-Based Cache Replacement and Bypassing Algorithms 13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)",content:' 1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory\n 2. Die-Stacked DRAM: Memory, Cache, or MemCache?\n 3. A Survey Of Techniques for Architecting DRAM Caches\n 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)\n 5. BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n 6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches\n 7. To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches\n 8. ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction\n\n----------------------------------------\n\n 9.  A Survey of Cache Bypassing Techniques\n 10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel\n 11. Bypass and Insertion Algorithms for Exclusive Last-level Caches\n 12. Counter-Based Cache Replacement and Bypassing Algorithms\n 13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)\n\n----------------------------------------\n\n\n# 1. Data Placement in HPC Architectures with Heterogeneous Off-chip Memory\n\n * Software manage DRAM and NVM\n\n 1. First touch policy\n    Alloc all pages in DRAM\n\n 2. Static profile-based policy\n\n 3. Spill Migration\n    LRU spill policy keeps track of last access time for each page in DRAM, and in case of eviction selects one that is least recently used. Spill migration policy first allocates a page in fast memory (in our case DRAM), and later evicts it to PCM. Spill profile-based policy can either spare a page from eviction if its future traffic is high, or victimize it if it is low, regardless of its previous access count.\n\n 4. Dynamic page migration\n    \n    \n    \n    When a page is first brought to the PCM we reset its access counter, regardless of how many times it was accessed in the DRAM. At the same time we keep track of the number of accesses for every page in the DRAM, as well as the average for all the pages (nDRAMavg). When a page in PCM is accessed, we compare its access counter (naccesses) with the average number of accesses to pages in DRAM. Back migration threshold (BMT) is a value that controls the aggressiveness of migration triggering. If it is set to zero, a page is migrated as soon as it is touched in PCM, so the DRAM acts as a typical cache. In this case we expect good performance as the system tends to always move active pages to DRAM, but due to a large number of migrations, number of writes to PCM may go high. On the other hand, if BMT is set to infinity the page never gets migrated back, and then the policy is equivalent to LRU spill. In between those extremes we would like to search for values that give good performace and low number of PCM writes.\n\n----------------------------------------\n\n\n# 2. Die-Stacked DRAM: Memory, Cache, or MemCache?\n\n * Part as Memory ans Part as Cache\n * Discuss and compared with Alloy Cache, Unison Cache, Banshee Cache, HMA\n * Hot Data Sets pages in memory HBM and transient pages in cache HBM\n\nCited from org paper: In this proposal, a software procedure pre-processes the application and determines hot pages,then asks the OS to map them to the memory portion of the die-stacked DRAM. The cache portion of the die-stacked DRAM is managed by hardware, caching data allocated in the off-chip memory.\n\nTo identify hot pages, we use a static profile-based approach before the execution of an application. A software procedure, incorporated into the compiler, pre-processes the application and sorts the pages based on their access frequency. Then it picks the top pages and asks the OS to map them to the memory portion of the die-stacked DRAM.\n\nAfter detection of hot pages, their details are coded into the program binary. Whenever the program gets executed, the Loader passes the required information of hot pages to the OS. Then, the OS tries to map such hot pages to physical locations that belong to the memory portion of the die-stacked DRAM. For the OS, allocating pages in the die-stacked and off-chip memory is similar to the same operations in Non-Uniform Memory Architecture (NUMA) [50] systems.\n\nIn this paper, they raised the issue that when process switches, previous hbm space allocated to a process might left inadequate space for the following process. Other orthogonal research " Various proposals (e.g., [13, 47, 54, 62, 78]) have suggested to optimize memory management in such situations typically by gradually or periodically migrating application pages between different types of memories based on factors like programming model, application’s criticality, sharing degree, and so on".\n\nThey identify the portion of hbm memory(the hot pages that need to be allocated to HBM) for hot pages by trying to allocate the maximum number of pages into hbm memory without worsening the cacheAHF.\n\n\n\n----------------------------------------\n\n\n# 4. Bumblebee: A MemCache Design for Die-stacked and Off-chip Heterogeneous Memory Systems (2023)\n\n# Me\n\n * Hybrid Memory\n * Blk/Page Size 2KB/64KB\n   64KB page size is due to the fact that it maps all memory in dram and hbm.\n   Every request will cam PRT and BLE.\n   In multi core simulation env,it have to support multi core read and write the SRAM.\n   \n * Distinguish Spacial Locality and Temporal Locality.\n   cacheHBM (cHBM) for temporal locality\n   memoryHBM (mHBM) for spacial locality\n   \n * Page Allocation.\n   Different from previous design that allocate all memory in HBM or DRAM. It allocate page according to its neighbour pages. But it does not mention how it interact with page table. If page is deallocate or written back to disk, the PRT should also be updated.\n * The ratio between cHBM and mHBM is flexible.\n\n\n\n * If memory footprint is high, all used by OS, all the HBM will be served as flat memory.\n\nCited from org paper: Program Statistics\n\n\n\nIn each remapping set, the hotness tracker includes a hot table and five parameters: the HBM occupied ratio (Rh), a hotness threshold (T) to decide if an off-chip DRAM page should be brought in HBM for high Rh condition, the number of cHBM pages (Nc), and the number of mHBM pages in which most blocks have/have not been accessed (Na/Nn).\n\n\n\nFor SL>0 (strong spatial locality), more hot data should be brought in mHBM to better exploit the spatial locality and utilize the memory bandwidth. For SL ≤ 0 (weak spatial locality), hot data should be cached in cHBM to reduce over-fetching.\n\nThe threshold T in the hotness tracker can alleviate this issue. If Rh is high, for SL>0, only pages whose hotness value is larger than T are permitted to be migrated to mHBM and for SL ≤ 0, only blocks in a page whose hotness value is larger than T are permitted to be cached in cHBM. Me From this aspect, SL means that number of mHBM pages that most blocks have been accessed is far larger than not been accessed. This means strong spatial locality.\n\n----------------------------------------\n\n\n# 5.BATMAN: Techniques for Maximizing System Bandwidth of Memory Systems with Stacked-DRAM\n\nInsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nAs the NM simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. We observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nWe leverage our key insight on controlling data movement and propose Bandwidth-Aware Tiered-Memory Management (BATMAN), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the NM and the FM. We define the desired access rate of the NM as the target access rate (TAR). TAR is the fraction of memory accesses serviced by the NM when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2X 2/3 4X 4/5 8X 8/9\n\nMe Bandwidth-Aware Tired-Memory Management tries to distritube memory according to HBM and DRAM bandwidth ratio. And also treat it as a threshold to refuse page migration.\n\nThis bandwidth division is also adopted in "Design and Implementation of Bandwidth-Aware Memory Placement and Migration Policies for Heterogeneous Memory".\n\n\n\n----------------------------------------\n\n\n# 6. BEAR: Techniques for Mitigating Bandwidth Bloat in Gigascale DRAM Caches\n\nYear:2015\n\nIdeally, we want the bandwidth consumed for such secondary operations to be negligible, and have almost all the bandwidth be available for transfer of useful data from the DRAM cache to the processor. BEAR integrates three components, one each for reducing the bandwidth consumed by miss detection, miss fill, and writeback probes.\n\n 1. Miss Probe (to detect a miss, we need to look up the tag store in the DRAM cache)\n 2. Miss Fill (on a cache miss the missed line is obtained from memory and filled in the cache)\n 3. Write back Probe (on a dirty eviction from the on-chip LLC identifying if that line is present in the DRAM cache)\n 4. Writeback Update (if writeback probe gives a hit, updating the content of the line in DRAM cache)\n 5. Writeback Fill (filling the writeback data in the cache, if a writeback probe gives a miss)\n\nThey define BloatFactor, the ratio of the total bandwidth consumed by the DRAM cache to the bandwidth required for transferring only the data lines to the processor chip.\n\n 1. Bandwidth Efficient Cache Fills We propose Bandwidth Aware Bypass (BAB) to reduce the bandwidth consumed by fill operations while limiting the loss in cache hit rate to a desired level.\n    \n    \n\n 2. Bandwidth Efficient Writeback Probe DRAM Cache Presence (DCP), reduces Writeback Probe by introducing state information in the on-chip Last Level Cache (LLC) to track if the line exists in the DRAM cache. Inclusive Cache\n    \n    \n\n 3. Bandwidth Efficient Miss Probe We reduce the bandwidth consumed by Miss Probe by leveraging the property of DRAM caches to streams multiple tags on each access. We buffer the tags of recently accessed adjacent cache line\'s tags in the Neighboring Tag Cache (NTC). Neighboring Tag Cache\n    \n    \n\nComment from To Update or Not to update Along the same lines, Chou et. al [6] propose a policy that bypasses the cache with 90% probability (we call this policy 90%-Bypass). Me This comment from to update or not to update is not accurate, the bear paper mentioned that "Overall, the speed up from probabilistic bypass is negligible, and we may deem PB to be ineffective at improving performance." Then it prefers set-duleling.\n\n----------------------------------------\n\n\n# 7.To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches\n\nYear: 2019\n\nMe Previous dram cache is stateless, due to the fact that maintaining state of cache would require significant bandwidth.\n\nCite from org paper We propose a stateful replacement/bypass policy called RRIP Age-On-Bypass (RRIP-AOB), that tracks reuse state for high-reuse lines, protects such lines by bypassing other lines, and Ages the state On cache Bypass.\n\nThe DRAM cache in KNL [4, 5],for example, employs an Always-Install policy. The DRAM cache places each tag information in the unused bits in the ECC space and streams out the data and tag (contained in ECC) on each access.\n\n\n\nOur goal is to increase the hit-rate of such DRAM caches. In fact, the DRAM cache only uses about 8-10 bits from the unused 28 bits in the ECC space, so we have 18-20 bits per line available for managing the DRAM cache intelligently.\n\nTo reduce significant bandwidth to update state, we propose Efficient Tracking of Reuse (ETR). ETR makes state tracking efficient by accurately tracking the state of only one line from a region, and using the state of that line to guide the replacement decisions for other lines in that region.\n\n 1. We propose a bypass version of RRIP (RRIP-AOB) suitable for caches with limited associativity. However, we find an effective replacement policy for DRAM caches must optimize not only hit-rate but also state update cost. We introduce two properties, coresidency and eviction-locality, that can be exploited to reduce state update cost for implementing intelligent replacement.\n    \n    \n    \n    Coresidency indicates that at any given time if a line is present, then several other line belonging to that 4KB region are also present in the cache. Eviction-Locality indicates that when a line gets evicted from the cache, the replacement-state of the other coresident lines belonging to that region tend to have similar replacement state as the line being evicted. Me Just a synonym for spacial locality. This granularity is 4KB. Doubt about its authenticity.\n\n 2. We propose Efficient Tracking of Reuse (ETR), a design that performs updates for only a subset of lines and uses their state to guide the replacement decisions of other lines.\n\n\n\nMe This is similar to set dueling.\n\nThe design of ETR consists of three parts: (1) Selecting a Representative-Line in the region. (2) Keeping accurate RRPV for only the Representative-Line. (3) Using the representative’s RRPV to infer coresident lines’ RRPV to make bypass decisions.\n\n----------------------------------------\n\n\n# 8.ACCORD: Enabling Associativity for Gigascale DRAM Caches by Coordinating Way-Install and Way-Prediction\n\nA method to optimize prediction way of dram.\n\n----------------------------------------\n\n\n# 9. A Survey of Cache Bypassing Techniques\n\n----------------------------------------\n\n\n# 10. The Evicted-Address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing - Not Read Yet Intel\n\n----------------------------------------\n\n\n# 11. Bypass and Insertion Algorithms for Exclusive Last-level Caches\n\n----------------------------------------\n\n\n# 12. Counter-Based Cache Replacement and Bypassing Algorithms\n\n----------------------------------------\n\n\n# 13. Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (LDS Prefetch)',normalizedContent:' 1. data placement in hpc architectures with heterogeneous off-chip memory\n 2. die-stacked dram: memory, cache, or memcache?\n 3. a survey of techniques for architecting dram caches\n 4. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems (2023)\n 5. batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n 6. bear: techniques for mitigating bandwidth bloat in gigascale dram caches\n 7. to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches\n 8. accord: enabling associativity for gigascale dram caches by coordinating way-install and way-prediction\n\n----------------------------------------\n\n 9.  a survey of cache bypassing techniques\n 10. the evicted-address filter: a unified mechanism to address both cache pollution and thrashing - not read yet intel\n 11. bypass and insertion algorithms for exclusive last-level caches\n 12. counter-based cache replacement and bypassing algorithms\n 13. techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems (lds prefetch)\n\n----------------------------------------\n\n\n# 1. data placement in hpc architectures with heterogeneous off-chip memory\n\n * software manage dram and nvm\n\n 1. first touch policy\n    alloc all pages in dram\n\n 2. static profile-based policy\n\n 3. spill migration\n    lru spill policy keeps track of last access time for each page in dram, and in case of eviction selects one that is least recently used. spill migration policy first allocates a page in fast memory (in our case dram), and later evicts it to pcm. spill profile-based policy can either spare a page from eviction if its future traffic is high, or victimize it if it is low, regardless of its previous access count.\n\n 4. dynamic page migration\n    \n    \n    \n    when a page is first brought to the pcm we reset its access counter, regardless of how many times it was accessed in the dram. at the same time we keep track of the number of accesses for every page in the dram, as well as the average for all the pages (ndramavg). when a page in pcm is accessed, we compare its access counter (naccesses) with the average number of accesses to pages in dram. back migration threshold (bmt) is a value that controls the aggressiveness of migration triggering. if it is set to zero, a page is migrated as soon as it is touched in pcm, so the dram acts as a typical cache. in this case we expect good performance as the system tends to always move active pages to dram, but due to a large number of migrations, number of writes to pcm may go high. on the other hand, if bmt is set to infinity the page never gets migrated back, and then the policy is equivalent to lru spill. in between those extremes we would like to search for values that give good performace and low number of pcm writes.\n\n----------------------------------------\n\n\n# 2. die-stacked dram: memory, cache, or memcache?\n\n * part as memory ans part as cache\n * discuss and compared with alloy cache, unison cache, banshee cache, hma\n * hot data sets pages in memory hbm and transient pages in cache hbm\n\ncited from org paper: in this proposal, a software procedure pre-processes the application and determines hot pages,then asks the os to map them to the memory portion of the die-stacked dram. the cache portion of the die-stacked dram is managed by hardware, caching data allocated in the off-chip memory.\n\nto identify hot pages, we use a static profile-based approach before the execution of an application. a software procedure, incorporated into the compiler, pre-processes the application and sorts the pages based on their access frequency. then it picks the top pages and asks the os to map them to the memory portion of the die-stacked dram.\n\nafter detection of hot pages, their details are coded into the program binary. whenever the program gets executed, the loader passes the required information of hot pages to the os. then, the os tries to map such hot pages to physical locations that belong to the memory portion of the die-stacked dram. for the os, allocating pages in the die-stacked and off-chip memory is similar to the same operations in non-uniform memory architecture (numa) [50] systems.\n\nin this paper, they raised the issue that when process switches, previous hbm space allocated to a process might left inadequate space for the following process. other orthogonal research " various proposals (e.g., [13, 47, 54, 62, 78]) have suggested to optimize memory management in such situations typically by gradually or periodically migrating application pages between different types of memories based on factors like programming model, application’s criticality, sharing degree, and so on".\n\nthey identify the portion of hbm memory(the hot pages that need to be allocated to hbm) for hot pages by trying to allocate the maximum number of pages into hbm memory without worsening the cacheahf.\n\n\n\n----------------------------------------\n\n\n# 4. bumblebee: a memcache design for die-stacked and off-chip heterogeneous memory systems (2023)\n\n# me\n\n * hybrid memory\n * blk/page size 2kb/64kb\n   64kb page size is due to the fact that it maps all memory in dram and hbm.\n   every request will cam prt and ble.\n   in multi core simulation env,it have to support multi core read and write the sram.\n   \n * distinguish spacial locality and temporal locality.\n   cachehbm (chbm) for temporal locality\n   memoryhbm (mhbm) for spacial locality\n   \n * page allocation.\n   different from previous design that allocate all memory in hbm or dram. it allocate page according to its neighbour pages. but it does not mention how it interact with page table. if page is deallocate or written back to disk, the prt should also be updated.\n * the ratio between chbm and mhbm is flexible.\n\n\n\n * if memory footprint is high, all used by os, all the hbm will be served as flat memory.\n\ncited from org paper: program statistics\n\n\n\nin each remapping set, the hotness tracker includes a hot table and five parameters: the hbm occupied ratio (rh), a hotness threshold (t) to decide if an off-chip dram page should be brought in hbm for high rh condition, the number of chbm pages (nc), and the number of mhbm pages in which most blocks have/have not been accessed (na/nn).\n\n\n\nfor sl>0 (strong spatial locality), more hot data should be brought in mhbm to better exploit the spatial locality and utilize the memory bandwidth. for sl ≤ 0 (weak spatial locality), hot data should be cached in chbm to reduce over-fetching.\n\nthe threshold t in the hotness tracker can alleviate this issue. if rh is high, for sl>0, only pages whose hotness value is larger than t are permitted to be migrated to mhbm and for sl ≤ 0, only blocks in a page whose hotness value is larger than t are permitted to be cached in chbm. me from this aspect, sl means that number of mhbm pages that most blocks have been accessed is far larger than not been accessed. this means strong spatial locality.\n\n----------------------------------------\n\n\n# 5.batman: techniques for maximizing system bandwidth of memory systems with stacked-dram\n\ninsights\n\n * bandwidth distribution\n * dram and hbm similar latency\n\nas the nm simply offers higher bandwidth, not lower latency,the performance of tiered-memory systems is determined by the utilization of system bandwidth. we observe that both system bandwidth and performance are maximized when memory accesses are distributed proportional to the bandwidth of each memory.\n\nwe leverage our key insight on controlling data movement and propose bandwidth-aware tiered-memory management (batman), which is a runtime mechanism that monitors memory access distribution and explicitly controls the data movement between the nm and the fm. we define the desired access rate of the nm as the target access rate (tar). tar is the fraction of memory accesses serviced by the nm when memory accesses to both memories are proportional to the respective bandwidth.\n\n\n\n2x 2/3 4x 4/5 8x 8/9\n\nme bandwidth-aware tired-memory management tries to distritube memory according to hbm and dram bandwidth ratio. and also treat it as a threshold to refuse page migration.\n\nthis bandwidth division is also adopted in "design and implementation of bandwidth-aware memory placement and migration policies for heterogeneous memory".\n\n\n\n----------------------------------------\n\n\n# 6. bear: techniques for mitigating bandwidth bloat in gigascale dram caches\n\nyear:2015\n\nideally, we want the bandwidth consumed for such secondary operations to be negligible, and have almost all the bandwidth be available for transfer of useful data from the dram cache to the processor. bear integrates three components, one each for reducing the bandwidth consumed by miss detection, miss fill, and writeback probes.\n\n 1. miss probe (to detect a miss, we need to look up the tag store in the dram cache)\n 2. miss fill (on a cache miss the missed line is obtained from memory and filled in the cache)\n 3. write back probe (on a dirty eviction from the on-chip llc identifying if that line is present in the dram cache)\n 4. writeback update (if writeback probe gives a hit, updating the content of the line in dram cache)\n 5. writeback fill (filling the writeback data in the cache, if a writeback probe gives a miss)\n\nthey define bloatfactor, the ratio of the total bandwidth consumed by the dram cache to the bandwidth required for transferring only the data lines to the processor chip.\n\n 1. bandwidth efficient cache fills we propose bandwidth aware bypass (bab) to reduce the bandwidth consumed by fill operations while limiting the loss in cache hit rate to a desired level.\n    \n    \n\n 2. bandwidth efficient writeback probe dram cache presence (dcp), reduces writeback probe by introducing state information in the on-chip last level cache (llc) to track if the line exists in the dram cache. inclusive cache\n    \n    \n\n 3. bandwidth efficient miss probe we reduce the bandwidth consumed by miss probe by leveraging the property of dram caches to streams multiple tags on each access. we buffer the tags of recently accessed adjacent cache line\'s tags in the neighboring tag cache (ntc). neighboring tag cache\n    \n    \n\ncomment from to update or not to update along the same lines, chou et. al [6] propose a policy that bypasses the cache with 90% probability (we call this policy 90%-bypass). me this comment from to update or not to update is not accurate, the bear paper mentioned that "overall, the speed up from probabilistic bypass is negligible, and we may deem pb to be ineffective at improving performance." then it prefers set-duleling.\n\n----------------------------------------\n\n\n# 7.to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches\n\nyear: 2019\n\nme previous dram cache is stateless, due to the fact that maintaining state of cache would require significant bandwidth.\n\ncite from org paper we propose a stateful replacement/bypass policy called rrip age-on-bypass (rrip-aob), that tracks reuse state for high-reuse lines, protects such lines by bypassing other lines, and ages the state on cache bypass.\n\nthe dram cache in knl [4, 5],for example, employs an always-install policy. the dram cache places each tag information in the unused bits in the ecc space and streams out the data and tag (contained in ecc) on each access.\n\n\n\nour goal is to increase the hit-rate of such dram caches. in fact, the dram cache only uses about 8-10 bits from the unused 28 bits in the ecc space, so we have 18-20 bits per line available for managing the dram cache intelligently.\n\nto reduce significant bandwidth to update state, we propose efficient tracking of reuse (etr). etr makes state tracking efficient by accurately tracking the state of only one line from a region, and using the state of that line to guide the replacement decisions for other lines in that region.\n\n 1. we propose a bypass version of rrip (rrip-aob) suitable for caches with limited associativity. however, we find an effective replacement policy for dram caches must optimize not only hit-rate but also state update cost. we introduce two properties, coresidency and eviction-locality, that can be exploited to reduce state update cost for implementing intelligent replacement.\n    \n    \n    \n    coresidency indicates that at any given time if a line is present, then several other line belonging to that 4kb region are also present in the cache. eviction-locality indicates that when a line gets evicted from the cache, the replacement-state of the other coresident lines belonging to that region tend to have similar replacement state as the line being evicted. me just a synonym for spacial locality. this granularity is 4kb. doubt about its authenticity.\n\n 2. we propose efficient tracking of reuse (etr), a design that performs updates for only a subset of lines and uses their state to guide the replacement decisions of other lines.\n\n\n\nme this is similar to set dueling.\n\nthe design of etr consists of three parts: (1) selecting a representative-line in the region. (2) keeping accurate rrpv for only the representative-line. (3) using the representative’s rrpv to infer coresident lines’ rrpv to make bypass decisions.\n\n----------------------------------------\n\n\n# 8.accord: enabling associativity for gigascale dram caches by coordinating way-install and way-prediction\n\na method to optimize prediction way of dram.\n\n----------------------------------------\n\n\n# 9. a survey of cache bypassing techniques\n\n----------------------------------------\n\n\n# 10. the evicted-address filter: a unified mechanism to address both cache pollution and thrashing - not read yet intel\n\n----------------------------------------\n\n\n# 11. bypass and insertion algorithms for exclusive last-level caches\n\n----------------------------------------\n\n\n# 12. counter-based cache replacement and bypassing algorithms\n\n----------------------------------------\n\n\n# 13. techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems (lds prefetch)',charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Dynamically Adapting  Page Migration Policies Based on Applications Memory Access Behaviors",frontmatter:{title:"Dynamically Adapting  Page Migration Policies Based on Applications Memory Access Behaviors",date:"2023-05-11T00:00:00.000Z",permalink:"/pages/24769f/"},regularPath:"/01.hbm/03.Dynamically_Adapting%20_Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.html",relativePath:"01.hbm/03.Dynamically_Adapting _Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.md",key:"v-b2fd417c",path:"/pages/24769f/",headersStr:null,content:'Year: 2021 Mem : HBM & PCM\n\n * migration friendly\n * migration unfriendly\n\nBased on previous research "On-the-fly Page Migration and Address Reconciliation for Heterogeneous Memory Systems" from the same author.\n\n 1. Adaptive migration polices Our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n 2. AR overheads can defeat the benefits of page migration To eliminate AR, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. AR: OS tables (translation look-aside buffers (TLBs), page tables) must also be updated since physical addresses (PAs) in such memory systems are based on the physical location of pages and a migration changes PAs: we call this process of changing PAs and updating system tables address reconciliation (AR).\n\nWe discovered that an exponential-shaped histogram indicates that very few pages receive most accesses and that those applications benefit by either placing those few pages in the faster (HBM) memory at the start of execution, or migrated to HBM on demand.\n\nMcf 3% of all pages cause 97% of memory accesses. Milc 65% of pages contribute to 82% of all accesses.\n\nIf most of all pages receive about the same number of accesses, implying that too many pages may be migrated if a fixed hotness threshold is used for migrating pages, and the migration overheads outweigh performance gains. Doubt about this statement.\n\nMigration of pages to faster memories results in performance gains if those pages continue to be heavily used, because these accesses will be satisfied by faster memories.\n\nLinux3 performs the following functions when the virtual to PA mapping of a page is changed.\n\n 1. flush_cache_page\n 2. change PTE\n 3. flush_tlb_page\n\n# Key Insights\n\nAdaptive migration polices: Previous page migration techniques relied on fixed hotness thresholds: a page is migrated from slow memories to faster memories when the number of times that page was accessed exceeds the hotness threshold. In contrast, we control page migration policies based on applications’ memory access behaviors. Our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n\nAR overheads can defeat the benefits of page migration: To eliminate AR, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. Reverse migration makes page migration invisible to the OS. However, reverse migrations can result in excessive data movement between slow and fast memories. In this work, we evaluate the effectiveness of the reverse migration technique.\n\n# Algorithm\n\nIf the count is high (too many pages have been migrated), we double the hotness threshold to reduce future migrations; likewise, if too few pages have been migrated in a twindow, we halve the hotness threshold to increase future Adaptive Migration Based on Number of Pages Migrated migrations. In our experiments, we used 4 million cycles as our twindow.6 We also limit the hotness threshold variations between 64 and 256.\n\nWe define the MBQ as the average number of accesses to pages that were recently migrated to HBM.\n\n\n\n * threshold adaption We increase the threshold if more than 240 pages have been migrated in a window and reduce the threshold if fewer than 160 pages have been migrated in a window.\n * pause and resume migration If the MBQ is less than a threshold (min_MBQ), then migrations are halted. migrations are resumed if the MBQ is greater than another threshold (max_MBQ).\n\nAdaptive Migration Based on the MBQ\n\n# Summary\n\nTwo Algorithm\n\n 1. Based on number of page migrated, if too many page is migrared, reduce migration by increase threshold.\n 2. Based on reference after migration, if too less after migration, reduce or stop.',normalizedContent:'year: 2021 mem : hbm & pcm\n\n * migration friendly\n * migration unfriendly\n\nbased on previous research "on-the-fly page migration and address reconciliation for heterogeneous memory systems" from the same author.\n\n 1. adaptive migration polices our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n 2. ar overheads can defeat the benefits of page migration to eliminate ar, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. ar: os tables (translation look-aside buffers (tlbs), page tables) must also be updated since physical addresses (pas) in such memory systems are based on the physical location of pages and a migration changes pas: we call this process of changing pas and updating system tables address reconciliation (ar).\n\nwe discovered that an exponential-shaped histogram indicates that very few pages receive most accesses and that those applications benefit by either placing those few pages in the faster (hbm) memory at the start of execution, or migrated to hbm on demand.\n\nmcf 3% of all pages cause 97% of memory accesses. milc 65% of pages contribute to 82% of all accesses.\n\nif most of all pages receive about the same number of accesses, implying that too many pages may be migrated if a fixed hotness threshold is used for migrating pages, and the migration overheads outweigh performance gains. doubt about this statement.\n\nmigration of pages to faster memories results in performance gains if those pages continue to be heavily used, because these accesses will be satisfied by faster memories.\n\nlinux3 performs the following functions when the virtual to pa mapping of a page is changed.\n\n 1. flush_cache_page\n 2. change pte\n 3. flush_tlb_page\n\n# key insights\n\nadaptive migration polices: previous page migration techniques relied on fixed hotness thresholds: a page is migrated from slow memories to faster memories when the number of times that page was accessed exceeds the hotness threshold. in contrast, we control page migration policies based on applications’ memory access behaviors. our technique increases or reduces the hotness thresholds to reduce or increase the number of pages migrated based on either the number of pages migrated over a window of observation or based on the observed benefits of page migrations (were pages accessed after the migration to faster memories).\n\nar overheads can defeat the benefits of page migration: to eliminate ar, we explore the benefit of reverse migrating pages to their original locations, particularly when the migrated pages are no longer heavily accessed. reverse migration makes page migration invisible to the os. however, reverse migrations can result in excessive data movement between slow and fast memories. in this work, we evaluate the effectiveness of the reverse migration technique.\n\n# algorithm\n\nif the count is high (too many pages have been migrated), we double the hotness threshold to reduce future migrations; likewise, if too few pages have been migrated in a twindow, we halve the hotness threshold to increase future adaptive migration based on number of pages migrated migrations. in our experiments, we used 4 million cycles as our twindow.6 we also limit the hotness threshold variations between 64 and 256.\n\nwe define the mbq as the average number of accesses to pages that were recently migrated to hbm.\n\n\n\n * threshold adaption we increase the threshold if more than 240 pages have been migrated in a window and reduce the threshold if fewer than 160 pages have been migrated in a window.\n * pause and resume migration if the mbq is less than a threshold (min_mbq), then migrations are halted. migrations are resumed if the mbq is greater than another threshold (max_mbq).\n\nadaptive migration based on the mbq\n\n# summary\n\ntwo algorithm\n\n 1. based on number of page migrated, if too many page is migrared, reduce migration by increase threshold.\n 2. based on reference after migration, if too less after migration, reduce or stop.',charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Cache Memory Compression",frontmatter:{title:"Cache Memory Compression",date:"2023-06-06T00:00:00.000Z",permalink:"/pages/2476bf/"},regularPath:"/01.hbm/05.cache_mem_compression.html",relativePath:"01.hbm/05.cache_mem_compression.md",key:"v-7f8db936",path:"/pages/2476bf/",headersStr:null,content:" 1. Compresso: Pragmatic Main Memory Compression\n 2. Translation-optimized Memory Compression for Capacity\n 3. Touche: Towards Ideal and Efficient Cache Compression By Mitigating Tag Area Overheads\n\n----------------------------------------\n\n# 1. Compresso: Pragmatic Main Memory Compression\n\nCite from the paper: We propose Compresso, with optimizations to reduce compressed data movement in a hardware compressed memory, while maintaining high compression ratio by repacking data at the right time.\n\nCompresso uses the modified BPC compression algorithm, achieving 1.85x average compression on a wide range of applications.\n\nCompresso uses the compression granularity of 64B.\n\nCompresso uses LinePack with 4 possible cache line sizes.\n\nWe compare variable-sized chunks (512B, 1KB, 2KB and 4KB) with 512B fixed-sized chunks. Compresso uses incremental allocation in 512B chunks,thereby allowing 8 page sizes (512B, 1KB, 1.5KB and so on).\n\n\n\nAdditional Data Movement:\n\n 1. split-access cachelines\n 2. changes in compressibility(overflows)\n 3. metadata access\n\n\n\nDifference in exception in LCP compression Instead, Compresso allows some number of such inflated cachelines to be stored uncompressed in the inflation room at the end of an MPA page, provided that there is space in that page (Fig. 5a). This is similar to the exception region in LCP, but is used for an entirely different reason—to reduce compression-related data movement, rather than to support a specific packing scheme.\n\n\n\nWe present the first main-memory compression architecture that is designed to run an unmodified operating system.\n\n----------------------------------------\n\n# 2. Translation-optimized Memory Compression for Capacity\n\nprior workscompress and pack/migrate data at a small - memory block-level - granularity; this introduces an additional block-level translation after the page-level virtual address translation. In general, the smaller the granularity of address translation, the higher the translation overhead.\n\nA promising solution is to only save memory from cold (i.e.,less recently accessed) pages without saving memory from hot (i.e., more recently accessed) pages (e.g., keep the hot pages uncompressed).\n\nTwo challenges:\n\n 1. after a compressed cold page becomes hot again, migrating the page to a full 4KB DRAM location still adds another level (albeit page-level, instead of block-level) of translation on top of existing virtual address translation. Solution we propose compressing page table blocks in hardware to opportunistically embed compression translations into them in a software-transparent manner to effectively prefetch compression translations during a page walk, instead of serially fetching them after the walk.\n\nFirst, CTE misses typically occur after PTE misses in TLB because CTEs, especially the page-level CTEs under an OS-inspired approach, have similar translation reach as PTEs. Second, we observe page table blocks (PTBs) are highly compressible because adjacent virtual pages often have identical status bits and the most significant bits in physical page numbers are unused. As such, to hide the latency of CTE misses, TMCC transparently compresses each PTB in hardware to free up space in the PTB to embed the CTEs of the 4KB pages (i.e., either data pages or page table pages) that the PTB points to; this enables each page walk to also prefetch the matching CTE required for fetching from DRAM either the end data or the next PTB.\n\n 2. only compressing cold data require compressing them very aggressively to achieve high overall memory savings. Solution we perform a large design space exploration across many hardware configurations and diverse workloads to derive and implement in HDL an ASIC Deflate that is specialized for memory.\n\nPrior new hardware managed translation entries as Compression Translation Entries (CTEs), as they are similar to OS page table entries (PTEs). Prior works cache CTEs in the memory controller via a dedicated CTE cache, similar to the TLBs dedicated to caching PTEs.\n\nlet hardware take on an OS-inspired approach: only save memory from cold (i.e., less recently accessed) pages without saving memory from hot (i.e., recently accessed) pages (e.g., keep the hot pages uncompressed), like OS memory compression. Solves the problem of\n\n 1. translation overheads that large and/or irregular workloads suffer from high PTE miss under hardware memory compression.\n 2. Fine-grained address translation\n\nAccesses to a compressed virtual page in ML2 incurs a page fault to wake up OS to pop a free physical page from ML1’s free list and migrate the virtual page to the page.\n\nML2 also keeps many free lists, each tracking sub-physical pages of a different size, to store any compressed virtual page in a practically ideal matching sub-physical page.\n\nML2 gracefully grows and shrinks relative to ML1 with increasing and decreasing memory usage. When everything can fit in memory uncompressed, ML2 shrinks to zero bytes in physical size so ML1 can have every physical page. Specifically, when ML2’s free list(s) get large (e.g., due to reducing memory usage), ML2 donates free physical pages from its free list(s) to ML1. OS also grows ML1 free list, when it gets small, by migrating cold virtual pages to ML2. Migrating a virtual page to ML2 shrinks one of ML2’s free lists. If a ML2 free list gets empty, ML1 gives cold victim physical pages to ML2 (i.e., track them in ML2 instead of ML1), so that ML2 can compress the virtual pages currently in the victim pages to free space in the victims to grow ML2’s free list(s).\n\n\n\nKey Idea: Based on our observations, we propose transparently compressing each PTB in hardware to free up space in the PTB to embed the CTEs of the 4KB pages (i.e., either data pages or page table pages) that the PTB points to; this enables each page walk access to also prefetch the matching CTE required either for the next page walk access (i.e., to the next PTB) or for the actual data (or instruction) access after the walk.\n\n\n\nA practical challenge is that after migrating a page (e.g., from ML1 to ML2 after the page becomes cold), the corresponding CTE embedded in the page’s PTB should be updated. However, hardware has no easy way to use the PPN of the migrating page to find/access the page’s PTB(s). TMCC addresses this challenge by lazily updating the CTE in the PTB later around when the PTB is naturally accessed by the page walker, instead of updating it at the time of migrating the page. However, this means that for the first page walker access to the PTB after migrating one of the pages that the PTB points to, the corresponding CTE is out-of-date. To ensure correctness, TMCC also accesses the correct CTE in DRAM (or in CTE cache) in parallel to verify the correctness of the DRAM access. Figure 8 compares and contrasts how TMCC serves an LLC miss that also misses in CTE cache with the baseline approach. Figure 9 provides n architectural overview of TMCC.\n\n\n\n\n\n\n\nMy Comment When os access compressed page, that page is migrated from ML2 to ML1. Hardware cannot update the PTB easily. Thus it utilizes lazily update. During the page table walk, it will buffer the piggybacked CTE into CTE buffer. And when data miss req happens, L2 extracts the PPN from the received request to lookup the CTE Buffer to obtain the CTE for MC to translate the PPN.\n\n----------------------------------------\n\n# 3. Touche: Towards Ideal and Efficient Cache Compression By Mitigating Tag Area Overheads\n\nThe first component, called the “Signature” (SIGN) engine, creates shortened signatures from the tag addresses of compressed blocks. Due to this, the SIGN engine can store multiple signatures in each tag entry. On a cache access, the physical cacheline is accessed only if there is a signature match (which has a negligible probability of false positive). The second component, called the “Tag Appended Data” (TADA) mechanism, stores the full tag addresses with data. TADA enables Touch´e to detect false positive signature matches by ensuring that the actual tag address is available for comparison. The third component, called the “Superblock Marker” (SMARK) mechanism, uses a unique marker in the tag entry to indicate the occurrence of compressed cache blocks from neighboring physical addresses in the same cacheline.\n\n\n\nOn average, 55% of the blocks can be compressed to less than 48 bytes in size. Furthermore, 17% of the lines can be compressed to be less than 16 bytes in size. Therefore, several workloads tend to have blocks with low entropy and can benefit from compression.\n\n\n\n\n\n\n\nFor instance, a cacheline cannot be marked both invalid and dirty at the same time. The tag manager uses this unused state to flag cachelines that contains compressed blocks. Thereafter, for a cacheline that stores compressed blocks, the 1st and 2nd bits of the tag address encodes its valid bit and dirty bit.\n\n\n\n\n\nThe tag manager then retrieves the 16-bit marker from the SMARK mechanism. It then informs the SIGN engine to ignore the last 2-bits (corresponding to four neighboring addresses) of the full tag address to generate a unique 9-bit signature.\n\nThis SMARK generate a random 16-bit marker and concated with signature. Since non-superblocks use 3 signature to identify blks, it should also use a tag to compare not just 0.\n\nIf this paper doest not support superblock 4 compressed blks in a super block, it can only store 3* 16B compressed block or 48B + 64B block, due to extra real tag stored in data, 43bit for each data.",normalizedContent:" 1. compresso: pragmatic main memory compression\n 2. translation-optimized memory compression for capacity\n 3. touche: towards ideal and efficient cache compression by mitigating tag area overheads\n\n----------------------------------------\n\n# 1. compresso: pragmatic main memory compression\n\ncite from the paper: we propose compresso, with optimizations to reduce compressed data movement in a hardware compressed memory, while maintaining high compression ratio by repacking data at the right time.\n\ncompresso uses the modified bpc compression algorithm, achieving 1.85x average compression on a wide range of applications.\n\ncompresso uses the compression granularity of 64b.\n\ncompresso uses linepack with 4 possible cache line sizes.\n\nwe compare variable-sized chunks (512b, 1kb, 2kb and 4kb) with 512b fixed-sized chunks. compresso uses incremental allocation in 512b chunks,thereby allowing 8 page sizes (512b, 1kb, 1.5kb and so on).\n\n\n\nadditional data movement:\n\n 1. split-access cachelines\n 2. changes in compressibility(overflows)\n 3. metadata access\n\n\n\ndifference in exception in lcp compression instead, compresso allows some number of such inflated cachelines to be stored uncompressed in the inflation room at the end of an mpa page, provided that there is space in that page (fig. 5a). this is similar to the exception region in lcp, but is used for an entirely different reason—to reduce compression-related data movement, rather than to support a specific packing scheme.\n\n\n\nwe present the first main-memory compression architecture that is designed to run an unmodified operating system.\n\n----------------------------------------\n\n# 2. translation-optimized memory compression for capacity\n\nprior workscompress and pack/migrate data at a small - memory block-level - granularity; this introduces an additional block-level translation after the page-level virtual address translation. in general, the smaller the granularity of address translation, the higher the translation overhead.\n\na promising solution is to only save memory from cold (i.e.,less recently accessed) pages without saving memory from hot (i.e., more recently accessed) pages (e.g., keep the hot pages uncompressed).\n\ntwo challenges:\n\n 1. after a compressed cold page becomes hot again, migrating the page to a full 4kb dram location still adds another level (albeit page-level, instead of block-level) of translation on top of existing virtual address translation. solution we propose compressing page table blocks in hardware to opportunistically embed compression translations into them in a software-transparent manner to effectively prefetch compression translations during a page walk, instead of serially fetching them after the walk.\n\nfirst, cte misses typically occur after pte misses in tlb because ctes, especially the page-level ctes under an os-inspired approach, have similar translation reach as ptes. second, we observe page table blocks (ptbs) are highly compressible because adjacent virtual pages often have identical status bits and the most significant bits in physical page numbers are unused. as such, to hide the latency of cte misses, tmcc transparently compresses each ptb in hardware to free up space in the ptb to embed the ctes of the 4kb pages (i.e., either data pages or page table pages) that the ptb points to; this enables each page walk to also prefetch the matching cte required for fetching from dram either the end data or the next ptb.\n\n 2. only compressing cold data require compressing them very aggressively to achieve high overall memory savings. solution we perform a large design space exploration across many hardware configurations and diverse workloads to derive and implement in hdl an asic deflate that is specialized for memory.\n\nprior new hardware managed translation entries as compression translation entries (ctes), as they are similar to os page table entries (ptes). prior works cache ctes in the memory controller via a dedicated cte cache, similar to the tlbs dedicated to caching ptes.\n\nlet hardware take on an os-inspired approach: only save memory from cold (i.e., less recently accessed) pages without saving memory from hot (i.e., recently accessed) pages (e.g., keep the hot pages uncompressed), like os memory compression. solves the problem of\n\n 1. translation overheads that large and/or irregular workloads suffer from high pte miss under hardware memory compression.\n 2. fine-grained address translation\n\naccesses to a compressed virtual page in ml2 incurs a page fault to wake up os to pop a free physical page from ml1’s free list and migrate the virtual page to the page.\n\nml2 also keeps many free lists, each tracking sub-physical pages of a different size, to store any compressed virtual page in a practically ideal matching sub-physical page.\n\nml2 gracefully grows and shrinks relative to ml1 with increasing and decreasing memory usage. when everything can fit in memory uncompressed, ml2 shrinks to zero bytes in physical size so ml1 can have every physical page. specifically, when ml2’s free list(s) get large (e.g., due to reducing memory usage), ml2 donates free physical pages from its free list(s) to ml1. os also grows ml1 free list, when it gets small, by migrating cold virtual pages to ml2. migrating a virtual page to ml2 shrinks one of ml2’s free lists. if a ml2 free list gets empty, ml1 gives cold victim physical pages to ml2 (i.e., track them in ml2 instead of ml1), so that ml2 can compress the virtual pages currently in the victim pages to free space in the victims to grow ml2’s free list(s).\n\n\n\nkey idea: based on our observations, we propose transparently compressing each ptb in hardware to free up space in the ptb to embed the ctes of the 4kb pages (i.e., either data pages or page table pages) that the ptb points to; this enables each page walk access to also prefetch the matching cte required either for the next page walk access (i.e., to the next ptb) or for the actual data (or instruction) access after the walk.\n\n\n\na practical challenge is that after migrating a page (e.g., from ml1 to ml2 after the page becomes cold), the corresponding cte embedded in the page’s ptb should be updated. however, hardware has no easy way to use the ppn of the migrating page to find/access the page’s ptb(s). tmcc addresses this challenge by lazily updating the cte in the ptb later around when the ptb is naturally accessed by the page walker, instead of updating it at the time of migrating the page. however, this means that for the first page walker access to the ptb after migrating one of the pages that the ptb points to, the corresponding cte is out-of-date. to ensure correctness, tmcc also accesses the correct cte in dram (or in cte cache) in parallel to verify the correctness of the dram access. figure 8 compares and contrasts how tmcc serves an llc miss that also misses in cte cache with the baseline approach. figure 9 provides n architectural overview of tmcc.\n\n\n\n\n\n\n\nmy comment when os access compressed page, that page is migrated from ml2 to ml1. hardware cannot update the ptb easily. thus it utilizes lazily update. during the page table walk, it will buffer the piggybacked cte into cte buffer. and when data miss req happens, l2 extracts the ppn from the received request to lookup the cte buffer to obtain the cte for mc to translate the ppn.\n\n----------------------------------------\n\n# 3. touche: towards ideal and efficient cache compression by mitigating tag area overheads\n\nthe first component, called the “signature” (sign) engine, creates shortened signatures from the tag addresses of compressed blocks. due to this, the sign engine can store multiple signatures in each tag entry. on a cache access, the physical cacheline is accessed only if there is a signature match (which has a negligible probability of false positive). the second component, called the “tag appended data” (tada) mechanism, stores the full tag addresses with data. tada enables touch´e to detect false positive signature matches by ensuring that the actual tag address is available for comparison. the third component, called the “superblock marker” (smark) mechanism, uses a unique marker in the tag entry to indicate the occurrence of compressed cache blocks from neighboring physical addresses in the same cacheline.\n\n\n\non average, 55% of the blocks can be compressed to less than 48 bytes in size. furthermore, 17% of the lines can be compressed to be less than 16 bytes in size. therefore, several workloads tend to have blocks with low entropy and can benefit from compression.\n\n\n\n\n\n\n\nfor instance, a cacheline cannot be marked both invalid and dirty at the same time. the tag manager uses this unused state to flag cachelines that contains compressed blocks. thereafter, for a cacheline that stores compressed blocks, the 1st and 2nd bits of the tag address encodes its valid bit and dirty bit.\n\n\n\n\n\nthe tag manager then retrieves the 16-bit marker from the smark mechanism. it then informs the sign engine to ignore the last 2-bits (corresponding to four neighboring addresses) of the full tag address to generate a unique 9-bit signature.\n\nthis smark generate a random 16-bit marker and concated with signature. since non-superblocks use 3 signature to identify blks, it should also use a tag to compare not just 0.\n\nif this paper doest not support superblock 4 compressed blks in a super block, it can only store 3* 16b compressed block or 48b + 64b block, due to extra real tag stored in data, 43bit for each data.",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"DRAM PCM NVM Cache",frontmatter:{title:"DRAM PCM NVM Cache",date:"2023-05-12T00:00:00.000Z",permalink:"/pages/24760e/"},regularPath:"/01.hbm/04.DRAM_PCM_NVM_Cache.html",relativePath:"01.hbm/04.DRAM_PCM_NVM_Cache.md",key:"v-9842a21a",path:"/pages/24760e/",headers:[{level:3,title:"Me",slug:"me",normalizedTitle:"me",charIndex:88}],headersStr:"Me",content:" 1. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures\n 2. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture\n 3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method\n 4. Page Placement in Hybrid Memory Systems\n\n----------------------------------------\n\n# 1. CLOCK-DWF: A Write-History-Aware Page Replacement Algorithm for Hybrid PCM and DRAM Memory Architectures\n\nLRU: Even though it requires only constant time and space overhead, LRU has a critical weakness in virtual memory environments. On every memory hit, LRU needs to move a page to the most recently used (MRU) position in the list. This involves list manipulations that cannot be handled by the paging unit hardware. CLOCK: Specifically, on a hit to a page, the paging unit hardware sets the reference bit of the page to 1 when a read or a write reference for that page occurs, and sets the dirty bit to 1 when a write reference occurs. Then, pages are maintained in a circular list. In the course of the scan, for every page with reference bit 1, CLOCK clears it to zero, without removing the page from the list.\n\nThe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. Even though CLOCK does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nLRU maintains the temporal locality. Frequency of Write Reference collect statistics of reference cnter.\n\nThe shape of the curves in these figures can be modeled as a monotonic decreasing function, implying that a more recently referenced page is more likely to be written in the near future.\n\nSpecifically, we can observe ranking inversion of temporal locality, i.e., a more recently used page shows a smaller fraction of writes for some ranking ranges.\n\nIn Fig. 3, x axis is the ranking by LRU. y axis represents the number of write references of the page ranking in x-axis.\n\nIn Fig. 4, the x-axis represents the ranking of pages based on their past write counts (black plot) and read/write counts (gray plot). The y-axis represents the number of writes occurring on that ranking.\n\nThe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. Even though CLOCK does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nThis indicates that frequency based estimations are more accurate compared to temporal locality based estimations for most cases. Specifically, frequency based stimations indicate that a wide range of top ranking pages, that is, pages that have been written to frequently in the past, are likely to be written to again in the future.\n\nIn summary, write frequency is generally a better estimator than temporal locality in predicting the re-reference likelihood of write references, but the very recent past write history is also a strong indicator of future writes.\n\nMe In pic4, the axis x is also ranked by number of reference. That's why its x axis can correlates with y axis. Maybe 80% rule can also explain this.\n\n----------------------------------------\n\n# 2. An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture\n\nContrary to CLOCK-DWF that places page faults issued by read requests on NVM, the proposed scheme moves all pages from disk to DRAM area. This is motivated by the fact that moving to either NVM or DRAM will result in a page write in NVM since the DRAM is always full and moving a data page to DRAM will issue an eviction to NVM. Therefore, the cost of moving to NVM or DRAM is the same in terms of writes in NVM. The newly accessed data pages have higher probability of access compared to the older data pages and moving this new page to DRAM will result in increase in DRAM hit ratio instead of NVM hit ratio.\n\nFirst,it requires an ordering scheme in order to identify data pages that are cold but will be accessed once in a long time. These data pages will reside long enough in NVM to have a high counter values and therefore will be moved to DRAM where they cannot compete with hot data pages and will return to NVM which makes their migration to DRAM without any benefits. Second, there is no difference between pages that are frequently accessed and typically reside near the head of the NVM LRU queue for the entire time and data pages which go back and forth in the queue.\n\nThe housekeeping information will be only stored for a few percentage of top positions in the NVM LRU queue. Once a data page moves to the end of this selected percentage of LRU, the corresponding counter will be reset to zero. This will handle both ordering scheme and identifying burst data accesses.\n\nFinding the data page in DRAM will result in a normal LRU housekeeping. Otherwise, the extra housekeeping information in NVM will be updated based on the request type. The read and write counters will be stored for readperc and writeperc top data pages in the NVM, respectively. [Still confused why they have readperc and writeperc]\n\nThe values of read threshold and write threshold determine how aggressive we plan to prevent the migrations with low probability of being useful.\n\nTo this end, we use two Least Recently Used (LRU) queues (one for DRAM and one for NVM) and optimize the LRU queue for NVM to prevent nonbeneficial migrations to DRAM.\n\n# 3. APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method\n\nComments on the previous paper: CLOCK-DWF [19]. CLOCK-DWF first proposes to load the write-request pages into DRAM. For new pages, if it is for a write request, it will be swapped into DRAM. Otherwise, it will be placed in NVRAM. For pages stored in NVRAM, if one is hit by a write request, it will be migrated from NVRAM to DRAM. At this time when DRAM is full, CLOCK-DWF will select a victim page that has the lowest number of writes or that has not been accessed for the longest period of time in DRAM to be evicted. Double LRU [20]. Double LRU recognizes the high migration cost between NVRAM and DRAM, and tries to restrict the number of page migrations by setting some threshold. It uses two separate LRU linked lists to manage pages in DRAM and NVRAM. For each page in NVRAM, it maintains a read/write request count. When a page is accessed, Double LRU checks its read/write request count, and if the count reaches a certain threshold, it will be migrated from NVRAM to DRAM; otherwise, it will remain in NVRAM. In DRAM, the page at the end of the LRU list is always selected as the victim. For new pages, Double LRU stores them directly in DRAM, assuming new pages will be accessed frequently in the near future. regardless of the read or write requests.\n\nUIMigrate consists of three parts: unified hot page identification, page migration, and self-adaptive adjustment.\n\nTo consider both the number of accesses and access time,we add the attenuation factor to quantify the hotness of each page, hoping to quickly reduce the access counts for pages that are accessed a long time ago. Thus, while updating page hotness upon each access, UIMigrate also uses an attenuation coefficient to lower the page popularity of old accesses.\n\nIf all DRAM pages should be accessed in one cycle, (acc_count.global-acc.countpage)/DRAMsize denotes the number of cycles the page that has not been accessed.\n\nthe attenuation coefficient d is closely related to the value of hotold and the number of cycles the page that has not been accessed since last time.\n\nUIMigrate sets a threshold, called new page threshold, to measure the hotness of each victim page. When the quantified hotness of a selected victim is larger than the preset threshold, it means that this victim page is too hot to be evicted, and so UIMigrate will store the new page in NVRAM. Otherwise, it will be migrated to DRAM.\n\nIn order to effectively adapt to the change of access patterns, UIMigrate adjusts migration thresholds (new page threshold, hot page threshold and cold page threshold) automatically to promote or suppress the page migrations, according to real-time migration revenue.\n\nWhen they are evicted from DRAM, UIMigrate calculates the migration revenue based on Equations (3) and (4). If the migration revenue is below zero, it means that the migration cost is greater than the benefit. In this case, UIMigrate will increase hot page threshold to prevent certain pages from getting hot in NVRAM and decrease cold page threshold to prevent some pages from becoming cold in DRAM, thus retaining more pages in NVRAM. For new pages, UIMigrate will also reduce new page threshold, so that more new pages will go to NVRAM instead of DRAM. When the calculated migration benefit is larger than the migration cost, UIMigare will reduce hot page threshold and increase cold page threshold to make migrate more pages to DRAM, and increase new page threshold to keep more new pages in DRAM.\n\n# 4. Page Placement in Hybrid Memory Systems\n\nGiven the characteristics of DRAM and PCM, RaPP seeks to (1) place performance-critical pages and frequently written pages in DRAM (2) place non-critical pages and rarely written pages in PCM (3) spread writes to PCM across many physical frames.\n\nUsing this information, RaPP dynamically ranks frames based on frequency and recency of accesses, as detailed below. Frames that rank high are called “popular”, and frames that rank low are called “unpopular”.\n\n# Algorithm\n\n 1. The descriptors in queue M − 1 represent the blocks that are most frequently used. On the first access to a block, its descriptor is placed in the tail of queue 0.\n 2. In addition, the block’s expiration time ExpirationTime is set to CurrentTime + LifeTime, where both times are measured in number of accesses and LifeT ime specifies the number of consecutive accesses that n must directed to other blocks before we expire the block.\n 3. Every time the block is accessed, its reference counter is incremented, its expiration time is reset to CurrentT ime + LifeT ime, and its descriptor is moved to the tail of its current queue.\n 4. The descriptor of a frequently used block is promoted to a higher queue (saturating at queue M − 1, of course) after a certain number of accesses to the block.\n 5. Specifically, if the descriptor is currently in queue i, it will be upgraded to queue i + 1 when its reference counter reaches 2i+1.\n 6. Conversely, MQ demotes blocks that have not been accessed recently. On each access, the descriptors at the heads of all M queues (representing the LRU block of each queue) are checked for expiration (CurrentT ime > ExpirationTime).\n\nIf a block descriptor expires, it is placed at the tail of the immediately inferior queue, and has its expiration time again set to CurrentTime + LifeTime.\n\nFirst,instead of counting all accesses, we only count an access if it occurs more than a threshold time (measured in memory cycles) after the last access to the same frame. This latter threshold is called the “filter threshold”. The MC stores the time of the last access in the descriptor for the frame. Using a 2-competitive approach, we set the filter threshold to be MigrationCost/MigrationThreshold, where MigrationCost is the uncontended number of memory cycles needed to migrate a page. (MigrationCost is roughly 1.6µs in our experiments.)\n\nSecond, we modified the demotion policy in the following ways: (a) we use time, not number of accesses, as the metric for demotion to reduce space requirements (in our experiments, we set LifeT ime to 100µs, which works well for our workloads); (b) we only demote from one queue at a time (in round-robin fashion) to reduce runtime overhead; (c) a DRAM frame that is demoted twice without any intervening accesses leaves the MQ queues and becomes a candidate to receive a popular PCM page.\n\nTo select a destination DRAM frame for a page, the MC maintains an LRU list of victim DRAM frames. The victim frames are not in any of the LRU queues (the list is initialized with all DRAM frames).\n\nTo effect a page migration to DRAM, the MC (1) migrates the page stored in the selected DRAM frame to one of the unranked PCM frames, (2) migrates the content of this latter frame to the most popular PCM frame, and finally (3) migrates the content of the most popular PCM frame to the selected DRAM frame.\n\n\n# Me\n\nWhy swap 3 time? not 2?",normalizedContent:" 1. clock-dwf: a write-history-aware page replacement algorithm for hybrid pcm and dram memory architectures\n 2. an operating system level data migration scheme in hybrid dram-nvm memory architecture\n 3. apmigration: improving performance of hybrid memory performance via an adaptive page migration method\n 4. page placement in hybrid memory systems\n\n----------------------------------------\n\n# 1. clock-dwf: a write-history-aware page replacement algorithm for hybrid pcm and dram memory architectures\n\nlru: even though it requires only constant time and space overhead, lru has a critical weakness in virtual memory environments. on every memory hit, lru needs to move a page to the most recently used (mru) position in the list. this involves list manipulations that cannot be handled by the paging unit hardware. clock: specifically, on a hit to a page, the paging unit hardware sets the reference bit of the page to 1 when a read or a write reference for that page occurs, and sets the dirty bit to 1 when a write reference occurs. then, pages are maintained in a circular list. in the course of the scan, for every page with reference bit 1, clock clears it to zero, without removing the page from the list.\n\nthe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. even though clock does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nlru maintains the temporal locality. frequency of write reference collect statistics of reference cnter.\n\nthe shape of the curves in these figures can be modeled as a monotonic decreasing function, implying that a more recently referenced page is more likely to be written in the near future.\n\nspecifically, we can observe ranking inversion of temporal locality, i.e., a more recently used page shows a smaller fraction of writes for some ranking ranges.\n\nin fig. 3, x axis is the ranking by lru. y axis represents the number of write references of the page ranking in x-axis.\n\nin fig. 4, the x-axis represents the ranking of pages based on their past write counts (black plot) and read/write counts (gray plot). the y-axis represents the number of writes occurring on that ranking.\n\nthe reference bit of each page is an indication of whether that page has recently been accessed or not; and pages not referenced upon the return of the clock-hand to that page will be replaced. even though clock does not replace the least recently used page, it replaces a page that has not been referenced recently, that is, through the cycle of the circular list, so that temporal locality is exploited to some extent.\n\nthis indicates that frequency based estimations are more accurate compared to temporal locality based estimations for most cases. specifically, frequency based stimations indicate that a wide range of top ranking pages, that is, pages that have been written to frequently in the past, are likely to be written to again in the future.\n\nin summary, write frequency is generally a better estimator than temporal locality in predicting the re-reference likelihood of write references, but the very recent past write history is also a strong indicator of future writes.\n\nme in pic4, the axis x is also ranked by number of reference. that's why its x axis can correlates with y axis. maybe 80% rule can also explain this.\n\n----------------------------------------\n\n# 2. an operating system level data migration scheme in hybrid dram-nvm memory architecture\n\ncontrary to clock-dwf that places page faults issued by read requests on nvm, the proposed scheme moves all pages from disk to dram area. this is motivated by the fact that moving to either nvm or dram will result in a page write in nvm since the dram is always full and moving a data page to dram will issue an eviction to nvm. therefore, the cost of moving to nvm or dram is the same in terms of writes in nvm. the newly accessed data pages have higher probability of access compared to the older data pages and moving this new page to dram will result in increase in dram hit ratio instead of nvm hit ratio.\n\nfirst,it requires an ordering scheme in order to identify data pages that are cold but will be accessed once in a long time. these data pages will reside long enough in nvm to have a high counter values and therefore will be moved to dram where they cannot compete with hot data pages and will return to nvm which makes their migration to dram without any benefits. second, there is no difference between pages that are frequently accessed and typically reside near the head of the nvm lru queue for the entire time and data pages which go back and forth in the queue.\n\nthe housekeeping information will be only stored for a few percentage of top positions in the nvm lru queue. once a data page moves to the end of this selected percentage of lru, the corresponding counter will be reset to zero. this will handle both ordering scheme and identifying burst data accesses.\n\nfinding the data page in dram will result in a normal lru housekeeping. otherwise, the extra housekeeping information in nvm will be updated based on the request type. the read and write counters will be stored for readperc and writeperc top data pages in the nvm, respectively. [still confused why they have readperc and writeperc]\n\nthe values of read threshold and write threshold determine how aggressive we plan to prevent the migrations with low probability of being useful.\n\nto this end, we use two least recently used (lru) queues (one for dram and one for nvm) and optimize the lru queue for nvm to prevent nonbeneficial migrations to dram.\n\n# 3. apmigration: improving performance of hybrid memory performance via an adaptive page migration method\n\ncomments on the previous paper: clock-dwf [19]. clock-dwf first proposes to load the write-request pages into dram. for new pages, if it is for a write request, it will be swapped into dram. otherwise, it will be placed in nvram. for pages stored in nvram, if one is hit by a write request, it will be migrated from nvram to dram. at this time when dram is full, clock-dwf will select a victim page that has the lowest number of writes or that has not been accessed for the longest period of time in dram to be evicted. double lru [20]. double lru recognizes the high migration cost between nvram and dram, and tries to restrict the number of page migrations by setting some threshold. it uses two separate lru linked lists to manage pages in dram and nvram. for each page in nvram, it maintains a read/write request count. when a page is accessed, double lru checks its read/write request count, and if the count reaches a certain threshold, it will be migrated from nvram to dram; otherwise, it will remain in nvram. in dram, the page at the end of the lru list is always selected as the victim. for new pages, double lru stores them directly in dram, assuming new pages will be accessed frequently in the near future. regardless of the read or write requests.\n\nuimigrate consists of three parts: unified hot page identification, page migration, and self-adaptive adjustment.\n\nto consider both the number of accesses and access time,we add the attenuation factor to quantify the hotness of each page, hoping to quickly reduce the access counts for pages that are accessed a long time ago. thus, while updating page hotness upon each access, uimigrate also uses an attenuation coefficient to lower the page popularity of old accesses.\n\nif all dram pages should be accessed in one cycle, (acc_count.global-acc.countpage)/dramsize denotes the number of cycles the page that has not been accessed.\n\nthe attenuation coefficient d is closely related to the value of hotold and the number of cycles the page that has not been accessed since last time.\n\nuimigrate sets a threshold, called new page threshold, to measure the hotness of each victim page. when the quantified hotness of a selected victim is larger than the preset threshold, it means that this victim page is too hot to be evicted, and so uimigrate will store the new page in nvram. otherwise, it will be migrated to dram.\n\nin order to effectively adapt to the change of access patterns, uimigrate adjusts migration thresholds (new page threshold, hot page threshold and cold page threshold) automatically to promote or suppress the page migrations, according to real-time migration revenue.\n\nwhen they are evicted from dram, uimigrate calculates the migration revenue based on equations (3) and (4). if the migration revenue is below zero, it means that the migration cost is greater than the benefit. in this case, uimigrate will increase hot page threshold to prevent certain pages from getting hot in nvram and decrease cold page threshold to prevent some pages from becoming cold in dram, thus retaining more pages in nvram. for new pages, uimigrate will also reduce new page threshold, so that more new pages will go to nvram instead of dram. when the calculated migration benefit is larger than the migration cost, uimigare will reduce hot page threshold and increase cold page threshold to make migrate more pages to dram, and increase new page threshold to keep more new pages in dram.\n\n# 4. page placement in hybrid memory systems\n\ngiven the characteristics of dram and pcm, rapp seeks to (1) place performance-critical pages and frequently written pages in dram (2) place non-critical pages and rarely written pages in pcm (3) spread writes to pcm across many physical frames.\n\nusing this information, rapp dynamically ranks frames based on frequency and recency of accesses, as detailed below. frames that rank high are called “popular”, and frames that rank low are called “unpopular”.\n\n# algorithm\n\n 1. the descriptors in queue m − 1 represent the blocks that are most frequently used. on the first access to a block, its descriptor is placed in the tail of queue 0.\n 2. in addition, the block’s expiration time expirationtime is set to currenttime + lifetime, where both times are measured in number of accesses and lifet ime specifies the number of consecutive accesses that n must directed to other blocks before we expire the block.\n 3. every time the block is accessed, its reference counter is incremented, its expiration time is reset to currentt ime + lifet ime, and its descriptor is moved to the tail of its current queue.\n 4. the descriptor of a frequently used block is promoted to a higher queue (saturating at queue m − 1, of course) after a certain number of accesses to the block.\n 5. specifically, if the descriptor is currently in queue i, it will be upgraded to queue i + 1 when its reference counter reaches 2i+1.\n 6. conversely, mq demotes blocks that have not been accessed recently. on each access, the descriptors at the heads of all m queues (representing the lru block of each queue) are checked for expiration (currentt ime > expirationtime).\n\nif a block descriptor expires, it is placed at the tail of the immediately inferior queue, and has its expiration time again set to currenttime + lifetime.\n\nfirst,instead of counting all accesses, we only count an access if it occurs more than a threshold time (measured in memory cycles) after the last access to the same frame. this latter threshold is called the “filter threshold”. the mc stores the time of the last access in the descriptor for the frame. using a 2-competitive approach, we set the filter threshold to be migrationcost/migrationthreshold, where migrationcost is the uncontended number of memory cycles needed to migrate a page. (migrationcost is roughly 1.6µs in our experiments.)\n\nsecond, we modified the demotion policy in the following ways: (a) we use time, not number of accesses, as the metric for demotion to reduce space requirements (in our experiments, we set lifet ime to 100µs, which works well for our workloads); (b) we only demote from one queue at a time (in round-robin fashion) to reduce runtime overhead; (c) a dram frame that is demoted twice without any intervening accesses leaves the mq queues and becomes a candidate to receive a popular pcm page.\n\nto select a destination dram frame for a page, the mc maintains an lru list of victim dram frames. the victim frames are not in any of the lru queues (the list is initialized with all dram frames).\n\nto effect a page migration to dram, the mc (1) migrates the page stored in the selected dram frame to one of the unranked pcm frames, (2) migrates the content of this latter frame to the most popular pcm frame, and finally (3) migrates the content of the most popular pcm frame to the selected dram frame.\n\n\n# me\n\nwhy swap 3 time? not 2?",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"memory-ecc",frontmatter:{title:"memory-ecc",date:"2023-05-31T17:24:24.000Z",permalink:"/pages/f07695/"},regularPath:"/01.hbm/06.memory%20ecc.html",relativePath:"01.hbm/06.memory ecc.md",key:"v-171f66aa",path:"/pages/f07695/",headersStr:null,content:"Ecc length for different length of data.\n\nhttps://perswww.kuleuven.be/~u0068190/Onderwijs/Extra_info/Hamming%20ecc.pdf\n\n\n\nhttp://www.sxlist.com/techref/method/error/hamming.htm\n\n 1. 4-bit data path requires 3 bits for ECC (8 entry table) (75% increase in size)\n 2. 8-bit data path requires 5 bits for ECC or 1 bit for parity.\n 3. 11-bit data path requires 4 bits for ECC (16 entry table)\n 4. 16-bit data path requires 6 bit for ECC or 2 bits for parity\n 5. 32-bit data path requires 7 bits for ECC or 4 bits for parity (21.8% increase in size)\n 6. 64-bit (8 byte) data path requires 8 bits for ECC and parity (12.5% increase in size)\n 7. 128-bit (16 bytes) data path requires 9 bits for ECC or 16 bits for parity (7% increase in size\n\nUse ECC bit for compression CRAM: Efficient Hardware-Based Memory Compression for Bandwidth Enhancement\n\nEnabling Technologies for Memory Compression: Metadata, Mapping, and Prediction\n\nUse ECC bit for DRAM Cache To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and Misses in Hybrid Memory Systems",normalizedContent:"ecc length for different length of data.\n\nhttps://perswww.kuleuven.be/~u0068190/onderwijs/extra_info/hamming%20ecc.pdf\n\n\n\nhttp://www.sxlist.com/techref/method/error/hamming.htm\n\n 1. 4-bit data path requires 3 bits for ecc (8 entry table) (75% increase in size)\n 2. 8-bit data path requires 5 bits for ecc or 1 bit for parity.\n 3. 11-bit data path requires 4 bits for ecc (16 entry table)\n 4. 16-bit data path requires 6 bit for ecc or 2 bits for parity\n 5. 32-bit data path requires 7 bits for ecc or 4 bits for parity (21.8% increase in size)\n 6. 64-bit (8 byte) data path requires 8 bits for ecc and parity (12.5% increase in size)\n 7. 128-bit (16 bytes) data path requires 9 bits for ecc or 16 bits for parity (7% increase in size\n\nuse ecc bit for compression cram: efficient hardware-based memory compression for bandwidth enhancement\n\nenabling technologies for memory compression: metadata, mapping, and prediction\n\nuse ecc bit for dram cache to update or not to update?: bandwidth-efficient intelligent replacement policies for dram caches tictoc: enabling bandwidth-efficient dram caching for both hits and misses in hybrid memory systems",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"hbm-latency",frontmatter:{title:"hbm-latency",date:"2023-05-31T17:24:24.000Z",permalink:"/pages/f07696/"},regularPath:"/01.hbm/07.hbm-latency.html",relativePath:"01.hbm/07.hbm-latency.md",key:"v-788071c5",path:"/pages/f07696/",headersStr:null,content:" 1. HBM latency\n\nIn flat mode on Knight’s Landing, MCDRAM latency is around 176 ns, while a DDR4 access has a latency of 147 ns.\n\nCited from blog: Knight’s Landing: Atom with AVX-512.\n\nhttps://chipsandcheese.com/2022/12/08/knights-landing-atom-with-avx-512/\n\nWe report 154.0 ns latency for HBM and 130.4 ns for DRAM.\n\nPaper: Exploring the Performance Benefit of Hybrid Memory System on HPC Environments.\n\n 2. The latency and bandwidth comparison of HBM and DRAM Paper: [UPC Phd Thesis] Memory Bandwidth and Latency in HPC: System Requirements and Performance Impact.",normalizedContent:" 1. hbm latency\n\nin flat mode on knight’s landing, mcdram latency is around 176 ns, while a ddr4 access has a latency of 147 ns.\n\ncited from blog: knight’s landing: atom with avx-512.\n\nhttps://chipsandcheese.com/2022/12/08/knights-landing-atom-with-avx-512/\n\nwe report 154.0 ns latency for hbm and 130.4 ns for dram.\n\npaper: exploring the performance benefit of hybrid memory system on hpc environments.\n\n 2. the latency and bandwidth comparison of hbm and dram paper: [upc phd thesis] memory bandwidth and latency in hpc: system requirements and performance impact.",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"compressibility prediction",frontmatter:{title:"compressibility prediction",date:"2024-04-04T11:42:24.000Z",permalink:"/pages/f07699/"},regularPath:"/01.hbm/09.compressibility_prediction.html",relativePath:"01.hbm/09.compressibility_prediction.md",key:"v-662ff4fd",path:"/pages/f07699/",headers:[{level:3,title:"1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO]",slug:"_1-attach-e-towards-ideal-memory-compression-by-mitigating-metadata-bandwidth-overheads-micro",normalizedTitle:"1. attach´e: towards ideal memory compression by mitigating metadata bandwidth overheads [micro]",charIndex:347},{level:3,title:"2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA]",slug:"_2-cram-enabling-transparent-memory-compression-for-commodity-memory-systems-hpca",normalizedTitle:"2. cram enabling transparent memory-compression for commodity memory systems [hpca]",charIndex:1311},{level:3,title:"3. MBZip: Multiblock Data Compression [TACO]",slug:"_3-mbzip-multiblock-data-compression-taco",normalizedTitle:"3. mbzip: multiblock data compression [taco]",charIndex:2596},{level:3,title:"4. Compresso: Pragmatic Main Memory Compression",slug:"_4-compresso-pragmatic-main-memory-compression",normalizedTitle:"4. compresso: pragmatic main memory compression",charIndex:245},{level:3,title:"5. Enabling Technologies for Memory Compression:Metadata, Mapping, and Prediction",slug:"_5-enabling-technologies-for-memory-compression-metadata-mapping-and-prediction",normalizedTitle:"5. enabling technologies for memory compression:metadata, mapping, and prediction",charIndex:4436}],headersStr:"1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO] 2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA] 3. MBZip: Multiblock Data Compression [TACO] 4. Compresso: Pragmatic Main Memory Compression 5. Enabling Technologies for Memory Compression:Metadata, Mapping, and Prediction",content:" 1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO 2018]\n 2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA 2019]\n 3. MBZip: Multiblock Data Compression [TACO 2017]\n 4. Compresso: Pragmatic Main Memory Compression [MICRO]\n\n----------------------------------------\n\n\n# 1. Attach´e: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads [MICRO]\n\nYear: 2018\n\nAttach´e does not use the free space made available by compression.\n\nCompression Predictor (COPR), predicts if the memory block is compressed.\n\nGlobal Indication(GI): GI is composed of eight two-bit saturating counters, each of which keeps track of the compressibility of 18th the memory space. GI can be used as an accurate indicator for predicting the compressibility within a memory space if there is abundant similarity in compressibility.\n\nPage-Level Predictor (PaPR): By exploiting the similarity in the compressibility of cachelines within an OS page [12], [18], [37], PaPR provides compression predictions at the page granularity.\n\nLine-Level Predictor (LiPR): LiPR is a set-associative cache structure indexed by the page number. LiPR uses the two-bit values of PaPR to determine if the neighboring cachelines have the same compressibility.\n\n\n# 2. CRAM Enabling Transparent Memory-Compression for Commodity Memory Systems [HPCA]\n\nYear: 2019\n\nTransparent Memory-Compression (TMC) can provide bandwidth benefits of memory compression in an OS-transparent manner by trying to exploit only the increased bandwidth and not the extra capacity.\n\nLine Location Predictor (LLP) that can determine the location of the line with 98% accuracy and a dynamic solution that disables compression if the benefits of compression are smaller than the overheads.\n\nIf we use HBM, we dont need to care too much about the bandwidth and metadata.\n\nWe propose a history-based Line Location Predictor (LLP), that can identify the correct location of the line with a high accuracy (98%). The LLP is based on the observation that lines within a page tend to have similar compressibility.\n\nLLP contains the Last Compressibility Table (LCT), that tracks the last compression status seen for a given index. The LCT is indexed with the hash of the page address. So, for a given access, the index corresponding to the page address is used to predict the compressibility, then line location.\n\n\n\nEven though the LLP is quite small, it provides an accuracy of 98%, much higher than the hit-rate of the metadata cache.\n\nLCP stores metadata inline with block.\n\n\n\n\n\n\n# 3. MBZip: Multiblock Data Compression [TACO]\n\nA write to a location in memory may not change the existing data in that location and is thus a redundant write. Such write requests to cache have been termed as silent stores [25]/writes [21].\n\nWe find that, on average, across 21 benchmarks, 9.6% of the writes are silent. More than 15% of the writes are silent in benchmarks such as bwaves, GemsFDTD, lbm, leslie3d, mcf, mesa, sjeng, soplex, vortex2, and zeusmp.\n\nIn such a scenario, we essentially issue one read request (to read the existing data) and no write request. However, if the write request is not silent, we add the overhead of a read request to the existing write request.\n\nWe observe that there is a strong correlation to a write being silent or nonsilent both across writes made to the same address during the course of the program execution and across writes to consecutive addresses. To exploit this correlation, we propose using a 2b bimodal predictor (indexed using the page addresses) to predict whether a write request is silent. The accuracy of our predictor (4kB structure) is around 94.4%, on average.\n\n\n# 4. Compresso: Pragmatic Main Memory Compression\n\n\n\nWe associate a 2-bit saturating counter with each entry in the metadata cache (Fig. 5b). The counter is incremented when any writeback to the associated page results in a cache line overflow and is decremented upon cache line underflows (i.e., new data being more compressible).\n\nAnother 3-bit global predictor changes state based on page overflows in the system. We speculatively increase a page’s size to the maximum (4KB) when the local as well as global predictors have the higher bit set.\n\nHence, a page is stored uncompressed if it receives multiple streaming cache line overflows during a phase when the overall system is experiencing page overflows\n\n\n# 5. Enabling Technologies for Memory Compression:Metadata, Mapping, and Prediction\n\n\n\n\n\nMetadata is coexist with data. Thus they have to predict, even read.\n\nReads are problematic because the size of the block is encoded in the block itself. Therefore, the read has to either be performed in two phases (read the metadata from the 0th chip, then read data from the appropriate subset of chips) or the read has to conservatively read data from all 9 chips in parallel.\n\nThe first is PCbased, where the PC of the load instruction serves as the index into a predictor table. This assumes that a load tends to access the same type of data record, with relatively uniform compressibility.\n\nThe second is page-based, where the physical page number serves as the index into a predictor table. This assumes that the data records in a single page are of a similar type and have uniform compressibility.\n\nOn a look-up, the highest-valued saturating counter indicates the predicted size of the block. In case of a tie, we conservatively predict the larger block size.\n\nThey keep a counter for each block compression length, and try to keep counter for each block length. Then predict by voting, following majority wins low.",normalizedContent:" 1. attach´e: towards ideal memory compression by mitigating metadata bandwidth overheads [micro 2018]\n 2. cram enabling transparent memory-compression for commodity memory systems [hpca 2019]\n 3. mbzip: multiblock data compression [taco 2017]\n 4. compresso: pragmatic main memory compression [micro]\n\n----------------------------------------\n\n\n# 1. attach´e: towards ideal memory compression by mitigating metadata bandwidth overheads [micro]\n\nyear: 2018\n\nattach´e does not use the free space made available by compression.\n\ncompression predictor (copr), predicts if the memory block is compressed.\n\nglobal indication(gi): gi is composed of eight two-bit saturating counters, each of which keeps track of the compressibility of 18th the memory space. gi can be used as an accurate indicator for predicting the compressibility within a memory space if there is abundant similarity in compressibility.\n\npage-level predictor (papr): by exploiting the similarity in the compressibility of cachelines within an os page [12], [18], [37], papr provides compression predictions at the page granularity.\n\nline-level predictor (lipr): lipr is a set-associative cache structure indexed by the page number. lipr uses the two-bit values of papr to determine if the neighboring cachelines have the same compressibility.\n\n\n# 2. cram enabling transparent memory-compression for commodity memory systems [hpca]\n\nyear: 2019\n\ntransparent memory-compression (tmc) can provide bandwidth benefits of memory compression in an os-transparent manner by trying to exploit only the increased bandwidth and not the extra capacity.\n\nline location predictor (llp) that can determine the location of the line with 98% accuracy and a dynamic solution that disables compression if the benefits of compression are smaller than the overheads.\n\nif we use hbm, we dont need to care too much about the bandwidth and metadata.\n\nwe propose a history-based line location predictor (llp), that can identify the correct location of the line with a high accuracy (98%). the llp is based on the observation that lines within a page tend to have similar compressibility.\n\nllp contains the last compressibility table (lct), that tracks the last compression status seen for a given index. the lct is indexed with the hash of the page address. so, for a given access, the index corresponding to the page address is used to predict the compressibility, then line location.\n\n\n\neven though the llp is quite small, it provides an accuracy of 98%, much higher than the hit-rate of the metadata cache.\n\nlcp stores metadata inline with block.\n\n\n\n\n\n\n# 3. mbzip: multiblock data compression [taco]\n\na write to a location in memory may not change the existing data in that location and is thus a redundant write. such write requests to cache have been termed as silent stores [25]/writes [21].\n\nwe find that, on average, across 21 benchmarks, 9.6% of the writes are silent. more than 15% of the writes are silent in benchmarks such as bwaves, gemsfdtd, lbm, leslie3d, mcf, mesa, sjeng, soplex, vortex2, and zeusmp.\n\nin such a scenario, we essentially issue one read request (to read the existing data) and no write request. however, if the write request is not silent, we add the overhead of a read request to the existing write request.\n\nwe observe that there is a strong correlation to a write being silent or nonsilent both across writes made to the same address during the course of the program execution and across writes to consecutive addresses. to exploit this correlation, we propose using a 2b bimodal predictor (indexed using the page addresses) to predict whether a write request is silent. the accuracy of our predictor (4kb structure) is around 94.4%, on average.\n\n\n# 4. compresso: pragmatic main memory compression\n\n\n\nwe associate a 2-bit saturating counter with each entry in the metadata cache (fig. 5b). the counter is incremented when any writeback to the associated page results in a cache line overflow and is decremented upon cache line underflows (i.e., new data being more compressible).\n\nanother 3-bit global predictor changes state based on page overflows in the system. we speculatively increase a page’s size to the maximum (4kb) when the local as well as global predictors have the higher bit set.\n\nhence, a page is stored uncompressed if it receives multiple streaming cache line overflows during a phase when the overall system is experiencing page overflows\n\n\n# 5. enabling technologies for memory compression:metadata, mapping, and prediction\n\n\n\n\n\nmetadata is coexist with data. thus they have to predict, even read.\n\nreads are problematic because the size of the block is encoded in the block itself. therefore, the read has to either be performed in two phases (read the metadata from the 0th chip, then read data from the appropriate subset of chips) or the read has to conservatively read data from all 9 chips in parallel.\n\nthe first is pcbased, where the pc of the load instruction serves as the index into a predictor table. this assumes that a load tends to access the same type of data record, with relatively uniform compressibility.\n\nthe second is page-based, where the physical page number serves as the index into a predictor table. this assumes that the data records in a single page are of a similar type and have uniform compressibility.\n\non a look-up, the highest-valued saturating counter indicates the predicted size of the block. in case of a tie, we conservatively predict the larger block size.\n\nthey keep a counter for each block compression length, and try to keep counter for each block length. then predict by voting, following majority wins low.",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"compression",frontmatter:{title:"compression",date:"2023-12-20T17:24:24.000Z",permalink:"/pages/f07698/"},regularPath:"/01.hbm/08.compression.html",relativePath:"01.hbm/08.compression.md",key:"v-25995465",path:"/pages/f07698/",headersStr:null,content:" 1. Unified Compilation for Lossless Compression and Sparse Computing\n\ncompute in compressed data && compression on sparse matrix\n\n 2.  Memory Access Granularity Aware Lossless Compression for GPUs\n\n 3.  Linearly Compressed Pages: A Low-Complexity, Low-Latency Main Memory Compression Framework\n\n 4.  FlatPack: Flexible Compaction of Compressed Memory\n\n 5.  Compresso: Pragmatic Main Memory Compression\n\n 6.  Compacted CPU/GPU Data Compression via Modified Virtual Address Translation\n\n 7.  CMH: Compression Management for Improving Capacity in the Hybrid Memory Cube\n\n 8.  Buri: Scaling Big-Memory Computing with Hardware-Based Memory Expansion\n\n 9.  Compress Objects, Not Cache Lines: An Object-Based Compressed Memory Hierarchy\n\n 10. Base-Delta-Immediate Compression:Practical Data Compression for On-Chip Caches\n\n 11. Frequent Pattern Compression: A Significance-Based Compression Scheme for L2 Caches\n\n 12. \n\n 13. BCD Deduplication: Effective Memory Compression using Partial Cache-Line Deduplication\n\n 14. Could Compression Be of General Use? Evaluating Memory Compression across Domains\n\n 15. Linearly Compressed Pages: A Low-Complexity, Low-Latency Main Memory Compression Framework\n\n 16. Compresso: Pragmatic Main Memory Compression\n\n 17. FlatPack: Flexible Compaction of Compressed Memory",normalizedContent:" 1. unified compilation for lossless compression and sparse computing\n\ncompute in compressed data && compression on sparse matrix\n\n 2.  memory access granularity aware lossless compression for gpus\n\n 3.  linearly compressed pages: a low-complexity, low-latency main memory compression framework\n\n 4.  flatpack: flexible compaction of compressed memory\n\n 5.  compresso: pragmatic main memory compression\n\n 6.  compacted cpu/gpu data compression via modified virtual address translation\n\n 7.  cmh: compression management for improving capacity in the hybrid memory cube\n\n 8.  buri: scaling big-memory computing with hardware-based memory expansion\n\n 9.  compress objects, not cache lines: an object-based compressed memory hierarchy\n\n 10. base-delta-immediate compression:practical data compression for on-chip caches\n\n 11. frequent pattern compression: a significance-based compression scheme for l2 caches\n\n 12. \n\n 13. bcd deduplication: effective memory compression using partial cache-line deduplication\n\n 14. could compression be of general use? evaluating memory compression across domains\n\n 15. linearly compressed pages: a low-complexity, low-latency main memory compression framework\n\n 16. compresso: pragmatic main memory compression\n\n 17. flatpack: flexible compaction of compressed memory",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"memory management",frontmatter:{title:"memory management",date:"2024-05-06T11:42:24.000Z",permalink:"/pages/f07692/"},regularPath:"/01.hbm/10.software_memory_paper.html",relativePath:"01.hbm/10.software_memory_paper.md",key:"v-065eff05",path:"/pages/f07692/",headersStr:null,content:" 1. [11 ]Hardware Memory Management for Future Mobile Hybrid Memory Systems\n 2. [4 DAC] OpenMem: Hardware/Software Cooperative Management for Mobile Memory System\n 3. [TACO] FlexPointer: Fast Address Translation Based on Range TLB and Tagged Pointers\n 4. [26] Branch Prediction Is Not a Solved Problem: Measurements, Opportunities, and Future Directions\n 5. [12] When Storage Response Time Catches Up With Overall Context Switch Overhead, What Is Next?\n 6. [225] Dynamic tracking of page miss ratio curve for memory management\n 7. [299] LVI: Hijacking Transient Execution with Load Value Injection",normalizedContent:" 1. [11 ]hardware memory management for future mobile hybrid memory systems\n 2. [4 dac] openmem: hardware/software cooperative management for mobile memory system\n 3. [taco] flexpointer: fast address translation based on range tlb and tagged pointers\n 4. [26] branch prediction is not a solved problem: measurements, opportunities, and future directions\n 5. [12] when storage response time catches up with overall context switch overhead, what is next?\n 6. [225] dynamic tracking of page miss ratio curve for memory management\n 7. [299] lvi: hijacking transient execution with load value injection",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"llvm front end",frontmatter:{title:"llvm front end",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000001/"},regularPath:"/02.compiler/01.llvm_frontend.html",relativePath:"02.compiler/01.llvm_frontend.md",key:"v-f3b213e2",path:"/pages/000001/",headers:[{level:3,title:"1. Clang parser will build an Abstract Syntax Tree(AST) and then goes on to emit LLVM IR",slug:"_1-clang-parser-will-build-an-abstract-syntax-tree-ast-and-then-goes-on-to-emit-llvm-ir",normalizedTitle:"1. clang parser will build an abstract syntax tree(ast) and then goes on to emit llvm ir",charIndex:129},{level:3,title:"2. LLVM Target Independent",slug:"_2-llvm-target-independent",normalizedTitle:"2. llvm target independent",charIndex:222},{level:3,title:"3. SelectionDAG Node",slug:"_3-selectiondag-node",normalizedTitle:"3. selectiondag node",charIndex:253},{level:3,title:"4. To emit machine instructions, LLVM will legalize the operation",slug:"_4-to-emit-machine-instructions-llvm-will-legalize-the-operation",normalizedTitle:"4. to emit machine instructions, llvm will legalize the operation",charIndex:481},{level:3,title:"5. Instruction selection from SDNode to MachineSNDode",slug:"_5-instruction-selection-from-sdnode-to-machinesndode",normalizedTitle:"5. instruction selection from sdnode to machinesndode",charIndex:725},{level:3,title:"6. Scheduling and emitting a MachineInstr",slug:"_6-scheduling-and-emitting-a-machineinstr",normalizedTitle:"6. scheduling and emitting a machineinstr",charIndex:988},{level:3,title:"7. Register Allocation",slug:"_7-register-allocation",normalizedTitle:"7. register allocation",charIndex:1245},{level:3,title:"8. From MachineInstruction to MCinst",slug:"_8-from-machineinstruction-to-mcinst",normalizedTitle:"8. from machineinstruction to mcinst",charIndex:1565},{level:3,title:"9. Build LLVM",slug:"_9-build-llvm",normalizedTitle:"9. build llvm",charIndex:1678}],headersStr:"1. Clang parser will build an Abstract Syntax Tree(AST) and then goes on to emit LLVM IR 2. LLVM Target Independent 3. SelectionDAG Node 4. To emit machine instructions, LLVM will legalize the operation 5. Instruction selection from SDNode to MachineSNDode 6. Scheduling and emitting a MachineInstr 7. Register Allocation 8. From MachineInstruction to MCinst 9. Build LLVM",content:'llvm front end demo Notes from Life of an instruction in LLVM https://blog.llvm.org/2012/11/life-of-instruction-in-llvm.html\n\n\n# 1. Clang parser will build an Abstract Syntax Tree(AST) and then goes on to emit LLVM IR\n\n\n# 2. LLVM Target Independent\n\n\n# 3. SelectionDAG Node\n\n\nSelectionDAGBuild creates SDGNode\nSelectionDAGIsel goes over all IR instructions and calls SelectionDAGBuilder::visit to Dispatch them\nWe can use -debug or -view to get log or dump image of the graph\n\n\n\n# 4. To emit machine instructions, LLVM will legalize the operation\n\n\nUse target-specific hooks to convert all operations and types into ones that the target actually supports. This is done by TargetLowering.\nSelectionDAGLegalize::LegalizeOp\n\n\n# 5. Instruction selection from SDNode to MachineSNDode\n\n\nSelectionDAGISel::Select\nSelectCode\nThis step will create MachineSDNode, a subclass of SDNode which holds the information required to construct an actual machine instruction, but still in DAG node form.\n\n\n# 6. Scheduling and emitting a MachineInstr\n\n\nTranslate SDNode into Machine Instructions with InstrEmitter::EmitMachineNode, emmit into MachineBasicBlock. Here the instruction are in linear form (MI). No DAG any more.\n-print-machineinstrs\nStill SSA form.\n\n\n# 7. Register Allocation\n\n\nFor instructions that can only support fixed registers, it is already allocated. Here the virtual registers are allocated into physical registers. This assignment is done by X86DAGToDAGISel::Select.\nAfter this, another round of optimization is conducted, TargetPassConfig::addMachinePasses.\n\n\n# 8. From MachineInstruction to MCinst\n\nJIT: AsmPrinter::EmitInstruction\nObj: ObjectStreamer::EmitInstruction\n\n\n\n# 9. Build LLVM\n\ncreate build&cd build\ncmake -S llvm -B . -DCMAKE_BUILD_TYPE=Debug -DLLVM_TARGETS_TO_BUILD="MSP430;RISCV" ../llvm\nmake -j 8\n\n\n# How to build LC3\n\ncmake -S llvm -B . -DCMAKE_BUILD_TYPE=Debug -DLLVM_EXPERIMENTAL_TARGETS_TO_BUILD="LC3" ../llvm\n',normalizedContent:'llvm front end demo notes from life of an instruction in llvm https://blog.llvm.org/2012/11/life-of-instruction-in-llvm.html\n\n\n# 1. clang parser will build an abstract syntax tree(ast) and then goes on to emit llvm ir\n\n\n# 2. llvm target independent\n\n\n# 3. selectiondag node\n\n\nselectiondagbuild creates sdgnode\nselectiondagisel goes over all ir instructions and calls selectiondagbuilder::visit to dispatch them\nwe can use -debug or -view to get log or dump image of the graph\n\n\n\n# 4. to emit machine instructions, llvm will legalize the operation\n\n\nuse target-specific hooks to convert all operations and types into ones that the target actually supports. this is done by targetlowering.\nselectiondaglegalize::legalizeop\n\n\n# 5. instruction selection from sdnode to machinesndode\n\n\nselectiondagisel::select\nselectcode\nthis step will create machinesdnode, a subclass of sdnode which holds the information required to construct an actual machine instruction, but still in dag node form.\n\n\n# 6. scheduling and emitting a machineinstr\n\n\ntranslate sdnode into machine instructions with instremitter::emitmachinenode, emmit into machinebasicblock. here the instruction are in linear form (mi). no dag any more.\n-print-machineinstrs\nstill ssa form.\n\n\n# 7. register allocation\n\n\nfor instructions that can only support fixed registers, it is already allocated. here the virtual registers are allocated into physical registers. this assignment is done by x86dagtodagisel::select.\nafter this, another round of optimization is conducted, targetpassconfig::addmachinepasses.\n\n\n# 8. from machineinstruction to mcinst\n\njit: asmprinter::emitinstruction\nobj: objectstreamer::emitinstruction\n\n\n\n# 9. build llvm\n\ncreate build&cd build\ncmake -s llvm -b . -dcmake_build_type=debug -dllvm_targets_to_build="msp430;riscv" ../llvm\nmake -j 8\n\n\n# how to build lc3\n\ncmake -s llvm -b . -dcmake_build_type=debug -dllvm_experimental_targets_to_build="lc3" ../llvm\n',charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Getting Started with LLVM Core Libraries-Notes Chap5 IR",frontmatter:{title:"Getting Started with LLVM Core Libraries-Notes Chap5 IR",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000002/"},regularPath:"/02.compiler/02.GetStartedLLVMChap5Notes.html",relativePath:"02.compiler/02.GetStartedLLVMChap5Notes.md",key:"v-0b217b45",path:"/pages/000002/",headers:[{level:3,title:"Chap5. LLVM Intermediate Representation",slug:"chap5-llvm-intermediate-representation",normalizedTitle:"chap5. llvm intermediate representation",charIndex:2}],headersStr:"Chap5. LLVM Intermediate Representation",content:'# Chap5. LLVM Intermediate Representation\n\n\n\n# 1. This IR has three equivalent forms:\n\n\n• An in-memory representation (the Instruction class, among others)\n• An on-disk representation that is encoded in a space-efficient form (the bitcode files)\n• An on-disk representation in a human-readable text form (the LLVM assembly files)\n\n\n# 2. LLVM still conveys some target-specific aspects\n\n\nProgram might implicitly include target-specific headers, like bits linux header folder.\n\n# 3. commands\n\n\nclang *.c -emit-llvm -c -o *.bc\nclang *.c -emit-llvm -S -c -o *.ll\nllvm-as *.ll -o *.bc\nllvm-dis *.bc -o *.ll\n\n//extract function from IR module\nllvm-extract -func=* *.bc -o *.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 4. LLVM IR Language Syntax\n\n\nmodule -> function -> block -> instruction • SSA(Static Single Assignment) Form • Thress Address Instruction • Infinite number of registers\n\ntarget datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"\n\n// type:<size>:<abi>:<preferred>\n// pointer 64bit 64bit 64 bit\n// p:64:64:64\n\n\n1\n2\n3\n4\n5\n\n\n# 5. Introducing llvm IR in-memory model\n\n\n * Module\n   \n   * Module::iterator iterates across functions in the module\n     \n   * begin(); end();\n     \n * Function\n   \n   * isDeclaration()\n     \n   * getArgumentList() or arg_begin(), arg_end()\n     \n   * Iterate through blocks: for (Function::iterator i = function.begin(), e = function.end(); i != e; ++i)\n     \n * BasicBlock\n   \n   * encapsulate all instructions\n   * iterates thorugh begin() and end()\n   * access predecessor or list through getSinglePredecessor\n * Instruction\n   \n   * Predicates: isAssociative(), isCommutative(), isIdempotent(), or isTerminator()\n   * getOpCode()\n   * access Operands() through op_begin() and op_end()\n     \n * Most powerful Value and User Interface\n   * Function and Intruction are subclasses of both Value and User.\n   * BasicBlock is a subclass of Value\n   * Value and User and be navigate through use-def and def-use chain\n   * Value defines a result can be used by others\n   * User means that this entity use one or more Value Interface.\n * Value & User\n   * Value defines use_begin() and use_end() to iterate through all Users def-use chain\n   * ReplaceAllUsesWith(Value *)\n   * User defines op_begin() and op_end() access all of the Value Interface it uses use-def chain\n   * ReplaceUsesOfWith(Value *From, Value *To)\n\n# 6. Compile-time and Link time Optimization\n\n\nopt -O3 sum.bc -o sum-O3.bc\nopt -std-compile-opts sum.bc -o sum-stdc.bc\n\nllvm-link file1.bc file2.bc file3.bc -o=all.bc\nopt -std-link-opts all.bc -o all-stdl.bc\n\nopt sum.bc -mem2reg -instcount -o sum-tmp.bc -stats\nopt sum.bc -time-passes -domtree -instcount -o sum-tmp.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 7. Discovering which passes matter\n\n\nopt -O1 sum-O0.ll -S -o sum-O1.ll\n\nclang -Xclang -print-stats -emit-llvm -O1 sum.c -c -o sum-O1.bc\n\nopt sum-O0.ll -stats -mem2reg -o sum-O1.ll\n\n\n1\n2\n3\n4\n5\n\n\n# 8. Pass Dependencies\n\n\n// full list of passes used when you request just the mem2reg pass\nopt sum-O0.ll -debug-pass=Structure -mem2reg -S -o sum-O1.ll\n\n\n1\n2\n\n\n# 9. Pass API\n\n\n * ModulePass runOnModule()\n * FunctionPass runOnFuction()\n * BasicBlockPass runOnBasicBlock()\n\nIf Unchanged, return false. Or else, return true. 10.',normalizedContent:'# chap5. llvm intermediate representation\n\n\n\n# 1. this ir has three equivalent forms:\n\n\n• an in-memory representation (the instruction class, among others)\n• an on-disk representation that is encoded in a space-efficient form (the bitcode files)\n• an on-disk representation in a human-readable text form (the llvm assembly files)\n\n\n# 2. llvm still conveys some target-specific aspects\n\n\nprogram might implicitly include target-specific headers, like bits linux header folder.\n\n# 3. commands\n\n\nclang *.c -emit-llvm -c -o *.bc\nclang *.c -emit-llvm -s -c -o *.ll\nllvm-as *.ll -o *.bc\nllvm-dis *.bc -o *.ll\n\n//extract function from ir module\nllvm-extract -func=* *.bc -o *.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 4. llvm ir language syntax\n\n\nmodule -> function -> block -> instruction • ssa(static single assignment) form • thress address instruction • infinite number of registers\n\ntarget datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-s128"\n\n// type:<size>:<abi>:<preferred>\n// pointer 64bit 64bit 64 bit\n// p:64:64:64\n\n\n1\n2\n3\n4\n5\n\n\n# 5. introducing llvm ir in-memory model\n\n\n * module\n   \n   * module::iterator iterates across functions in the module\n     \n   * begin(); end();\n     \n * function\n   \n   * isdeclaration()\n     \n   * getargumentlist() or arg_begin(), arg_end()\n     \n   * iterate through blocks: for (function::iterator i = function.begin(), e = function.end(); i != e; ++i)\n     \n * basicblock\n   \n   * encapsulate all instructions\n   * iterates thorugh begin() and end()\n   * access predecessor or list through getsinglepredecessor\n * instruction\n   \n   * predicates: isassociative(), iscommutative(), isidempotent(), or isterminator()\n   * getopcode()\n   * access operands() through op_begin() and op_end()\n     \n * most powerful value and user interface\n   * function and intruction are subclasses of both value and user.\n   * basicblock is a subclass of value\n   * value and user and be navigate through use-def and def-use chain\n   * value defines a result can be used by others\n   * user means that this entity use one or more value interface.\n * value & user\n   * value defines use_begin() and use_end() to iterate through all users def-use chain\n   * replacealluseswith(value *)\n   * user defines op_begin() and op_end() access all of the value interface it uses use-def chain\n   * replaceusesofwith(value *from, value *to)\n\n# 6. compile-time and link time optimization\n\n\nopt -o3 sum.bc -o sum-o3.bc\nopt -std-compile-opts sum.bc -o sum-stdc.bc\n\nllvm-link file1.bc file2.bc file3.bc -o=all.bc\nopt -std-link-opts all.bc -o all-stdl.bc\n\nopt sum.bc -mem2reg -instcount -o sum-tmp.bc -stats\nopt sum.bc -time-passes -domtree -instcount -o sum-tmp.bc\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 7. discovering which passes matter\n\n\nopt -o1 sum-o0.ll -s -o sum-o1.ll\n\nclang -xclang -print-stats -emit-llvm -o1 sum.c -c -o sum-o1.bc\n\nopt sum-o0.ll -stats -mem2reg -o sum-o1.ll\n\n\n1\n2\n3\n4\n5\n\n\n# 8. pass dependencies\n\n\n// full list of passes used when you request just the mem2reg pass\nopt sum-o0.ll -debug-pass=structure -mem2reg -s -o sum-o1.ll\n\n\n1\n2\n\n\n# 9. pass api\n\n\n * modulepass runonmodule()\n * functionpass runonfuction()\n * basicblockpass runonbasicblock()\n\nif unchanged, return false. or else, return true. 10.',charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Getting Started with LLVM Core Libraries-Notes Chap6 Backend",frontmatter:{title:"Getting Started with LLVM Core Libraries-Notes Chap6 Backend",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000003/"},regularPath:"/02.compiler/03.GetStartedLLVMChap6Notes.html",relativePath:"02.compiler/03.GetStartedLLVMChap6Notes.md",key:"v-2d0923f6",path:"/pages/000003/",headers:[{level:3,title:"Chap6. The Backend",slug:"chap6-the-backend",normalizedTitle:"chap6. the backend",charIndex:2},{level:3,title:"1. Using the backend tools",slug:"_1-using-the-backend-tools",normalizedTitle:"1. using the backend tools",charIndex:1060},{level:3,title:"2. Learning backend struture",slug:"_2-learning-backend-struture",normalizedTitle:"2. learning backend struture",charIndex:1232},{level:3,title:"3. Knowing backend libraries",slug:"_3-knowing-backend-libraries",normalizedTitle:"3. knowing backend libraries",charIndex:1627},{level:3,title:"4. Learning how to use TableGen for LLVM backends",slug:"_4-learning-how-to-use-tablegen-for-llvm-backends",normalizedTitle:"4. learning how to use tablegen for llvm backends",charIndex:2024},{level:3,title:"5. Instruction Selection Phase",slug:"_5-instruction-selection-phase",normalizedTitle:"5. instruction selection phase",charIndex:3229},{level:3,title:"6. Lowering",slug:"_6-lowering",normalizedTitle:"6. lowering",charIndex:4179},{level:3,title:"7. DAG Combine and legalization",slug:"_7-dag-combine-and-legalization",normalizedTitle:"7. dag combine and legalization",charIndex:4889},{level:3,title:"8. DAG-to-DAG instruction selection",slug:"_8-dag-to-dag-instruction-selection",normalizedTitle:"8. dag-to-dag instruction selection",charIndex:6153},{level:3,title:"9. Scheduler",slug:"_9-scheduler",normalizedTitle:"9. scheduler",charIndex:8783},{level:3,title:"10. Machine Instructions",slug:"_10-machine-instructions",normalizedTitle:"10. machine instructions",charIndex:9629},{level:3,title:"11. Register Allocation",slug:"_11-register-allocation",normalizedTitle:"11. register allocation",charIndex:10221},{level:3,title:"12. Prologue and epilogue",slug:"_12-prologue-and-epilogue",normalizedTitle:"12. prologue and epilogue",charIndex:13492},{level:3,title:"13. Frame indexs",slug:"_13-frame-indexs",normalizedTitle:"13. frame indexs",charIndex:13837},{level:3,title:"13. Understanding machine code framework",slug:"_13-understanding-machine-code-framework",normalizedTitle:"13. understanding machine code framework",charIndex:14047},{level:3,title:"Summary",slug:"summary",normalizedTitle:"summary",charIndex:14509}],headersStr:"Chap6. The Backend 1. Using the backend tools 2. Learning backend struture 3. Knowing backend libraries 4. Learning how to use TableGen for LLVM backends 5. Instruction Selection Phase 6. Lowering 7. DAG Combine and legalization 8. DAG-to-DAG instruction selection 9. Scheduler 10. Machine Instructions 11. Register Allocation 12. Prologue and epilogue 13. Frame indexs 13. Understanding machine code framework Summary",content:'# Chap6. The Backend\n\n\n\nWhite box Essential Gray Block For generated code efficiency\n\n# 1. Instructon Selection\n\n\n * Convert IR to target-specific SelectionDAG(Directed Acyclic Graph)\n   * Block->DAG\n   * Instruction->Node\n   * Edge contains dataflow dependence and control dependence and glue.\n * LLVM use DAG to employ tree-based pattern-matching instruction selection.\n * IN the end of this phase, IR node are converted to target-machine(machine instructions) nodes.\n\n# 2. Pre-register Allocation(RA) scheduling,the first instruction scheduling.\n\n\n * This is to explore instruction-level parallelism\n * The instructions are converted to MachineInstr three-address representation.\n\n# 3. Reguster Allocation\n\n\n# 4. Post-register Allocation(RA) Instruction Scheduling, the second instruction scheduling\n\n * Now we have real register information, we can combine information of extra hazards and delays of real register to opmitize code.\n\n# 5. Code Emission\n\n * Convert MachineInstr to MCInst\n * Emit Assembly Code\n * Emit Binary blobs to object code format\n\n\n# 1. Using the backend tools\n\nllc *bc -o *.s\nllc *.bc -filetype=obj -o *.o\n\nllc *.bc -march=mips -filetype=obj -o *.o\n\n// how march options\nllc -version\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 2. Learning backend struture\n\n * CodeGen: Instruction selection, scheduler,register allocation\n * MC: assembly parser, disassembler\n * TableGen\n * Target/*.cpp *.h *.td\n\nNotice:\n\nIselLowering is for Selection DAG Node lowering\nIselDAGtoDAG is for instruction selection.\n\n\nTargetLowering is called first for target-specific call and ret.\nThe major instruction selection is in ISelDAGtoDAG.\n\n\n\n\n# 3. Knowing backend libraries\n\n * AsmParser.a\n * AsmPrinter.a\n * CodeGen.a\n    * majority of the target-dependent functionality of the backend, as following：\n    * specific register handling rules, instruction selection, and scheduling\n\n * Desc.a\n    * low-level MC infrastructure and is responsible for registering target-specific MC objects such as MCCodeEmitter\n\n * Info.a\n * Disassembler.a\n\n\n# 4. Learning how to use TableGen for LLVM backends\n\n * instruction formats,\n * instructions,\n * registers,\n * pattern-matching DAGs,\n * instruction selection matching order,\n * calling conventions,\n * target CPU properties (supported Instruction Set Architecture (ISA) features and processor families).\n\ninsns.td\n\n\n\nGenerate code using llvm-tblgen\n\n\n\nTarget Properties: .td\nRegisters: RegisterInfo.td\n\n\n$ cd <llvm_source>/lib/Target/X86\n$ llvm-tblgen -gen-register-info X86.td -I ../../../include\n\n\n1\n2\n\n\nInstruction format: InstrFormat.td\nInstructions: InstrInfo.td\n\n\ninclude/llvm/Target/Target.td\n\n\n1\n\n\n\n\ndag in the above picture represents selectDAG for opcodes, registers or constants during instruction selection phase.\n\n\nSparcInstrInfo.td\n\n\n1\n\n\n\n\nWe can get how the template parameters are assigned to class Instruction.\n\n * OutOperandList\n * InOperandList\n * AsmString\n * Pattern\n\ncd <llvm_sources>/lib/Target/Sparc\nllvm-tblgen -print-records Sparc.td -I ../../../include | grep XNORrr -A 10\n\n\n1\n2\n\n\nThe difference between the first and second need to be checked.\n\n * GenDAGISel.inc\n * GenInstrInfo.inc\n * GenAsmWriter.inc\n * GenCodeEmitter.inc\n * GenDisassemblerTables.inc\n * GenAsmMatcher.inc\n\n\n# 5. Instruction Selection Phase\n\nLLVM IR -> SelectionDAG(SDNode)\n\n 1. Create DAG, in which node carry IR op\n 2. Nodes go through lowering, DAG combiner, and legalization phases.\n 3. Instruction selection perform DAG-to-DAG conversion, using node pattern matching and transforms SelectionDAG node into nodes representing target instructions.\n\nMost expensive ones in backend\n\n# 5.1 SelectionDAG class\n\n * DAG for each basic block\n * SDNode for instruction or operand\n\n\n\n * The black arrows represent regular edges showing a dataflow dependence.\n * The dashed blue arrows represent non-dataflow chains that exist to enforce order between two otherwise unrelated instructions.\n * The red edge guarantees that its adjacent nodes must be glued together\n\nPlease notice:\n\n * CopyFromReg: This is for getting value out of scope.\n * CopyToReg: This node copies a value to a specific register without supplying any concrete value for other nodes to consume.\n\n\n# 6. Lowering\n\n\n\n 1. SelectionDAGBuilder in SelectionDAGIsel.cpp visits every fuction and creates SelectionDAG for each basic block\n 2. During 1), special IR such as call and ret needs TargetLowering class for the first time for info like: pass call arg and how to return.\n 3. Only a smalle subset are lowered in this way. Majority are matched and replaces at instruction selection.\n\n> For instance, in SelectionDAG from sum.bc, the X86TargetLowering::LowerReturn() method (see lib/Target/X86/X86ISelLowering.cpp) is used to lower the IR ret instruction.\n> While doing this, it generates the X86ISD::RET_FLAG node, which copies the function result to EAX a-target-specific way to handle the function return.\n\n\n# 7. DAG Combine and legalization\n\n * DAG Combine\n   * Optimization for simpler code\n   * Target Independent: lib/CodeGen/SelectionDAG/DAGCombiner.cpp\n   * Target Dependnet: lib/Target/<Target_Name>/ISelLowering.cpp setTargetDAGCombine()\n\nsetTargetDAGCombine({ISD::SDIVREM, ISD::UDIVREM, ISD::SELECT, ISD::AND,\n                       ISD::OR, ISD::ADD, ISD::SUB, ISD::AssertZext, ISD::SHL});\n\nstatic SDValue performADDCombine(SDNode *N, SelectionDAG &DAG,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 const MipsSubtarget &Subtarget) {\n  ...\n  // (add v0, (add v1, abs_lo(tjt))) => (add (add v0, v1), abs_lo(tjt))\n  SDValue Add = N->getOperand(1);\n\n  if (Add.getOpcode() != ISD::ADD)\n    return SDValue();\n\n  SDValue Lo = Add.getOperand(1);\n  ...\n  EVT ValTy = N->getValueType(0);\n  SDLoc DL(N);\n\n  SDValue Add1 = DAG.getNode(ISD::ADD, DL, ValTy, N->getOperand(0),\n                             Add.getOperand(0));\n  return DAG.getNode(ISD::ADD, DL, ValTy, Add1, Lo);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n * Legalization\n\n * Support legal types: scalar: promote, expand, soften. vec split, scalarized or widened\n * Also it can be customized\n\nPromote\nExpand(library call)\nCustom\n\n\n\n\n# 8. DAG-to-DAG instruction selection\n\nTransform target-independent nodes to target-specific nodes by using pattern matching.\n\nCopyToReg, CopyFromReg and Register nodes are untouched until Register Allocation.\n\n# 8.1 Pattern Matching\n\nlib/Target/Sparc/SparcISelDAGToDAG.cpp\n\nSelect()  in SelectionDAGISel subclass\n\n\n1\n2\n3\n\n\nSelect():\n\n * receive an SDNode parameter to be matched\n * return SDNnode value representing a phycical instruction\n\nSelection() will call TableGen generateed SelectCode method.\nTableGen also contains MatcherTable, mapping ISD and ISD to physical-instruction node.\nThis table is generated by InstrInfo.td\nThe table are contained in <build_dir>/lib/Target/Sparc/SparcGenDAGISel.inc.\n\n\nWe can add other customized matching code prior to selectCode().\n\nCurDAG->getMachineNode() will create a node with phsycial instruction SP::SPAri CurDAG->SelectNodeTo() will create an instruction node and changes all use of * result to point to the "Opcode" result.\n\nvoid SparcDAGToDAGISel::Select(SDNode *N) {\n  ...\n  case ISD::UDIV: {\n    // sdivx / udivx handle 64-bit divides.\n    if (N->getValueType(0) == MVT::i64)\n      break;\n    // FIXME: should use a custom expander to expose the SRA to the dag.\n    SDValue DivLHS = N->getOperand(0);\n    SDValue DivRHS = N->getOperand(1);\n\n    // Set the Y register to the high-part.\n    SDValue TopPart;\n    if (N->getOpcode() == ISD::SDIV) {\n      TopPart = SDValue(CurDAG->getMachineNode(SP::SRAri, dl, MVT::i32, DivLHS,\n                                   CurDAG->getTargetConstant(31, dl, MVT::i32)),\n                        0);\n    } else {\n      TopPart = CurDAG->getRegister(SP::G0, MVT::i32);\n    }\n    TopPart = CurDAG->getCopyToReg(CurDAG->getEntryNode(), dl, SP::Y, TopPart,\n                                   SDValue())\n                  .getValue(1);\n\n    // FIXME: Handle div by immediate.\n    unsigned Opcode = N->getOpcode() == ISD::SDIV ? SP::SDIVrr : SP::UDIVrr;\n    CurDAG->SelectNodeTo(N, Opcode, MVT::i32, DivLHS, DivRHS, TopPart);\n    return;\n  }\n  }\n\n  SelectCode(N);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# 8.2 Visualizing the instruction selection process\n\nLLC                         PHASE\n-view-dag-combine1-dags     Before DAG combine 1\n-view-legalize-types-dags   Before legalize type\n-view-dag-combine-lt-dags   After legalize type 2 and before DAG combine\n-view-legalize-dags         Before legalization\n-view-dag-combine2-dags     Before DAG combine 2\n-view-isel-dags             Before instruction selection\n-view-sched-dags            After instruction selection and before scheduling\n\n\n# 9. Scheduler\n\nPre-register allocation works on SelectionDAG nodes(SDNodes).\n\n\n<llvm_source>/ lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp\n\nDifferent Algorithms: llc -pre-RA-sched=\n\n# 9.1 Instruction Itineraries\n\n<llvm_source>/include/llvm/Target/TargetItinerary.td\n<llvm_source>/lib/Target/ARM/ARMScheduleA8.td\n\n\nRepresent instruction latencya and hardware pipeline information.\n\n\n\n# 9.2 Hazard Detection\n\n> The ScheduleHazardRecognizer class provides an interface for hazard recognizer implementations and the ScoreboardHazardRecognizer subclass implements the scoreboard hazard recognizer (see the file <llvm_source>/lib/CodeGen/ScoreboardHazardRecognizer.cpp), which is LLVM\'s default recognizer.\n\n# 9.3 Scheduling Units\n\nThis scheduler runs before and after register allocation, which process both SDNode instruction and MachineInstr.\n\n\n# 10. Machine Instructions\n\nThe InstrEmitter pass, which runs after scheduling, transforms SDNode format into MachineInstr format.\nMI format is sequence of instructions rather than DAG.\n\n\nMI contains significant meta-information about an instruction:\n\n\n * it stores used and defined registers.\n * it distinguishes between register and memory operands (among other types).\n * it stores the instruction type (branch, return, call, and terminator, among others)\n * it stores predicates such as whether it is commutable or not, and so on.\n\n\n\nllc -print-machineinstrs\n\nllc -print-machineinstrs=\n\n\n# 11. Register Allocation\n\n * some MI code fragments might already use physical registers even before register allocation.\n   * Machine instructions that nee specific register\n   * ABI requirement\n * Destruct SSA form of IR\n\n4 register allocation algoritm\n\n-regalloc=<pbqp/greedy/basic/fast>\n\nby default, it will be basic(linear scan).\n\n\n\n# 11.1 Register Coalescer\n\nlib/CodeGen/RegisterCoalescer.cpp\n\n\n * A machine Function Pass, joinAllIntervals will iterate a work list of copy functions.\n * joinCopy creates CoalescerPair instances from copy machine instructions and coalesces copies wasy.\n\nBefore the coalescer, the phi node elimination pass runs.\nllc -print-machine-insts=phi-node-elimination will show this.\n\nMachine Instruction will be indexed with 0B, 16B, 32B(slot indexes).\n\nlive variable analysis pass runs before coalescing, thus the code is annotated with live variable information\n\n * which points each register is defined and killed\n * This is useful for ust to know which registers interfere with one other, that is are alive at the same time and need to live in distinct physical register.(Similar to graph coloring)\n\nCoalescer will also look for register copies, try to join the interval of the source register with the interval of the destination register.\nThe above is based on live interval analysis(different from live variable analysis).\n\nllc -march=sparc -debug-only=regalloc *.bc\n\n\n1\n\n\n# 11.2 Virtual Register Rewrite\n\n * Register Allocation Pass will selects the physical registers to be used for each virtual one.\n   \n * VirtRegMap contains mapping from virt to phy register.\n * VirtRegRewriter class implemented in <llvm_source>/lib/CodeGen/VirtRegMap.cpp—uses VirtRegMap and replaces virtual register references with physical ones.\n * Spill Code is also generated.\n * reg = COPY reg are deleted.\n\n> The register allocator and the instruction scheduler are sworn enemies in any compiler.\n> The job of the register allocator is to keep live ranges as short as possible, reducing the number of edges of the interference graph and thus reducing the number of necessary registers to avoid spills. To do this, the register allocator prefers to schedule instructions in a serial fashion (putting an instruction that depends on the other right next to it) because in this way the code uses less registers.\n> The job of the scheduler is the opposite: to extract instruction-level parallelism, it needs to keep alive as much unrelated and parallel computations as possible, requiring a much larger number of registers to hold intermediary values and increasing the number of interferences among live ranges.\n\n# 11.3 Target Hooks\n\n 1. TargetRegisterInfo includes if it is reserved or not, its parent register classes, and whether it is physical or virtual\n 2. InstrInfo\n     * isLoadFromStackSlot() and isStoreToStackSlot() are used during spill code generation to discover whether the machine instruction is a memory access to a stack slot.\n     * Spiller use using the storeRegToStackSlot() and loadRegFromStackSlot() methods with target-specific memory access instructions.\n     * copyPhyReg() method will also generate target-specific register copy.\n\nThe BuildMI() method is used everywhere in the code generator to generate machine instructions.\n\n\n# 12. Prologue and epilogue\n\n 1. Prologue sets up the stack frame and callee-saved registers during the beginning of a function.\n 2. Epilogue cleans up the stack frame prior to function return. They are target-specific, defined in FrameLowering::emitPrologue() and FrameLowering::emitEpilogue() at <llvm_source>/lib/Target//FrameLowering.cpp)\n\n\n# 13. Frame indexs\n\n<llvm_source>/lib/Target//RegisterInfo.cpp contains eliminateFrameIndex().\nIt will replace each frame index to real stack offset for all machine instructions that contain stack reference.\n\n\n# 13. Understanding machine code framework\n\nconvert machine instruction into machine code instructions(MC instructions).\n\n// show MC inst\nllc *.bc -march=x86-64 -show-mc-inst -o -\n\n// show assemble encoding\necho "movq 48879(,%riz), %rax" | llvm-mc -triple=x86_64 --show-encoding\n # encoding: [0x48,0x8b,0x04,0x25,0xef,0xbe,0x00,0x00]\n\n// disassemble\necho "0x8d 0x4c 0x24 0x04" | llvm-mc --disassemble -triple=x86_64\n leal 4(%rsp), %ecx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# Summary\n\n> https://jonathan2251.github.io/lbd/_images/9.png',normalizedContent:'# chap6. the backend\n\n\n\nwhite box essential gray block for generated code efficiency\n\n# 1. instructon selection\n\n\n * convert ir to target-specific selectiondag(directed acyclic graph)\n   * block->dag\n   * instruction->node\n   * edge contains dataflow dependence and control dependence and glue.\n * llvm use dag to employ tree-based pattern-matching instruction selection.\n * in the end of this phase, ir node are converted to target-machine(machine instructions) nodes.\n\n# 2. pre-register allocation(ra) scheduling,the first instruction scheduling.\n\n\n * this is to explore instruction-level parallelism\n * the instructions are converted to machineinstr three-address representation.\n\n# 3. reguster allocation\n\n\n# 4. post-register allocation(ra) instruction scheduling, the second instruction scheduling\n\n * now we have real register information, we can combine information of extra hazards and delays of real register to opmitize code.\n\n# 5. code emission\n\n * convert machineinstr to mcinst\n * emit assembly code\n * emit binary blobs to object code format\n\n\n# 1. using the backend tools\n\nllc *bc -o *.s\nllc *.bc -filetype=obj -o *.o\n\nllc *.bc -march=mips -filetype=obj -o *.o\n\n// how march options\nllc -version\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n\n# 2. learning backend struture\n\n * codegen: instruction selection, scheduler,register allocation\n * mc: assembly parser, disassembler\n * tablegen\n * target/*.cpp *.h *.td\n\nnotice:\n\nisellowering is for selection dag node lowering\niseldagtodag is for instruction selection.\n\n\ntargetlowering is called first for target-specific call and ret.\nthe major instruction selection is in iseldagtodag.\n\n\n\n\n# 3. knowing backend libraries\n\n * asmparser.a\n * asmprinter.a\n * codegen.a\n    * majority of the target-dependent functionality of the backend, as following：\n    * specific register handling rules, instruction selection, and scheduling\n\n * desc.a\n    * low-level mc infrastructure and is responsible for registering target-specific mc objects such as mccodeemitter\n\n * info.a\n * disassembler.a\n\n\n# 4. learning how to use tablegen for llvm backends\n\n * instruction formats,\n * instructions,\n * registers,\n * pattern-matching dags,\n * instruction selection matching order,\n * calling conventions,\n * target cpu properties (supported instruction set architecture (isa) features and processor families).\n\ninsns.td\n\n\n\ngenerate code using llvm-tblgen\n\n\n\ntarget properties: .td\nregisters: registerinfo.td\n\n\n$ cd <llvm_source>/lib/target/x86\n$ llvm-tblgen -gen-register-info x86.td -i ../../../include\n\n\n1\n2\n\n\ninstruction format: instrformat.td\ninstructions: instrinfo.td\n\n\ninclude/llvm/target/target.td\n\n\n1\n\n\n\n\ndag in the above picture represents selectdag for opcodes, registers or constants during instruction selection phase.\n\n\nsparcinstrinfo.td\n\n\n1\n\n\n\n\nwe can get how the template parameters are assigned to class instruction.\n\n * outoperandlist\n * inoperandlist\n * asmstring\n * pattern\n\ncd <llvm_sources>/lib/target/sparc\nllvm-tblgen -print-records sparc.td -i ../../../include | grep xnorrr -a 10\n\n\n1\n2\n\n\nthe difference between the first and second need to be checked.\n\n * gendagisel.inc\n * geninstrinfo.inc\n * genasmwriter.inc\n * gencodeemitter.inc\n * gendisassemblertables.inc\n * genasmmatcher.inc\n\n\n# 5. instruction selection phase\n\nllvm ir -> selectiondag(sdnode)\n\n 1. create dag, in which node carry ir op\n 2. nodes go through lowering, dag combiner, and legalization phases.\n 3. instruction selection perform dag-to-dag conversion, using node pattern matching and transforms selectiondag node into nodes representing target instructions.\n\nmost expensive ones in backend\n\n# 5.1 selectiondag class\n\n * dag for each basic block\n * sdnode for instruction or operand\n\n\n\n * the black arrows represent regular edges showing a dataflow dependence.\n * the dashed blue arrows represent non-dataflow chains that exist to enforce order between two otherwise unrelated instructions.\n * the red edge guarantees that its adjacent nodes must be glued together\n\nplease notice:\n\n * copyfromreg: this is for getting value out of scope.\n * copytoreg: this node copies a value to a specific register without supplying any concrete value for other nodes to consume.\n\n\n# 6. lowering\n\n\n\n 1. selectiondagbuilder in selectiondagisel.cpp visits every fuction and creates selectiondag for each basic block\n 2. during 1), special ir such as call and ret needs targetlowering class for the first time for info like: pass call arg and how to return.\n 3. only a smalle subset are lowered in this way. majority are matched and replaces at instruction selection.\n\n> for instance, in selectiondag from sum.bc, the x86targetlowering::lowerreturn() method (see lib/target/x86/x86isellowering.cpp) is used to lower the ir ret instruction.\n> while doing this, it generates the x86isd::ret_flag node, which copies the function result to eax a-target-specific way to handle the function return.\n\n\n# 7. dag combine and legalization\n\n * dag combine\n   * optimization for simpler code\n   * target independent: lib/codegen/selectiondag/dagcombiner.cpp\n   * target dependnet: lib/target/<target_name>/isellowering.cpp settargetdagcombine()\n\nsettargetdagcombine({isd::sdivrem, isd::udivrem, isd::select, isd::and,\n                       isd::or, isd::add, isd::sub, isd::assertzext, isd::shl});\n\nstatic sdvalue performaddcombine(sdnode *n, selectiondag &dag,\n                                 targetlowering::dagcombinerinfo &dci,\n                                 const mipssubtarget &subtarget) {\n  ...\n  // (add v0, (add v1, abs_lo(tjt))) => (add (add v0, v1), abs_lo(tjt))\n  sdvalue add = n->getoperand(1);\n\n  if (add.getopcode() != isd::add)\n    return sdvalue();\n\n  sdvalue lo = add.getoperand(1);\n  ...\n  evt valty = n->getvaluetype(0);\n  sdloc dl(n);\n\n  sdvalue add1 = dag.getnode(isd::add, dl, valty, n->getoperand(0),\n                             add.getoperand(0));\n  return dag.getnode(isd::add, dl, valty, add1, lo);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n * legalization\n\n * support legal types: scalar: promote, expand, soften. vec split, scalarized or widened\n * also it can be customized\n\npromote\nexpand(library call)\ncustom\n\n\n\n\n# 8. dag-to-dag instruction selection\n\ntransform target-independent nodes to target-specific nodes by using pattern matching.\n\ncopytoreg, copyfromreg and register nodes are untouched until register allocation.\n\n# 8.1 pattern matching\n\nlib/target/sparc/sparciseldagtodag.cpp\n\nselect()  in selectiondagisel subclass\n\n\n1\n2\n3\n\n\nselect():\n\n * receive an sdnode parameter to be matched\n * return sdnnode value representing a phycical instruction\n\nselection() will call tablegen generateed selectcode method.\ntablegen also contains matchertable, mapping isd and isd to physical-instruction node.\nthis table is generated by instrinfo.td\nthe table are contained in <build_dir>/lib/target/sparc/sparcgendagisel.inc.\n\n\nwe can add other customized matching code prior to selectcode().\n\ncurdag->getmachinenode() will create a node with phsycial instruction sp::spari curdag->selectnodeto() will create an instruction node and changes all use of * result to point to the "opcode" result.\n\nvoid sparcdagtodagisel::select(sdnode *n) {\n  ...\n  case isd::udiv: {\n    // sdivx / udivx handle 64-bit divides.\n    if (n->getvaluetype(0) == mvt::i64)\n      break;\n    // fixme: should use a custom expander to expose the sra to the dag.\n    sdvalue divlhs = n->getoperand(0);\n    sdvalue divrhs = n->getoperand(1);\n\n    // set the y register to the high-part.\n    sdvalue toppart;\n    if (n->getopcode() == isd::sdiv) {\n      toppart = sdvalue(curdag->getmachinenode(sp::srari, dl, mvt::i32, divlhs,\n                                   curdag->gettargetconstant(31, dl, mvt::i32)),\n                        0);\n    } else {\n      toppart = curdag->getregister(sp::g0, mvt::i32);\n    }\n    toppart = curdag->getcopytoreg(curdag->getentrynode(), dl, sp::y, toppart,\n                                   sdvalue())\n                  .getvalue(1);\n\n    // fixme: handle div by immediate.\n    unsigned opcode = n->getopcode() == isd::sdiv ? sp::sdivrr : sp::udivrr;\n    curdag->selectnodeto(n, opcode, mvt::i32, divlhs, divrhs, toppart);\n    return;\n  }\n  }\n\n  selectcode(n);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\n# 8.2 visualizing the instruction selection process\n\nllc                         phase\n-view-dag-combine1-dags     before dag combine 1\n-view-legalize-types-dags   before legalize type\n-view-dag-combine-lt-dags   after legalize type 2 and before dag combine\n-view-legalize-dags         before legalization\n-view-dag-combine2-dags     before dag combine 2\n-view-isel-dags             before instruction selection\n-view-sched-dags            after instruction selection and before scheduling\n\n\n# 9. scheduler\n\npre-register allocation works on selectiondag nodes(sdnodes).\n\n\n<llvm_source>/ lib/codegen/selectiondag/scheduledagsdnodes.cpp\n\ndifferent algorithms: llc -pre-ra-sched=\n\n# 9.1 instruction itineraries\n\n<llvm_source>/include/llvm/target/targetitinerary.td\n<llvm_source>/lib/target/arm/armschedulea8.td\n\n\nrepresent instruction latencya and hardware pipeline information.\n\n\n\n# 9.2 hazard detection\n\n> the schedulehazardrecognizer class provides an interface for hazard recognizer implementations and the scoreboardhazardrecognizer subclass implements the scoreboard hazard recognizer (see the file <llvm_source>/lib/codegen/scoreboardhazardrecognizer.cpp), which is llvm\'s default recognizer.\n\n# 9.3 scheduling units\n\nthis scheduler runs before and after register allocation, which process both sdnode instruction and machineinstr.\n\n\n# 10. machine instructions\n\nthe instremitter pass, which runs after scheduling, transforms sdnode format into machineinstr format.\nmi format is sequence of instructions rather than dag.\n\n\nmi contains significant meta-information about an instruction:\n\n\n * it stores used and defined registers.\n * it distinguishes between register and memory operands (among other types).\n * it stores the instruction type (branch, return, call, and terminator, among others)\n * it stores predicates such as whether it is commutable or not, and so on.\n\n\n\nllc -print-machineinstrs\n\nllc -print-machineinstrs=\n\n\n# 11. register allocation\n\n * some mi code fragments might already use physical registers even before register allocation.\n   * machine instructions that nee specific register\n   * abi requirement\n * destruct ssa form of ir\n\n4 register allocation algoritm\n\n-regalloc=<pbqp/greedy/basic/fast>\n\nby default, it will be basic(linear scan).\n\n\n\n# 11.1 register coalescer\n\nlib/codegen/registercoalescer.cpp\n\n\n * a machine function pass, joinallintervals will iterate a work list of copy functions.\n * joincopy creates coalescerpair instances from copy machine instructions and coalesces copies wasy.\n\nbefore the coalescer, the phi node elimination pass runs.\nllc -print-machine-insts=phi-node-elimination will show this.\n\nmachine instruction will be indexed with 0b, 16b, 32b(slot indexes).\n\nlive variable analysis pass runs before coalescing, thus the code is annotated with live variable information\n\n * which points each register is defined and killed\n * this is useful for ust to know which registers interfere with one other, that is are alive at the same time and need to live in distinct physical register.(similar to graph coloring)\n\ncoalescer will also look for register copies, try to join the interval of the source register with the interval of the destination register.\nthe above is based on live interval analysis(different from live variable analysis).\n\nllc -march=sparc -debug-only=regalloc *.bc\n\n\n1\n\n\n# 11.2 virtual register rewrite\n\n * register allocation pass will selects the physical registers to be used for each virtual one.\n   \n * virtregmap contains mapping from virt to phy register.\n * virtregrewriter class implemented in <llvm_source>/lib/codegen/virtregmap.cpp—uses virtregmap and replaces virtual register references with physical ones.\n * spill code is also generated.\n * reg = copy reg are deleted.\n\n> the register allocator and the instruction scheduler are sworn enemies in any compiler.\n> the job of the register allocator is to keep live ranges as short as possible, reducing the number of edges of the interference graph and thus reducing the number of necessary registers to avoid spills. to do this, the register allocator prefers to schedule instructions in a serial fashion (putting an instruction that depends on the other right next to it) because in this way the code uses less registers.\n> the job of the scheduler is the opposite: to extract instruction-level parallelism, it needs to keep alive as much unrelated and parallel computations as possible, requiring a much larger number of registers to hold intermediary values and increasing the number of interferences among live ranges.\n\n# 11.3 target hooks\n\n 1. targetregisterinfo includes if it is reserved or not, its parent register classes, and whether it is physical or virtual\n 2. instrinfo\n     * isloadfromstackslot() and isstoretostackslot() are used during spill code generation to discover whether the machine instruction is a memory access to a stack slot.\n     * spiller use using the storeregtostackslot() and loadregfromstackslot() methods with target-specific memory access instructions.\n     * copyphyreg() method will also generate target-specific register copy.\n\nthe buildmi() method is used everywhere in the code generator to generate machine instructions.\n\n\n# 12. prologue and epilogue\n\n 1. prologue sets up the stack frame and callee-saved registers during the beginning of a function.\n 2. epilogue cleans up the stack frame prior to function return. they are target-specific, defined in framelowering::emitprologue() and framelowering::emitepilogue() at <llvm_source>/lib/target//framelowering.cpp)\n\n\n# 13. frame indexs\n\n<llvm_source>/lib/target//registerinfo.cpp contains eliminateframeindex().\nit will replace each frame index to real stack offset for all machine instructions that contain stack reference.\n\n\n# 13. understanding machine code framework\n\nconvert machine instruction into machine code instructions(mc instructions).\n\n// show mc inst\nllc *.bc -march=x86-64 -show-mc-inst -o -\n\n// show assemble encoding\necho "movq 48879(,%riz), %rax" | llvm-mc -triple=x86_64 --show-encoding\n # encoding: [0x48,0x8b,0x04,0x25,0xef,0xbe,0x00,0x00]\n\n// disassemble\necho "0x8d 0x4c 0x24 0x04" | llvm-mc --disassemble -triple=x86_64\n leal 4(%rsp), %ecx\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n# summary\n\n> https://jonathan2251.github.io/lbd/_images/9.png',charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"Learning LLVM Diary 00",frontmatter:{title:"Learning LLVM Diary 00",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000004/"},regularPath:"/02.compiler/04.%20LearningLLVMDiary0.html",relativePath:"02.compiler/04. LearningLLVMDiary0.md",key:"v-4d042c96",path:"/pages/000004/",headers:[{level:3,title:"3. Knowledge Fragments",slug:"_3-knowledge-fragments",normalizedTitle:"3. knowledge fragments",charIndex:7893}],headersStr:"3. Knowledge Fragments",content:"# 1. Difference between ISelDAGToDAG and ISelLowering\n\nIn LLVM's backend, ISelLowering and ISelDAGToDAG are two important classes that play different roles in the instruction selection process. Here's the difference between them and the phases in which they are called:\n\nISelDAGToDAG\n\n * Role: ISelDAGToDAG stands for \"Instruction Selection DAG to DAG\". This class is responsible for translating the Selection DAG (Directed Acyclic Graph) into a sequence of target-specific instructions.\n\n * Phase: ISelDAGToDAG is called during the \"Instruction Selection\" phase of the compiler backend. This phase comes after the DAG Legalization phase and before Register Allocation.\n\n * Functions: In ISelDAGToDAG, you will find functions like Select, which is responsible for selecting target instructions for each node in the DAG.\n\nTypical Functions:\n\nSelect: Select target instructions for nodes in the DAG.\nSelectNode: Implement target-specific node selection.\nPreprocessISelDAG: Prepare for instruction selection.\nPostprocessISelDAG: Clean up after instruction selection.\n\n\n1\n2\n3\n4\n\n\nISelLowering\n\n * Role: ISelLowering stands for \"Instruction Selection Lowering\". This class provides target-specific information and handling for aspects of instruction selection during the Selection DAG Construction phase.\n\n * Phase: ISelLowering is called during the \"Selection DAG Construction\" phase of the compiler backend. This phase transforms the generic LLVM IR into a machine-independent representation, the Selection DAG.\n\n * Functions: In ISelLowering, you will find functions that define how LLVM IR operations are translated into target-specific instructions. This includes custom lowering of specific LLVM IR operations, defining patterns for converting LLVM IR to target nodes, and providing information about target-specific features and constraints.\n\nTypical Functions:\n\nLowerCall: Lower calls to target-specific calling conventions.\nLowerReturn: Lower return instructions to target-specific sequences.\nEmitInstrWithCustomInserter: Handle target-specific instructions with custom insertion logic.\ngetTargetNodeName: Provide human-readable names for target nodes.\n\n\n1\n2\n3\n4\n\n\nPhases in Summary\n\n * Selection DAG Construction:\n   \n   * ISelLowering is called during this phase.\n   * This phase transforms the generic LLVM IR into a machine-independent representation (the Selection DAG).\n   * Functions in ISelLowering handle how LLVM IR operations are translated into target-specific instructions.\n\n * DAG Legalization:\n   \n   * Various transformations to ensure DAG conforms to target-specific constraints.\n   * Typically, no specific user-defined classes are involved in this phase.\n\n * Instruction Selection (ISelDAGToDAG):\n   \n   * ISelDAGToDAG is called during this phase.\n   * Translates the Selection DAG into a sequence of target-specific instructions.\n   * Functions in ISelDAGToDAG handle how DAG nodes are selected and transformed into machine instructions.\n\n * Register Allocation:\n   \n   * Assigns virtual registers to physical registers.\n   * Ensures that the generated code does not use more registers than available.\n\n * Frame Lowering:\n   \n   * Manages the function's stack frame.\n   * Sets up the stack frame, manages the frame pointer, and handles stack frame operations.\n\n * Prologue and Epilogue Emission:\n   \n   * Generates the machine code for the function's entry and exit sequences.\n   * Includes setting up the stack frame, saving/restoring callee-saved registers, etc.\n\nIn summary, ISelLowering is called during the Selection DAG Construction phase to handle translation of LLVM IR to target-specific instructions. ISelDAGToDAG is called during the Instruction Selection phase to translate the Selection DAG to a sequence of target-specific instructions. These phases work together to convert the LLVM IR into machine code for the target architecture.\n\n----------------------------------------\n\n# 2. Analysis Pass and Tranform Pass\n\nAnalysis Passes in LLVM:\n\n * Purpose:\n\n * Analysis passes in LLVM are used to gather information about the program without modifying it.\n * They analyze the program's code structure, control flow, data flow, and other properties.\n * This information is used by subsequent optimization passes to make informed decisions.\n\n * Characteristics:\n   \n   * Do not modify the program.\n   * Collect information about the program.\n   * Used by other passes to guide optimizations.\n   * Typically run early in the optimization pipeline.\n\n * Major Analysis Passes:\n   \n   * DominatorTree Analysis:\n     \n     * Computes the dominator tree for a function.\n     * Helps in various optimizations such as loop optimization, control flow analysis, etc.\n   \n   * LoopInfo Analysis:\n     \n     * Provides information about loops in a function.\n     * Used by loop optimization passes for loop transformations.\n   \n   * ScalarEvolution Analysis:\n     \n     * Analyzes and characterizes scalar expressions in loops.\n     * Helps in loop transformations like loop unrolling, loop vectorization, etc.\n   \n   * MemorySSA Analysis:\n     \n     * Provides a memory SSA representation of the program.\n     * Used in optimizations related to memory access analysis, alias analysis, etc.\n   \n   * AliasAnalysis Analysis:\n     \n     * Determines the aliasing relationship between memory accesses.\n     * Helps in optimizations that depend on memory aliasing information.\n\nTransform Passes in LLVM:\n\n * Purpose: Transform passes in LLVM modify the program's IR to improve its performance or reduce its size. They apply optimizations and transformations to the code.\n\n * Characteristics:\n   \n   * Modify the program's IR.\n   * Apply optimizations and transformations.\n   * Can introduce new code or modify existing code.\n   * Typically run after analysis passes.\n\n * Major Transform Passes:\n   \n   * Instruction Combining:\n     \n     * Combines multiple instructions into simpler forms.\n     * Reduces the number of instructions and improves code readability.\n   \n   * Dead Code Elimination:\n     \n     * Removes code that is guaranteed to have no effect on program output.\n     * Improves code size and execution speed.\n   \n   * Loop Unrolling:\n     \n     * Duplicates the loop body multiple times to reduce loop overhead.\n     * Improves instruction-level parallelism.\n   \n   * Function Inlining:\n     \n     * Replaces a function call with the body of the called function.\n     * Reduces function call overhead and enables further optimizations.\n   \n   * Constant Propagation:\n     \n     * Propagates constant values through the program.\n     * Enables further optimizations by replacing variables with constants.\n   \n   * Vectorization (Loop Vectorization):\n     \n     * Converts scalar operations in loops into SIMD (Single Instruction, Multiple Data) operations.\n     * Improves performance by exploiting parallelism in hardware.\n   \n   * SROA (Scalar Replacement of Aggregates):\n     \n     * Breaks down aggregates (like structs) into individual scalar variables.\n     * Improves optimization opportunities by working on individual scalar variables.\n\nSummary:\n\n * Analysis Passes:\n   * Gather information about the program.\n   * Do not modify the program.\n   * Used by other passes for optimizations.\n   * Examples: DominatorTree, LoopInfo, ScalarEvolution, MemorySSA, AliasAnalysis.\n * Transform Passes:\n   * Modify the program's IR.\n   * Apply optimizations and transformations.\n   * Examples: Instruction Combining, Dead Code Elimination, Loop Unrolling, Function Inlining, Constant Propagation, Vectorization, SROA (Scalar Replacement of Aggregates). These are just a few examples of major analysis and transform passes in LLVM. The LLVM infrastructure provides a wide range of passes for various optimizations and analyses, allowing users to construct custom optimization pipelines tailored to their specific needs.\n\n----------------------------------------\n\n\n# 3. Knowledge Fragments\n\n 1. SROA is claimed to replace mem2reg.\n    discourse.llvm\n\n 2. Alias Analysis\n    Alias Analysis (aka Pointer Analysis) is a class of techniques which attempt to determine whether or not two pointers ever can point to the same object in memory.\n    Traditionally, alias analyses respond to a query with a Must, May, or No alias response, indicating that two pointers always point to the same object, might point to the same object, or are known to never point to the same object.\n    \n\n 3. RISCV Instruction Set Might deserves further explore.\n    Berkely RISCV Instruction Set Manual\n    \n\n 4. How LLVM Optimizes a Function\n    How LLVM Optimizes a Function\n    This blog trace through different passes in llvm IR.\n    \n\n 5. CambridgeSlides\n    Cambridge Modern Compiler Design\n    \n    * Introduction\n    * Modern intermediate representations\n    * LLVM IR and transform pipeline\n    * Modern processor architectures\n    * Dynamic dispatch and duck typing\n    * Autovectorisation\n    * Garbage collection\n    * JIT Compilation",normalizedContent:"# 1. difference between iseldagtodag and isellowering\n\nin llvm's backend, isellowering and iseldagtodag are two important classes that play different roles in the instruction selection process. here's the difference between them and the phases in which they are called:\n\niseldagtodag\n\n * role: iseldagtodag stands for \"instruction selection dag to dag\". this class is responsible for translating the selection dag (directed acyclic graph) into a sequence of target-specific instructions.\n\n * phase: iseldagtodag is called during the \"instruction selection\" phase of the compiler backend. this phase comes after the dag legalization phase and before register allocation.\n\n * functions: in iseldagtodag, you will find functions like select, which is responsible for selecting target instructions for each node in the dag.\n\ntypical functions:\n\nselect: select target instructions for nodes in the dag.\nselectnode: implement target-specific node selection.\npreprocessiseldag: prepare for instruction selection.\npostprocessiseldag: clean up after instruction selection.\n\n\n1\n2\n3\n4\n\n\nisellowering\n\n * role: isellowering stands for \"instruction selection lowering\". this class provides target-specific information and handling for aspects of instruction selection during the selection dag construction phase.\n\n * phase: isellowering is called during the \"selection dag construction\" phase of the compiler backend. this phase transforms the generic llvm ir into a machine-independent representation, the selection dag.\n\n * functions: in isellowering, you will find functions that define how llvm ir operations are translated into target-specific instructions. this includes custom lowering of specific llvm ir operations, defining patterns for converting llvm ir to target nodes, and providing information about target-specific features and constraints.\n\ntypical functions:\n\nlowercall: lower calls to target-specific calling conventions.\nlowerreturn: lower return instructions to target-specific sequences.\nemitinstrwithcustominserter: handle target-specific instructions with custom insertion logic.\ngettargetnodename: provide human-readable names for target nodes.\n\n\n1\n2\n3\n4\n\n\nphases in summary\n\n * selection dag construction:\n   \n   * isellowering is called during this phase.\n   * this phase transforms the generic llvm ir into a machine-independent representation (the selection dag).\n   * functions in isellowering handle how llvm ir operations are translated into target-specific instructions.\n\n * dag legalization:\n   \n   * various transformations to ensure dag conforms to target-specific constraints.\n   * typically, no specific user-defined classes are involved in this phase.\n\n * instruction selection (iseldagtodag):\n   \n   * iseldagtodag is called during this phase.\n   * translates the selection dag into a sequence of target-specific instructions.\n   * functions in iseldagtodag handle how dag nodes are selected and transformed into machine instructions.\n\n * register allocation:\n   \n   * assigns virtual registers to physical registers.\n   * ensures that the generated code does not use more registers than available.\n\n * frame lowering:\n   \n   * manages the function's stack frame.\n   * sets up the stack frame, manages the frame pointer, and handles stack frame operations.\n\n * prologue and epilogue emission:\n   \n   * generates the machine code for the function's entry and exit sequences.\n   * includes setting up the stack frame, saving/restoring callee-saved registers, etc.\n\nin summary, isellowering is called during the selection dag construction phase to handle translation of llvm ir to target-specific instructions. iseldagtodag is called during the instruction selection phase to translate the selection dag to a sequence of target-specific instructions. these phases work together to convert the llvm ir into machine code for the target architecture.\n\n----------------------------------------\n\n# 2. analysis pass and tranform pass\n\nanalysis passes in llvm:\n\n * purpose:\n\n * analysis passes in llvm are used to gather information about the program without modifying it.\n * they analyze the program's code structure, control flow, data flow, and other properties.\n * this information is used by subsequent optimization passes to make informed decisions.\n\n * characteristics:\n   \n   * do not modify the program.\n   * collect information about the program.\n   * used by other passes to guide optimizations.\n   * typically run early in the optimization pipeline.\n\n * major analysis passes:\n   \n   * dominatortree analysis:\n     \n     * computes the dominator tree for a function.\n     * helps in various optimizations such as loop optimization, control flow analysis, etc.\n   \n   * loopinfo analysis:\n     \n     * provides information about loops in a function.\n     * used by loop optimization passes for loop transformations.\n   \n   * scalarevolution analysis:\n     \n     * analyzes and characterizes scalar expressions in loops.\n     * helps in loop transformations like loop unrolling, loop vectorization, etc.\n   \n   * memoryssa analysis:\n     \n     * provides a memory ssa representation of the program.\n     * used in optimizations related to memory access analysis, alias analysis, etc.\n   \n   * aliasanalysis analysis:\n     \n     * determines the aliasing relationship between memory accesses.\n     * helps in optimizations that depend on memory aliasing information.\n\ntransform passes in llvm:\n\n * purpose: transform passes in llvm modify the program's ir to improve its performance or reduce its size. they apply optimizations and transformations to the code.\n\n * characteristics:\n   \n   * modify the program's ir.\n   * apply optimizations and transformations.\n   * can introduce new code or modify existing code.\n   * typically run after analysis passes.\n\n * major transform passes:\n   \n   * instruction combining:\n     \n     * combines multiple instructions into simpler forms.\n     * reduces the number of instructions and improves code readability.\n   \n   * dead code elimination:\n     \n     * removes code that is guaranteed to have no effect on program output.\n     * improves code size and execution speed.\n   \n   * loop unrolling:\n     \n     * duplicates the loop body multiple times to reduce loop overhead.\n     * improves instruction-level parallelism.\n   \n   * function inlining:\n     \n     * replaces a function call with the body of the called function.\n     * reduces function call overhead and enables further optimizations.\n   \n   * constant propagation:\n     \n     * propagates constant values through the program.\n     * enables further optimizations by replacing variables with constants.\n   \n   * vectorization (loop vectorization):\n     \n     * converts scalar operations in loops into simd (single instruction, multiple data) operations.\n     * improves performance by exploiting parallelism in hardware.\n   \n   * sroa (scalar replacement of aggregates):\n     \n     * breaks down aggregates (like structs) into individual scalar variables.\n     * improves optimization opportunities by working on individual scalar variables.\n\nsummary:\n\n * analysis passes:\n   * gather information about the program.\n   * do not modify the program.\n   * used by other passes for optimizations.\n   * examples: dominatortree, loopinfo, scalarevolution, memoryssa, aliasanalysis.\n * transform passes:\n   * modify the program's ir.\n   * apply optimizations and transformations.\n   * examples: instruction combining, dead code elimination, loop unrolling, function inlining, constant propagation, vectorization, sroa (scalar replacement of aggregates). these are just a few examples of major analysis and transform passes in llvm. the llvm infrastructure provides a wide range of passes for various optimizations and analyses, allowing users to construct custom optimization pipelines tailored to their specific needs.\n\n----------------------------------------\n\n\n# 3. knowledge fragments\n\n 1. sroa is claimed to replace mem2reg.\n    discourse.llvm\n\n 2. alias analysis\n    alias analysis (aka pointer analysis) is a class of techniques which attempt to determine whether or not two pointers ever can point to the same object in memory.\n    traditionally, alias analyses respond to a query with a must, may, or no alias response, indicating that two pointers always point to the same object, might point to the same object, or are known to never point to the same object.\n    \n\n 3. riscv instruction set might deserves further explore.\n    berkely riscv instruction set manual\n    \n\n 4. how llvm optimizes a function\n    how llvm optimizes a function\n    this blog trace through different passes in llvm ir.\n    \n\n 5. cambridgeslides\n    cambridge modern compiler design\n    \n    * introduction\n    * modern intermediate representations\n    * llvm ir and transform pipeline\n    * modern processor architectures\n    * dynamic dispatch and duck typing\n    * autovectorisation\n    * garbage collection\n    * jit compilation",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:'Add New Instruction "ACE" to LLVM',frontmatter:{title:'Add New Instruction "ACE" to LLVM',date:"2023-11-21T00:00:00.000Z",permalink:"/pages/000005/"},regularPath:"/02.compiler/05.%20addInstACE.html",relativePath:"02.compiler/05. addInstACE.md",key:"v-49e4dae0",path:"/pages/000005/",headers:[{level:3,title:"1. Add Register Class in RISCV",slug:"_1-add-register-class-in-riscv",normalizedTitle:"1. add register class in riscv",charIndex:2}],headersStr:"1. Add Register Class in RISCV",content:'# 1. Add Register Class in RISCV\n\n// RegACE - 4-bit register for RISC-V ACE inst\nclass RISCVRegACE<bits<16> Enc, string n, list<string> alt = []> : Register<n> {\n  let HWEncoding = Enc;\n  let AltNames = alt;\n}\n\n// Define the ACE registers\nlet RegAltNameIndices = [ABIRegAltName] in {\n  //foreach Index = !range(0, 16, 1) in {\n  //  def ACE#Index : RISCVRegACE<Index, "ace_reg_"#Index, ["ace_reg_"#Index]>, DwarfRegNum<[!add(Index, 128)]>;\n  //}\n  def ACE0  : RISCVRegACE<0, "ace_reg_0", ["ace_reg_0"]>, DwarfRegNum<[128]>;\n  def ACE1  : RISCVRegACE<1, "ace_reg_1", ["ace_reg_1"]>, DwarfRegNum<[129]>;\n  def ACE2  : RISCVRegACE<2, "ace_reg_2", ["ace_reg_2"]>, DwarfRegNum<[130]>;\n  def ACE3  : RISCVRegACE<3, "ace_reg_3", ["ace_reg_3"]>, DwarfRegNum<[131]>;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } GPR:{ *:[i32] m1:[i32 i64] }:$src)\nIncluded from /home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCV.td:30:\n/home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.td:1915:1: error: In anonymous_8306: Could not infer all types in pattern!\ndef : Pat<(trunc GPR:$src), (COPY GPR:$src)>;\n^\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } GPR:{ *:[i32] m1:[i32 i64] }:$src)\nanonymous_8306: \t(COPY:{ *:[i16] m1:[i16 i32] } GPR:{ *:[i32] m1:[i32 i64] }:$src)\nIncluded from /home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCV.td:30:\n/home/qishao/Project/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.td:1915:1: error: In anonymous_8306: Could not infer all types in pattern result!\ndef : Pat<(trunc GPR:$src), (COPY GPR:$src)>;\n^\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nThe failed issues after I def ACE Register Class\n\ndef ACE : RegisterClass<"RISCV", [i16], 16, (add\n    (sequence "ACE%u", 0, 3)\n)>;\n\n\n\n1\n2\n3\n4\n',normalizedContent:'# 1. add register class in riscv\n\n// regace - 4-bit register for risc-v ace inst\nclass riscvregace<bits<16> enc, string n, list<string> alt = []> : register<n> {\n  let hwencoding = enc;\n  let altnames = alt;\n}\n\n// define the ace registers\nlet regaltnameindices = [abiregaltname] in {\n  //foreach index = !range(0, 16, 1) in {\n  //  def ace#index : riscvregace<index, "ace_reg_"#index, ["ace_reg_"#index]>, dwarfregnum<[!add(index, 128)]>;\n  //}\n  def ace0  : riscvregace<0, "ace_reg_0", ["ace_reg_0"]>, dwarfregnum<[128]>;\n  def ace1  : riscvregace<1, "ace_reg_1", ["ace_reg_1"]>, dwarfregnum<[129]>;\n  def ace2  : riscvregace<2, "ace_reg_2", ["ace_reg_2"]>, dwarfregnum<[130]>;\n  def ace3  : riscvregace<3, "ace_reg_3", ["ace_reg_3"]>, dwarfregnum<[131]>;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } gpr:{ *:[i32] m1:[i32 i64] }:$src)\nincluded from /home/qishao/project/llvm-project/llvm/lib/target/riscv/riscv.td:30:\n/home/qishao/project/llvm-project/llvm/lib/target/riscv/riscvinstrinfo.td:1915:1: error: in anonymous_8306: could not infer all types in pattern!\ndef : pat<(trunc gpr:$src), (copy gpr:$src)>;\n^\nanonymous_8306: \t(trunc:{ *:[i16] m1:[i16 i32] } gpr:{ *:[i32] m1:[i32 i64] }:$src)\nanonymous_8306: \t(copy:{ *:[i16] m1:[i16 i32] } gpr:{ *:[i32] m1:[i32 i64] }:$src)\nincluded from /home/qishao/project/llvm-project/llvm/lib/target/riscv/riscv.td:30:\n/home/qishao/project/llvm-project/llvm/lib/target/riscv/riscvinstrinfo.td:1915:1: error: in anonymous_8306: could not infer all types in pattern result!\ndef : pat<(trunc gpr:$src), (copy gpr:$src)>;\n^\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nthe failed issues after i def ace register class\n\ndef ace : registerclass<"riscv", [i16], 16, (add\n    (sequence "ace%u", 0, 3)\n)>;\n\n\n\n1\n2\n3\n4\n',charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"How does LLVM perform instruction combine",frontmatter:{title:"How does LLVM perform instruction combine",output:{html_document:{code_folding:"hide"}},date:"2024-03-17T00:00:00.000Z",permalink:"/pages/000006/"},regularPath:"/02.compiler/06.Value&Use.html",relativePath:"02.compiler/06.Value&Use.md",key:"v-c73e5956",path:"/pages/000006/",headers:[{level:3,title:"1. CodeFile",slug:"_1-codefile",normalizedTitle:"1. codefile",charIndex:2},{level:3,title:"2. Values & User",slug:"_2-values-user",normalizedTitle:"2. values &amp; user",charIndex:null},{level:3,title:"3. Q & A in Stackoverflow",slug:"_3-q-a-in-stackoverflow",normalizedTitle:"3. q &amp; a in stackoverflow",charIndex:null},{level:3,title:"Reference",slug:"reference",normalizedTitle:"reference",charIndex:7957}],headersStr:"1. CodeFile 2. Values & User 3. Q & A in Stackoverflow Reference",content:"# 1. CodeFile\n\nlib/Target/Mips/MipsISelLowering.cpp\n\nAll DAG Combine is called here.\n\nSDValue  MipsTargetLowering::PerformDAGCombine(SDNode *N, DAGCombinerInfo &DCI)\n  const {\n  SelectionDAG &DAG = DCI.DAG;\n  unsigned Opc = N->getOpcode();\n\n  switch (Opc) {\n  default: break;\n  case ISD::SDIVREM:\n  case ISD::UDIVREM:\n    return performDivRemCombine(N, DAG, DCI, Subtarget);\n  case ISD::SELECT:\n    return performSELECTCombine(N, DAG, DCI, Subtarget);\n  case MipsISD::CMovFP_F:\n  case MipsISD::CMovFP_T:\n    return performCMovFPCombine(N, DAG, DCI, Subtarget);\n  case ISD::AND:\n    return performANDCombine(N, DAG, DCI, Subtarget);\n  case ISD::OR:\n    return performORCombine(N, DAG, DCI, Subtarget);\n  case ISD::ADD:\n    return performADDCombine(N, DAG, DCI, Subtarget);\n  case ISD::SHL:\n    return performSHLCombine(N, DAG, DCI, Subtarget);\n  case ISD::SUB:\n    return performSUBCombine(N, DAG, DCI, Subtarget);\n  }\n\n  return SDValue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\nDefine of performSUBCombine\n\n * we can look into the process.\n * ISD::SUB will call performSUBCombine\n * It will combine (sub v0 (mul v1, v2)) into (msub v1, v2, v0)\n * The intresting thing is that, current SDNode Opcode is sub and if precedent node is mul, it will combine it into msub.\n\nHow to identify the precedent node?\n\n 1. SDValue Mult = ROOTNode->getOperand(1); // multi SDValue\n 2. SDValue AddOperand = ROOTNode->getOperand(0); // add SDValue\n 3. how about previous instruction?\n 4. ROOTNode->getOperand(0) will point to previous instruction\n\nstatic SDValue performSUBCombine(SDNode *N, SelectionDAG &DAG,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 const MipsSubtarget &Subtarget) {\n  // (sub v0 (mul v1, v2)) => (msub v1, v2, v0)\n  if (DCI.isBeforeLegalizeOps()) {\n    if (Subtarget.hasMips32() && !Subtarget.hasMips32r6() &&\n        !Subtarget.inMips16Mode() && N->getValueType(0) == MVT::i64)\n      return performMADD_MSUBCombine(N, DAG, Subtarget);\n\n    return SDValue();\n  }\n\n  return SDValue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nstatic SDValue performMADD_MSUBCombine(SDNode *ROOTNode, SelectionDAG &CurDAG,\n                                       const MipsSubtarget &Subtarget) {\n  SDValue Mult = ROOTNode->getOperand(0).getOpcode() == ISD::MUL\n                     ? ROOTNode->getOperand(0)\n                     : ROOTNode->getOperand(1);\n\n  SDValue AddOperand = ROOTNode->getOperand(0).getOpcode() == ISD::MUL\n                     ? ROOTNode->getOperand(1)\n                     : ROOTNode->getOperand(0);\n\n  // Transform this to a MADD only if the user of this node is the add.\n  // If there are other users of the mul, this function returns here.\n  if (!Mult.hasOneUse())\n    return SDValue();\n\n  // maddu and madd are unusual instructions in that on MIPS64 bits 63..31\n  // must be in canonical form, i.e. sign extended. For MIPS32, the operands\n  // of the multiply must have 32 or more sign bits, otherwise we cannot\n  // perform this optimization. We have to check this here as we're performing\n  // this optimization pre-legalization.\n  SDValue MultLHS = Mult->getOperand(0);\n  SDValue MultRHS = Mult->getOperand(1);\n\n  bool IsSigned = MultLHS->getOpcode() == ISD::SIGN_EXTEND &&\n                  MultRHS->getOpcode() == ISD::SIGN_EXTEND;\n  bool IsUnsigned = MultLHS->getOpcode() == ISD::ZERO_EXTEND &&\n                    MultRHS->getOpcode() == ISD::ZERO_EXTEND;\n\n  if (!IsSigned && !IsUnsigned)\n    return SDValue();\n\n  // Initialize accumulator.\n  SDLoc DL(ROOTNode);\n  SDValue TopHalf;\n  SDValue BottomHalf;\n  BottomHalf = CurDAG.getNode(ISD::EXTRACT_ELEMENT, DL, MVT::i32, AddOperand,\n                              CurDAG.getIntPtrConstant(0, DL));\n\n  TopHalf = CurDAG.getNode(ISD::EXTRACT_ELEMENT, DL, MVT::i32, AddOperand,\n                           CurDAG.getIntPtrConstant(1, DL));\n  SDValue ACCIn = CurDAG.getNode(MipsISD::MTLOHI, DL, MVT::Untyped,\n                                  BottomHalf,\n                                  TopHalf);\n\n  // Create MipsMAdd(u) / MipsMSub(u) node.\n  bool IsAdd = ROOTNode->getOpcode() == ISD::ADD;\n  unsigned Opcode = IsAdd ? (IsUnsigned ? MipsISD::MAddu : MipsISD::MAdd)\n                          : (IsUnsigned ? MipsISD::MSubu : MipsISD::MSub);\n  SDValue MAddOps[3] = {\n      CurDAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Mult->getOperand(0)),\n      CurDAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Mult->getOperand(1)), ACCIn};\n  EVT VTs[2] = {MVT::i32, MVT::i32};\n  SDValue MAdd = CurDAG.getNode(Opcode, DL, VTs, MAddOps);\n\n  SDValue ResLo = CurDAG.getNode(MipsISD::MFLO, DL, MVT::i32, MAdd);\n  SDValue ResHi = CurDAG.getNode(MipsISD::MFHI, DL, MVT::i32, MAdd);\n  SDValue Combined =\n      CurDAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64, ResLo, ResHi);\n  return Combined;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n# 2. Values & User\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 3. Q & A in Stackoverflow\n\nQ&A\nSince Instruction is derived from Value it inherits both functions users and uses. The difference is that a user of Value has the Value as one of its operands.\n\nWhen you are calling uses you get a list of all Use instances holding a reference from the Value to each of the users of the particular Value. Calling users gives you a list of User directly. The following code shows how to use users and uses.\n\nfor(auto U : V->users()){  // U is of type User*\n     if (auto I = dyn_cast<Instruction>(U)){\n        // an instruction uses V\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\nYou can see users as a shortcut because you can do the same with uses:\n\nfor(auto U : V->uses()){  // U is of type Use*\n     if (auto I = dyn_cast<Instruction>(U.getUser())){\n        // an instruction uses V\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\nCommonly it is enough to use users to get all dependencies of a Value.\n\nAll Values used by a Value are the operands. This direction of dependency is not part of a Value's use list.\n\nWe have still not presented the most powerful aspect of the LLVM IR (enabled by the SSA form): the Value and User interfaces; these allow you to easily navigate the use-def and def-use chains. In the LLVM in-memory IR, a class that inherits from Value means that it defines a result that can be used by others, whereas a subclass of User means that this entity uses one or more Value interfaces. Function and Instruction are subclasses of both Value and User, while BasicBlock is a subclass of just Value. To understand this, let's analyze these two classes in depth:\n\n• The Value class defines the use_begin() and use_end() methods to allow you to iterate through Users, offering an easy way to access its def-use chain. For every Value class, you can also access its name through the getName() method. This models the fact that any LLVM value can have a distinct identifier associated with it. For example, %add1 can identify the result of an add instruction, BB1 can identify a basic block, and myfunc can identify a function. Value also has a powerful method called replaceAllUsesWith(Value *), which navigates through all of the users of this value and replaces it with some other value. This is a good example of how the SSA form allows you to easily substitute instructions and write fast optimizations. You can view the full interface at LLVM Value Class.\n\n• The User class has the op_begin() and op_end() methods that allows you to quickly access all of the Value interfaces that it uses. Note that this represents the use-def chain. You can also use a helper method called replaceUsesOfWith(Value *From, Value *To) to replace any of its used values. You can view the full interface at LLVM User Class.\n\nFor short, use_begin() iterator points to users. and op_begin() points to operand values. but the value is the basic class of instruction. By Refer to a value, you can get the producer's instructin.\n\n\n# Reference\n\nHow to Write an LLVM Backend #4: Instruction Selection\n\nMore on the LLVM Compiler\n\nIntroduction to LLVM (II)\n\n深入浅出 LLVM之 Value 、User 、Use 源码解析",normalizedContent:"# 1. codefile\n\nlib/target/mips/mipsisellowering.cpp\n\nall dag combine is called here.\n\nsdvalue  mipstargetlowering::performdagcombine(sdnode *n, dagcombinerinfo &dci)\n  const {\n  selectiondag &dag = dci.dag;\n  unsigned opc = n->getopcode();\n\n  switch (opc) {\n  default: break;\n  case isd::sdivrem:\n  case isd::udivrem:\n    return performdivremcombine(n, dag, dci, subtarget);\n  case isd::select:\n    return performselectcombine(n, dag, dci, subtarget);\n  case mipsisd::cmovfp_f:\n  case mipsisd::cmovfp_t:\n    return performcmovfpcombine(n, dag, dci, subtarget);\n  case isd::and:\n    return performandcombine(n, dag, dci, subtarget);\n  case isd::or:\n    return performorcombine(n, dag, dci, subtarget);\n  case isd::add:\n    return performaddcombine(n, dag, dci, subtarget);\n  case isd::shl:\n    return performshlcombine(n, dag, dci, subtarget);\n  case isd::sub:\n    return performsubcombine(n, dag, dci, subtarget);\n  }\n\n  return sdvalue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\ndefine of performsubcombine\n\n * we can look into the process.\n * isd::sub will call performsubcombine\n * it will combine (sub v0 (mul v1, v2)) into (msub v1, v2, v0)\n * the intresting thing is that, current sdnode opcode is sub and if precedent node is mul, it will combine it into msub.\n\nhow to identify the precedent node?\n\n 1. sdvalue mult = rootnode->getoperand(1); // multi sdvalue\n 2. sdvalue addoperand = rootnode->getoperand(0); // add sdvalue\n 3. how about previous instruction?\n 4. rootnode->getoperand(0) will point to previous instruction\n\nstatic sdvalue performsubcombine(sdnode *n, selectiondag &dag,\n                                 targetlowering::dagcombinerinfo &dci,\n                                 const mipssubtarget &subtarget) {\n  // (sub v0 (mul v1, v2)) => (msub v1, v2, v0)\n  if (dci.isbeforelegalizeops()) {\n    if (subtarget.hasmips32() && !subtarget.hasmips32r6() &&\n        !subtarget.inmips16mode() && n->getvaluetype(0) == mvt::i64)\n      return performmadd_msubcombine(n, dag, subtarget);\n\n    return sdvalue();\n  }\n\n  return sdvalue();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nstatic sdvalue performmadd_msubcombine(sdnode *rootnode, selectiondag &curdag,\n                                       const mipssubtarget &subtarget) {\n  sdvalue mult = rootnode->getoperand(0).getopcode() == isd::mul\n                     ? rootnode->getoperand(0)\n                     : rootnode->getoperand(1);\n\n  sdvalue addoperand = rootnode->getoperand(0).getopcode() == isd::mul\n                     ? rootnode->getoperand(1)\n                     : rootnode->getoperand(0);\n\n  // transform this to a madd only if the user of this node is the add.\n  // if there are other users of the mul, this function returns here.\n  if (!mult.hasoneuse())\n    return sdvalue();\n\n  // maddu and madd are unusual instructions in that on mips64 bits 63..31\n  // must be in canonical form, i.e. sign extended. for mips32, the operands\n  // of the multiply must have 32 or more sign bits, otherwise we cannot\n  // perform this optimization. we have to check this here as we're performing\n  // this optimization pre-legalization.\n  sdvalue multlhs = mult->getoperand(0);\n  sdvalue multrhs = mult->getoperand(1);\n\n  bool issigned = multlhs->getopcode() == isd::sign_extend &&\n                  multrhs->getopcode() == isd::sign_extend;\n  bool isunsigned = multlhs->getopcode() == isd::zero_extend &&\n                    multrhs->getopcode() == isd::zero_extend;\n\n  if (!issigned && !isunsigned)\n    return sdvalue();\n\n  // initialize accumulator.\n  sdloc dl(rootnode);\n  sdvalue tophalf;\n  sdvalue bottomhalf;\n  bottomhalf = curdag.getnode(isd::extract_element, dl, mvt::i32, addoperand,\n                              curdag.getintptrconstant(0, dl));\n\n  tophalf = curdag.getnode(isd::extract_element, dl, mvt::i32, addoperand,\n                           curdag.getintptrconstant(1, dl));\n  sdvalue accin = curdag.getnode(mipsisd::mtlohi, dl, mvt::untyped,\n                                  bottomhalf,\n                                  tophalf);\n\n  // create mipsmadd(u) / mipsmsub(u) node.\n  bool isadd = rootnode->getopcode() == isd::add;\n  unsigned opcode = isadd ? (isunsigned ? mipsisd::maddu : mipsisd::madd)\n                          : (isunsigned ? mipsisd::msubu : mipsisd::msub);\n  sdvalue maddops[3] = {\n      curdag.getnode(isd::truncate, dl, mvt::i32, mult->getoperand(0)),\n      curdag.getnode(isd::truncate, dl, mvt::i32, mult->getoperand(1)), accin};\n  evt vts[2] = {mvt::i32, mvt::i32};\n  sdvalue madd = curdag.getnode(opcode, dl, vts, maddops);\n\n  sdvalue reslo = curdag.getnode(mipsisd::mflo, dl, mvt::i32, madd);\n  sdvalue reshi = curdag.getnode(mipsisd::mfhi, dl, mvt::i32, madd);\n  sdvalue combined =\n      curdag.getnode(isd::build_pair, dl, mvt::i64, reslo, reshi);\n  return combined;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n\n# 2. values & user\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 3. q & a in stackoverflow\n\nq&a\nsince instruction is derived from value it inherits both functions users and uses. the difference is that a user of value has the value as one of its operands.\n\nwhen you are calling uses you get a list of all use instances holding a reference from the value to each of the users of the particular value. calling users gives you a list of user directly. the following code shows how to use users and uses.\n\nfor(auto u : v->users()){  // u is of type user*\n     if (auto i = dyn_cast<instruction>(u)){\n        // an instruction uses v\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\nyou can see users as a shortcut because you can do the same with uses:\n\nfor(auto u : v->uses()){  // u is of type use*\n     if (auto i = dyn_cast<instruction>(u.getuser())){\n        // an instruction uses v\n     }\n}\n\n\n1\n2\n3\n4\n5\n\n\ncommonly it is enough to use users to get all dependencies of a value.\n\nall values used by a value are the operands. this direction of dependency is not part of a value's use list.\n\nwe have still not presented the most powerful aspect of the llvm ir (enabled by the ssa form): the value and user interfaces; these allow you to easily navigate the use-def and def-use chains. in the llvm in-memory ir, a class that inherits from value means that it defines a result that can be used by others, whereas a subclass of user means that this entity uses one or more value interfaces. function and instruction are subclasses of both value and user, while basicblock is a subclass of just value. to understand this, let's analyze these two classes in depth:\n\n• the value class defines the use_begin() and use_end() methods to allow you to iterate through users, offering an easy way to access its def-use chain. for every value class, you can also access its name through the getname() method. this models the fact that any llvm value can have a distinct identifier associated with it. for example, %add1 can identify the result of an add instruction, bb1 can identify a basic block, and myfunc can identify a function. value also has a powerful method called replacealluseswith(value *), which navigates through all of the users of this value and replaces it with some other value. this is a good example of how the ssa form allows you to easily substitute instructions and write fast optimizations. you can view the full interface at llvm value class.\n\n• the user class has the op_begin() and op_end() methods that allows you to quickly access all of the value interfaces that it uses. note that this represents the use-def chain. you can also use a helper method called replaceusesofwith(value *from, value *to) to replace any of its used values. you can view the full interface at llvm user class.\n\nfor short, use_begin() iterator points to users. and op_begin() points to operand values. but the value is the basic class of instruction. by refer to a value, you can get the producer's instructin.\n\n\n# reference\n\nhow to write an llvm backend #4: instruction selection\n\nmore on the llvm compiler\n\nintroduction to llvm (ii)\n\n深入浅出 llvm之 value 、user 、use 源码解析",charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"Understand llvm with its source code Part 1",frontmatter:{title:"Understand llvm with its source code Part 1",output:{html_document:{code_folding:"hide"}},date:"2024-07-05T00:00:00.000Z",permalink:"/pages/000007/"},regularPath:"/02.compiler/07.%20UnderstaningLLVMwithSourceCode.html",relativePath:"02.compiler/07. UnderstaningLLVMwithSourceCode.md",key:"v-89581d5c",path:"/pages/000007/",headers:[{level:3,title:"1. Create SelectionDAG",slug:"_1-create-selectiondag",normalizedTitle:"1. create selectiondag",charIndex:2},{level:3,title:"2. Legalization",slug:"_2-legalization",normalizedTitle:"2. legalization",charIndex:1458},{level:3,title:"3. Instruction Selection",slug:"_3-instruction-selection",normalizedTitle:"3. instruction selection",charIndex:5867},{level:3,title:"4. Instruction Scheduler",slug:"_4-instruction-scheduler",normalizedTitle:"4. instruction scheduler",charIndex:10907}],headersStr:"1. Create SelectionDAG 2. Legalization 3. Instruction Selection 4. Instruction Scheduler",content:'# 1. Create SelectionDAG\n\nSelectionDAG Builder calls visit() function to build SDNode\n\n// CodeGen/SelectionDAG/SelectionDAGBuilder.cpp\nvoid SelectionDAGBuilder::visit(unsigned Opcode, const User &I) {\n  // Note: this doesn\'t use InstVisitor, because it has to work with\n  // ConstantExpr\'s in addition to instructions.\n  switch (Opcode) {\n  default: llvm_unreachable("Unknown instruction type encountered!");\n    // Build the switch statement using the Instruction.def file.\n#define HANDLE_INST(NUM, OPCODE, CLASS) \\\n    case Instruction::OPCODE: visit##OPCODE((const CLASS&)I); break;\n#include "llvm/IR/Instruction.def"\n  }\n}\n\n// include/llvm/IR/Instruction.def\nHANDLE_BINARY_INST(20, SDiv , BinaryOperator)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nvisitSDiv(const User &I) will create SDValue as operand and SDNode for each IR\n\nvoid SelectionDAGBuilder::visitSDiv(const User &I) {\n  SDValue Op1 = getValue(I.getOperand(0));\n  SDValue Op2 = getValue(I.getOperand(1));\n\n  SDNodeFlags Flags;\n  Flags.setExact(isa<PossiblyExactOperator>(&I) &&\n                 cast<PossiblyExactOperator>(&I)->isExact());\n  setValue(&I, DAG.getNode(ISD::SDIV, getCurSDLoc(), Op1.getValueType(), Op1,\n                           Op2, Flags));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nSDNode also contains dependencies in SDValue, SDValue include following:\n\n 1. data dependency\n 2. chain dependency. For example. order load, store insturction to the same address\n 3. glue of instructions.\n\n\n# 2. Legalization\n\nLegalization will legalize SDNode operation that is unsupported target into supported Node. It includes legalization of operation and operand. As to operation, it includes 3 major operation:\n\n 1. Expansion expand one op into series of op\n 2. Promotion promote data type\n 3. Custom\n\n// CodeGen/SelectionDAG/LegalizeDAG.cpp\n\n/// Return a legal replacement for the given operation, with all legal operands.\nvoid SelectionDAGLegalize::LegalizeOp(SDNode *Node) {\n.....\n    case TargetLowering::Expand:\n      if (ExpandNode(Node))\n        return;\n      [[fallthrough]];\n    case TargetLowering::LibCall:\n      ConvertNodeToLibcall(Node);\n      return;\n    case TargetLowering::Promote:\n      PromoteNode(Node);\n      return;\n    }\n\n  switch (Node->getOpcode()) {\n  case ISD::LOAD:\n    return LegalizeLoadOps(Node);\n  case ISD::STORE:\n    return LegalizeStoreOps(Node);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nLook closely to legalize of stop operation, it considered Expand, Custom and Promote.\n\nvoid SelectionDAGLegalize::LegalizeStoreOps(SDNode *Node) {\n    SDValue Value = ST->getValue();\n    MVT VT = Value.getSimpleValueType();\n    switch (TLI.getOperationAction(ISD::STORE, VT)) {\n    case TargetLowering::Legal: {\n      // If this is an unaligned store and the target doesn\'t support it, expand it.\n      EVT MemVT = ST->getMemoryVT();\n      const DataLayout &DL = DAG.getDataLayout();\n      if (!TLI.allowsMemoryAccessForAlignment(*DAG.getContext(), DL, MemVT,\n                                              *ST->getMemOperand())) {\n        SDValue Result = TLI.expandUnalignedStore(ST, DAG);\n        ReplaceNode(SDValue(ST, 0), Result);\n      }\n      break;\n    }\n    case TargetLowering::Custom: {\n      SDValue Res = TLI.LowerOperation(SDValue(Node, 0), DAG);\n      if (Res && Res != SDValue(Node, 0))\n        ReplaceNode(SDValue(Node, 0), Res);\n      return;\n    }\n    case TargetLowering::Promote: {\n      MVT NVT = TLI.getTypeToPromoteTo(ISD::STORE, VT);\n      Value = DAG.getNode(ISD::BITCAST, dl, NVT, Value);\n      SDValue Result = DAG.getStore(Chain, dl, Value, Ptr, ST->getPointerInfo(),\n                                    ST->getOriginalAlign(), MMOFlags, AAInfo);\n      ReplaceNode(SDValue(Node, 0), Result);\n      break;\n    }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\nIt also includes LibCall now.\n\n// include/llvm/CodeGen/TargetLowering.h\nclass TargetLoweringBase {\npublic:\n  /// This enum indicates whether operations are valid for a target, and if not,\n  /// what action should be used to make them valid.\n  enum LegalizeAction : uint8_t {\n    Legal,      // The target natively supports this operation.\n    Promote,    // This operation should be executed in a larger type.\n    Expand,     // Try to expand this to other ops, otherwise use a libcall.\n    LibCall,    // Don\'t try to expand this to other ops, always use a libcall.\n    Custom      // Use the LowerOperation hook to implement custom lowering.\n  };\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nAs to legalize operand, in ./CodeGen/SelectionDAG/LegalizeTypes.h, it shows supported functions.\n\n  //===--------------------------------------------------------------------===//\n  // Integer Promotion Support: LegalizeIntegerTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Integer Expansion Support: LegalizeIntegerTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Float to Integer Conversion Support: LegalizeFloatTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Float Expansion Support: LegalizeFloatTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Scalarization Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Vector Splitting Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Vector Widening Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n  // Vector Widening Utilities Support: LegalizeVectorTypes.cpp\n  //===--------------------------------------------------------------------===//\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 3. Instruction Selection\n\nCodeGen/SelectionDAG/SelectionDAGISel.cpp\nvoid SelectionDAGISel::CodeGenAndEmitDAG() {\n  // Pre-type legalization allow creation of any node types.\n  CurDAG->NewNodesMustHaveLegalTypes = false;\n  // Run the DAG combiner in pre-legalize mode.\n  CurDAG->Combine(BeforeLegalizeTypes, AA, OptLevel);\n  // Second step, hack on the DAG until it only uses operations and types that\n  // the target supports.\n  Changed = CurDAG->LegalizeTypes();\n\n  // Only allow creation of legal node types.\n  CurDAG->NewNodesMustHaveLegalTypes = true;\n   // Run the DAG combiner in post-type-legalize mode.\n  NamedRegionTimer T("combine_lt", "DAG Combining after legalize types",\n                         GroupName, GroupDescription, TimePassesIsEnabled);\n  CurDAG->Combine(AfterLegalizeTypes, AA, OptLevel);\n\n  Changed = CurDAG->LegalizeVectors();\n  // Run the DAG combiner in post-type-legalize mode.\n  NamedRegionTimer T("combine_lv", "DAG Combining after legalize vectors",\n                         GroupName, GroupDescription, TimePassesIsEnabled);\n  CurDAG->Combine(AfterLegalizeVectorOps, AA, OptLevel);\n\n  CurDAG->Legalize();\n\n  // Run the DAG combiner in post-legalize mode.\n  NamedRegionTimer T("combine2", "DAG Combining 2", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n  CurDAG->Combine(AfterLegalizeDAG, AA, OptLevel);\n  \n  ComputeLiveOutVRegInfo();\n\n  DoInstructionSelection();\n\n  // Schedule machine code.\n  ScheduleDAGSDNodes *Scheduler = CreateScheduler();\n  {\n    NamedRegionTimer T("sched", "Instruction Scheduling", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n    Scheduler->Run(CurDAG, FuncInfo->MBB);\n  }\n\n  // Emit machine code to BB.  This can change \'BB\' to the last block being\n  // inserted into.\n  MachineBasicBlock *FirstMBB = FuncInfo->MBB, *LastMBB;\n  {\n    NamedRegionTimer T("emit", "Instruction Creation", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n\n    // FuncInfo->InsertPt is passed by reference and set to the end of the\n    // scheduled instructions.\n    LastMBB = FuncInfo->MBB = Scheduler->EmitSchedule(FuncInfo->InsertPt);\n  }\n\n  // If the block was split, make sure we update any references that are used to\n  // update PHI nodes later on.\n  if (FirstMBB != LastMBB)\n    SDB->UpdateSplitBlock(FirstMBB, LastMBB);\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n//File include/llvm/CodeGen/SelectionDAGISel.h\n// Main hook for targets to transform nodes into machine nodes.\nvirtual void Select(SDNode *N) = 0;\n\n//File CodeGen/SelectionDAG/SelectionDAGISel.cpp\nvoid SelectionDAGISel::DoInstructionSelection() {\n  LLVM_DEBUG(dbgs() << "===== Instruction selection begins: "\n                    << printMBBReference(*FuncInfo->MBB) << " \'"\n                    << FuncInfo->MBB->getName() << "\'\\n");\n  PreprocessISelDAG();\n\n  // Select target instructions for the DAG.\n  // Number all nodes with a topological order and set DAGSize.\n  DAGSize = CurDAG->AssignTopologicalOrder();\n\n  // The AllNodes list is now topological-sorted. Visit the\n  // nodes by starting at the end of the list (the root of the\n  // graph) and preceding back toward the beginning (the entry\n  // node).\n  while (ISelPosition != CurDAG->allnodes_begin()) {\n    SDNode *Node = &*--ISelPosition;\n    ...\n    \n    Select(Node);\n  }\n\n  CurDAG->setRoot(Dummy.getValue());\n  \n  LLVM_DEBUG(dbgs() << "\\n===== Instruction selection ends:\\n");\n  PostprocessISelDAG();\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nSelect(SDNode *N) will be overide by target backend:\n\n//File Target/MSP430/MSP430ISelDAGToDAG.cpp\n\n  #include "MSP430GenDAGISel.inc"\nvoid MSP430DAGToDAGISel::Select(SDNode *Node) {\n  // Few custom selection stuff.\n  switch (Node->getOpcode()) {\n  default: break;\n  case ISD::LOAD:\n    if (tryIndexedLoad(Node))\n      return;\n    // Other cases are autogenerated.\n    break;\n  case ISD::ADD:\n    if (tryIndexedBinOp(Node, Node->getOperand(0), Node->getOperand(1),\n                        MSP430::ADD8rp, MSP430::ADD16rp))\n      return;\n    else if (tryIndexedBinOp(Node, Node->getOperand(1), Node->getOperand(0),\n                             MSP430::ADD8rp, MSP430::ADD16rp))\n      return;\n  // Select the default instruction\n  SelectCode(Node);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nIn the end, it is the SelectCode function. This function will be generated in XXXGenDAGISel.inc from XXXInstrInfo.td. For example, it would be like this:\n\n  SDNode *SelectCode(SDValue N) {\n    ...\n    MVT::ValueType NVT = N.getNode()->getValueType(0);\n    switch (N.getOpcode()) {\n    case ISD::STORE: {\n      switch (NVT) {\n      default:\n        return Select_ISD_STORE(N);\n        break;\n      }\n      break;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nAfter the instruction selection, the LLVM IR will be represented in form of machineDAG.\n\n\n# 4. Instruction Scheduler\n\nIt includes 3 scheulder\n\n 1. ScheduleDAG. This is in the stage of instruction selection.\n\nScheduleDAGSDNodes.cpp\nScheduleDAGFast.cpp\nScheduleDAGRRList.cpp\nScheduleDAGVLIW.cpp\n\n\n1\n2\n3\n4\n\n\nFile ScheduleDAGRRList.cpp\n//===- ScheduleDAGRRList.cpp - Reg pressure reduction list scheduler ------===//\n// This implements bottom-up and top-down register pressure reduction list\n// schedulers, using standard algorithms.  The basic approach uses a priority\n// queue of available nodes to schedule.  One at a time, nodes are taken from\n// the priority queue (thus in priority order), checked for legality to\n// schedule, and emitted if legal.\n//\n//===----------------------------------------------------------------------===//\nburrListDAGScheduler("list-burr",\n                    "Bottom-up register reduction list scheduling",\n                    createBURRListDAGScheduler);\n\nsourceListDAGScheduler("source",\n                    "Similar to list-burr but schedules in source "\n                    "order when possible",\n                    createSourceListDAGScheduler);\n\nhybridListDAGScheduler("list-hybrid",\n                    "Bottom-up register pressure aware list scheduling "\n                    "which tries to balance latency and register pressure",\n                    createHybridListDAGScheduler);\n\nILPListDAGScheduler("list-ilp",\n                    "Bottom-up register pressure aware list scheduling "\n                    "which tries to balance ILP and register pressure",\n                    createILPListDAGScheduler);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\nIn every DAGScheduler, the flow is similar, the only difference is the construction of priority queue.\n\nScheduleDAGSDNodes *\nllvm::createHybridListDAGScheduler(SelectionDAGISel *IS,\n                                   CodeGenOptLevel OptLevel) {\n  HybridBURRPriorityQueue *PQ =\n    new HybridBURRPriorityQueue(*IS->MF, true, false, TII, TRI, TLI);\n\n  ScheduleDAGRRList *SD = new ScheduleDAGRRList(*IS->MF, true, PQ, OptLevel);\n  PQ->setScheduleDAG(SD);\n  return SD;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nfunction CodeGenAdnEmitDAG contains founction CreateScheduler, it can call target-defined scheduler and also the schedulers defined above.\n\nHere llvm utilize template, it can be seen that the template parameter is different scheduler policy, it defines them as struct.\n\nIn the "template class RegReductionPriorityQueue", SF will be instantiated as "Picker", the ick will influence "isReady" and "pop".\n\nusing BURegReductionPriorityQueue = RegReductionPriorityQueue<bu_ls_rr_sort>;\nusing SrcRegReductionPriorityQueue = RegReductionPriorityQueue<src_ls_rr_sort>;\nusing HybridBURRPriorityQueue = RegReductionPriorityQueue<hybrid_ls_rr_sort>;\nusing ILPBURRPriorityQueue = RegReductionPriorityQueue<ilp_ls_rr_sort>;\n\n// src_ls_rr_sort - Priority function for source order scheduler.\nstruct src_ls_rr_sort : public queue_sort {\n  enum {\n    IsBottomUp = true,\n    HasReadyFilter = false\n  };\n  RegReductionPQBase *SPQ;\n  src_ls_rr_sort(RegReductionPQBase *spq) : SPQ(spq) {}\n  bool operator()(SUnit* left, SUnit* right) const;\n};\n\n// hybrid_ls_rr_sort - Priority function for hybrid scheduler.\nstruct hybrid_ls_rr_sort : public queue_sort {\n  enum {\n    IsBottomUp = true,\n    HasReadyFilter = false\n  };\n  RegReductionPQBase *SPQ;\n  hybrid_ls_rr_sort(RegReductionPQBase *spq) : SPQ(spq) {}\n  bool isReady(SUnit *SU, unsigned CurCycle) const;\n  bool operator()(SUnit* left, SUnit* right) const;\n};\n\n//===----------------------------------------------------------------------===//\n//                RegReductionPriorityQueue Definition\n//===----------------------------------------------------------------------===//\n//\n// This is a SchedulingPriorityQueue that schedules using Sethi Ullman numbers\n// to reduce register pressure.\n//\ntemplate<class SF>\nclass RegReductionPriorityQueue : public RegReductionPQBase {\n  SF Picker;\npublic:\n  RegReductionPriorityQueue(MachineFunction &mf,\n                            bool tracksrp,\n                            bool srcorder,\n                            const TargetInstrInfo *tii,\n                            const TargetRegisterInfo *tri,\n                            const TargetLowering *tli)\n    : RegReductionPQBase(mf, SF::HasReadyFilter, tracksrp, srcorder,\n                         tii, tri, tli),\n      Picker(this) {}\n\n  bool isBottomUp() const override { return SF::IsBottomUp; }\n\n  bool isReady(SUnit *U) const override {\n    return Picker.HasReadyFilter && Picker.isReady(U, getCurCycle());\n  }\n\n  SUnit *pop() override {\n    if (Queue.empty()) return nullptr;\n\n    SUnit *V = popFromQueue(Queue, Picker, scheduleDAG);\n    V->NodeQueueId = 0;\n    return V;\n  }\n};\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\nIn Instruction Selection, it calls creater scheduler and Run. For simplicity, I delete all the trivial code, like reset or clear, or clear function in the code.\n\n  // Schedule machine code.\n  ScheduleDAGSDNodes *Scheduler = CreateScheduler();\n  {\n    NamedRegionTimer T("sched", "Instruction Scheduling", GroupName,\n                       GroupDescription, TimePassesIsEnabled);\n    Scheduler->Run(CurDAG, FuncInfo->MBB);\n  }\n\n// File CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp\n/// Run - perform scheduling.\nvoid ScheduleDAGSDNodes::Run(SelectionDAG *dag, MachineBasicBlock *bb) {\n   ....\n  // Invoke the target\'s selection of scheduler.\n  Schedule();\n}\n\n\n// File CodeGen/SelectionDAG/ScheduleDAGRRList.cpp\n/// Schedule - Schedule the DAG using list scheduling.\nvoid ScheduleDAGRRList::Schedule() {\n  LLVM_DEBUG(dbgs() << "********** List Scheduling " << printMBBReference(*BB)\n                    << " \'" << BB->getName() << "\' **********\\n");\n  BuildSchedGraph(nullptr);\n  Topo.MarkDirty();\n  AvailableQueue->initNodes(SUnits);\n  // Execute the actual scheduling loop.\n  ListScheduleBottomUp();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\nIn Schedule() function, it contains the following code:\n\n 1. BuildSchedGraph. This function creates SUnit Graph\n\n// File CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp\n/// BuildSchedGraph - Build the SUnit graph from the selection dag that we\n/// are input.  This SUnit graph is similar to the SelectionDAG, but\n/// excludes nodes that aren\'t interesting to scheduling, and represents\n/// glued together nodes with a single SUnit.\nvoid ScheduleDAGSDNodes::BuildSchedGraph(AAResults *AA) {\n  // Cluster certain nodes which should be scheduled together.\n  ClusterNodes();\n  // Populate the SUnits array.\n  // During scheduling, the NodeId field of SDNode is used to map SDNodes\n  // to their associated SUnits by holding SUnits table indices\n  // Multiple SDNodes might be associated to one SUnit.\n  BuildSchedUnits();\n  // Compute all the scheduling dependencies between nodes.\n  AddSchedEdges();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nBuildSchedUnits will also calculate the latency for each Sunit, using computeLatency\n\n 2. ListScheduleBottomUp()\n\n// ListScheduleBottomUp - The main loop of list scheduling for bottom-up\n// schedulers.\nvoid ScheduleDAGRRList::ListScheduleBottomUp() {\n  // Release any predecessors of the special Exit node.\n  ReleasePredecessors(&ExitSU);\n  // Add root to Available queue.\n  if (!SUnits.empty()) {\n    SUnit *RootSU = &SUnits[DAG->getRoot().getNode()->getNodeId()];\n    RootSU->isAvailable = true;\n    AvailableQueue->push(RootSU);\n  }\n\n  // While Available queue is not empty, grab the node with the highest\n  // priority. If it is not ready put it back.  Schedule the node.\n  Sequence.reserve(SUnits.size());\n  while (!AvailableQueue->empty() || !Interferences.empty()) {\n    // Pick the best node to schedule taking all constraints into\n    // consideration.\n    // Return a node that can be scheduled in this cycle. Requirements:\n    // (1) Ready: latency has been satisfied\n    // (2) No Hazards: resources are available\n    // (3) No Interferences: may unschedule to break register interferences.\n    SUnit *SU = PickNodeToScheduleBottomUp();\n    /// Move the scheduler state forward until the specified node\'s dependents are\n    /// ready and can be scheduled with no resource conflicts.\n    AdvancePastStalls(SU);\n    // ScheduleNodeBottomUp - Add the node to the schedule. Decrement the pending\n    // count of its predecessors. If a predecessor pending count is zero, add it to\n    // the Available queue.\n    ScheduleNodeBottomUp(SU);\n\n    while (AvailableQueue->empty() && !PendingQueue.empty()) {\n      AdvanceToCycle(std::max(CurCycle + 1, MinAvailableCycle));\n    }\n  }\n\n  // Reverse the order if it is bottom up.\n  std::reverse(Sequence.begin(), Sequence.end());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nAdvanceToCycle(unsigned NextCycle) would be good function to be noticed. For example, in AdvancePastStalls(), the scheduler will advance to the cycle when the chosedn SUnit is ready.',normalizedContent:'# 1. create selectiondag\n\nselectiondag builder calls visit() function to build sdnode\n\n// codegen/selectiondag/selectiondagbuilder.cpp\nvoid selectiondagbuilder::visit(unsigned opcode, const user &i) {\n  // note: this doesn\'t use instvisitor, because it has to work with\n  // constantexpr\'s in addition to instructions.\n  switch (opcode) {\n  default: llvm_unreachable("unknown instruction type encountered!");\n    // build the switch statement using the instruction.def file.\n#define handle_inst(num, opcode, class) \\\n    case instruction::opcode: visit##opcode((const class&)i); break;\n#include "llvm/ir/instruction.def"\n  }\n}\n\n// include/llvm/ir/instruction.def\nhandle_binary_inst(20, sdiv , binaryoperator)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\nvisitsdiv(const user &i) will create sdvalue as operand and sdnode for each ir\n\nvoid selectiondagbuilder::visitsdiv(const user &i) {\n  sdvalue op1 = getvalue(i.getoperand(0));\n  sdvalue op2 = getvalue(i.getoperand(1));\n\n  sdnodeflags flags;\n  flags.setexact(isa<possiblyexactoperator>(&i) &&\n                 cast<possiblyexactoperator>(&i)->isexact());\n  setvalue(&i, dag.getnode(isd::sdiv, getcursdloc(), op1.getvaluetype(), op1,\n                           op2, flags));\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nsdnode also contains dependencies in sdvalue, sdvalue include following:\n\n 1. data dependency\n 2. chain dependency. for example. order load, store insturction to the same address\n 3. glue of instructions.\n\n\n# 2. legalization\n\nlegalization will legalize sdnode operation that is unsupported target into supported node. it includes legalization of operation and operand. as to operation, it includes 3 major operation:\n\n 1. expansion expand one op into series of op\n 2. promotion promote data type\n 3. custom\n\n// codegen/selectiondag/legalizedag.cpp\n\n/// return a legal replacement for the given operation, with all legal operands.\nvoid selectiondaglegalize::legalizeop(sdnode *node) {\n.....\n    case targetlowering::expand:\n      if (expandnode(node))\n        return;\n      [[fallthrough]];\n    case targetlowering::libcall:\n      convertnodetolibcall(node);\n      return;\n    case targetlowering::promote:\n      promotenode(node);\n      return;\n    }\n\n  switch (node->getopcode()) {\n  case isd::load:\n    return legalizeloadops(node);\n  case isd::store:\n    return legalizestoreops(node);\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\nlook closely to legalize of stop operation, it considered expand, custom and promote.\n\nvoid selectiondaglegalize::legalizestoreops(sdnode *node) {\n    sdvalue value = st->getvalue();\n    mvt vt = value.getsimplevaluetype();\n    switch (tli.getoperationaction(isd::store, vt)) {\n    case targetlowering::legal: {\n      // if this is an unaligned store and the target doesn\'t support it, expand it.\n      evt memvt = st->getmemoryvt();\n      const datalayout &dl = dag.getdatalayout();\n      if (!tli.allowsmemoryaccessforalignment(*dag.getcontext(), dl, memvt,\n                                              *st->getmemoperand())) {\n        sdvalue result = tli.expandunalignedstore(st, dag);\n        replacenode(sdvalue(st, 0), result);\n      }\n      break;\n    }\n    case targetlowering::custom: {\n      sdvalue res = tli.loweroperation(sdvalue(node, 0), dag);\n      if (res && res != sdvalue(node, 0))\n        replacenode(sdvalue(node, 0), res);\n      return;\n    }\n    case targetlowering::promote: {\n      mvt nvt = tli.gettypetopromoteto(isd::store, vt);\n      value = dag.getnode(isd::bitcast, dl, nvt, value);\n      sdvalue result = dag.getstore(chain, dl, value, ptr, st->getpointerinfo(),\n                                    st->getoriginalalign(), mmoflags, aainfo);\n      replacenode(sdvalue(node, 0), result);\n      break;\n    }\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\nit also includes libcall now.\n\n// include/llvm/codegen/targetlowering.h\nclass targetloweringbase {\npublic:\n  /// this enum indicates whether operations are valid for a target, and if not,\n  /// what action should be used to make them valid.\n  enum legalizeaction : uint8_t {\n    legal,      // the target natively supports this operation.\n    promote,    // this operation should be executed in a larger type.\n    expand,     // try to expand this to other ops, otherwise use a libcall.\n    libcall,    // don\'t try to expand this to other ops, always use a libcall.\n    custom      // use the loweroperation hook to implement custom lowering.\n  };\n...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nas to legalize operand, in ./codegen/selectiondag/legalizetypes.h, it shows supported functions.\n\n  //===--------------------------------------------------------------------===//\n  // integer promotion support: legalizeintegertypes.cpp\n  //===--------------------------------------------------------------------===//\n  // integer expansion support: legalizeintegertypes.cpp\n  //===--------------------------------------------------------------------===//\n  // float to integer conversion support: legalizefloattypes.cpp\n  //===--------------------------------------------------------------------===//\n  // float expansion support: legalizefloattypes.cpp\n  //===--------------------------------------------------------------------===//\n  // scalarization support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n  // vector splitting support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n  // vector widening support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n  // vector widening utilities support: legalizevectortypes.cpp\n  //===--------------------------------------------------------------------===//\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n# 3. instruction selection\n\ncodegen/selectiondag/selectiondagisel.cpp\nvoid selectiondagisel::codegenandemitdag() {\n  // pre-type legalization allow creation of any node types.\n  curdag->newnodesmusthavelegaltypes = false;\n  // run the dag combiner in pre-legalize mode.\n  curdag->combine(beforelegalizetypes, aa, optlevel);\n  // second step, hack on the dag until it only uses operations and types that\n  // the target supports.\n  changed = curdag->legalizetypes();\n\n  // only allow creation of legal node types.\n  curdag->newnodesmusthavelegaltypes = true;\n   // run the dag combiner in post-type-legalize mode.\n  namedregiontimer t("combine_lt", "dag combining after legalize types",\n                         groupname, groupdescription, timepassesisenabled);\n  curdag->combine(afterlegalizetypes, aa, optlevel);\n\n  changed = curdag->legalizevectors();\n  // run the dag combiner in post-type-legalize mode.\n  namedregiontimer t("combine_lv", "dag combining after legalize vectors",\n                         groupname, groupdescription, timepassesisenabled);\n  curdag->combine(afterlegalizevectorops, aa, optlevel);\n\n  curdag->legalize();\n\n  // run the dag combiner in post-legalize mode.\n  namedregiontimer t("combine2", "dag combining 2", groupname,\n                       groupdescription, timepassesisenabled);\n  curdag->combine(afterlegalizedag, aa, optlevel);\n  \n  computeliveoutvreginfo();\n\n  doinstructionselection();\n\n  // schedule machine code.\n  scheduledagsdnodes *scheduler = createscheduler();\n  {\n    namedregiontimer t("sched", "instruction scheduling", groupname,\n                       groupdescription, timepassesisenabled);\n    scheduler->run(curdag, funcinfo->mbb);\n  }\n\n  // emit machine code to bb.  this can change \'bb\' to the last block being\n  // inserted into.\n  machinebasicblock *firstmbb = funcinfo->mbb, *lastmbb;\n  {\n    namedregiontimer t("emit", "instruction creation", groupname,\n                       groupdescription, timepassesisenabled);\n\n    // funcinfo->insertpt is passed by reference and set to the end of the\n    // scheduled instructions.\n    lastmbb = funcinfo->mbb = scheduler->emitschedule(funcinfo->insertpt);\n  }\n\n  // if the block was split, make sure we update any references that are used to\n  // update phi nodes later on.\n  if (firstmbb != lastmbb)\n    sdb->updatesplitblock(firstmbb, lastmbb);\n\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n//file include/llvm/codegen/selectiondagisel.h\n// main hook for targets to transform nodes into machine nodes.\nvirtual void select(sdnode *n) = 0;\n\n//file codegen/selectiondag/selectiondagisel.cpp\nvoid selectiondagisel::doinstructionselection() {\n  llvm_debug(dbgs() << "===== instruction selection begins: "\n                    << printmbbreference(*funcinfo->mbb) << " \'"\n                    << funcinfo->mbb->getname() << "\'\\n");\n  preprocessiseldag();\n\n  // select target instructions for the dag.\n  // number all nodes with a topological order and set dagsize.\n  dagsize = curdag->assigntopologicalorder();\n\n  // the allnodes list is now topological-sorted. visit the\n  // nodes by starting at the end of the list (the root of the\n  // graph) and preceding back toward the beginning (the entry\n  // node).\n  while (iselposition != curdag->allnodes_begin()) {\n    sdnode *node = &*--iselposition;\n    ...\n    \n    select(node);\n  }\n\n  curdag->setroot(dummy.getvalue());\n  \n  llvm_debug(dbgs() << "\\n===== instruction selection ends:\\n");\n  postprocessiseldag();\n  }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n\n\nselect(sdnode *n) will be overide by target backend:\n\n//file target/msp430/msp430iseldagtodag.cpp\n\n  #include "msp430gendagisel.inc"\nvoid msp430dagtodagisel::select(sdnode *node) {\n  // few custom selection stuff.\n  switch (node->getopcode()) {\n  default: break;\n  case isd::load:\n    if (tryindexedload(node))\n      return;\n    // other cases are autogenerated.\n    break;\n  case isd::add:\n    if (tryindexedbinop(node, node->getoperand(0), node->getoperand(1),\n                        msp430::add8rp, msp430::add16rp))\n      return;\n    else if (tryindexedbinop(node, node->getoperand(1), node->getoperand(0),\n                             msp430::add8rp, msp430::add16rp))\n      return;\n  // select the default instruction\n  selectcode(node);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\nin the end, it is the selectcode function. this function will be generated in xxxgendagisel.inc from xxxinstrinfo.td. for example, it would be like this:\n\n  sdnode *selectcode(sdvalue n) {\n    ...\n    mvt::valuetype nvt = n.getnode()->getvaluetype(0);\n    switch (n.getopcode()) {\n    case isd::store: {\n      switch (nvt) {\n      default:\n        return select_isd_store(n);\n        break;\n      }\n      break;\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nafter the instruction selection, the llvm ir will be represented in form of machinedag.\n\n\n# 4. instruction scheduler\n\nit includes 3 scheulder\n\n 1. scheduledag. this is in the stage of instruction selection.\n\nscheduledagsdnodes.cpp\nscheduledagfast.cpp\nscheduledagrrlist.cpp\nscheduledagvliw.cpp\n\n\n1\n2\n3\n4\n\n\nfile scheduledagrrlist.cpp\n//===- scheduledagrrlist.cpp - reg pressure reduction list scheduler ------===//\n// this implements bottom-up and top-down register pressure reduction list\n// schedulers, using standard algorithms.  the basic approach uses a priority\n// queue of available nodes to schedule.  one at a time, nodes are taken from\n// the priority queue (thus in priority order), checked for legality to\n// schedule, and emitted if legal.\n//\n//===----------------------------------------------------------------------===//\nburrlistdagscheduler("list-burr",\n                    "bottom-up register reduction list scheduling",\n                    createburrlistdagscheduler);\n\nsourcelistdagscheduler("source",\n                    "similar to list-burr but schedules in source "\n                    "order when possible",\n                    createsourcelistdagscheduler);\n\nhybridlistdagscheduler("list-hybrid",\n                    "bottom-up register pressure aware list scheduling "\n                    "which tries to balance latency and register pressure",\n                    createhybridlistdagscheduler);\n\nilplistdagscheduler("list-ilp",\n                    "bottom-up register pressure aware list scheduling "\n                    "which tries to balance ilp and register pressure",\n                    createilplistdagscheduler);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\nin every dagscheduler, the flow is similar, the only difference is the construction of priority queue.\n\nscheduledagsdnodes *\nllvm::createhybridlistdagscheduler(selectiondagisel *is,\n                                   codegenoptlevel optlevel) {\n  hybridburrpriorityqueue *pq =\n    new hybridburrpriorityqueue(*is->mf, true, false, tii, tri, tli);\n\n  scheduledagrrlist *sd = new scheduledagrrlist(*is->mf, true, pq, optlevel);\n  pq->setscheduledag(sd);\n  return sd;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\nfunction codegenadnemitdag contains founction createscheduler, it can call target-defined scheduler and also the schedulers defined above.\n\nhere llvm utilize template, it can be seen that the template parameter is different scheduler policy, it defines them as struct.\n\nin the "template class regreductionpriorityqueue", sf will be instantiated as "picker", the ick will influence "isready" and "pop".\n\nusing buregreductionpriorityqueue = regreductionpriorityqueue<bu_ls_rr_sort>;\nusing srcregreductionpriorityqueue = regreductionpriorityqueue<src_ls_rr_sort>;\nusing hybridburrpriorityqueue = regreductionpriorityqueue<hybrid_ls_rr_sort>;\nusing ilpburrpriorityqueue = regreductionpriorityqueue<ilp_ls_rr_sort>;\n\n// src_ls_rr_sort - priority function for source order scheduler.\nstruct src_ls_rr_sort : public queue_sort {\n  enum {\n    isbottomup = true,\n    hasreadyfilter = false\n  };\n  regreductionpqbase *spq;\n  src_ls_rr_sort(regreductionpqbase *spq) : spq(spq) {}\n  bool operator()(sunit* left, sunit* right) const;\n};\n\n// hybrid_ls_rr_sort - priority function for hybrid scheduler.\nstruct hybrid_ls_rr_sort : public queue_sort {\n  enum {\n    isbottomup = true,\n    hasreadyfilter = false\n  };\n  regreductionpqbase *spq;\n  hybrid_ls_rr_sort(regreductionpqbase *spq) : spq(spq) {}\n  bool isready(sunit *su, unsigned curcycle) const;\n  bool operator()(sunit* left, sunit* right) const;\n};\n\n//===----------------------------------------------------------------------===//\n//                regreductionpriorityqueue definition\n//===----------------------------------------------------------------------===//\n//\n// this is a schedulingpriorityqueue that schedules using sethi ullman numbers\n// to reduce register pressure.\n//\ntemplate<class sf>\nclass regreductionpriorityqueue : public regreductionpqbase {\n  sf picker;\npublic:\n  regreductionpriorityqueue(machinefunction &mf,\n                            bool tracksrp,\n                            bool srcorder,\n                            const targetinstrinfo *tii,\n                            const targetregisterinfo *tri,\n                            const targetlowering *tli)\n    : regreductionpqbase(mf, sf::hasreadyfilter, tracksrp, srcorder,\n                         tii, tri, tli),\n      picker(this) {}\n\n  bool isbottomup() const override { return sf::isbottomup; }\n\n  bool isready(sunit *u) const override {\n    return picker.hasreadyfilter && picker.isready(u, getcurcycle());\n  }\n\n  sunit *pop() override {\n    if (queue.empty()) return nullptr;\n\n    sunit *v = popfromqueue(queue, picker, scheduledag);\n    v->nodequeueid = 0;\n    return v;\n  }\n};\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\nin instruction selection, it calls creater scheduler and run. for simplicity, i delete all the trivial code, like reset or clear, or clear function in the code.\n\n  // schedule machine code.\n  scheduledagsdnodes *scheduler = createscheduler();\n  {\n    namedregiontimer t("sched", "instruction scheduling", groupname,\n                       groupdescription, timepassesisenabled);\n    scheduler->run(curdag, funcinfo->mbb);\n  }\n\n// file codegen/selectiondag/scheduledagsdnodes.cpp\n/// run - perform scheduling.\nvoid scheduledagsdnodes::run(selectiondag *dag, machinebasicblock *bb) {\n   ....\n  // invoke the target\'s selection of scheduler.\n  schedule();\n}\n\n\n// file codegen/selectiondag/scheduledagrrlist.cpp\n/// schedule - schedule the dag using list scheduling.\nvoid scheduledagrrlist::schedule() {\n  llvm_debug(dbgs() << "********** list scheduling " << printmbbreference(*bb)\n                    << " \'" << bb->getname() << "\' **********\\n");\n  buildschedgraph(nullptr);\n  topo.markdirty();\n  availablequeue->initnodes(sunits);\n  // execute the actual scheduling loop.\n  listschedulebottomup();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\nin schedule() function, it contains the following code:\n\n 1. buildschedgraph. this function creates sunit graph\n\n// file codegen/selectiondag/scheduledagsdnodes.cpp\n/// buildschedgraph - build the sunit graph from the selection dag that we\n/// are input.  this sunit graph is similar to the selectiondag, but\n/// excludes nodes that aren\'t interesting to scheduling, and represents\n/// glued together nodes with a single sunit.\nvoid scheduledagsdnodes::buildschedgraph(aaresults *aa) {\n  // cluster certain nodes which should be scheduled together.\n  clusternodes();\n  // populate the sunits array.\n  // during scheduling, the nodeid field of sdnode is used to map sdnodes\n  // to their associated sunits by holding sunits table indices\n  // multiple sdnodes might be associated to one sunit.\n  buildschedunits();\n  // compute all the scheduling dependencies between nodes.\n  addschededges();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n\nbuildschedunits will also calculate the latency for each sunit, using computelatency\n\n 2. listschedulebottomup()\n\n// listschedulebottomup - the main loop of list scheduling for bottom-up\n// schedulers.\nvoid scheduledagrrlist::listschedulebottomup() {\n  // release any predecessors of the special exit node.\n  releasepredecessors(&exitsu);\n  // add root to available queue.\n  if (!sunits.empty()) {\n    sunit *rootsu = &sunits[dag->getroot().getnode()->getnodeid()];\n    rootsu->isavailable = true;\n    availablequeue->push(rootsu);\n  }\n\n  // while available queue is not empty, grab the node with the highest\n  // priority. if it is not ready put it back.  schedule the node.\n  sequence.reserve(sunits.size());\n  while (!availablequeue->empty() || !interferences.empty()) {\n    // pick the best node to schedule taking all constraints into\n    // consideration.\n    // return a node that can be scheduled in this cycle. requirements:\n    // (1) ready: latency has been satisfied\n    // (2) no hazards: resources are available\n    // (3) no interferences: may unschedule to break register interferences.\n    sunit *su = picknodetoschedulebottomup();\n    /// move the scheduler state forward until the specified node\'s dependents are\n    /// ready and can be scheduled with no resource conflicts.\n    advancepaststalls(su);\n    // schedulenodebottomup - add the node to the schedule. decrement the pending\n    // count of its predecessors. if a predecessor pending count is zero, add it to\n    // the available queue.\n    schedulenodebottomup(su);\n\n    while (availablequeue->empty() && !pendingqueue.empty()) {\n      advancetocycle(std::max(curcycle + 1, minavailablecycle));\n    }\n  }\n\n  // reverse the order if it is bottom up.\n  std::reverse(sequence.begin(), sequence.end());\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\nadvancetocycle(unsigned nextcycle) would be good function to be noticed. for example, in advancepaststalls(), the scheduler will advance to the cycle when the chosedn sunit is ready.',charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"Operand Collector",frontmatter:{title:"Operand Collector",date:"2022-07-18T17:25:49.000Z",permalink:"/pages/cc7034/"},regularPath:"/03.gpu/01.operand_collector.html",relativePath:"03.gpu/01.operand_collector.md",key:"v-6ffd4505",path:"/pages/cc7034/",headersStr:null,content:"Warped-Compression: Enabling Power Efficient GPUs through Register Compression\n\neach register bank entry can store up to four 32-bit register values. All thread registers in a warp are statically allocated on consecutive banks with the same entry index. Therefore, to read one operand of a warp instruction,a buffering unit called operand collector needs to access up to eight register banks with the same index within each bank. While operands from different banks may be concurrently read, operands that access the same bank lead to bank conflicts.\n\nCORF: Coalescing Operand Register File for GPUs\n\nFigure 1 shows our baseline register file organization for the Fermi generation of Nvidia GPUs. It has a register file size of 128 KB per SM split across four banks. A bank is made up of 8 sub-banks that are 128 bits wide each. All 32 registers belonging to the 32 threads in the same warp are statically allocated to consecutive sub-banks (in a single bank) with the same entry index. Thus, a full register for all the threads within a warp can be striped using one entry of one bank, allowing it to be operated on in a single cycle. Each bank can store up to 256 warp-registers.\n\nSummary They all assume that 128bit entry in each bank will supply 4 register to 4 thread in 32 thread per swap.\n\nWarped-Compression assumes that 8 bank will supply 32 registers for a warp. CORF assumes that 8 subbank in each bank will supply 32 registers for a warp.",normalizedContent:"warped-compression: enabling power efficient gpus through register compression\n\neach register bank entry can store up to four 32-bit register values. all thread registers in a warp are statically allocated on consecutive banks with the same entry index. therefore, to read one operand of a warp instruction,a buffering unit called operand collector needs to access up to eight register banks with the same index within each bank. while operands from different banks may be concurrently read, operands that access the same bank lead to bank conflicts.\n\ncorf: coalescing operand register file for gpus\n\nfigure 1 shows our baseline register file organization for the fermi generation of nvidia gpus. it has a register file size of 128 kb per sm split across four banks. a bank is made up of 8 sub-banks that are 128 bits wide each. all 32 registers belonging to the 32 threads in the same warp are statically allocated to consecutive sub-banks (in a single bank) with the same entry index. thus, a full register for all the threads within a warp can be striped using one entry of one bank, allowing it to be operated on in a single cycle. each bank can store up to 256 warp-registers.\n\nsummary they all assume that 128bit entry in each bank will supply 4 register to 4 thread in 32 thread per swap.\n\nwarped-compression assumes that 8 bank will supply 32 registers for a warp. corf assumes that 8 subbank in each bank will supply 32 registers for a warp.",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Precision Exception",frontmatter:{title:"Precision Exception",date:"2023-11-11T00:00:00.000Z",permalink:"/pages/14769f/"},regularPath:"/03.gpu/03.Precise%20Exception.html",relativePath:"03.gpu/03.Precise Exception.md",key:"v-f96426b0",path:"/pages/14769f/",headers:[{level:3,title:"1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception [2012]",slug:"_1-supporting-virtual-memory-in-gpgpu-without-supporting-precise-exception-2012",normalizedTitle:"1. supporting virtual memory in gpgpu without supporting precise exception [2012]",charIndex:489},{level:3,title:"6. Efficient Exception Handling Support for GPUs [2017]",slug:"_6-efficient-exception-handling-support-for-gpus-2017",normalizedTitle:"6. efficient exception handling support for gpus [2017]",charIndex:2960}],headersStr:"1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception [2012] 6. Efficient Exception Handling Support for GPUs [2017]",content:" 1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception 2012\n 2. Idempotent Processor Architecture 2011\n 3. iGPU: Exception Support and Speculative Execution on GPUs 2012\n 4. Implementing Virtual Memory in a Vector Processor with Software Restart Markers 2006 Not Read\n 5. Imprecise Store Exceptions 2023 ISCA\n 6. Efficient Exception Handling Support for GPUs 2017\n 7. Simple Out of Order Core for GPGPUs\n 8. Other Papers.\n\n----------------------------------------\n\n\n# 1. Supporting Virtual Memory in GPGPU without Supporting Precise Exception [2012]\n\n👍\n\nIntroduction: GPU is designed for grahics. Supporting precise exceptions is not needed at all and it is extremely expensive due to the high number of registers. Other Designs:\n\n 1. Software restart Remarker Implementing virtual memory in a vector processor with software restart markers.[4] 2006 Reducing Exception Management Overhead with Software Restart Markers 2008\n 2. Idempotent Idempotent processor architecture [2] 2011 igpu: Exception support and speculative execution on gpus. [3] 2012\n\na) set start_maker set start_marker indicates a place where a program can be restarted after a page fault exception handler is serviced.\n\nb) LD.pfchk An LD.pfchk instruction sets pfbit, when it generates a page fault. The pfbit registers behave like predicate registers in IA-64. Instructions that can potentially change program’s states are predicated with pfbit.\n\nc) sw_call sw_call is composed of barrier and call instructions. When a processor fetches an sw_call instruction, it enforces an execution barrier.\n\nInstructions after sw_call can be fetched/renamed, but none of the instructions will be executed. call instructions invoke page fault handler. Implementing this execution barrier is very easy, but it reduces the benefit of a fully out-of-order scheduling processor.\n\nLD.pfchk will set pfbits. Instructions that can potentially change program's state are predicated with pfbit. Similar to idempotent processors, instructions that can be safely reexecuted without changing the program’s results do not need to be predicated. If all instructions are predicated, those instructions cannot be executed until the load instruction is completed, thereby degrading performance significantly.\n\n 1. Not all load/store instruction will be set as LD.pfck. Compiler's job to distinguish Static, Malloc, Large Arrays, Stack Operations, Pointers, and so on\n 2. Only those instructions that can safely reexecuted can be predicated.\n\n/* original C-code */\nfor (int ii=0; ii<N; ii++)\na[ii] = b[ii]*2;\n/* new code */\nfor (int ii=0; ii<N; ii++) {\nif (!(ii%kk)) {\n// kk = page size%(size of(a[0]))\npfchk(&(a[0])+ii*kk));\npfchk(&(b[0])+ii*kk));\n}\na[ii] = b[ii]*2;\n}\nvoid pfchk(int addr) {\n/* use intrinsics to insert assembly code */\nset start_marker;\nLD.pfchk(addr);\n(pfbit) sw_call(start_marker);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n----------------------------------------\n\n\n# 6. Efficient Exception Handling Support for GPUs [2017]\n\n👍 👍 👍 👍\n\nThis paper summerize [1] [2] [3] [4] and discuss why altough GPU solves the problem of dependency, it still meets the problem of RAW Hazard on replay\n\n\n\nIn short, since R4 has been read by C, D can issue and might overwrite R4 before C is done. Thus if we resume from C, C might read the value of new R4, which means hazard.\n\nIt propose three method to solve this:\n\n 1. Warp Disable\n\n\n\n 2. Replay Queue\n\n\n\n 3. Operand Log\n\n\n\nThis is a good paper, that deserves reading throughly. 👏",normalizedContent:" 1. supporting virtual memory in gpgpu without supporting precise exception 2012\n 2. idempotent processor architecture 2011\n 3. igpu: exception support and speculative execution on gpus 2012\n 4. implementing virtual memory in a vector processor with software restart markers 2006 not read\n 5. imprecise store exceptions 2023 isca\n 6. efficient exception handling support for gpus 2017\n 7. simple out of order core for gpgpus\n 8. other papers.\n\n----------------------------------------\n\n\n# 1. supporting virtual memory in gpgpu without supporting precise exception [2012]\n\n👍\n\nintroduction: gpu is designed for grahics. supporting precise exceptions is not needed at all and it is extremely expensive due to the high number of registers. other designs:\n\n 1. software restart remarker implementing virtual memory in a vector processor with software restart markers.[4] 2006 reducing exception management overhead with software restart markers 2008\n 2. idempotent idempotent processor architecture [2] 2011 igpu: exception support and speculative execution on gpus. [3] 2012\n\na) set start_maker set start_marker indicates a place where a program can be restarted after a page fault exception handler is serviced.\n\nb) ld.pfchk an ld.pfchk instruction sets pfbit, when it generates a page fault. the pfbit registers behave like predicate registers in ia-64. instructions that can potentially change program’s states are predicated with pfbit.\n\nc) sw_call sw_call is composed of barrier and call instructions. when a processor fetches an sw_call instruction, it enforces an execution barrier.\n\ninstructions after sw_call can be fetched/renamed, but none of the instructions will be executed. call instructions invoke page fault handler. implementing this execution barrier is very easy, but it reduces the benefit of a fully out-of-order scheduling processor.\n\nld.pfchk will set pfbits. instructions that can potentially change program's state are predicated with pfbit. similar to idempotent processors, instructions that can be safely reexecuted without changing the program’s results do not need to be predicated. if all instructions are predicated, those instructions cannot be executed until the load instruction is completed, thereby degrading performance significantly.\n\n 1. not all load/store instruction will be set as ld.pfck. compiler's job to distinguish static, malloc, large arrays, stack operations, pointers, and so on\n 2. only those instructions that can safely reexecuted can be predicated.\n\n/* original c-code */\nfor (int ii=0; ii<n; ii++)\na[ii] = b[ii]*2;\n/* new code */\nfor (int ii=0; ii<n; ii++) {\nif (!(ii%kk)) {\n// kk = page size%(size of(a[0]))\npfchk(&(a[0])+ii*kk));\npfchk(&(b[0])+ii*kk));\n}\na[ii] = b[ii]*2;\n}\nvoid pfchk(int addr) {\n/* use intrinsics to insert assembly code */\nset start_marker;\nld.pfchk(addr);\n(pfbit) sw_call(start_marker);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n----------------------------------------\n\n\n# 6. efficient exception handling support for gpus [2017]\n\n👍 👍 👍 👍\n\nthis paper summerize [1] [2] [3] [4] and discuss why altough gpu solves the problem of dependency, it still meets the problem of raw hazard on replay\n\n\n\nin short, since r4 has been read by c, d can issue and might overwrite r4 before c is done. thus if we resume from c, c might read the value of new r4, which means hazard.\n\nit propose three method to solve this:\n\n 1. warp disable\n\n\n\n 2. replay queue\n\n\n\n 3. operand log\n\n\n\nthis is a good paper, that deserves reading throughly. 👏",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"GPU WARP Scheduler",frontmatter:{title:"GPU WARP Scheduler",date:"2023-09-20T00:00:00.000Z",permalink:"/pages/2476ae/"},regularPath:"/03.gpu/02.warp_execution.html",relativePath:"03.gpu/02.warp_execution.md",key:"v-b25c491a",path:"/pages/2476ae/",headers:[{level:3,title:"1. Thread Block Compaction for Efficient SIMT Control Flow",slug:"_1-thread-block-compaction-for-efficient-simt-control-flow",normalizedTitle:"1. thread block compaction for efficient simt control flow",charIndex:1}],headersStr:"1. Thread Block Compaction for Efficient SIMT Control Flow",content:" 1. Thread Block Compaction for Efficient SIMT Control Flow\n\n----------------------------------------\n\n\n# 1. Thread Block Compaction for Efficient SIMT Control Flow",normalizedContent:" 1. thread block compaction for efficient simt control flow\n\n----------------------------------------\n\n\n# 1. thread block compaction for efficient simt control flow",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Unified Memory Paper List",frontmatter:{title:"Unified Memory Paper List",date:"2023-11-11T00:00:00.000Z",permalink:"/pages/44771e/"},regularPath:"/03.gpu/04.Unified_Memory.html",relativePath:"03.gpu/04.Unified_Memory.md",key:"v-5849f946",path:"/pages/44771e/",headers:[{level:3,title:"1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory",slug:"_1-holistic-performance-analysis-and-optimization-of-unified-virtual-holistic-performance-analysis-and-optimization-of-unified-virtual-memory",normalizedTitle:"1. holistic performance analysis and optimization of unified virtual holistic performance analysis and optimization of unified virtual memory",charIndex:3797},{level:3,title:"3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions",slug:"_3-oversubscribing-gpu-unified-virtual-memory-implications-and-suggestions",normalizedTitle:"3. oversubscribing gpu unified virtual memory: implications and suggestions",charIndex:4084},{level:3,title:"4. Performance Evaluation of Advanced Features in CUDA Unified Memory",slug:"_4-performance-evaluation-of-advanced-features-in-cuda-unified-memory",normalizedTitle:"4. performance evaluation of advanced features in cuda unified memory",charIndex:6364},{level:3,title:"5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory",slug:"_5-interplay-between-hardware-prefetcher-and-page-eviction-policy-in-cpu-gpu-unified-virtual-memory",normalizedTitle:"5. interplay between hardware prefetcher and page eviction policy in cpu-gpu unified virtual memory",charIndex:9887},{level:3,title:"7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads 2020",slug:"_7-batch-aware-unified-memory-management-in-gpus-for-irregular-workloads-2020",normalizedTitle:"7. batch-aware unified memory management in gpus for irregular workloads 2020",charIndex:11620},{level:3,title:"10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019",slug:"_10-machine-learning-guided-optimal-use-of-gpu-unified-memory-2019",normalizedTitle:"10. machine learning guided optimal use of gpu unified memory 2019",charIndex:18629},{level:3,title:"14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory [2024] 👍👍👍👍",slug:"_14-fine-grain-quantitative-analysis-of-demand-paging-in-unified-virtual-memory-2024",normalizedTitle:"14. fine-grain quantitative analysis of demand paging in unified virtual memory [2024] 👍👍👍👍",charIndex:21192}],headersStr:"1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory 3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions 4. Performance Evaluation of Advanced Features in CUDA Unified Memory 5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory 7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads 2020 10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019 14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory [2024] 👍👍👍👍",content:' 1.  Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory\n 2.  In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing\n 3.  Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions\n 4.  Performance Evaluation of Advanced Features in CUDA Unified Memory\n 5.  Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory\n 6.  Unified Memory: GPGPU-Sim/UVM Smart Integration\n 7.  Batch-Aware Unified Memory Management in GPUs for Irregular Workloads\n 8.  An Intelligent Framework for Oversubscription Management in CPU-GPU Unified Memory\n 9.  Architectural Support for Address Translation on GPUs Designing Memory Management Units for CPU/GPUs with Unified Address Spaces\n 10. Machine Learning Guided Optimal Use of GPU Unified Memory\n 11. Towards High Performance Paged Memory for GPUs\n 12. [Virtualization] Virtual Thread: Maximizing Thread-Level Parallelism beyond GPU Scheduling Limit.\n 13. [Virtualization] A Survey of GPU Multitasking Methods Supported by Hardware Architecture\n 14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory\n\nPlan to read\n\n 1. Early-Adaptor: An Adaptive Framework for Proactive UVM Memory Management\n 2. Liberator: A Data Reuse Framework for Out-of-Memory Graph Computing on GPUs\n 3. [HPCA] Enabling Large Dynamic Neural Network Training with Learning-based Memory Management\n 4. GPUswap: Enabling Oversubscription of GPU Memory through Transparent Swapping\n\n----------------------------------------\n\nUnified Memory History copied from Evolution of Nvidia GPU from microarchitectures Pascal to Ampere\n\nCUDA 4 introduced UVA (Unified Virtual Addressing) to provide a single virtual memory address space for both CPU and GPU memory and enable pointers to be accessed from GPU code no matter where in the system they reside. UVA enables Zero-Copy memory, a pinned CPU memory accessible by GPU code directly, over PCIe, without the need for memory copy. This provides some of the convince of Unified Memory, but at the cost of worse performance, because GPU always accesses it with PCIe’s low bandwidth and high latency.[1]\n\nLater, CUDA 6 introduced Unified Memory, which creates a pool of managed memory that programs running on the CPU and GPU can access without explicit data movement. However, only when CPU and GPU processes are not running together because of the limitation of the Kepler and Maxwell GPU microarchitecture. Also, the Unified Memory address space was limited to the size of the GPU memory.[1, 3]\n\nCUDA 8 and Pascal microarchitectures improve Unified Memory functionality by adding 49-bit virtual addressing and page faulting capability. The larger 49-bit virtual addresses are sufficient to enable GPUs to access the entire system memory plus the memory of all GPUs in the system. Because of the memory page faulting functionality, the CUDA system software does not need to synchronize all managed memory allocations to the GPU before each kernel lunch. Instead, when a thread running on GPU faults on non-resident memory access(demanding page), it stalls until the page can be migrated and the page table updated. Alternatively, the page may be mapped for remote access over PCIe or NVLink interconnects.[1, 3, 6]\n\nThese new features of Unified Memory enable oversubscription of memory, which means that application running on a GPU can use data sets larger than ten their device memory.[1] While the Unified Memory model makes GPU programming more convenient, it comes at a cost; handling page faults and page migrations can be expensive. CUDA 8 addresses this issue with features like prefetch and memory advice.\n\n----------------------------------------\n\n\n# 1. Holistic Performance Analysis and Optimization of Unified Virtual Holistic Performance Analysis and Optimization of Unified Virtual Memory\n\nSame author with In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing\n\n----------------------------------------\n\n\n# 3. Oversubscribing GPU Unified Virtual Memory: Implications and Suggestions\n\nUVM supports memory oversubscription, giving GPU programs the ability to use a larger amount of memory than the physical memory, without worrying about the problem of memory shortage.\n\nAdvanced optimization techniques, mainly prefetching and memory usage hints [1], can be used to fine-tune the performance of UVM applications, mitigating the overheads caused by UVM.\n\n\n\n2）Prefetching and Hints Prefetching and UVM hints are the major approaches provided by CUDA, with the hope that page faults and memory thrashing could be prevented by fine-tuning the behavior of UVM at runtime.\n\nBy calling cudaMemPrefetchAsync (PF), a memory block could be prefetched to GPU. UVM hints provide informed decisions on page handling by indicating the access patterns of data.\n\nChanging UVM hints is done by invoking cudaMemAdvise with one of the following policies：\n\n• cudaMemAdviseSetAccessedBy (AB) implies that the device keeps a direct mapping in its page table. When the data is migrated, the mapping is re-established.\n• cudaMemAdviseSetPreferredLocation (PL) pins the data and prevents the page to be migrated, which is useful when the page is mainly accessed on one side.\n• cudaMemAdviseSetReadMostly (RM) indicates the data region is read-intensive. It creates a read-only copy of the page on the faulting side, allowing on current access on both sides.\n\n\nOnly one policy (AB, PL, or RM) could be specified for each memory block, but each policy can be used along with prefetching.\n\nSuggestions: To ensure performance under all oversubscription conditions, programmer needs to choose the UVM hints dynamically based on the application’s memory usage and available GPU memory. As a prerequisite, the size of the FALL pages needs to be estimated or measured by experiment. Before kernel launch, the program should first check the size of available GPU memory (e.g. via the cudaMemGetInfo API). If no oversubscription will happen, or the available memory is larger than the size of FALL pages, the programmer could set hints based on the conclusions provided by related researches [24]. Otherwise, based on our findings, applying the hint AB is a preferable choice.\n\n----------------------------------------\n\n\n# 4. Performance Evaluation of Advanced Features in CUDA Unified Memory\n\nCUDA has introduced new features for optimizing the data migration on UM, i.e., memory advises and prefetch. Instead of solely relying on page faults, the memory advises feature allows the programmer to provide data access pattern for each memory object so that the runtime can optimize migration decisions. The prefetch proactively triggers asynchronous data migration to GPU before the data is accessed, which reduces page faults and, consequently, the overhead in handling page faults.\n\n-Using memory advises improves application performance in oversubscription execution on the Intel platform and in-memory executions on the IBM platform.\n\n-UM prefetch provides a significant performance improvement on the Intel-Volta/Pascal-PCI-E based systems while it does not show a performance improvement on the Power9-Volta-NVLink based system\n\nUM was first introduced in CUDA 6.0 [21]. Only until the recent Nvidia Pascal microarchitecture that has hardware support for page faults.\n\n\n\n• cudaMemAdviseSetAccessedBy establishes a direct mapping of data to a specified device. Figure 2c illustrates an example of a physical page on GPU being remotely access from the host. When cudaMemAdviseSetPreferredLocation is applied, CUDA runtime tries to build a direct mapping to the page to avoid data migration so that the destination can access data remotely. Differently from cudaMemAdviseSetPreferredLocation, this cudaMemAdviseSetAccessedBy does not try to pin pages on a specific device; instead, its main effect is to establish mapping on the remote device. This advice takes effect on the creation of the memory pages. The mapping will be re-established after the pages are migrated.\n\n• cudaMemAdviseSetPreferredLocation sets the preferred physical location of pages. This advice pins a page and prevents it from migrating to other memories. Figure 2b illustrates a page preferred on the host side, and GPU uses remote mapping to access the page. This advice established a direct (remote) mapping to the memory page. When accessing the page remotely, data is fetched through the remote memory instead of generating a page fault. If the underlying hardware does not support the remote mapping, the page will be migrated as in the standard UM. cudaMemAdviseSetPreferredLocation is useful for applications with little data sharing between CPU and GPU, i.e., part of the application is executed completely on the GPU, and the rest of the application executes on the host. Data that is being used mostly by the GPU can be pinned to the GPU with the advice, avoiding memory thrashing.\n\n• cudaMemAdviseSetReadMostly implies a read-intensive data region. In the basic UM, accessing a page on a remote side triggers page migration. However, with cudaMemAdviseSetReadMostly, a read-only duplicate of the page will be created on the faulting side, which prevents page faults and data migration in the future. Figure 2a illustrates an example, where the second access (step 5) has no page fault and is local access. This mechanism, however, results in a high overhead if there is any update to this memory region because all copies of the corresponding page will be invalidated to preserve consistency between different copies. Thus, this advice is often used in read-only data structures, such as lookup tables and application parameters.\n\nIn general, we found both memory advises and prefetch to be simple and effective.\n\n----------------------------------------\n\n\n# 5. Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory\n\nCons in traditional GPU: Complicated asynchronous user-directed constructs to overlap data migration and kernel execution are used to address this issue. The second challenge is memory over-subscription. When the working set of the GPU kernel cannot fit in the device memory, the programmers have to painstakingly redefine the data structures and tile the data to transfer back and forth in chunks.\n\nThis flow is inspired by -> 11. Towards High Performance Paged Memory for GPU.\n\n1 Scheduled threads generate global memory accesses.\n\n2 Each SM has its own load/store unit. Every load/store unit has its own TLB. Load/store unit performs a TLB look up to find whether the translation for the issued memory access is cached in TLB or not. A TLB miss is relayed to the GMMU.\n\n3 The GMMU walks through the page table looking for a PTE corresponding to the requested page with valid flag set. A far-fault occurs if there is no PTE for the requested page or the valid flag is not set. Then the far-fault is registered in the Far-fault Miss Status Handling Registers (MSHRs).\n\n4 The page is scheduled for transfer over CPU-GPU PCI-e interconnect.\n\n5 A 4KB page is allocated on demand and data is migrated from host to device memory.\n\n6 The MSHRs are consulted to notify the corresponding load/store unit and the memory access is replayed. A new PTE entry is added to the page table with valid\n\n\n\nThis paper introduces random, sequential and tree-based Neighborhood prefetcher in detail.\n\nAnd come up with pre-eviction for tree-based Neighborhood, different from LRU eviction used in Nvidia.\n\n\n\n----------------------------------------\n\n\n# 7. Batch-Aware Unified Memory Management in GPUs for Irregular Workloads 2020\n\nPropose:\n\n(1) increases the batch size (i.e., the number of page faults handled together), thereby amortizing the GPU runtime fault handling time, and reduces the number of batches by supporting CPU-like thread block context switching\n\nThread Oversubscription (TO), a CPU-like thread block context switching technique, to effectively amortize the GPU runtime fault handling time by increasing the batch size (i.e., the number of page faults handled together).\n\n(2) takes page eviction off the critical path with no hardware changes by overlapping evictions with CPU-to-GPU page migrations. Unobtrusive Eviction (UE) to take GPU page evictions off the critical path with no hardware changes based on the idea of overlapping page evictions with CPU-to-GPU page migrations.\n\nPrior work reports that page fault handling latency ranges from 20µs to 50µs [53]. We find that these numbers are conservative and can be worse depending on the applications and systems. Unfortunately, this page fault latency, which is in the order of microseconds, cannot be easily hidden even with ample thread-level parallelism (TLP) in GPUs, especially when GPU memory is oversubscribed.\n\n\n\nThe GPU runtime processes a group of GPU page faults together, rather than processing each individual one, in order to amortize the overhead of multiple round-trip latencies over the PCIe bus and to avoid invoking multiple interrupt service routines (ISRs) in the operating system (OS). To efficiently process an excessive number of page faults, the GPU runtime performs a series of operations such as preprocessing all the page faults and inserting page prefetching requests, which takes a significant amount of time (in the range of tens to hundreds of microseconds). Once all the operations (e.g., CPU page table walks for all the page faults, page allocation and eviction scheduling, etc.) are finished, page migrations between the CPU and the GPU begin.\n\nThis page fault handling is expensive because (1) it requires long latency communications between the CPU and GPU over the PCIe bus, and (2) the GPU runtime performs a very expensive fault handling service routine.\n\nTo amortize the overhead, the GPU runtime processes a group of page faults together, which we refer to as batch processing.\n\nWhen a page fault exception is raised by the GPU memory management unit (MMU), the GPU runtime begins to handle the exception, shown in 1.\n\n\n\nFrom this, we conclude that page evictions and new page allocations are serialized in modern GPUs to prevent the new pages from overwriting the evicted pages. Note that an eviction is required on every page fault once the pages resident in the GPU’s memory are at capacity.\n\n\n\nThis preprocessing includes sorting the page faults in ascending order of page addresses (to accelerate the page table walks) and the analysis of page addresses to insert page prefetching requests.1 We refer to the time taken by the GPU runtime to perform a collection of operations to handle many page faults together as GPU runtime fault handling time.\n\nhttps://github.com/acsl-technion/gaia_nvidia/blob/e23e4d926f576c2c4169664b6add89e1368ee849/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c#L787\n\n// Fault cache preprocessing for fault coalescing\n//\n// This function generates an ordered view of the given fault_cache in which faults are sorted by VA space, fault\n// address (aligned to 4K) and access type "intrusiveness" (atomic - write - read - prefetch). In order to minimize\n// the number of instance_ptr to VA space translations we perform a first sort by instance_ptr.\n//\n// This function returns NV_WARN_MORE_PROCESSING_REQUIRED if a fault buffer flush occurred during instance_ptr\n// translation and executed successfully, or the error code if it failed. NV_OK otherwise.\n//\n// Current scheme:\n// 1) sort by instance_ptr\n// 2) translate all instance_ptrs to VA spaces\n// 3) sort by va_space, fault address (GPU already reports 4K-aligned address) and access type\nstatic NV_STATUS preprocess_fault_batch(uvm_gpu_t *gpu, uvm_fault_service_batch_context_t *batch_context)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nThe batch processing time is measured to be in the range of 223µs to 553µs with a median of 313µs, of which, GPU runtime fault handling accounts for an average of 46.69% of the time (measured to be in the range of 50µs to 430µs with a median of 140µs).\n\n1）Thread Oversubscription\n\nWe enable thread oversubscription from the beginning of the execution by allocating one additional thread block to each SM ( 1 ). The thread block additionally allocated to each SM is inactive at first. It is important to note that the number of active thread blocks does not exceed that of the baseline, which is determined by the physical resource constraints. Once all of the warps in an active thread block are stalled due to page faults, the thread oversubscription mechanism context switches the active (but stalled) thread block with an inactive thread block ( 2 ). The thread oversubscription mechanism can be detrimental if it causes premature evictions. To prevent this, the GPU runtime monitors the premature eviction rates by periodically estimating the running average of the lifetime of pages by tracking when each page is allocated and evicted. We use the running average as an indicator of premature evictions. If the running average is decreased by a certain threshold, the thread oversubscription mechanism does not allow any more context switching by decrementing (and limiting) the number of concurrently runnable thread blocks ( 3 ).6 Otherwise, thread oversubscription allocates one additional thread block to each SM in an incremental manner.\n\n\n\n 2. Unobstrusive Eviction\n\nWhen a page fault interrupt is raised by the GPU MMU, the top-half interrupt service routine (ISR) responds. It checks whether the number of GPU resident pages is at capacity via the GPU memory status tracker. If so, it sends a preemptive eviction request to the GPU. The rest of the fault handling (e.g., preprocessing of the page faults, CPU-side page table walks) is performed by the bottom-half ISR.\n\n\n\n\n\nWhen the GPU runtime begins a batch’s processing, it checks the GPU memory status. If it is at capacity, it initiates a single page eviction ( 1 ). Once page X is evicted from the GPU’s memory, both CPU and GPU page tables are updated ( 2 ). Unlike the baseline case (Figure 4), page A can be migrated to the GPU memory without any delay ( 3 ). At the same time, page Y can be evicted using bidirectional transfers. Since the data transfer speed from the GPU to CPU memory is faster than the other way around [29], eviction is completely unobtrusive and migrations to the GPU can occur without any delay.\n\nIn short, thread oversubscription increase the batch size by switching in in-active thread block. and unobstrusive eviction avoid the serialization of swap pages between host and device.\n\n----------------------------------------\n\n\n# 10. Machine Learning Guided Optimal Use of GPU Unified Memory 2019\n\nTo enable better performance of UM, CUDA allows developers to give the UM driver additional advice on managing a given GPU memory range via an API function named cudaMemAdvise(const void *, size_t, enum cudaMemoryAdvise, int). The first two parameters of this function accept a pointer to a memory range with a specified size. The memory range should be allocated via cudaMallocManaged or declared via __managed__variables. The third parameter sets the advice for the memory range. The last parameter indicates the associated device’s id, which can indicate either a CPU or GPU device. The details and differences of these four kinds of advice are presented as follows:\n\n• Default: This represents the default on-demand page migration to accessing processor, using the first-touch policy.\n\n• cudaMemAdviseSetReadMostly: This advice is used for the data which is mostly going to be read from and only occasionally written to. The UM driver may create read-only copies of the data in a processor’s memory when that processor accesses it. If this region encounters any write requests, then only the write occurred page will be valid and other copies will be invalid.\n\n• cudaMemAdviseSetPreferredLocation: Once a target device is specified, this device memory can be set as the preferred location for the allocated data. The host memory can also be specified as the preferred location. Setting the preferred location does not cause data to migrate to that location immediately. The policy only guides what will happen when a fault occurs on the specified memory region: if data is already in the preferred location, the faulting processor will try to directly establish a mapping to the region without causing page migration. Otherwise, the data will be migrated to the processor accessing it if the data is not in the preferred location or if a direct mapping cannot be established.\n\n• cudaMemAdviseSetAccessedBy: This advice implies that the data will be accessed by a specified CPU or GPU device. It has no impact on the data location and will not cause data migration. It only causes the data to be always mapped in the specified processor’s page tables, when applicable. The mapping will be accordingly updated if the data is migrated somehow. This advice is useful to indicate that avoiding faults is important for some data, especially when the data is accessed by a GPU within a system containing multiple GPUs with peer-to-peer access enabled.\n\n----------------------------------------\n\n\n# 14. Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory [2024] 👍👍👍👍\n\nSame author: In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing[2021]\n\n\n\nThe UVM host driver on the host is open source with dependencies on the proprietary nvidia driver/resource manager and the host OS for memory management. This driver is a runtime fault servicing engine and the memory manager for managed memory allocations.\n\n\n 1. the fault is generated and handled by the hardware thread’s corresponding µTLB. The thread may continue executing instructions not blocked by a memory dependency. The fault propagates to the GPU memory management unit (GMMU), which writes the corresponding fault information into the GPU Fault Buffer and sends a hardware interrupt to the host. The fault buffer acts as a circular array, configured and managed by the UVM driver.\n    \n 2. The nvidia-uvm driver fetches the fault information, caches it on the host, and services the faults through\n 3. page processing: page table update and TLB shootdown on the host and GPU page table update\n 4. page migration: involves page migration.\n\nThe GPU exposes two functionalities to the host via the GPU command push-buffer—host-to GPU memory copy and fault replay.\nAs part of the fault servicing process, the driver instructs the GPU to copy pages into its memory, generally using high-performance hardware “copy engines.”\nOnce the GPU’s page tables are updated and the data is successfully migrated, the driver issues a fault replay, which clears the waiting status of µTLB, causing them to “replay” the prior miss.\n\nFault Handling:\n\nFirst, the GPU sends an interrupt over the interconnect to alert the host UVM driver of a page fault. The interrupt wakes up a worker thread to begin fault servicing if none is awake.\nSecond, the host retrieves the complete fault information from the GPU Fault Buffer.\nThe default fault retrieval policy reads faults until the batch size limit (i.e., 256 faults) is reached or no faults remain in the buffer.\n\n\nThese VABlocks serve as logical boundaries; the driver processes all batch faults within a single VABlock together, and each VABlock within a batch requires a distinct processing step.\n\n\n\n\n\nNotes:\n\n 1. when prefetching is not enabled, Service Faults is the major part of delay in CPU-GPU system. In this case, even NVlink does not matter. The reason is that unmapping and tlb-shut down in multi-cpu costs a lot.\n    \n 2. Pretching reduce the overhead by reduction of page fault and also increase the efficiency of NVlink.\n    \n 3. Oversubscription worse the case by finding empty space failed first and then evictim block to GPU. This worsen the performance.\n    \n\n1 & 2 explained:\n\n(1) unmapping host-side data takes place on the fault path and incurs significant overhead\n(2) certain hostside parallelizations of an application using UVM can exaggerate these unmapping costs.\nThe host OS performs this operation, and the costs likely stem from issues with virtual mappings across CPU cores, flushing dirty pages from caches and TLBs, NUMA, and other memory-adjacent issues.\nAdditionally, these operations do not take place in bulk due to the logical separation of VABlocks within UVM.\nThis is an area that deserves particular scrutiny as HMM also performs host page unmapping on the fault path using host OS mechanisms, implying a similar cost could be applied to all devices when using HMM [15, 26].\n\ncompared to cpu-gpu case, GPU-GPU on-demand page migration is faster due to the actual page table updates offloaded to the source GPU.\nFault servicing includes operations such as page unmapping and TLB shootdown on the source device.\nGPU page table updates and TLB shootdown are hardware based and relatively much faster.\n\n\n 3. Explained Process: (1) fail allocation\n    (2) evict a VABlock and migrate the data back to the host\n    (3) restart the block migration process, including host unmapping, data transfer, GPU mapping, page population, a process by which pages are filled with zero values before data is migrated to them.\n\nInterestingly, oversubscription diminishes the benefits of NVLink2. Oversubscription, as it is currently implemented, always evicts pages back to the host memory. This causes the CPU-GPU PCIe interconnect to become active for data eviction.\n\n👉 In short, in CPU-GPU system, service faults are major issue due to tlb shutdown and page table update. This even diminish the power of NVLink. Memory Oversubscription worsen the situation by failing to allocate memory in GPU, find eviction and eviction to CPU, adding these operation worsen the performance.\n👉 GPU-GPU does not have the service faults problem since page table update and tlb shutdown are handled by faster gpu hardware.\n👉 Besides, prefetching helps to improve performance a lot by reducing fault and better bandwitdh efficiency.\n',normalizedContent:' 1.  holistic performance analysis and optimization of unified virtual holistic performance analysis and optimization of unified virtual memory\n 2.  in-depth analyses of unified virtual memory system for gpu accelerated computing\n 3.  oversubscribing gpu unified virtual memory: implications and suggestions\n 4.  performance evaluation of advanced features in cuda unified memory\n 5.  interplay between hardware prefetcher and page eviction policy in cpu-gpu unified virtual memory\n 6.  unified memory: gpgpu-sim/uvm smart integration\n 7.  batch-aware unified memory management in gpus for irregular workloads\n 8.  an intelligent framework for oversubscription management in cpu-gpu unified memory\n 9.  architectural support for address translation on gpus designing memory management units for cpu/gpus with unified address spaces\n 10. machine learning guided optimal use of gpu unified memory\n 11. towards high performance paged memory for gpus\n 12. [virtualization] virtual thread: maximizing thread-level parallelism beyond gpu scheduling limit.\n 13. [virtualization] a survey of gpu multitasking methods supported by hardware architecture\n 14. fine-grain quantitative analysis of demand paging in unified virtual memory\n\nplan to read\n\n 1. early-adaptor: an adaptive framework for proactive uvm memory management\n 2. liberator: a data reuse framework for out-of-memory graph computing on gpus\n 3. [hpca] enabling large dynamic neural network training with learning-based memory management\n 4. gpuswap: enabling oversubscription of gpu memory through transparent swapping\n\n----------------------------------------\n\nunified memory history copied from evolution of nvidia gpu from microarchitectures pascal to ampere\n\ncuda 4 introduced uva (unified virtual addressing) to provide a single virtual memory address space for both cpu and gpu memory and enable pointers to be accessed from gpu code no matter where in the system they reside. uva enables zero-copy memory, a pinned cpu memory accessible by gpu code directly, over pcie, without the need for memory copy. this provides some of the convince of unified memory, but at the cost of worse performance, because gpu always accesses it with pcie’s low bandwidth and high latency.[1]\n\nlater, cuda 6 introduced unified memory, which creates a pool of managed memory that programs running on the cpu and gpu can access without explicit data movement. however, only when cpu and gpu processes are not running together because of the limitation of the kepler and maxwell gpu microarchitecture. also, the unified memory address space was limited to the size of the gpu memory.[1, 3]\n\ncuda 8 and pascal microarchitectures improve unified memory functionality by adding 49-bit virtual addressing and page faulting capability. the larger 49-bit virtual addresses are sufficient to enable gpus to access the entire system memory plus the memory of all gpus in the system. because of the memory page faulting functionality, the cuda system software does not need to synchronize all managed memory allocations to the gpu before each kernel lunch. instead, when a thread running on gpu faults on non-resident memory access(demanding page), it stalls until the page can be migrated and the page table updated. alternatively, the page may be mapped for remote access over pcie or nvlink interconnects.[1, 3, 6]\n\nthese new features of unified memory enable oversubscription of memory, which means that application running on a gpu can use data sets larger than ten their device memory.[1] while the unified memory model makes gpu programming more convenient, it comes at a cost; handling page faults and page migrations can be expensive. cuda 8 addresses this issue with features like prefetch and memory advice.\n\n----------------------------------------\n\n\n# 1. holistic performance analysis and optimization of unified virtual holistic performance analysis and optimization of unified virtual memory\n\nsame author with in-depth analyses of unified virtual memory system for gpu accelerated computing\n\n----------------------------------------\n\n\n# 3. oversubscribing gpu unified virtual memory: implications and suggestions\n\nuvm supports memory oversubscription, giving gpu programs the ability to use a larger amount of memory than the physical memory, without worrying about the problem of memory shortage.\n\nadvanced optimization techniques, mainly prefetching and memory usage hints [1], can be used to fine-tune the performance of uvm applications, mitigating the overheads caused by uvm.\n\n\n\n2）prefetching and hints prefetching and uvm hints are the major approaches provided by cuda, with the hope that page faults and memory thrashing could be prevented by fine-tuning the behavior of uvm at runtime.\n\nby calling cudamemprefetchasync (pf), a memory block could be prefetched to gpu. uvm hints provide informed decisions on page handling by indicating the access patterns of data.\n\nchanging uvm hints is done by invoking cudamemadvise with one of the following policies：\n\n• cudamemadvisesetaccessedby (ab) implies that the device keeps a direct mapping in its page table. when the data is migrated, the mapping is re-established.\n• cudamemadvisesetpreferredlocation (pl) pins the data and prevents the page to be migrated, which is useful when the page is mainly accessed on one side.\n• cudamemadvisesetreadmostly (rm) indicates the data region is read-intensive. it creates a read-only copy of the page on the faulting side, allowing on current access on both sides.\n\n\nonly one policy (ab, pl, or rm) could be specified for each memory block, but each policy can be used along with prefetching.\n\nsuggestions: to ensure performance under all oversubscription conditions, programmer needs to choose the uvm hints dynamically based on the application’s memory usage and available gpu memory. as a prerequisite, the size of the fall pages needs to be estimated or measured by experiment. before kernel launch, the program should first check the size of available gpu memory (e.g. via the cudamemgetinfo api). if no oversubscription will happen, or the available memory is larger than the size of fall pages, the programmer could set hints based on the conclusions provided by related researches [24]. otherwise, based on our findings, applying the hint ab is a preferable choice.\n\n----------------------------------------\n\n\n# 4. performance evaluation of advanced features in cuda unified memory\n\ncuda has introduced new features for optimizing the data migration on um, i.e., memory advises and prefetch. instead of solely relying on page faults, the memory advises feature allows the programmer to provide data access pattern for each memory object so that the runtime can optimize migration decisions. the prefetch proactively triggers asynchronous data migration to gpu before the data is accessed, which reduces page faults and, consequently, the overhead in handling page faults.\n\n-using memory advises improves application performance in oversubscription execution on the intel platform and in-memory executions on the ibm platform.\n\n-um prefetch provides a significant performance improvement on the intel-volta/pascal-pci-e based systems while it does not show a performance improvement on the power9-volta-nvlink based system\n\num was first introduced in cuda 6.0 [21]. only until the recent nvidia pascal microarchitecture that has hardware support for page faults.\n\n\n\n• cudamemadvisesetaccessedby establishes a direct mapping of data to a specified device. figure 2c illustrates an example of a physical page on gpu being remotely access from the host. when cudamemadvisesetpreferredlocation is applied, cuda runtime tries to build a direct mapping to the page to avoid data migration so that the destination can access data remotely. differently from cudamemadvisesetpreferredlocation, this cudamemadvisesetaccessedby does not try to pin pages on a specific device; instead, its main effect is to establish mapping on the remote device. this advice takes effect on the creation of the memory pages. the mapping will be re-established after the pages are migrated.\n\n• cudamemadvisesetpreferredlocation sets the preferred physical location of pages. this advice pins a page and prevents it from migrating to other memories. figure 2b illustrates a page preferred on the host side, and gpu uses remote mapping to access the page. this advice established a direct (remote) mapping to the memory page. when accessing the page remotely, data is fetched through the remote memory instead of generating a page fault. if the underlying hardware does not support the remote mapping, the page will be migrated as in the standard um. cudamemadvisesetpreferredlocation is useful for applications with little data sharing between cpu and gpu, i.e., part of the application is executed completely on the gpu, and the rest of the application executes on the host. data that is being used mostly by the gpu can be pinned to the gpu with the advice, avoiding memory thrashing.\n\n• cudamemadvisesetreadmostly implies a read-intensive data region. in the basic um, accessing a page on a remote side triggers page migration. however, with cudamemadvisesetreadmostly, a read-only duplicate of the page will be created on the faulting side, which prevents page faults and data migration in the future. figure 2a illustrates an example, where the second access (step 5) has no page fault and is local access. this mechanism, however, results in a high overhead if there is any update to this memory region because all copies of the corresponding page will be invalidated to preserve consistency between different copies. thus, this advice is often used in read-only data structures, such as lookup tables and application parameters.\n\nin general, we found both memory advises and prefetch to be simple and effective.\n\n----------------------------------------\n\n\n# 5. interplay between hardware prefetcher and page eviction policy in cpu-gpu unified virtual memory\n\ncons in traditional gpu: complicated asynchronous user-directed constructs to overlap data migration and kernel execution are used to address this issue. the second challenge is memory over-subscription. when the working set of the gpu kernel cannot fit in the device memory, the programmers have to painstakingly redefine the data structures and tile the data to transfer back and forth in chunks.\n\nthis flow is inspired by -> 11. towards high performance paged memory for gpu.\n\n1 scheduled threads generate global memory accesses.\n\n2 each sm has its own load/store unit. every load/store unit has its own tlb. load/store unit performs a tlb look up to find whether the translation for the issued memory access is cached in tlb or not. a tlb miss is relayed to the gmmu.\n\n3 the gmmu walks through the page table looking for a pte corresponding to the requested page with valid flag set. a far-fault occurs if there is no pte for the requested page or the valid flag is not set. then the far-fault is registered in the far-fault miss status handling registers (mshrs).\n\n4 the page is scheduled for transfer over cpu-gpu pci-e interconnect.\n\n5 a 4kb page is allocated on demand and data is migrated from host to device memory.\n\n6 the mshrs are consulted to notify the corresponding load/store unit and the memory access is replayed. a new pte entry is added to the page table with valid\n\n\n\nthis paper introduces random, sequential and tree-based neighborhood prefetcher in detail.\n\nand come up with pre-eviction for tree-based neighborhood, different from lru eviction used in nvidia.\n\n\n\n----------------------------------------\n\n\n# 7. batch-aware unified memory management in gpus for irregular workloads 2020\n\npropose:\n\n(1) increases the batch size (i.e., the number of page faults handled together), thereby amortizing the gpu runtime fault handling time, and reduces the number of batches by supporting cpu-like thread block context switching\n\nthread oversubscription (to), a cpu-like thread block context switching technique, to effectively amortize the gpu runtime fault handling time by increasing the batch size (i.e., the number of page faults handled together).\n\n(2) takes page eviction off the critical path with no hardware changes by overlapping evictions with cpu-to-gpu page migrations. unobtrusive eviction (ue) to take gpu page evictions off the critical path with no hardware changes based on the idea of overlapping page evictions with cpu-to-gpu page migrations.\n\nprior work reports that page fault handling latency ranges from 20µs to 50µs [53]. we find that these numbers are conservative and can be worse depending on the applications and systems. unfortunately, this page fault latency, which is in the order of microseconds, cannot be easily hidden even with ample thread-level parallelism (tlp) in gpus, especially when gpu memory is oversubscribed.\n\n\n\nthe gpu runtime processes a group of gpu page faults together, rather than processing each individual one, in order to amortize the overhead of multiple round-trip latencies over the pcie bus and to avoid invoking multiple interrupt service routines (isrs) in the operating system (os). to efficiently process an excessive number of page faults, the gpu runtime performs a series of operations such as preprocessing all the page faults and inserting page prefetching requests, which takes a significant amount of time (in the range of tens to hundreds of microseconds). once all the operations (e.g., cpu page table walks for all the page faults, page allocation and eviction scheduling, etc.) are finished, page migrations between the cpu and the gpu begin.\n\nthis page fault handling is expensive because (1) it requires long latency communications between the cpu and gpu over the pcie bus, and (2) the gpu runtime performs a very expensive fault handling service routine.\n\nto amortize the overhead, the gpu runtime processes a group of page faults together, which we refer to as batch processing.\n\nwhen a page fault exception is raised by the gpu memory management unit (mmu), the gpu runtime begins to handle the exception, shown in 1.\n\n\n\nfrom this, we conclude that page evictions and new page allocations are serialized in modern gpus to prevent the new pages from overwriting the evicted pages. note that an eviction is required on every page fault once the pages resident in the gpu’s memory are at capacity.\n\n\n\nthis preprocessing includes sorting the page faults in ascending order of page addresses (to accelerate the page table walks) and the analysis of page addresses to insert page prefetching requests.1 we refer to the time taken by the gpu runtime to perform a collection of operations to handle many page faults together as gpu runtime fault handling time.\n\nhttps://github.com/acsl-technion/gaia_nvidia/blob/e23e4d926f576c2c4169664b6add89e1368ee849/kernel/nvidia-uvm/uvm8_gpu_replayable_faults.c#l787\n\n// fault cache preprocessing for fault coalescing\n//\n// this function generates an ordered view of the given fault_cache in which faults are sorted by va space, fault\n// address (aligned to 4k) and access type "intrusiveness" (atomic - write - read - prefetch). in order to minimize\n// the number of instance_ptr to va space translations we perform a first sort by instance_ptr.\n//\n// this function returns nv_warn_more_processing_required if a fault buffer flush occurred during instance_ptr\n// translation and executed successfully, or the error code if it failed. nv_ok otherwise.\n//\n// current scheme:\n// 1) sort by instance_ptr\n// 2) translate all instance_ptrs to va spaces\n// 3) sort by va_space, fault address (gpu already reports 4k-aligned address) and access type\nstatic nv_status preprocess_fault_batch(uvm_gpu_t *gpu, uvm_fault_service_batch_context_t *batch_context)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nthe batch processing time is measured to be in the range of 223µs to 553µs with a median of 313µs, of which, gpu runtime fault handling accounts for an average of 46.69% of the time (measured to be in the range of 50µs to 430µs with a median of 140µs).\n\n1）thread oversubscription\n\nwe enable thread oversubscription from the beginning of the execution by allocating one additional thread block to each sm ( 1 ). the thread block additionally allocated to each sm is inactive at first. it is important to note that the number of active thread blocks does not exceed that of the baseline, which is determined by the physical resource constraints. once all of the warps in an active thread block are stalled due to page faults, the thread oversubscription mechanism context switches the active (but stalled) thread block with an inactive thread block ( 2 ). the thread oversubscription mechanism can be detrimental if it causes premature evictions. to prevent this, the gpu runtime monitors the premature eviction rates by periodically estimating the running average of the lifetime of pages by tracking when each page is allocated and evicted. we use the running average as an indicator of premature evictions. if the running average is decreased by a certain threshold, the thread oversubscription mechanism does not allow any more context switching by decrementing (and limiting) the number of concurrently runnable thread blocks ( 3 ).6 otherwise, thread oversubscription allocates one additional thread block to each sm in an incremental manner.\n\n\n\n 2. unobstrusive eviction\n\nwhen a page fault interrupt is raised by the gpu mmu, the top-half interrupt service routine (isr) responds. it checks whether the number of gpu resident pages is at capacity via the gpu memory status tracker. if so, it sends a preemptive eviction request to the gpu. the rest of the fault handling (e.g., preprocessing of the page faults, cpu-side page table walks) is performed by the bottom-half isr.\n\n\n\n\n\nwhen the gpu runtime begins a batch’s processing, it checks the gpu memory status. if it is at capacity, it initiates a single page eviction ( 1 ). once page x is evicted from the gpu’s memory, both cpu and gpu page tables are updated ( 2 ). unlike the baseline case (figure 4), page a can be migrated to the gpu memory without any delay ( 3 ). at the same time, page y can be evicted using bidirectional transfers. since the data transfer speed from the gpu to cpu memory is faster than the other way around [29], eviction is completely unobtrusive and migrations to the gpu can occur without any delay.\n\nin short, thread oversubscription increase the batch size by switching in in-active thread block. and unobstrusive eviction avoid the serialization of swap pages between host and device.\n\n----------------------------------------\n\n\n# 10. machine learning guided optimal use of gpu unified memory 2019\n\nto enable better performance of um, cuda allows developers to give the um driver additional advice on managing a given gpu memory range via an api function named cudamemadvise(const void *, size_t, enum cudamemoryadvise, int). the first two parameters of this function accept a pointer to a memory range with a specified size. the memory range should be allocated via cudamallocmanaged or declared via __managed__variables. the third parameter sets the advice for the memory range. the last parameter indicates the associated device’s id, which can indicate either a cpu or gpu device. the details and differences of these four kinds of advice are presented as follows:\n\n• default: this represents the default on-demand page migration to accessing processor, using the first-touch policy.\n\n• cudamemadvisesetreadmostly: this advice is used for the data which is mostly going to be read from and only occasionally written to. the um driver may create read-only copies of the data in a processor’s memory when that processor accesses it. if this region encounters any write requests, then only the write occurred page will be valid and other copies will be invalid.\n\n• cudamemadvisesetpreferredlocation: once a target device is specified, this device memory can be set as the preferred location for the allocated data. the host memory can also be specified as the preferred location. setting the preferred location does not cause data to migrate to that location immediately. the policy only guides what will happen when a fault occurs on the specified memory region: if data is already in the preferred location, the faulting processor will try to directly establish a mapping to the region without causing page migration. otherwise, the data will be migrated to the processor accessing it if the data is not in the preferred location or if a direct mapping cannot be established.\n\n• cudamemadvisesetaccessedby: this advice implies that the data will be accessed by a specified cpu or gpu device. it has no impact on the data location and will not cause data migration. it only causes the data to be always mapped in the specified processor’s page tables, when applicable. the mapping will be accordingly updated if the data is migrated somehow. this advice is useful to indicate that avoiding faults is important for some data, especially when the data is accessed by a gpu within a system containing multiple gpus with peer-to-peer access enabled.\n\n----------------------------------------\n\n\n# 14. fine-grain quantitative analysis of demand paging in unified virtual memory [2024] 👍👍👍👍\n\nsame author: in-depth analyses of unified virtual memory system for gpu accelerated computing[2021]\n\n\n\nthe uvm host driver on the host is open source with dependencies on the proprietary nvidia driver/resource manager and the host os for memory management. this driver is a runtime fault servicing engine and the memory manager for managed memory allocations.\n\n\n 1. the fault is generated and handled by the hardware thread’s corresponding µtlb. the thread may continue executing instructions not blocked by a memory dependency. the fault propagates to the gpu memory management unit (gmmu), which writes the corresponding fault information into the gpu fault buffer and sends a hardware interrupt to the host. the fault buffer acts as a circular array, configured and managed by the uvm driver.\n    \n 2. the nvidia-uvm driver fetches the fault information, caches it on the host, and services the faults through\n 3. page processing: page table update and tlb shootdown on the host and gpu page table update\n 4. page migration: involves page migration.\n\nthe gpu exposes two functionalities to the host via the gpu command push-buffer—host-to gpu memory copy and fault replay.\nas part of the fault servicing process, the driver instructs the gpu to copy pages into its memory, generally using high-performance hardware “copy engines.”\nonce the gpu’s page tables are updated and the data is successfully migrated, the driver issues a fault replay, which clears the waiting status of µtlb, causing them to “replay” the prior miss.\n\nfault handling:\n\nfirst, the gpu sends an interrupt over the interconnect to alert the host uvm driver of a page fault. the interrupt wakes up a worker thread to begin fault servicing if none is awake.\nsecond, the host retrieves the complete fault information from the gpu fault buffer.\nthe default fault retrieval policy reads faults until the batch size limit (i.e., 256 faults) is reached or no faults remain in the buffer.\n\n\nthese vablocks serve as logical boundaries; the driver processes all batch faults within a single vablock together, and each vablock within a batch requires a distinct processing step.\n\n\n\n\n\nnotes:\n\n 1. when prefetching is not enabled, service faults is the major part of delay in cpu-gpu system. in this case, even nvlink does not matter. the reason is that unmapping and tlb-shut down in multi-cpu costs a lot.\n    \n 2. pretching reduce the overhead by reduction of page fault and also increase the efficiency of nvlink.\n    \n 3. oversubscription worse the case by finding empty space failed first and then evictim block to gpu. this worsen the performance.\n    \n\n1 & 2 explained:\n\n(1) unmapping host-side data takes place on the fault path and incurs significant overhead\n(2) certain hostside parallelizations of an application using uvm can exaggerate these unmapping costs.\nthe host os performs this operation, and the costs likely stem from issues with virtual mappings across cpu cores, flushing dirty pages from caches and tlbs, numa, and other memory-adjacent issues.\nadditionally, these operations do not take place in bulk due to the logical separation of vablocks within uvm.\nthis is an area that deserves particular scrutiny as hmm also performs host page unmapping on the fault path using host os mechanisms, implying a similar cost could be applied to all devices when using hmm [15, 26].\n\ncompared to cpu-gpu case, gpu-gpu on-demand page migration is faster due to the actual page table updates offloaded to the source gpu.\nfault servicing includes operations such as page unmapping and tlb shootdown on the source device.\ngpu page table updates and tlb shootdown are hardware based and relatively much faster.\n\n\n 3. explained process: (1) fail allocation\n    (2) evict a vablock and migrate the data back to the host\n    (3) restart the block migration process, including host unmapping, data transfer, gpu mapping, page population, a process by which pages are filled with zero values before data is migrated to them.\n\ninterestingly, oversubscription diminishes the benefits of nvlink2. oversubscription, as it is currently implemented, always evicts pages back to the host memory. this causes the cpu-gpu pcie interconnect to become active for data eviction.\n\n👉 in short, in cpu-gpu system, service faults are major issue due to tlb shutdown and page table update. this even diminish the power of nvlink. memory oversubscription worsen the situation by failing to allocate memory in gpu, find eviction and eviction to cpu, adding these operation worsen the performance.\n👉 gpu-gpu does not have the service faults problem since page table update and tlb shutdown are handled by faster gpu hardware.\n👉 besides, prefetching helps to improve performance a lot by reducing fault and better bandwitdh efficiency.\n',charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"TensorCore Paper List",frontmatter:{title:"TensorCore Paper List",date:"2023-11-17T00:00:00.000Z",permalink:"/pages/44871e/"},regularPath:"/03.gpu/05.TensorCore.html",relativePath:"03.gpu/05.TensorCore.md",key:"v-2861ec8b",path:"/pages/44871e/",headers:[{level:3,title:"1. Modeling Deep Learning Accelerator Enabled GPUs",slug:"_1-modeling-deep-learning-accelerator-enabled-gpus",normalizedTitle:"1. modeling deep learning accelerator enabled gpus",charIndex:1}],headersStr:"1. Modeling Deep Learning Accelerator Enabled GPUs",content:" 1. Modeling Deep Learning Accelerator Enabled GPUs 2019\n 2. Dissecting Tensor Cores via Microbenchmarks: Latency, Throughput and Numeric Behaviors 2023\n 3. Demystifying Tensor Cores to Optimize Half-Precision Matrix Multiply 2020\n 4. CS 380 - GPU and GPGPU ProgrammingLecture 26: Programming Tensor Cores\n 5. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmark 2018\n\n----------------------------------------\n\n\n# 1. Modeling Deep Learning Accelerator Enabled GPUs",normalizedContent:" 1. modeling deep learning accelerator enabled gpus 2019\n 2. dissecting tensor cores via microbenchmarks: latency, throughput and numeric behaviors 2023\n 3. demystifying tensor cores to optimize half-precision matrix multiply 2020\n 4. cs 380 - gpu and gpgpu programminglecture 26: programming tensor cores\n 5. dissecting the nvidia volta gpu architecture via microbenchmark 2018\n\n----------------------------------------\n\n\n# 1. modeling deep learning accelerator enabled gpus",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Memory Behaviour Paper List",frontmatter:{title:"Memory Behaviour Paper List",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/45871e/"},regularPath:"/03.gpu/06.MemoryBehaviour.html",relativePath:"03.gpu/06.MemoryBehaviour.md",key:"v-04292a45",path:"/pages/45871e/",headers:[{level:3,title:"1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior",slug:"_1-a-comparative-analysis-of-microarchitecture-effects-on-cpu-and-gpu-memory-system-behavior",normalizedTitle:"1. a comparative analysis of microarchitecture effects on cpu and gpu memory system behavior",charIndex:1}],headersStr:"1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior",content:" 1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior 2014\n 2. \n\n----------------------------------------\n\n\n# 1. A Comparative Analysis of Microarchitecture Effects on CPU and GPU Memory System Behavior\n\nCPU cores must extract very wide ILP in order to expose MLP to the memory hierarchy, and this MLP can be limited to lower levels of the memory hierarchy due to L1 cache locality. On the other hand, GPU cores and caches aim to mitigate MLP limitations, allowing the programmer to focus their efforts on leveraging the available MLP.\n\n\n\nRodinia is categorized into the above features.\n\n * Pipleline\n * Iterative\n * Head Reads(I/c)\n\nA fairly common factor in compute and memory op count differences between system configurations is due to register handling. For x86 CPU applications, the small architected register set (16) can cause register spilling to the stack and recomputation of previously computed values.\n\nIn contrast, GPU cores have some flexibility in register use due to their core multithreading. By running fewer GPU threads per core and late binding register specifiers to physical registers, there is more flexibility for each thread to access more registers, which can avoid spilling and recomputation.\n\nMemory Access Pattern\n\nCPU cores use a small set of deep per-thread instruction windows, and high-frequency pipelines and caches to expose parallel memory accesses. In contrast, GPU cores expose parallel memory accesses by executing 100s–1000s more threads at lower frequencies, and threads are grouped for smaller perthread instruction windows and memory request coalescing.\n\nwhile CPU cores rely heavily on L1 caches to capture locality, GPU cores capture most locality with coalescing and lessen the L1 cache responsibilities by providing scratch memory.\n\nBeyond the L1 caches, the memory systems tend to capture very similar locality. Further, we see that different core threading and cache filtering result in extreme differences in instantaneous memory access rates; CPU caches tend to filter accesses down to regular intervals, while GPU cores tend to issue bursts of accesses.\n\n\n\nScratch memory: GPU cores provide scratch memory, which can function as local storage for groups of threads to expand the space of local storage with register-like accessibility. In CUDA benchmarks that use the GPU scratch memory, kernels are typically organized into three stages: (1) read a small portion of data from global memory into the scratch memory, (2) compute on the data in scratch memory, and (3) write results back to global memory.\n\nSince GPU request coalescing behaves similarly to CPU single-instruction, multiple-data (SIMD) vectorization, vectorization reduces the total number of memory accesses by 1.32–1.69× (1.44× geometric mean), and that most of the eliminated accesses are to heap data.\n\nOverall, GPU scratch memory and request coalescing reduce the number of global memory accesses by 18–100× compared to CPU applications (27× in the geometric mean).\n\nCompared to CPU cores, this reduction alleviates pressure on caches, which in turn allows GPU cores to operate at lower frequencies while still serving data to threads at rates comparable to or greater than CPU cores.\n\nSpatial Locality\n\nCPU threads have extremely high spatial locality, typically striding through all elements in a heap cache line in subsequent algorithm loop iterations. These access patterns, which also include accesses to stack/local memory that is persistent over many loop iterations, result in high L1 cache hit rates that even exceed those expected by simple strided read memory access.\n\nFor GPU, small number of remaining spatially local accesses is likely due to separate thread groups accessing the same data rather than thread groups being unable to fully coalesce accesses.\n\nTemporal Locality\n\nFor CPU, this leaves the L2 caches mostly responsible for capturing temporally local accesses to data shared across cores rather than temporally or spatially local accesses to data previously evicted from the L1 caches due to limited capacity.\n\nFor GPU, This indicates that instead of competing for L1 capacity, GPU threads from separate cores are generating most of the temporally local accesses to single cache lines, similar to the CPU L2.\n\nBased on the above observations, we find that CPU and GPU L1 caches have very different importance, though their filtering roles are similar.\n\nIn the aggregate for data-parallel workloads, CPU L1 caches have many responsibilities; they must be designed to capture both the spatial locality for heap data accesses and the temporal locality of stack accesses. Fortunately for data-parallel workloads, these responsibilities rarely conflict given sufficient L1 capacity, so CPU L1s are quite effective and important for capturing locality.\n\nFor GPU applications, register and scratch memory can shift local variable accesses away from the caches, which eliminates the L1 responsibility for capturing temporally local stack requests. Further, GPU coalescing greatly reduces the importance of spatial locality across separate heap accesses, so the L1 caches are mostly responsible for capturing the small number of temporally local accesses from separate GPU threads on the same core(this is different from cpu), diminishing the overall responsibility of the GPU L1s compared to CPU L1s.\n\nBandwidth Demands\n\n\n\nThe key takeaway here is that GPU burst access behavior results from the way that GPUs group and launch threads. Specifically, at the beginning of a kernel, all capable thread block contexts begin executing at roughly the same time, so this can cause very large bursts of independent accesses.\n\nFollowing this initial burst, smaller but still significant access bursts occur each time a new thread block begins executing or when thread groups pass synchronization events.\n\nBy contrast, CPU cache access filtering tends to modulate the core’s ability to issue nearly as many parallel accesses to off-chip memory.\n\nLatency Sensitivity and Bandwidth Sensitivity\n\n\n\nThis Figue show CPU is sensitive to latency, but gpu to bandwidth.\n\nInteresting thoughts*\n\n-cache shared by CPU and GPU -interconnect and off-chip memory scheduling\n\nThese should take different characteristic of CPU and GPU into consideration.",normalizedContent:" 1. a comparative analysis of microarchitecture effects on cpu and gpu memory system behavior 2014\n 2. \n\n----------------------------------------\n\n\n# 1. a comparative analysis of microarchitecture effects on cpu and gpu memory system behavior\n\ncpu cores must extract very wide ilp in order to expose mlp to the memory hierarchy, and this mlp can be limited to lower levels of the memory hierarchy due to l1 cache locality. on the other hand, gpu cores and caches aim to mitigate mlp limitations, allowing the programmer to focus their efforts on leveraging the available mlp.\n\n\n\nrodinia is categorized into the above features.\n\n * pipleline\n * iterative\n * head reads(i/c)\n\na fairly common factor in compute and memory op count differences between system configurations is due to register handling. for x86 cpu applications, the small architected register set (16) can cause register spilling to the stack and recomputation of previously computed values.\n\nin contrast, gpu cores have some flexibility in register use due to their core multithreading. by running fewer gpu threads per core and late binding register specifiers to physical registers, there is more flexibility for each thread to access more registers, which can avoid spilling and recomputation.\n\nmemory access pattern\n\ncpu cores use a small set of deep per-thread instruction windows, and high-frequency pipelines and caches to expose parallel memory accesses. in contrast, gpu cores expose parallel memory accesses by executing 100s–1000s more threads at lower frequencies, and threads are grouped for smaller perthread instruction windows and memory request coalescing.\n\nwhile cpu cores rely heavily on l1 caches to capture locality, gpu cores capture most locality with coalescing and lessen the l1 cache responsibilities by providing scratch memory.\n\nbeyond the l1 caches, the memory systems tend to capture very similar locality. further, we see that different core threading and cache filtering result in extreme differences in instantaneous memory access rates; cpu caches tend to filter accesses down to regular intervals, while gpu cores tend to issue bursts of accesses.\n\n\n\nscratch memory: gpu cores provide scratch memory, which can function as local storage for groups of threads to expand the space of local storage with register-like accessibility. in cuda benchmarks that use the gpu scratch memory, kernels are typically organized into three stages: (1) read a small portion of data from global memory into the scratch memory, (2) compute on the data in scratch memory, and (3) write results back to global memory.\n\nsince gpu request coalescing behaves similarly to cpu single-instruction, multiple-data (simd) vectorization, vectorization reduces the total number of memory accesses by 1.32–1.69× (1.44× geometric mean), and that most of the eliminated accesses are to heap data.\n\noverall, gpu scratch memory and request coalescing reduce the number of global memory accesses by 18–100× compared to cpu applications (27× in the geometric mean).\n\ncompared to cpu cores, this reduction alleviates pressure on caches, which in turn allows gpu cores to operate at lower frequencies while still serving data to threads at rates comparable to or greater than cpu cores.\n\nspatial locality\n\ncpu threads have extremely high spatial locality, typically striding through all elements in a heap cache line in subsequent algorithm loop iterations. these access patterns, which also include accesses to stack/local memory that is persistent over many loop iterations, result in high l1 cache hit rates that even exceed those expected by simple strided read memory access.\n\nfor gpu, small number of remaining spatially local accesses is likely due to separate thread groups accessing the same data rather than thread groups being unable to fully coalesce accesses.\n\ntemporal locality\n\nfor cpu, this leaves the l2 caches mostly responsible for capturing temporally local accesses to data shared across cores rather than temporally or spatially local accesses to data previously evicted from the l1 caches due to limited capacity.\n\nfor gpu, this indicates that instead of competing for l1 capacity, gpu threads from separate cores are generating most of the temporally local accesses to single cache lines, similar to the cpu l2.\n\nbased on the above observations, we find that cpu and gpu l1 caches have very different importance, though their filtering roles are similar.\n\nin the aggregate for data-parallel workloads, cpu l1 caches have many responsibilities; they must be designed to capture both the spatial locality for heap data accesses and the temporal locality of stack accesses. fortunately for data-parallel workloads, these responsibilities rarely conflict given sufficient l1 capacity, so cpu l1s are quite effective and important for capturing locality.\n\nfor gpu applications, register and scratch memory can shift local variable accesses away from the caches, which eliminates the l1 responsibility for capturing temporally local stack requests. further, gpu coalescing greatly reduces the importance of spatial locality across separate heap accesses, so the l1 caches are mostly responsible for capturing the small number of temporally local accesses from separate gpu threads on the same core(this is different from cpu), diminishing the overall responsibility of the gpu l1s compared to cpu l1s.\n\nbandwidth demands\n\n\n\nthe key takeaway here is that gpu burst access behavior results from the way that gpus group and launch threads. specifically, at the beginning of a kernel, all capable thread block contexts begin executing at roughly the same time, so this can cause very large bursts of independent accesses.\n\nfollowing this initial burst, smaller but still significant access bursts occur each time a new thread block begins executing or when thread groups pass synchronization events.\n\nby contrast, cpu cache access filtering tends to modulate the core’s ability to issue nearly as many parallel accesses to off-chip memory.\n\nlatency sensitivity and bandwidth sensitivity\n\n\n\nthis figue show cpu is sensitive to latency, but gpu to bandwidth.\n\ninteresting thoughts*\n\n-cache shared by cpu and gpu -interconnect and off-chip memory scheduling\n\nthese should take different characteristic of cpu and gpu into consideration.",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Large Language Model Paper List",frontmatter:{title:"Large Language Model Paper List",date:"2023-12-19T00:00:00.000Z",permalink:"/pages/458720/"},regularPath:"/03.gpu/08.LLM.html",relativePath:"03.gpu/08.LLM.md",key:"v-65bd31e5",path:"/pages/458720/",headers:[{level:3,title:"1. Efficient Memory Management for Large Language Model Serving with PagedAttention",slug:"_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention",normalizedTitle:"1. efficient memory management for large language model serving with pagedattention",charIndex:1},{level:3,title:"2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory",slug:"_2-llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory",normalizedTitle:"2. llm in a flash: efficient large language model inference with limited memory",charIndex:93},{level:3,title:"1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",slug:"_1-a-survey-on-hallucination-in-large-language-models-principles-taxonomy-challenges-and-open-questions",normalizedTitle:"1. a survey on hallucination in large language models: principles, taxonomy, challenges, and open questions",charIndex:4386}],headersStr:"1. Efficient Memory Management for Large Language Model Serving with PagedAttention 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory 1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",content:" 1. Efficient Memory Management for Large Language Model Serving with PagedAttention [2023]\n 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory [Apple 2023]\n\n----------------------------------------\n\n\n# 1. Efficient Memory Management for Large Language Model Serving with PagedAttention\n\nDisscussed the GEMM in prompt and GEMV in auto regression. In GEMV, LLM is memory bound. There is lot of fragment in KVCache. It also quantize the memory necessity for parameter in KV Cache. They came up the method similar to paging in OS to manage KV in KV cache, reducing the fragment.\n\n\n# 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory\n\nUpproject matrix and downprojection matrix: https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/ Related paper: Parameter-Efficient Transfer Learning for NLP This introduce low-rank.\n\nsliding window.\n\n 1. high sparsity in FeedForward Layers, more than 90% Selectively only load parameters from memory either no-zero input or predicted have non-zero output\n\n 2. Minimize data transfer and maximize flash memory throughout Window sliding: Load parameters for only the past few tokens, reusing activations from recently computed tokens. This sliding window approach reduces the number of IO requests to load weights. Row-column bundling: We store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. This increases throughput by reading larger chunks.\n\n 3. Predict FFN sparsity and avoid loading zeroed-out parameter to minimize the number of weights to be transferred from flash memory to DRAM.\n\n 4. Static memory preallocation\n\nAlso a model to predict the tradeoff between loading less data and reading larger chunks\n\nLoad only 2% of FFN layer from flash\n\n 1. Larger chunk Although throughput growth is not linear (larger chunks take longer to transfer), the latency for the initial byte becomes a smaller fraction of the total request time, resulting in more efficient data reading.\n\n 2. Load From Flash\n\n2.1 inherent sparsity found in Feed-Forward Network (FFN) model\n\nSelective Persistence Strategy Retain the embeddings and matrices within the attention mechanism of the transformer constant.Attentions weights 1/3 of the model size.For the Feed-Forward Network (FFN) portions, only the non-sparse segments are dynamically loaded into DRAM as needed.\n\nAnticipating ReLU Sparsity\nRelu activation can induce 90% sparsity. Optimize preceding layer, up project by low-rank predictor to identify the zeroed elements post-ReLU.\nIn contrast to their work, our predictor needs only the output of the current layer’s attention module and not the previous layer’s FFN module.\n\nNeuron Data Management via Sliding Window Technique Our approach focuses on managing neuron data by employing a Sliding Window Technique. This methodology entails maintaining neuron data only for a recent subset of input tokens in the memory.\nThe key aspect of this technique is the selective loading of neuron data that differs between the current input token and its immediate predecessors.\nFrees up memory resources previously allocated to neuron data from older tokens that are no longer within the sliding window\n\nLet sagg(k) denote the cumulative use of neuron data across a sequence of k input tokens. This reduction in data loading is counterbalanced by the memory cost associated with storing sagg(k). In determining the size of the sliding window, the aim is to maximize it within the constraints imposed by the available memory capacity.\n\n2.2 Improve Transfer Throughput with Increased Chunk Sizes\n\nBundling Columns and Rows for upward and downward projection\n\nBundling Based on Co-activation fetch neuron with its cloest friend. But there is WARM-GUY problem.\n\n2.3 Optimized Data Management in DRAM\n\nWhen a substantial portion (approximately 25%) of the Feed-Forward Networks (FFNs) in DRAM needs to be rewritten.\n\nWhen introducing data for new neurons, reallocating the matrix and appending new matrices can lead to significant overhead due to the need for rewriting existing neurons data in DRAM. This involves the preallocation of all necessary memory and the establishment of a corresponding data structure for efficient management.\n\n----------------------------------------\n\nLLM Principles\n\n\n# 1. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\n\n",normalizedContent:" 1. efficient memory management for large language model serving with pagedattention [2023]\n 2. llm in a flash: efficient large language model inference with limited memory [apple 2023]\n\n----------------------------------------\n\n\n# 1. efficient memory management for large language model serving with pagedattention\n\ndisscussed the gemm in prompt and gemv in auto regression. in gemv, llm is memory bound. there is lot of fragment in kvcache. it also quantize the memory necessity for parameter in kv cache. they came up the method similar to paging in os to manage kv in kv cache, reducing the fragment.\n\n\n# 2. llm in a flash: efficient large language model inference with limited memory\n\nupproject matrix and downprojection matrix: https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/ related paper: parameter-efficient transfer learning for nlp this introduce low-rank.\n\nsliding window.\n\n 1. high sparsity in feedforward layers, more than 90% selectively only load parameters from memory either no-zero input or predicted have non-zero output\n\n 2. minimize data transfer and maximize flash memory throughout window sliding: load parameters for only the past few tokens, reusing activations from recently computed tokens. this sliding window approach reduces the number of io requests to load weights. row-column bundling: we store a concatenated row and column of the up-projection and down-projection layers to read bigger contiguous chunks from flash memory. this increases throughput by reading larger chunks.\n\n 3. predict ffn sparsity and avoid loading zeroed-out parameter to minimize the number of weights to be transferred from flash memory to dram.\n\n 4. static memory preallocation\n\nalso a model to predict the tradeoff between loading less data and reading larger chunks\n\nload only 2% of ffn layer from flash\n\n 1. larger chunk although throughput growth is not linear (larger chunks take longer to transfer), the latency for the initial byte becomes a smaller fraction of the total request time, resulting in more efficient data reading.\n\n 2. load from flash\n\n2.1 inherent sparsity found in feed-forward network (ffn) model\n\nselective persistence strategy retain the embeddings and matrices within the attention mechanism of the transformer constant.attentions weights 1/3 of the model size.for the feed-forward network (ffn) portions, only the non-sparse segments are dynamically loaded into dram as needed.\n\nanticipating relu sparsity\nrelu activation can induce 90% sparsity. optimize preceding layer, up project by low-rank predictor to identify the zeroed elements post-relu.\nin contrast to their work, our predictor needs only the output of the current layer’s attention module and not the previous layer’s ffn module.\n\nneuron data management via sliding window technique our approach focuses on managing neuron data by employing a sliding window technique. this methodology entails maintaining neuron data only for a recent subset of input tokens in the memory.\nthe key aspect of this technique is the selective loading of neuron data that differs between the current input token and its immediate predecessors.\nfrees up memory resources previously allocated to neuron data from older tokens that are no longer within the sliding window\n\nlet sagg(k) denote the cumulative use of neuron data across a sequence of k input tokens. this reduction in data loading is counterbalanced by the memory cost associated with storing sagg(k). in determining the size of the sliding window, the aim is to maximize it within the constraints imposed by the available memory capacity.\n\n2.2 improve transfer throughput with increased chunk sizes\n\nbundling columns and rows for upward and downward projection\n\nbundling based on co-activation fetch neuron with its cloest friend. but there is warm-guy problem.\n\n2.3 optimized data management in dram\n\nwhen a substantial portion (approximately 25%) of the feed-forward networks (ffns) in dram needs to be rewritten.\n\nwhen introducing data for new neurons, reallocating the matrix and appending new matrices can lead to significant overhead due to the need for rewriting existing neurons data in dram. this involves the preallocation of all necessary memory and the establishment of a corresponding data structure for efficient management.\n\n----------------------------------------\n\nllm principles\n\n\n# 1. a survey on hallucination in large language models: principles, taxonomy, challenges, and open questions\n\n",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"GPU Virtualization Paper List",frontmatter:{title:"GPU Virtualization Paper List",date:"2023-12-18T00:00:00.000Z",permalink:"/pages/45871f/"},regularPath:"/03.gpu/07.GPUVirtualization.html",relativePath:"03.gpu/07.GPUVirtualization.md",key:"v-d22a70f6",path:"/pages/45871f/",headers:[{level:3,title:"1.TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments",slug:"_1-timegraph-gpu-scheduling-for-real-time-multi-tasking-environments",normalizedTitle:"1.timegraph: gpu scheduling for real-time multi-tasking environments",charIndex:500},{level:2,title:"Done.",slug:"done",normalizedTitle:"done.",charIndex:573},{level:3,title:"2.Hardware Compute Partitioning on NVIDIA GPUs",slug:"_2-hardware-compute-partitioning-on-nvidia-gpus",normalizedTitle:"2.hardware compute partitioning on nvidia gpus",charIndex:583},{level:3,title:"3.GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation]",slug:"_3-gpuvm-why-not-virtualizing-gpus-at-the-hypervisor-year-2014-citation",normalizedTitle:"3.gpuvm: why not virtualizing gpus at the hypervisor [year 2014 citation]",charIndex:683},{level:3,title:"4.Implementing Open-Source CUDA Runtime",slug:"_4-implementing-open-source-cuda-runtime",normalizedTitle:"4.implementing open-source cuda runtime",charIndex:884},{level:3,title:"5.Gdev: First-Class GPU Resource Management in the Operating System",slug:"_5-gdev-first-class-gpu-resource-management-in-the-operating-system",normalizedTitle:"5.gdev: first-class gpu resource management in the operating system",charIndex:977}],headersStr:"1.TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments Done. 2.Hardware Compute Partitioning on NVIDIA GPUs 3.GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation] 4.Implementing Open-Source CUDA Runtime 5.Gdev: First-Class GPU Resource Management in the Operating System",content:" 1. TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments [Citation 372]\n 2. Hardware Compute Partitioning on NVIDIA GPUs [Year 2022 Citation 3]\n 3. GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation 142]\n 4. Implementing Open-Source CUDA Runtime [Year 2013 Citation 13]\n 5. Gdev: First-Class GPU Resource Management in the Operating System [Year 2012 Citation 272]\n\n1.3.4.5 are written by the same author: Shinpei Kato.\n\n----------------------------------------\n\n\n# 1.TimeGraph: GPU Scheduling for Real-Time Multi-Tasking Environments\n\n\n# Done.\n\n\n# 2.Hardware Compute Partitioning on NVIDIA GPUs\n\nDone.\n\n----------------------------------------\n\n\n# 3.GPUvm: Why Not Virtualizing GPUs at the Hypervisor [Year 2014 Citation]\n\nhttps://cseweb.ucsd.edu/~yiying/cse291j-winter20/reading/GPU-Virtualization.pdf\n\n----------------------------------------\n\n\n# 4.Implementing Open-Source CUDA Runtime\n\nDone.\n\n----------------------------------------\n\n\n# 5.Gdev: First-Class GPU Resource Management in the Operating System\n\nDone.",normalizedContent:" 1. timegraph: gpu scheduling for real-time multi-tasking environments [citation 372]\n 2. hardware compute partitioning on nvidia gpus [year 2022 citation 3]\n 3. gpuvm: why not virtualizing gpus at the hypervisor [year 2014 citation 142]\n 4. implementing open-source cuda runtime [year 2013 citation 13]\n 5. gdev: first-class gpu resource management in the operating system [year 2012 citation 272]\n\n1.3.4.5 are written by the same author: shinpei kato.\n\n----------------------------------------\n\n\n# 1.timegraph: gpu scheduling for real-time multi-tasking environments\n\n\n# done.\n\n\n# 2.hardware compute partitioning on nvidia gpus\n\ndone.\n\n----------------------------------------\n\n\n# 3.gpuvm: why not virtualizing gpus at the hypervisor [year 2014 citation]\n\nhttps://cseweb.ucsd.edu/~yiying/cse291j-winter20/reading/gpu-virtualization.pdf\n\n----------------------------------------\n\n\n# 4.implementing open-source cuda runtime\n\ndone.\n\n----------------------------------------\n\n\n# 5.gdev: first-class gpu resource management in the operating system\n\ndone.",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"GPU Simulator",frontmatter:{title:"GPU Simulator",date:"2024-01-01T00:00:00.000Z",permalink:"/pages/458721/"},regularPath:"/03.gpu/09.Simulator.html",relativePath:"03.gpu/09.Simulator.md",key:"v-e33712b6",path:"/pages/458721/",headers:[{level:3,title:"1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis",slug:"_1-delta-gpu-performance-model-for-deep-learning-applications-with-in-depth-memory-system-traffic-analysis",normalizedTitle:"1.delta: gpu performance model for deep learning applications with in-depth memory system traffic analysis",charIndex:263},{level:3,title:"2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]",slug:"_2-lost-in-abstraction-pitfalls-of-analyzing-gpus-at-the-intermediate-language-level-hpca",normalizedTitle:"2.lost in abstraction: pitfalls of analyzing gpus at the intermediate language level [hpca]",charIndex:401}],headersStr:"1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis 2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]",content:" 1. DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis [Citation 39]\n 2. Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]\n\n----------------------------------------\n\n\n# 1.DeLTA: GPU Performance Model for Deep Learning Applications with In-depth Memory System Traffic Analysis\n\nStill reading in process.\n\n\n# 2.Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level [HPCA]\n\nDone. Gem5 GPU Introduction.\n\nReference Materials\n\nAMD_gem5_APU_simulator_isca_2018_gem5_wiki.pdf",normalizedContent:" 1. delta: gpu performance model for deep learning applications with in-depth memory system traffic analysis [citation 39]\n 2. lost in abstraction: pitfalls of analyzing gpus at the intermediate language level [hpca]\n\n----------------------------------------\n\n\n# 1.delta: gpu performance model for deep learning applications with in-depth memory system traffic analysis\n\nstill reading in process.\n\n\n# 2.lost in abstraction: pitfalls of analyzing gpus at the intermediate language level [hpca]\n\ndone. gem5 gpu introduction.\n\nreference materials\n\namd_gem5_apu_simulator_isca_2018_gem5_wiki.pdf",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Architectural Survey",frontmatter:{title:"Architectural Survey",date:"2024-03-30T00:00:00.000Z",permalink:"/pages/458722/"},regularPath:"/03.gpu/10.%20Architectural%20Survey.html",relativePath:"03.gpu/10. Architectural Survey.md",key:"v-a0731602",path:"/pages/458722/",headers:[{level:2,title:"Control flow divergence",slug:"control-flow-divergence",normalizedTitle:"control flow divergence",charIndex:643},{level:3,title:"1. Regrouping Divergent warps",slug:"_1-regrouping-divergent-warps",normalizedTitle:"1. regrouping divergent warps",charIndex:1633},{level:3,title:"2.  Large Warp/CTA compaction",slug:"_2-large-warp-cta-compaction",normalizedTitle:"2.  large warp/cta compaction",charIndex:null},{level:3,title:"3. Multi-path execution",slug:"_3-multi-path-execution",normalizedTitle:"3. multi-path execution",charIndex:2727},{level:3,title:"4. MIMD-like architecture",slug:"_4-mimd-like-architecture",normalizedTitle:"4. mimd-like architecture",charIndex:2815},{level:3,title:"5. Dynamic kernels/threads",slug:"_5-dynamic-kernels-threads",normalizedTitle:"5. dynamic kernels/threads",charIndex:3221},{level:2,title:"Efficient utilization of memory bandwidth",slug:"efficient-utilization-of-memory-bandwidth",normalizedTitle:"efficient utilization of memory bandwidth",charIndex:5054},{level:3,title:"1. Alleviating cache thrashing, and resource contention",slug:"_1-alleviating-cache-thrashing-and-resource-contention",normalizedTitle:"1. alleviating cache thrashing, and resource contention",charIndex:5100},{level:3,title:"2. High-bandwidth many-thread-aware memory hierarchy",slug:"_2-high-bandwidth-many-thread-aware-memory-hierarchy",normalizedTitle:"2. high-bandwidth many-thread-aware memory hierarchy",charIndex:11365},{level:2,title:"Increasing parallelism and improving execution pipelining",slug:"increasing-parallelism-and-improving-execution-pipelining",normalizedTitle:"increasing parallelism and improving execution pipelining",charIndex:14420},{level:3,title:"1. Reducing resource fragmentation and increasing parallelism",slug:"_1-reducing-resource-fragmentation-and-increasing-parallelism",normalizedTitle:"1. reducing resource fragmentation and increasing parallelism",charIndex:15371},{level:3,title:"2. GPU multitasking",slug:"_2-gpu-multitasking",normalizedTitle:"2. gpu multitasking",charIndex:15701},{level:3,title:"3. Exploiting scalar and value similarity opportunities",slug:"_3-exploiting-scalar-and-value-similarity-opportunities",normalizedTitle:"3. exploiting scalar and value similarity opportunities",charIndex:16480},{level:3,title:"4. Improving execution pipelining",slug:"_4-improving-execution-pipelining",normalizedTitle:"4. improving execution pipelining",charIndex:17379},{level:2,title:"Enhancing GPGPU programmability",slug:"enhancing-gpgpu-programmability",normalizedTitle:"enhancing gpgpu programmability",charIndex:18458},{level:3,title:"1. Coherence and consistency model",slug:"_1-coherence-and-consistency-model",normalizedTitle:"1. coherence and consistency model",charIndex:18496},{level:3,title:"2. Transactional memory",slug:"_2-transactional-memory",normalizedTitle:"2. transactional memory",charIndex:18795},{level:3,title:"3. Deterministic GPU",slug:"_3-deterministic-gpu",normalizedTitle:"3. deterministic gpu",charIndex:19057},{level:3,title:"4. Memory management",slug:"_4-memory-management",normalizedTitle:"4. memory management",charIndex:19082},{level:2,title:"CPU–GPU heterogeneous architecture",slug:"cpu-gpu-heterogeneous-architecture",normalizedTitle:"cpu–gpu heterogeneous architecture",charIndex:19645},{level:3,title:"1. Impacts of CPU–GPU integration",slug:"_1-impacts-of-cpu-gpu-integration",normalizedTitle:"1. impacts of cpu–gpu integration",charIndex:19684},{level:3,title:"2. CPU–GPU programmability",slug:"_2-cpu-gpu-programmability",normalizedTitle:"2. cpu–gpu programmability",charIndex:20168},{level:3,title:"3. Exploiting heterogeneity",slug:"_3-exploiting-heterogeneity",normalizedTitle:"3. exploiting heterogeneity",charIndex:20369},{level:3,title:"4. Shared resources management",slug:"_4-shared-resources-management",normalizedTitle:"4. shared resources management",charIndex:20696}],headersStr:"Control flow divergence 1. Regrouping Divergent warps 2.  Large Warp/CTA compaction 3. Multi-path execution 4. MIMD-like architecture 5. Dynamic kernels/threads Efficient utilization of memory bandwidth 1. Alleviating cache thrashing, and resource contention 2. High-bandwidth many-thread-aware memory hierarchy Increasing parallelism and improving execution pipelining 1. Reducing resource fragmentation and increasing parallelism 2. GPU multitasking 3. Exploiting scalar and value similarity opportunities 4. Improving execution pipelining Enhancing GPGPU programmability 1. Coherence and consistency model 2. Transactional memory 3. Deterministic GPU 4. Memory management CPU–GPU heterogeneous architecture 1. Impacts of CPU–GPU integration 2. CPU–GPU programmability 3. Exploiting heterogeneity 4. Shared resources management",content:' 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity\n\n----------------------------------------\n\n\n# 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity\n\nFour major improvement\n\n * mitigating the impact of control flow divergence\n * alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main memory\n * increasing the available parallelism and concurrency\n * improving pipeline execution and exploiting scalarization opportunities.\n\n\n\n\n# Control flow divergence\n\n 1. First, GPUs employ PDOM stack-based mechanism that serializes the execution of divergent paths. This serialization of divergent paths reduces the available thread level parallelism (i.e., the number of active warps at a time) which limits the ability of GPUs to hide long memory instruction latency.\n 2. Control divergence limits the number of active threads in the running warps. As a result, SIMD execution units are not efficiently utilized when a diverged warp is executed.\n 3. Control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. Memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.\n 4. Irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per CTAs) to some GPU cores are larger than others.\n\n\n\n\n# 1. Regrouping Divergent warps\n\n\nInstead, DWF dynamically re-forms divergent warps into new non-divergent warps on the fly.\nMoreover, DWF does not reconverge diverged warp at IPDOM in order to amortize coalesced memory address of converged warps.\n\n\n\n\n\n\n# 2. Large Warp/CTA compaction\n\n * Thread Block Compaction (TBC)\n   Allows a group of warps, that belong to the same thread block, to share the same PDOM stack.\n   However, TBC stalls all warps within a CTA on any potentially divergent branch until all warps reach the branch point.\n   \n   \n\nThe major difference between 1) and TBC is that 1) can only merge threads in a warp when they are ready in a queue. Thus it miss some potentials.\n\nTBC replace per-warp convergence stack with in-threadblock stack.\n\n * CAPRI\n   CAPRI dynamically identifies the compaction effectiveness of a branch and only stalls threads that are predicted to benefit from compaction.\n   \n\n * SLP proposed SIMD lane permutation (SLP) as an optimization to expand the applicability of compaction in case of conventional compaction technique is ineffective.\n   \n   \n\n\n# 3. Multi-path execution\n\n\n * DPS\n   Dual-path Stack\n   \n * Multi-path Execution\n   \n\n\n# 4. MIMD-like architecture\n\n\nRogers et al. [194] observed that regular applications perform better with a wider warp size, whereas divergent applications achieve better performance with a smaller warp size.\nVWS groups sets of these smaller warps together by ganging their execution in the warp scheduler and thus amortizing the energy consumed by fetch, decode, and warp scheduling across more threads.\n\n\n# 5. Dynamic kernels/threads\n\n\nRelated Paper: Characterization and Analysis of Dynamic Parallelism in Unstructured GPU Applications. [108]\nDynamic Thread Block Launch: A Lightweight Execution Mechanism to Support Irregular Applications on GPUs. [85]\nBy wang jing NVIDIA\n\n👍 👍 👍 These two paper has very thorough explanation of how kernels are launched, kernel parameters are gained and how thread create subkernel.\n\n\n\n\n\nCUDA enables dynamic parallsim, creating subkernels from each thread.\n\n> Copied from "Characterization"\n> \n> When a child kernel is launched, the parameter buffer pointer of the kernel is retrieved through the device runtime API cudaGetParameterBuffer.\n> \n> Then the argument values are stored in the parameter buffer and the kernel is launched by calling cudaLaunchDevice.\n> \n> After that, the device runtime manager appends the child kernels to an execution queue and dispatches the kernel to SMXs according to a certain scheduling policy.\n> \n> The CDP kernel launching overhead comprises of kernel parameter parsing, calling cudaGetParameterBuffer and cudaLaunchDevice, as well as the process that device runtime manager setups,enqueues, manages and dispatches the child kernels.\n> \n> however, the huge kernel launching overhead could negate the performance benefit of DFP. The overhead is due to the large number of launched kernels, the associated memory footprint and the low number of running warps per core.The CPU launches GPU kernels by dispatching kernel launching commands. Kernel parameters are passed from CPU to GPU at the kernel launching time and stored in the GPU global.\n> \n> Wang et al. [236] proposed new mechanism, called Dynamic Thread Block Launch (DTBL), that employs light-weight thread block rather than heavy-weight device kernel for DFP.\n\n----------------------------------------\n\n\n# Efficient utilization of memory bandwidth\n\n\n# 1. Alleviating cache thrashing, and resource contention\n\n# 1. Two-level warp scheduling\n\n * TLRR\n   \n\nThey proposed two-level round-robin warp scheduling (TL-RR), in which the warps are split into fetch groups.\nTL-RR executes only one fetch group at a time and it schedules warps from the same fetch group in a round-robin fashion.\nWhen the running warps reach a long latency operation, then the next fetch group is prioritized.\nThey try to alleviate the issue of threads in all warps arrive the same memory latency instruction at the same time.\n\n\n\n\n\n\n * OWL OWL augments the TL-RR with CTA-awareness, such that warps are split into groups of CTAs basis rather than warps basis, resulting in increased intra-CTA locality.\n   OWL gives a group of CTAs higher priority when their data exist at the L1 cache such that they get the opportunity to reuse it, therefore improving L1 hit rates and alleviating cache contention.\n   \n\n# 2. Coarse-grained CTA throttling\n\n\n\n\n\nDYNCTA\nNeither More Nor Less: Optimizing Thread-level Parallelism for GPGPUs\n👍 👍 Illustrated CTA and WARP mapping.\n\n * Always executing the maximum possible number of CTAs on a GPU core (i.e., increasing TLP to the maximum) does not always lead to better performance.\n * To alleviate resource contention, they proposed dynamic CTA scheduling mechanism (DYNCTA), which aims to allocate the optimal number of CTAs per GPU core that alleviate memory contention according to an application characteristics.\n * DYNCTA dynamically adjusts over sampling periods the number of active CTAs per GPU core that reduces the memory latency without sacrificing the available TLP.\n\nLCS\nIn contrast to DYNCTA that monitors the workload behavior for the entire kernel execution, LCS leverages GTO scheduler to find the optimal number of thread blocks at the early beginning of kernel execution.\n\n# 3. Fine-grained warp throttling\n\ndue to the massive multithreading and the limited capacity of L1 cache, divergent GPGPU applications cause severe cache contention.\n\n\n * CCWS\n   uses a victim tag array, called lost locality detector, to detect warps that have lost locality due to thrashing. These warps are prioritized till they exploit their locality while other warps are descheduled.\n   \n * DAWS\n   DAWS is a divergence-based cache footprint predictor to calculate the amount of locality in loops required by each warp.\n   DAWS uses these predictions to prioritize a group of warps such that the cache footprint of these warps do not exceed the capacity of the L1 cache.\n   \n\n# 4. Throttling and cache bypassing\n\nprevious CTA or warp throttling techniques leave memory bandwidth and other chip resources (L2 cache, interconnection and execution units) significantly underutilized.\n\n * PCAL\n   At the beginning of kernel execution, PCAL executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources. Thus, PCAL reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.\n * CCA\n   CCA improves DAWS by allowing extra inactive warps and some streaming memory instructions from the active warps to bypass the L1 cache and utilize on-chip resources.\n\n# 5. Critical warp awareness\n\nsome warps may be assigned more workload and exhibit longer latency compared to other warps within the same Thread Block. Hence, fast warps are idle at a synchronization barrier or at the end of kernel execution until the critical (i.e., the slowest) warp finishes execution. Thus, the overall execution time is dominated by the performance of these critical warps.\n\nCAWA dynamically identifies critical warps and coordinates warp scheduling and cache prioritization to accelerate the critical warp execution.\n\n * Workload Imbalance In a GPGPU kernel function, tasks are not always uniformly distributed to each thread/warp, and thereby some threads/warps have heavier workloads than others. Intuitively, the threads/warps with heavier workloads require longer time to process their tasks. Consequently, warps with heavier workloads often become the slowest running/critical warps.\n * Diverging Branch Behavior At runtime, warps can undergo different con trol paths leading to different number of dynamic instructions across different warps. This problem could be worsened if threads in a warp also take diverging control paths, i.e., the branch divergence problem, leading to a larger instruction execution gap between warps.\n * Contention in the Memory Subsystem Jog et al. ob served that the memory subsystem has a significant impact on GPGPU applications [15, 16]. . Jia et al. also pointed out that interference in the Ll data cache as well as in the interconnection between the Ll data caches and the L2 cache are the major factors that limit GPU performance.More than 60% of the cache blocks that could be reused by the slower-running, critical warps are evicted before the re-references by the critical warps.\n * Latency Introduced by the Warp Scheduler Because of the particular warp execution order determined by the scheduler, when a warp becomes ready for execution, it can experience up to N cycles of scheduling delay, where N represents the number of warps.\n\n# 6. Cache management and bypassing\n\nGCache To detect thrashing, they equip L2 cache tag array with extra bits (victim bits) to provide L1 cache with some information about the hot lines that have been evicted before. An adaptive cache replacement policy is used by L1 cache to protect these hot lines.\n\n# 7. Ordering buffers\n\nThe idea of MRPB is two-fold.\nFirst, a FIFO requests buffer is used to reorder memory references so that requests from the same warp are grouped and sent to the cache together in a more cache-friendly order. This results in drastically reducing cache contention and improving use of the limited per-thread cache capacity.\nSecond, MRPB allows memory request that encounters associativity stall to bypass L1 cache.\n\n\n\n\n\n\n# 8. Resource tuning\n\nEqualizer, a dynamic runtime system that tunes number of thread blocks, core and memory frequency to match the requirements of the running kernel, leading to efficient execution and energy saving.\n\n\n# 2. High-bandwidth many-thread-aware memory hierarchy\n\n# 1. Mitigating off-chip bandwidth bottleneck\n\nLAMAR Emerging irregular workloads benefit from fine-grain (FG) memory access by avoiding unnecessary data transfers, that may be happened under CG policy,\n\nthey proposed a locality-aware memory hierarchy (LAMAR) that adaptively tunes the memory access granularity for the running kernel.\n\nLAMAR employs CG accesses for kernels with high temporal and spatial locality, while applying FG accesses for irregular divergent workloads in attempt to reduce memory over-fetching.\n\nCABA Vijaykumar et al. [231] proposed, Core-Assisted Bottleneck Acceleration (CABA) framework, that exploits the underutilized computational resources to perform useful work and alleviate different bottlenecks in GPU execution.\n\nFor instance, to alleviate memory bandwidth bottleneck, CABA dynamically creates assist warps that execute with the original warps side by side on the same GPU core.\n\nAssist warps opportunistically use idle computational units to perform data decompression for the incoming compressed cache blocks and compression for the outgoing cache blocks, leading to less transferring data from memory and mitigating memory bandwidth problem.\n\nApproximation An approximation technique in which the GPU drops some portion of load requests which miss in the cache after approximating their values.\n\n# 2. Memory divergence normalization\n\nOrchestrated Scheduling and Prefetching for GPGPUs\n\nthey proposed prefetch-aware warp scheduling, that coordinates simple data prefetcher and warp scheduling in an intelligent manner such that the scheduling of two consecutive warps are separated in time, and thus prefetching becomes more effective.\n\n\n\n# 3. Interconnection network\n\n# 4. Main memory scheduling\n\ninterconnection network which is between cores and memory controllers can destroy memory access row-buffer locality.\n\nTo reserve row locality and reduce complexity circuit design of FR-FCFS DRAM controller, they employ an interconnection arbitration scheme to prioritize memory requests accessing the same row first.\n\n# 5. Heterogeneous memory management\n\nAgarwal et al. [5] showed that applying traditional Linux page placements policies, which have been used for CPUonly NUMA systems and aim to minimize the memory request latency, may not be effective in CPU–GPU NUMA systems. This is due to the fact that GPU performance is more sensitive to memory bandwidth.\n\nBandwidth-aware placement that maximizes GPU performance by balancing page placement across the memories based on the aggregate memory bandwidth available in a system.\n\n# 6. CPU–GPU memory transfer overhead\n\nfine-grained CPU–GPU synchronization enabled by a hardware-managed full-empty bits to track when regions of data have been transferred.\n\nThus, the GPU is able to start execution once the required block of data is available.\n\nSoftware level APIs are proposed to allow programmer to launch kernel earlier and overlap data transfer with execution.\n\n----------------------------------------\n\n\n# Increasing parallelism and improving execution pipelining\n\nSome applications have a low number of active thread blocks due to the small input size or the unavailability of some required resources in SM (e.g. registers or shared memory), thus they fail to efficiently utilize the execution units. This results in inefficient utilization of execution unit and hinders the GPU ability to hide long memory latency.\n\n\nPrevious works proposed new techniques in order to reduce resource fragmentation and run the maximum number of warps per core.\n\n\nFurther, other approaches proposed running multiple applications on the same GPU to exploit these underutilized resources and increase overall throughput.\n\n\nAnother way to improve execution efficiency and increase parallelism is to exploit scalar opportunities and value similarity between the running warps such that scalar instructions can be executed con currently along with other SIMT instructions.\n\n\n\n# 1. Reducing resource fragmentation and increasing parallelism\n\n👍 👍 👍\n\nUnifyingthey proposed a unified local memory which integrates the register file, L1 cache, and scratchpad memory into one large on-chip storage. Then, the hardware can dynamically partition the on-chip storage according to each application’s needs.\n\n\n\n\n\n\n# 2. GPU multitasking\n\nBetter Utilization and Virtualization\n\n\n * multiple applica tions execute simultaneously on different cores within the same GPU substrate.\n   \n * mixed concurrent kernels execution, in which two applications execute concurrently on the same core. especially, mixture of memory-intensive and compute-intensive workloads\n   \n\nDefault strategy may cause high-priority application suffering from a long latency to execute. a task preemption strategy is required to improve GPU ultitasking\n\n * context switching and draining\n * To further reduce preemption latency, Park et al. [178] intro duced core flushing which drops an execution of a thread block without context saving and re-executes the dropped thread block from the beginning when it is relaunched.\n\n\n# 3. Exploiting scalar and value similarity opportunities\n\nmany GPGPU workloads have scalar instructions in which computation is identical across mul tiple threads within the same warp instruction (i.e., operands are identical for all the threads in a warp).\n\nmodern GPU mi croarchitecture, like AMD’s GCN [10], leverages these scalar op portunities by statically detecting scalar instructions and executing them on a separate scalar unit attached with each GPU core.\n\nA vector is defined as an affine, when the vector contains a consecutive strided values, i.e., the vector values can be represented as V(i) = b + i ∗ s, where b is the base, s is the stride and i is the thread index.\n\n👍 👍 👍 Microarchitectural mechanisms to exploit value structure in SIMT architectures\n\nThis Paper has detailed explanation of microarchitecture in gpu execution core, how ALU and register files operates.\n\n\n\n\n\n\n# 4. Improving execution pipelining\n\nmany GPGPU applications do not have enough active threads that are ready to issue instructions and hide short read-after-write (RAW) dependencies caused by deep execution pipeline stages.\n\n * a low-power forwarded network that can considerably improve the performance of many compute-intensive GPGPU ap plications.\n\n\n\n * improve GPU performance by splitting the existing 32-bit datapath into two 16-bit datapath slices. As a result, the GPU instruction throughput can be increased by issuing dual 16-bit instructions from two different warps in parallel using the sliced 32-bit datapath.\n\n * a pre-execution approach for improving GPU latency hiding and performance by employing run-ahead out of-order execution [158].\n\n\n\n\n\nwhen a warp stalls for a long-latency operation such as off-chip memory accesses, it continues to fetch and pre-execute successive instructions that are not on the long latency dependence chain resulting in hiding processing delay of operations and performance improvement.\n\n----------------------------------------\n\n\n# Enhancing GPGPU programmability\n\n\n\n\n# 1. Coherence and consistency model\n\nCurrent GPUs lack hardware cache coherence and require dis abling of private L1 caches or employing software-based bulk coherence decisions (i.e., flush/invalidate all private L1 caches at synchronization points) if an application needs coherent memory view.\n\n\n# 2. Transactional memory\n\nKILO TM does not rely on cache coherence nor global atomic operations.\n\nInstead, it detects conflicts via a fine-grain value-based approach that supports thousands of concurrent transactions and requires negligible storage overhead.\n\n\n# 3. Deterministic GPU\n\n\n# 4. Memory management\n\nKim et al. [109] proposed GPUdmm, a high-performance dynamic memory management for GPU architecture. GPUdmm enables dynamic memory management for discrete GPU environ ments by using GPU memory as a cache of CPU memory with on demand CPU–GPU data transfers.\n\n\n\nPichai et al. [183] 👍 👍 👍 augmenting CCWS and TBC with TLB-awareness and a few simple adjustments can recover most of this lost performance and move address translation overheads into a range considered acceptable in the CPU world.\n\n----------------------------------------\n\n\n# CPU–GPU heterogeneous architecture\n\n\n# 1. Impacts of CPU–GPU integration\n\nremaining CPU code tends to have lower instruction-level parallelism (ILP), more complex load/store operations to prefetch and more difficult branch rediction.\n\n\nFurther, the serial code will not benefit significantly from SIMD instructions or increasing the number of CPU cores, owing to the limited availability of thread level parallelism (TLP) and data-level parallelism (DLP) that will be already captured and exploited by the GPU instead.\n\n\n# 2. CPU–GPU programmability\n\nHeterogeneous System Coherence for Integrated CPU-GPU Systems\n\nthey replace the fine-grained 64B-block-level directory with a coarse-grained 1KB-region-level directory.\n\n\n# 3. Exploiting heterogeneity\n\nCOMPASS uses idle GPU core resources to act as data prefetchers for CPU execution and success fully improve the memory performance of single-thread applications.\nWoo and Lee [247] proposed to collaboratively utilize CPU resources to act as programmable data prefetchers for GPGPU applications.\n\n\n# 4. Shared resources management\n\nTwo kinds of approaches have been explored to mitigate inter ference:\n\n * application-aware resource management\n * throttling based management.\n\n\n\nSMS decouples memory controller into three stages.\n\n\n * The first stage of SMS groups requests based on row buffer locality.\n * At the second stage, SMS ensures fairness between CPU and GPU memory requests by applying CPU-biased shortest job first scheduling policy or GPU-biased round robin scheduling policy. A dynamically configurable parameter is used to select between the two policies based on the system’s needs.\n * The last stage consists of simple per-bank FIFO queue to issue low-level memory commands.\n\nTAP: A TLP-Aware Cache Management Policy for a CPU-GPU Heterogeneous\n\n\n\n * A core-sampling technique, which applies a different cache management policy to each GPU core and regularly collects statistics on the performance\n   of these cores to see how these polices affect GPU applications.\n   \n * GPU cores typically access caches much more frequently than CPU cores.\n   \n * enforces a similar cache lifetime to both CPU and GPGPU appli cations and prevent GPGPU application to monopolize the shared cache.\n\nOne (CM-CPU) for boosting CPU performance in the presence of GPU interference.\nThe other (CM-BAL) for improving both CPU and GPU performance in a balanced manner and thus overall system performance.\n\n\n\n\npropose GPU concurrency management that dynamically throttles/boosts TLP (i.e., number of active warps) of GPU cores in order to minimize shared resources interference between CPU and GPU.',normalizedContent:' 1. a survey of architectural approaches for improving gpgpu performance, programmability and heterogeneity\n\n----------------------------------------\n\n\n# 1. a survey of architectural approaches for improving gpgpu performance, programmability and heterogeneity\n\nfour major improvement\n\n * mitigating the impact of control flow divergence\n * alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main memory\n * increasing the available parallelism and concurrency\n * improving pipeline execution and exploiting scalarization opportunities.\n\n\n\n\n# control flow divergence\n\n 1. first, gpus employ pdom stack-based mechanism that serializes the execution of divergent paths. this serialization of divergent paths reduces the available thread level parallelism (i.e., the number of active warps at a time) which limits the ability of gpus to hide long memory instruction latency.\n 2. control divergence limits the number of active threads in the running warps. as a result, simd execution units are not efficiently utilized when a diverged warp is executed.\n 3. control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.\n 4. irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per ctas) to some gpu cores are larger than others.\n\n\n\n\n# 1. regrouping divergent warps\n\n\ninstead, dwf dynamically re-forms divergent warps into new non-divergent warps on the fly.\nmoreover, dwf does not reconverge diverged warp at ipdom in order to amortize coalesced memory address of converged warps.\n\n\n\n\n\n\n# 2. large warp/cta compaction\n\n * thread block compaction (tbc)\n   allows a group of warps, that belong to the same thread block, to share the same pdom stack.\n   however, tbc stalls all warps within a cta on any potentially divergent branch until all warps reach the branch point.\n   \n   \n\nthe major difference between 1) and tbc is that 1) can only merge threads in a warp when they are ready in a queue. thus it miss some potentials.\n\ntbc replace per-warp convergence stack with in-threadblock stack.\n\n * capri\n   capri dynamically identifies the compaction effectiveness of a branch and only stalls threads that are predicted to benefit from compaction.\n   \n\n * slp proposed simd lane permutation (slp) as an optimization to expand the applicability of compaction in case of conventional compaction technique is ineffective.\n   \n   \n\n\n# 3. multi-path execution\n\n\n * dps\n   dual-path stack\n   \n * multi-path execution\n   \n\n\n# 4. mimd-like architecture\n\n\nrogers et al. [194] observed that regular applications perform better with a wider warp size, whereas divergent applications achieve better performance with a smaller warp size.\nvws groups sets of these smaller warps together by ganging their execution in the warp scheduler and thus amortizing the energy consumed by fetch, decode, and warp scheduling across more threads.\n\n\n# 5. dynamic kernels/threads\n\n\nrelated paper: characterization and analysis of dynamic parallelism in unstructured gpu applications. [108]\ndynamic thread block launch: a lightweight execution mechanism to support irregular applications on gpus. [85]\nby wang jing nvidia\n\n👍 👍 👍 these two paper has very thorough explanation of how kernels are launched, kernel parameters are gained and how thread create subkernel.\n\n\n\n\n\ncuda enables dynamic parallsim, creating subkernels from each thread.\n\n> copied from "characterization"\n> \n> when a child kernel is launched, the parameter buffer pointer of the kernel is retrieved through the device runtime api cudagetparameterbuffer.\n> \n> then the argument values are stored in the parameter buffer and the kernel is launched by calling cudalaunchdevice.\n> \n> after that, the device runtime manager appends the child kernels to an execution queue and dispatches the kernel to smxs according to a certain scheduling policy.\n> \n> the cdp kernel launching overhead comprises of kernel parameter parsing, calling cudagetparameterbuffer and cudalaunchdevice, as well as the process that device runtime manager setups,enqueues, manages and dispatches the child kernels.\n> \n> however, the huge kernel launching overhead could negate the performance benefit of dfp. the overhead is due to the large number of launched kernels, the associated memory footprint and the low number of running warps per core.the cpu launches gpu kernels by dispatching kernel launching commands. kernel parameters are passed from cpu to gpu at the kernel launching time and stored in the gpu global.\n> \n> wang et al. [236] proposed new mechanism, called dynamic thread block launch (dtbl), that employs light-weight thread block rather than heavy-weight device kernel for dfp.\n\n----------------------------------------\n\n\n# efficient utilization of memory bandwidth\n\n\n# 1. alleviating cache thrashing, and resource contention\n\n# 1. two-level warp scheduling\n\n * tlrr\n   \n\nthey proposed two-level round-robin warp scheduling (tl-rr), in which the warps are split into fetch groups.\ntl-rr executes only one fetch group at a time and it schedules warps from the same fetch group in a round-robin fashion.\nwhen the running warps reach a long latency operation, then the next fetch group is prioritized.\nthey try to alleviate the issue of threads in all warps arrive the same memory latency instruction at the same time.\n\n\n\n\n\n\n * owl owl augments the tl-rr with cta-awareness, such that warps are split into groups of ctas basis rather than warps basis, resulting in increased intra-cta locality.\n   owl gives a group of ctas higher priority when their data exist at the l1 cache such that they get the opportunity to reuse it, therefore improving l1 hit rates and alleviating cache contention.\n   \n\n# 2. coarse-grained cta throttling\n\n\n\n\n\ndyncta\nneither more nor less: optimizing thread-level parallelism for gpgpus\n👍 👍 illustrated cta and warp mapping.\n\n * always executing the maximum possible number of ctas on a gpu core (i.e., increasing tlp to the maximum) does not always lead to better performance.\n * to alleviate resource contention, they proposed dynamic cta scheduling mechanism (dyncta), which aims to allocate the optimal number of ctas per gpu core that alleviate memory contention according to an application characteristics.\n * dyncta dynamically adjusts over sampling periods the number of active ctas per gpu core that reduces the memory latency without sacrificing the available tlp.\n\nlcs\nin contrast to dyncta that monitors the workload behavior for the entire kernel execution, lcs leverages gto scheduler to find the optimal number of thread blocks at the early beginning of kernel execution.\n\n# 3. fine-grained warp throttling\n\ndue to the massive multithreading and the limited capacity of l1 cache, divergent gpgpu applications cause severe cache contention.\n\n\n * ccws\n   uses a victim tag array, called lost locality detector, to detect warps that have lost locality due to thrashing. these warps are prioritized till they exploit their locality while other warps are descheduled.\n   \n * daws\n   daws is a divergence-based cache footprint predictor to calculate the amount of locality in loops required by each warp.\n   daws uses these predictions to prioritize a group of warps such that the cache footprint of these warps do not exceed the capacity of the l1 cache.\n   \n\n# 4. throttling and cache bypassing\n\nprevious cta or warp throttling techniques leave memory bandwidth and other chip resources (l2 cache, interconnection and execution units) significantly underutilized.\n\n * pcal\n   at the beginning of kernel execution, pcal executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources. thus, pcal reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.\n * cca\n   cca improves daws by allowing extra inactive warps and some streaming memory instructions from the active warps to bypass the l1 cache and utilize on-chip resources.\n\n# 5. critical warp awareness\n\nsome warps may be assigned more workload and exhibit longer latency compared to other warps within the same thread block. hence, fast warps are idle at a synchronization barrier or at the end of kernel execution until the critical (i.e., the slowest) warp finishes execution. thus, the overall execution time is dominated by the performance of these critical warps.\n\ncawa dynamically identifies critical warps and coordinates warp scheduling and cache prioritization to accelerate the critical warp execution.\n\n * workload imbalance in a gpgpu kernel function, tasks are not always uniformly distributed to each thread/warp, and thereby some threads/warps have heavier workloads than others. intuitively, the threads/warps with heavier workloads require longer time to process their tasks. consequently, warps with heavier workloads often become the slowest running/critical warps.\n * diverging branch behavior at runtime, warps can undergo different con trol paths leading to different number of dynamic instructions across different warps. this problem could be worsened if threads in a warp also take diverging control paths, i.e., the branch divergence problem, leading to a larger instruction execution gap between warps.\n * contention in the memory subsystem jog et al. ob served that the memory subsystem has a significant impact on gpgpu applications [15, 16]. . jia et al. also pointed out that interference in the ll data cache as well as in the interconnection between the ll data caches and the l2 cache are the major factors that limit gpu performance.more than 60% of the cache blocks that could be reused by the slower-running, critical warps are evicted before the re-references by the critical warps.\n * latency introduced by the warp scheduler because of the particular warp execution order determined by the scheduler, when a warp becomes ready for execution, it can experience up to n cycles of scheduling delay, where n represents the number of warps.\n\n# 6. cache management and bypassing\n\ngcache to detect thrashing, they equip l2 cache tag array with extra bits (victim bits) to provide l1 cache with some information about the hot lines that have been evicted before. an adaptive cache replacement policy is used by l1 cache to protect these hot lines.\n\n# 7. ordering buffers\n\nthe idea of mrpb is two-fold.\nfirst, a fifo requests buffer is used to reorder memory references so that requests from the same warp are grouped and sent to the cache together in a more cache-friendly order. this results in drastically reducing cache contention and improving use of the limited per-thread cache capacity.\nsecond, mrpb allows memory request that encounters associativity stall to bypass l1 cache.\n\n\n\n\n\n\n# 8. resource tuning\n\nequalizer, a dynamic runtime system that tunes number of thread blocks, core and memory frequency to match the requirements of the running kernel, leading to efficient execution and energy saving.\n\n\n# 2. high-bandwidth many-thread-aware memory hierarchy\n\n# 1. mitigating off-chip bandwidth bottleneck\n\nlamar emerging irregular workloads benefit from fine-grain (fg) memory access by avoiding unnecessary data transfers, that may be happened under cg policy,\n\nthey proposed a locality-aware memory hierarchy (lamar) that adaptively tunes the memory access granularity for the running kernel.\n\nlamar employs cg accesses for kernels with high temporal and spatial locality, while applying fg accesses for irregular divergent workloads in attempt to reduce memory over-fetching.\n\ncaba vijaykumar et al. [231] proposed, core-assisted bottleneck acceleration (caba) framework, that exploits the underutilized computational resources to perform useful work and alleviate different bottlenecks in gpu execution.\n\nfor instance, to alleviate memory bandwidth bottleneck, caba dynamically creates assist warps that execute with the original warps side by side on the same gpu core.\n\nassist warps opportunistically use idle computational units to perform data decompression for the incoming compressed cache blocks and compression for the outgoing cache blocks, leading to less transferring data from memory and mitigating memory bandwidth problem.\n\napproximation an approximation technique in which the gpu drops some portion of load requests which miss in the cache after approximating their values.\n\n# 2. memory divergence normalization\n\norchestrated scheduling and prefetching for gpgpus\n\nthey proposed prefetch-aware warp scheduling, that coordinates simple data prefetcher and warp scheduling in an intelligent manner such that the scheduling of two consecutive warps are separated in time, and thus prefetching becomes more effective.\n\n\n\n# 3. interconnection network\n\n# 4. main memory scheduling\n\ninterconnection network which is between cores and memory controllers can destroy memory access row-buffer locality.\n\nto reserve row locality and reduce complexity circuit design of fr-fcfs dram controller, they employ an interconnection arbitration scheme to prioritize memory requests accessing the same row first.\n\n# 5. heterogeneous memory management\n\nagarwal et al. [5] showed that applying traditional linux page placements policies, which have been used for cpuonly numa systems and aim to minimize the memory request latency, may not be effective in cpu–gpu numa systems. this is due to the fact that gpu performance is more sensitive to memory bandwidth.\n\nbandwidth-aware placement that maximizes gpu performance by balancing page placement across the memories based on the aggregate memory bandwidth available in a system.\n\n# 6. cpu–gpu memory transfer overhead\n\nfine-grained cpu–gpu synchronization enabled by a hardware-managed full-empty bits to track when regions of data have been transferred.\n\nthus, the gpu is able to start execution once the required block of data is available.\n\nsoftware level apis are proposed to allow programmer to launch kernel earlier and overlap data transfer with execution.\n\n----------------------------------------\n\n\n# increasing parallelism and improving execution pipelining\n\nsome applications have a low number of active thread blocks due to the small input size or the unavailability of some required resources in sm (e.g. registers or shared memory), thus they fail to efficiently utilize the execution units. this results in inefficient utilization of execution unit and hinders the gpu ability to hide long memory latency.\n\n\nprevious works proposed new techniques in order to reduce resource fragmentation and run the maximum number of warps per core.\n\n\nfurther, other approaches proposed running multiple applications on the same gpu to exploit these underutilized resources and increase overall throughput.\n\n\nanother way to improve execution efficiency and increase parallelism is to exploit scalar opportunities and value similarity between the running warps such that scalar instructions can be executed con currently along with other simt instructions.\n\n\n\n# 1. reducing resource fragmentation and increasing parallelism\n\n👍 👍 👍\n\nunifyingthey proposed a unified local memory which integrates the register file, l1 cache, and scratchpad memory into one large on-chip storage. then, the hardware can dynamically partition the on-chip storage according to each application’s needs.\n\n\n\n\n\n\n# 2. gpu multitasking\n\nbetter utilization and virtualization\n\n\n * multiple applica tions execute simultaneously on different cores within the same gpu substrate.\n   \n * mixed concurrent kernels execution, in which two applications execute concurrently on the same core. especially, mixture of memory-intensive and compute-intensive workloads\n   \n\ndefault strategy may cause high-priority application suffering from a long latency to execute. a task preemption strategy is required to improve gpu ultitasking\n\n * context switching and draining\n * to further reduce preemption latency, park et al. [178] intro duced core flushing which drops an execution of a thread block without context saving and re-executes the dropped thread block from the beginning when it is relaunched.\n\n\n# 3. exploiting scalar and value similarity opportunities\n\nmany gpgpu workloads have scalar instructions in which computation is identical across mul tiple threads within the same warp instruction (i.e., operands are identical for all the threads in a warp).\n\nmodern gpu mi croarchitecture, like amd’s gcn [10], leverages these scalar op portunities by statically detecting scalar instructions and executing them on a separate scalar unit attached with each gpu core.\n\na vector is defined as an affine, when the vector contains a consecutive strided values, i.e., the vector values can be represented as v(i) = b + i ∗ s, where b is the base, s is the stride and i is the thread index.\n\n👍 👍 👍 microarchitectural mechanisms to exploit value structure in simt architectures\n\nthis paper has detailed explanation of microarchitecture in gpu execution core, how alu and register files operates.\n\n\n\n\n\n\n# 4. improving execution pipelining\n\nmany gpgpu applications do not have enough active threads that are ready to issue instructions and hide short read-after-write (raw) dependencies caused by deep execution pipeline stages.\n\n * a low-power forwarded network that can considerably improve the performance of many compute-intensive gpgpu ap plications.\n\n\n\n * improve gpu performance by splitting the existing 32-bit datapath into two 16-bit datapath slices. as a result, the gpu instruction throughput can be increased by issuing dual 16-bit instructions from two different warps in parallel using the sliced 32-bit datapath.\n\n * a pre-execution approach for improving gpu latency hiding and performance by employing run-ahead out of-order execution [158].\n\n\n\n\n\nwhen a warp stalls for a long-latency operation such as off-chip memory accesses, it continues to fetch and pre-execute successive instructions that are not on the long latency dependence chain resulting in hiding processing delay of operations and performance improvement.\n\n----------------------------------------\n\n\n# enhancing gpgpu programmability\n\n\n\n\n# 1. coherence and consistency model\n\ncurrent gpus lack hardware cache coherence and require dis abling of private l1 caches or employing software-based bulk coherence decisions (i.e., flush/invalidate all private l1 caches at synchronization points) if an application needs coherent memory view.\n\n\n# 2. transactional memory\n\nkilo tm does not rely on cache coherence nor global atomic operations.\n\ninstead, it detects conflicts via a fine-grain value-based approach that supports thousands of concurrent transactions and requires negligible storage overhead.\n\n\n# 3. deterministic gpu\n\n\n# 4. memory management\n\nkim et al. [109] proposed gpudmm, a high-performance dynamic memory management for gpu architecture. gpudmm enables dynamic memory management for discrete gpu environ ments by using gpu memory as a cache of cpu memory with on demand cpu–gpu data transfers.\n\n\n\npichai et al. [183] 👍 👍 👍 augmenting ccws and tbc with tlb-awareness and a few simple adjustments can recover most of this lost performance and move address translation overheads into a range considered acceptable in the cpu world.\n\n----------------------------------------\n\n\n# cpu–gpu heterogeneous architecture\n\n\n# 1. impacts of cpu–gpu integration\n\nremaining cpu code tends to have lower instruction-level parallelism (ilp), more complex load/store operations to prefetch and more difficult branch rediction.\n\n\nfurther, the serial code will not benefit significantly from simd instructions or increasing the number of cpu cores, owing to the limited availability of thread level parallelism (tlp) and data-level parallelism (dlp) that will be already captured and exploited by the gpu instead.\n\n\n# 2. cpu–gpu programmability\n\nheterogeneous system coherence for integrated cpu-gpu systems\n\nthey replace the fine-grained 64b-block-level directory with a coarse-grained 1kb-region-level directory.\n\n\n# 3. exploiting heterogeneity\n\ncompass uses idle gpu core resources to act as data prefetchers for cpu execution and success fully improve the memory performance of single-thread applications.\nwoo and lee [247] proposed to collaboratively utilize cpu resources to act as programmable data prefetchers for gpgpu applications.\n\n\n# 4. shared resources management\n\ntwo kinds of approaches have been explored to mitigate inter ference:\n\n * application-aware resource management\n * throttling based management.\n\n\n\nsms decouples memory controller into three stages.\n\n\n * the first stage of sms groups requests based on row buffer locality.\n * at the second stage, sms ensures fairness between cpu and gpu memory requests by applying cpu-biased shortest job first scheduling policy or gpu-biased round robin scheduling policy. a dynamically configurable parameter is used to select between the two policies based on the system’s needs.\n * the last stage consists of simple per-bank fifo queue to issue low-level memory commands.\n\ntap: a tlp-aware cache management policy for a cpu-gpu heterogeneous\n\n\n\n * a core-sampling technique, which applies a different cache management policy to each gpu core and regularly collects statistics on the performance\n   of these cores to see how these polices affect gpu applications.\n   \n * gpu cores typically access caches much more frequently than cpu cores.\n   \n * enforces a similar cache lifetime to both cpu and gpgpu appli cations and prevent gpgpu application to monopolize the shared cache.\n\none (cm-cpu) for boosting cpu performance in the presence of gpu interference.\nthe other (cm-bal) for improving both cpu and gpu performance in a balanced manner and thus overall system performance.\n\n\n\n\npropose gpu concurrency management that dynamically throttles/boosts tlp (i.e., number of active warps) of gpu cores in order to minimize shared resources interference between cpu and gpu.',charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper",frontmatter:{title:"Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper",date:"2024-08-10T00:00:00.000Z",permalink:"/pages/458724/"},regularPath:"/03.gpu/11.IntegratedCPUGPUMemory.html",relativePath:"03.gpu/11.IntegratedCPUGPUMemory.md",key:"v-75a07a01",path:"/pages/458724/",headers:[{level:2,title:"Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper",slug:"harnessing-integrated-cpu-gpu-system-memory-for-hpc-a-first-look-into-grace-hopper",normalizedTitle:"harnessing integrated cpu-gpu system memory for hpc: a first look into grace hopper",charIndex:2},{level:3,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:100},{level:3,title:"Grace Hooper Unified Memory System",slug:"grace-hooper-unified-memory-system",normalizedTitle:"grace hooper unified memory system",charIndex:1293},{level:3,title:"Methodology",slug:"methodology",normalizedTitle:"methodology",charIndex:8699},{level:3,title:"Overview",slug:"overview",normalizedTitle:"overview",charIndex:9633},{level:3,title:"CPU-GPU Integrated System Page Table",slug:"cpu-gpu-integrated-system-page-table",normalizedTitle:"cpu-gpu integrated system page table",charIndex:12057},{level:3,title:"System Page Size",slug:"system-page-size",normalizedTitle:"system page size",charIndex:14458},{level:3,title:"Page Migration",slug:"page-migration",normalizedTitle:"page migration",charIndex:16671},{level:3,title:"Memory Oversubscription",slug:"memory-oversubscription",normalizedTitle:"memory oversubscription",charIndex:19172}],headersStr:"Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper Introduction Grace Hooper Unified Memory System Methodology Overview CPU-GPU Integrated System Page Table System Page Size Page Migration Memory Oversubscription",content:"# Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper\n\n👍 👍 👍\n\n\n# Introduction\n\n# limitation of state-of-art\n\n 1. UVM large overheads in handling page faults in GPU and suffers from read/write amplification due to page-level swapping.\n 2. Data object offloading requires offline profiling and application refactoring, limiting solution generality.\n 3. The performance of both solutions is constrained by data transfer bottlenecks between the CPU and GPU.\n\n# Grace Hopper Superchip\n\n * NVLink-C2C (chip-to-chip) cache-coherent interconnect\n * a single virtual memory space is shared between the CPU and GPU (i.e., system memory)\n * address translation is accelerated by hardware.\n   1. direct remote accesses at cacheline granularity\n   2. heuristic-guided page migrations.\n\nBy leveraging cacheline level access and Address Translation Service (ATS), which enables full access to all CPU and GPU memory allocations, the system memory eliminates the page-fault handling overhead needed in managed memory in UVM, and minimizes the need for memory migrations.\n\nWhile managed memory splits the virtual memory space into both the system page table and GPU page table, system memory relies on a single system-wide page table, shared between the CPU and the GPU.\n\n\n# Grace Hooper Unified Memory System\n\n * system-allocated memory\n * CUDA managed memory\n\n# Memory Subsystem\n\nThe CPU is connected to 480 GB of LPDDR5X memory, while the GPU is equipped with 96 GB of HBM3 memory.\nThese two processors, GPU and CPU, are interconnected via the Nvidia NVLink-C2C interconnect.\n\nResults show that the GPU’s HBM3 memory achieved a bandwidth of 3.4 TB/s, compared to its theoretical bandwidth of 4 TB/s.\nThe CPU’s LPDDR5X memory reached a bandwidth of 486 GB/s, close to its theoretical bandwidth of 500 GB/s.\n\nWe achieved a bandwidth of 375 GB/s for host-to-device (H2D) transfers and 297 GB/s for device-to-host (D2H) transfers, compared to the interconnect’s theoretical bandwidth of 450 GB/s.\n\n# NVLink-C2C Interconnect\n\nIn the Grace Hopper system, a processor (CPU or GPU) can directly access the other processor’s physical memory over the NVLink-C2C interconnect.\nCacheline granularity, with transfer sizes as small as 64 bytes on the CPU side and 128 bytes on the GPU side.\n\n# System-level Address Translation\n\nGrace CPU features a unique hardware unit called the System Memory Management Unit (SMMU).\n\nThe SMMU is responsible for translating virtual addresses to physical addresses by performing page table walks.\nCompared to a traditional MMU, the SMMU provides additional support for virtual-to-physical address translation requests from the GPU.\n\nThis is the flow that GPU TLB cached mapping and GPU wish to access physical memory stored in CPU memory system.\n\n\n\nAccess Flow:\n\n * A GPU thread accesses a virtual address.\n * The data is not cached in the GPU cache hierarchy. This generates a cache miss.\n * The virtual address is looked up in the GPU TLBs (Translation Lookaside Buffers) for virtual-to-physical translation.\n   As the translation is already cached, it is used to perform an access to physical memory.\n * The GMMU initiates a direct memory access (DMA) over the NVLink-C2C interconnect, at the cacheline granularity.\n * The requested access is performed from CPU memory, and send back to the GPU.\n * The access is completed, and memory is cached in the regular GPU cache hierarchy.\n\nCompared to pre-Grace Hopper systems, which rely on GPU page fault handling to access CPU memory, this new approach has two main implications.\n\n * First, GPU accesses to CPU-located memory no longer systematically trigger GPU page faults.\n * Second, page faults are now generated by the SMMU and can be directly handled by the operating system’s page fault handling mechanism, simplifying the overall process.\n\n# Memory Management in Grace Hopper\n\ntwo distinct page tables\n\n * system-wide page table\n * GPU-exclusive page table\n\nMemory allocation\n\n * allocations in CPU physical memory only\n * allocations in GPU physical memory only\n * allocations that can reside in either CPU or GPU physical memory\n\na system-wide page table, located in CPU memory.\nThe operating system directly accesses this page table, creates and manages page table entries (PTEs).\nThe SMMU uses this page table to provide virtual-to-physical address translation for both the CPU (when required by user applications) and the GPU (when requested over the NVLink-C2C interconnect).\nMemory pages in the system-wide page table can be physically located in either CPU or GPU memory, and they use the system page size, which is defined at the operating system level and constrained by the CPU architecture capabilities.\nWhen using the Grace CPU, the page size is either 4 KB or 64 KB.\n\nGPU-exclusive Page Table The Grace Hopper system retains the local GPU page table from previous generations of Nvidia GPUs.\nThis page table, located in GPU memory and only accessible by the GPU, stores virtual-to-physical translations for cudaMalloc allocations and cudaMallocManaged allocations when the physical location of the managed memory is on the GPU.\nThe page size used by this page table is 2 MB.\n\n# System-Allocated Memory\n\nIn general, when malloc is called, the operating system creates page table entries in the system page table without assigning physical memory to those pages. During the first access to a virtual address in the allocation, known as first-touch, a page fault is triggered since the accessed virtual page is not mapped to physical memory. Classic first-touch. On Grace Hopper, this process applies to both CPU and GPU first-touch accesses.\n\n * When a GPU thread generates a first-touch access to a virtual address, a GPU TLB miss is triggered.\n * As a result, the GPU’s ATSTBU (Translation Buffer Unit) generates an address translation request and sends it to the SMMU over NVLink-C2C.\n * To answer the request, the SMMU performs a page table walk in the system page table.\n * If no physical memory is allocated to the page, the SMMU issues a page fault.\n * OS handles the fault by updating the page table entry to point to GPU physical memory, as the first-touch originated from a GPU thread.\n * Once the physical address is stored in the GPU’s TLB, GPU threads can perform memory access using direct memory access to the physical memory address, potentially located in CPU memory, over NVLink-C2C.\n\n# Automatic Delayed Access-counter-based Migrations\n\nFor system-allocated memory, the Grace Hopper system can be configured to automatically migrate memory regions between GPU and CPU physical memory.\nThe default migration strategy, detailed in Nvidia’s open-source GPU driver, relies on hardware counters to track GPU accesses to memory ranges.\nWhen a counter value exceeds a user-defined threshold (by default, 256), the GPU issues a notification in the form of a hardware interrupt, which is handled by the GPU driver on the CPU.\nThe driver then determines whether to migrate the pages belonging to the associated virtual memory region.\n\n# CUDA Managed Memory\n\nCUDA managed memory is primarily a software abstraction, implemented as part of the CUDA runtime libraries and the Nvidia GPU driver. Programmers create managed memory allocations using the cudaMallocManaged() function.\nSimilar to malloc, for post-Pascal systems, the virtual memory is not immediately mapped to physical memory.\nInstead, the location of the first-touch triggers this mapping operation.\n\n> Please Notice that CUDA Managed Memory doesn't guarantee memory is allocated in GPU. It just means that the memory is allocated by this API.\n\n# On-demand page migration\n\nCUDA managed memory relies on on-demand page migration to enable both GPU and CPU to access the shared virtual memory range.\nWhen the GPU tries to access a page, a page fault is triggered if a GPU TLB miss occurs and the GMMU fails to find the virtual address in the GPU-exclusive page table.\nThis page fault causes a page migration from CPU memory to GPU memory.\nwhen GPU memory is overwhelmed, pages can also be evicted to CPU memory.\\\n\nCoherent dynamic memory allocation was introduced on Power9 platforms in CUDA 9.2. This feature is supported by the ATS, which enables hardware-level address translations by allowing direct communication between CPU and GPU MMUs and eliminates the need for software-level address translation.\n\n# Speculative prefetching\n\nbefore they are accessed, in order to reduce the page fault handling overhead of CUDA managed memory on the critical path.\nThese strategies include explicit prefetching, triggered through the cudaMemPrefetchAsync API, and implicit prefetching performed by GPU hardware prefetchers.\n\n\n# Methodology\n\nA snippet of code transformation from a typical CUDA code with explicit memory copy to Unified Memory.\n\nWe derive two versions for each application,\n\n * one using CUDA managed memory\n * one using system-allocated memory.\n\nFor this purpose, we first identify candidate memory allocations to replace, by locating explicit host-to-device data movements in the code.\nWe replace the destination and source buffers in those data transfers by a single buffer, allocated using one of the two unified memory allocators, either the system-level allocator (malloc) or CUDA managed memory allocator (cudaMallocManaged).\nGPU-only buffers, which are never meant be accessed by the CPU, and are typically only used for storing intermediary results on the GPU, are still allocated with cudaMalloc.\n\nPhases:\nGPU context initialization and argument parsing, allocation, CPU-side buffers initialization, computation, and de-allocation.\n\n\n# Overview\n\ncategorized into two classes.\n\n * In some applications, the system memory version outperforms the managed memory version.\n   The managed memory will trigger page faults when the GPU accesses data that is not in GPU memory, and start on-demand page migration.\n   As pointed in multiple existing works [2, 9], the page fault handling can cause higher overhead than the data migration itself.\n   The new cache-coherent NVlink-C2C enables direct data access to CPU memory at cacheline level without involving the expensive page fault mechanism, attributing to the observed speedup.\n   the system memory version even outperforms the original explicit version. The significant difference in the allocation and de-allocation time depending on the type of memory management in use.\n * In contrast, for SRAD and Quantum Volume simulations of 21-23 qubits, the managed memory version outperforms the system memory version.\n\nOur in-depth analysis in Section 5 identifies the main factors coming from\n\n * the data structures that are initialized on GPU\n * the different sizes of the integrated system pages and GPU-exclusive pages.\n\nWe also identified a difference in behavior for the GPU context initialization.\nIn the traditional explicit version and managed memory version, memory allocations, and data transfer are done through specific CUDA APIs before kernel launches, which implicitly initialize GPU context.\nHowever, in the system memory version, due to the absence of explicit CUDA memory allocation and data copy API calls, GPU context initialization occurs within the first kernel launch, apparently prolonging the computation time.\n\n\n\nCUDA Managed Mode: once in the computation phase, GPU access to data triggers page migration, and a steep decrease in system memory and a sharp increase in GPU memory usage is observed. hotspot represents a typical class of existing GPU applications, where data structures used in GPU computation are initialized on CPU.\n\n\n\nIn this application, the end-to-end execution is significantly prolonged in the system memory version, compared to the managed memory version.\nHowever, we also notice that the main difference is only constrained in the initialization phase,\nwhere the GPU memory usage slowly ramps up in the system memory version (orange) but quickly reaches the peak in the managed memory version (blue).\nIn fact, the computation phase in both versions are similar.\n\n\n# CPU-GPU Integrated System Page Table\n\nThe following two factor will affect integrated page table.\nSystem memory uses a first-touch placement policy and pages always reside in the system page table.\nManaged memory also uses a first-touch placement policy but pages may reside in either the system page table or GPU page table, depending on its physical location.\n\n# CPU-side Initialization\n\nCommon HPC perform data initialization, often including pre-processing, on the CPU before offloading data onto the GPU for computation.\n\nIn such a pattern, the first-touch policy will cause pages to be placed on the CPU during initialization.\nWhen the computation phase starts, in managed memory, data is migrated on demand to the GPU memory often with additional pages from speculative prefetching, which will result in both traffic on the NVLink-C2C and increased GPU memory utilization.\nInstead, in the system memory, data will not be migrated on access but deferred, which will result in only traffic on the NVLink-C2C link and no immediately increased GPU memory utilization. using system memory (left) and CUDA managed memory (right).\n\nIn short:\n\n * Managed, introduce additional prefetch\n * System, access-counter-based migration\n\n# GPU-side Initialization\n\n> Please Notice that Malloc and CUDA can both allocate memory, then initialized by GPU, which is far more different.\n\nWith CUDA managed memory, the initialization is much shorter than that in the system memory version, and no page migration is performed during the computation phase, as the first touch by GPU has directly mapped data to GPU memory.\n\nWith system memory, the GPU first-touch policy triggers a replayable page fault, as the page being first-touched is neither present in the GMMU page table, nor through address translation.\nThe CPU then handles the page fault and populates the system page table, therefore slowing down the initialization time on the GPU.\n\nIn short:\n\n * Managed, no page migration\n * System, CPU intervene the process, handle page fault and populate the system page table.\n\n\n\nsystem-allocated memory performs better in cases of CPU-side initialization as the page faults are both triggered and handled on the CPU side.\nIn the GPU-side initialization, page table initialization on the CPU-side significantly slows down the execution. In the latter cases, we observed that CUDA managed memory performed better.\n\n\n# System Page Size\n\nAll pages in a system allocation use the system page size, while only pages resident on CPU memory in the managed memory uses the system page size.\nThe system page sizes mainly impacts the page initialization overhead that often occurs in application initialization phase, and migration performance between CPU and GPU memory that often occurs in the computation phase.\n\nWe run each application in the system memory version by configuring the system pages in 4 KB and 64 KB.\n\nA noticeable difference between 4 KB and 64 KB pages lies in the de-allocation time, which is significantly higher in 4 KB system pages, for all applications.\n\nRodinia applications, with the exception of SRAD, exhibit lower compute time for 4 KB pages compared to 64 KB pages (1.1×-2.1×).\n\n\n\nOne possible reason for the lower performance in 64 KB pages pages is the granularity of migrated pages may cause amplification, resulting in unused data being migrated.\nThis performance loss could also partially be attributed to the automatic migrations that might incur temporary latency increase when the computation accesses on pages that are being migrated, reducing performance.\nIn Rodinia applications, this is particularly noticeable as applications have a short computation time, where migrated data may not be sufficiently reused.\n\n\n\nWith an increasing problem size, the speedup in the managed memory version is decreasing while the speedup in the system memory version is increasing.\n\nIn CUDA managed memory, when using 64 KB pages, the execution time is 10% lower than with 4 KB pages. This limited impact of the system page size is expected, as Qiskit has GPU-side data initialization, and CUDA managed memory uses the GPU page table for GPU-resident data, with a constant 2 MB page size, independent of system page size.\n\nWhile the computation time remains stable between page sizes, the initialization time is drastically reduced with 64 KB pages, with a 5× improvement.\nThis difference highlights the cost of GPU-side page initialization, where memory pages are first-touched on the GPU-side, and page table initialization is performed on the CPU-side, representing a notable bottleneck in the application.\n\n\n# Page Migration\n\nwe compare the new automatic access-counter-based strategy in system-allocated memory on Grace Hopper with the on-demand page migration strategy in CUDA managed memory.\n\n * access-counter-based An application needs to have access patterns that can clearly expose hot pages to exploit the access-counter-based strategy in system-allocated memory.\n   We examined all the test applications and choose SRAD as this application uses an iterative algorithm in its computation phase.\n   Therefore, with a sufficient number of iterations, the access-counter-based page migration should migrate pages repeatedly accessed during computation iterations into GPU memory. \\\n * on-demand migration the on-demand page migration in the managed system version should migrate all accessed pages on their first access.\n\nFor the managed memory version, due to page migration in the first iteration, the execution time of this iteration is significantly higher than the other iterations.\n\nIn the system memory version, from a performance standpoint, the computational phase consists of three sub-phases, as separated by dashed line on Figure 10.\n\n * The first phase corresponds to the first iteration, with high execution time, primarily caused by the overhead of GPU first-touch on system-allocated data, as memory pages must be initialized on the CPU-side.\n * The second phase (iteration 2-4), exhibits a decreasing iteration time but still slower than that of the managed memory version.\n * In the final phase (iteration 5 and above), the iteration time stabilizes and outperform the managed memory version.\n\n\n\nIn the managed memory version, all reads are performed from GPU memory, even for the first iteration, where pages are being migrated, and exhibit non-zero reads over NVLink-C2C.\nThis is because in managed memory, pages are first migrated, and then read from local GPU memory.\nIn the system memory version, we observe that memory reads over NVLink-C2C decreases as reads from GPU memory increases gradually in iteration 1-4.\nThis observation confirms that the ccess-pattern-based automatic migrations are being triggered in this stage, which hinders performance in this period.\nAfter the entire working set has been migrated to GPU memory, that is, for iterations 5-12, memory reads over NVLink-C2C remain nearly zero while reads from GPU memory stabilize at 1.5 GB per iteration.\nConsequently, the performance in iterations 5-12 improves to outperform that of the managed memory version.\n\n\n# Memory Oversubscription\n\n * First, pages can be evicted from GPU memory, and the required pages can be migrated into GPU. This is the expected behavior for CUDA managed memory.\n * In addition, as Grace Hopper supports direct memory access over NVLink-C2C, data in CPU memory can be remotely accessed without migration.\n\n\n\nThe system memory version of Rodinia applications, BFS, hotspot, needle, pathfinder, are less affected by oversubscription than the managed versions, as indicated by the increased speedup at increased over-subscription.\nThis trend is because that the system-memory version always places data on CPU memory, and performs accesses over NVLink-C2C link.\nHowever, in the managed memory version, data is being migrated to the GPU, and evicted when the GPU memory has been exhausted.\nThis eviction and migration process significantly impacts the performance.\n\nFor the 34-qubits quantum volume simulation (about 130% GPU memory oversubscription), a significant slowdown with managed memory is observed compared to the explicit copy version. Further analysis reveals that no page is migrated and all data is accessed over NVLink-C2C at a low bandwidth.\nWe optimize the managed memory version using CUDA managed memory prefetching to transform the majority of data access to be read locally from GPU memory.\n\nIn previous in-memory scenarios, CUDA managed memory in both 4 KB and 64 KB pages exhibits similar execution times.\nHowever, in oversubscription scenarios, the system page size shows a high impact on execution time.\n\n\n\nIn the 34-qubit quantum simulation, switching from 4 KB to 64 KB system pages shortens initialization and accelerates page migration by 58%.\nInterestingly, the 30-qubit simulation shows a different preference on the system page size, nearly 3× slower computation when using 64 KB system pages as shown in Figure 13.\\ This is unexpected, as the page size for GPU-resident memory is 2 MB in managed memory, and is not modified by the system page size. We suggest that this difference is due to some pages being evicted to CPU memory where the system page size is used. In the case of 64 KB pages, when those pages are migrated back to the GPU, the amount of migrated memory at a time is higher than 4 KB, affecting performance.",normalizedContent:"# harnessing integrated cpu-gpu system memory for hpc: a first look into grace hopper\n\n👍 👍 👍\n\n\n# introduction\n\n# limitation of state-of-art\n\n 1. uvm large overheads in handling page faults in gpu and suffers from read/write amplification due to page-level swapping.\n 2. data object offloading requires offline profiling and application refactoring, limiting solution generality.\n 3. the performance of both solutions is constrained by data transfer bottlenecks between the cpu and gpu.\n\n# grace hopper superchip\n\n * nvlink-c2c (chip-to-chip) cache-coherent interconnect\n * a single virtual memory space is shared between the cpu and gpu (i.e., system memory)\n * address translation is accelerated by hardware.\n   1. direct remote accesses at cacheline granularity\n   2. heuristic-guided page migrations.\n\nby leveraging cacheline level access and address translation service (ats), which enables full access to all cpu and gpu memory allocations, the system memory eliminates the page-fault handling overhead needed in managed memory in uvm, and minimizes the need for memory migrations.\n\nwhile managed memory splits the virtual memory space into both the system page table and gpu page table, system memory relies on a single system-wide page table, shared between the cpu and the gpu.\n\n\n# grace hooper unified memory system\n\n * system-allocated memory\n * cuda managed memory\n\n# memory subsystem\n\nthe cpu is connected to 480 gb of lpddr5x memory, while the gpu is equipped with 96 gb of hbm3 memory.\nthese two processors, gpu and cpu, are interconnected via the nvidia nvlink-c2c interconnect.\n\nresults show that the gpu’s hbm3 memory achieved a bandwidth of 3.4 tb/s, compared to its theoretical bandwidth of 4 tb/s.\nthe cpu’s lpddr5x memory reached a bandwidth of 486 gb/s, close to its theoretical bandwidth of 500 gb/s.\n\nwe achieved a bandwidth of 375 gb/s for host-to-device (h2d) transfers and 297 gb/s for device-to-host (d2h) transfers, compared to the interconnect’s theoretical bandwidth of 450 gb/s.\n\n# nvlink-c2c interconnect\n\nin the grace hopper system, a processor (cpu or gpu) can directly access the other processor’s physical memory over the nvlink-c2c interconnect.\ncacheline granularity, with transfer sizes as small as 64 bytes on the cpu side and 128 bytes on the gpu side.\n\n# system-level address translation\n\ngrace cpu features a unique hardware unit called the system memory management unit (smmu).\n\nthe smmu is responsible for translating virtual addresses to physical addresses by performing page table walks.\ncompared to a traditional mmu, the smmu provides additional support for virtual-to-physical address translation requests from the gpu.\n\nthis is the flow that gpu tlb cached mapping and gpu wish to access physical memory stored in cpu memory system.\n\n\n\naccess flow:\n\n * a gpu thread accesses a virtual address.\n * the data is not cached in the gpu cache hierarchy. this generates a cache miss.\n * the virtual address is looked up in the gpu tlbs (translation lookaside buffers) for virtual-to-physical translation.\n   as the translation is already cached, it is used to perform an access to physical memory.\n * the gmmu initiates a direct memory access (dma) over the nvlink-c2c interconnect, at the cacheline granularity.\n * the requested access is performed from cpu memory, and send back to the gpu.\n * the access is completed, and memory is cached in the regular gpu cache hierarchy.\n\ncompared to pre-grace hopper systems, which rely on gpu page fault handling to access cpu memory, this new approach has two main implications.\n\n * first, gpu accesses to cpu-located memory no longer systematically trigger gpu page faults.\n * second, page faults are now generated by the smmu and can be directly handled by the operating system’s page fault handling mechanism, simplifying the overall process.\n\n# memory management in grace hopper\n\ntwo distinct page tables\n\n * system-wide page table\n * gpu-exclusive page table\n\nmemory allocation\n\n * allocations in cpu physical memory only\n * allocations in gpu physical memory only\n * allocations that can reside in either cpu or gpu physical memory\n\na system-wide page table, located in cpu memory.\nthe operating system directly accesses this page table, creates and manages page table entries (ptes).\nthe smmu uses this page table to provide virtual-to-physical address translation for both the cpu (when required by user applications) and the gpu (when requested over the nvlink-c2c interconnect).\nmemory pages in the system-wide page table can be physically located in either cpu or gpu memory, and they use the system page size, which is defined at the operating system level and constrained by the cpu architecture capabilities.\nwhen using the grace cpu, the page size is either 4 kb or 64 kb.\n\ngpu-exclusive page table the grace hopper system retains the local gpu page table from previous generations of nvidia gpus.\nthis page table, located in gpu memory and only accessible by the gpu, stores virtual-to-physical translations for cudamalloc allocations and cudamallocmanaged allocations when the physical location of the managed memory is on the gpu.\nthe page size used by this page table is 2 mb.\n\n# system-allocated memory\n\nin general, when malloc is called, the operating system creates page table entries in the system page table without assigning physical memory to those pages. during the first access to a virtual address in the allocation, known as first-touch, a page fault is triggered since the accessed virtual page is not mapped to physical memory. classic first-touch. on grace hopper, this process applies to both cpu and gpu first-touch accesses.\n\n * when a gpu thread generates a first-touch access to a virtual address, a gpu tlb miss is triggered.\n * as a result, the gpu’s atstbu (translation buffer unit) generates an address translation request and sends it to the smmu over nvlink-c2c.\n * to answer the request, the smmu performs a page table walk in the system page table.\n * if no physical memory is allocated to the page, the smmu issues a page fault.\n * os handles the fault by updating the page table entry to point to gpu physical memory, as the first-touch originated from a gpu thread.\n * once the physical address is stored in the gpu’s tlb, gpu threads can perform memory access using direct memory access to the physical memory address, potentially located in cpu memory, over nvlink-c2c.\n\n# automatic delayed access-counter-based migrations\n\nfor system-allocated memory, the grace hopper system can be configured to automatically migrate memory regions between gpu and cpu physical memory.\nthe default migration strategy, detailed in nvidia’s open-source gpu driver, relies on hardware counters to track gpu accesses to memory ranges.\nwhen a counter value exceeds a user-defined threshold (by default, 256), the gpu issues a notification in the form of a hardware interrupt, which is handled by the gpu driver on the cpu.\nthe driver then determines whether to migrate the pages belonging to the associated virtual memory region.\n\n# cuda managed memory\n\ncuda managed memory is primarily a software abstraction, implemented as part of the cuda runtime libraries and the nvidia gpu driver. programmers create managed memory allocations using the cudamallocmanaged() function.\nsimilar to malloc, for post-pascal systems, the virtual memory is not immediately mapped to physical memory.\ninstead, the location of the first-touch triggers this mapping operation.\n\n> please notice that cuda managed memory doesn't guarantee memory is allocated in gpu. it just means that the memory is allocated by this api.\n\n# on-demand page migration\n\ncuda managed memory relies on on-demand page migration to enable both gpu and cpu to access the shared virtual memory range.\nwhen the gpu tries to access a page, a page fault is triggered if a gpu tlb miss occurs and the gmmu fails to find the virtual address in the gpu-exclusive page table.\nthis page fault causes a page migration from cpu memory to gpu memory.\nwhen gpu memory is overwhelmed, pages can also be evicted to cpu memory.\\\n\ncoherent dynamic memory allocation was introduced on power9 platforms in cuda 9.2. this feature is supported by the ats, which enables hardware-level address translations by allowing direct communication between cpu and gpu mmus and eliminates the need for software-level address translation.\n\n# speculative prefetching\n\nbefore they are accessed, in order to reduce the page fault handling overhead of cuda managed memory on the critical path.\nthese strategies include explicit prefetching, triggered through the cudamemprefetchasync api, and implicit prefetching performed by gpu hardware prefetchers.\n\n\n# methodology\n\na snippet of code transformation from a typical cuda code with explicit memory copy to unified memory.\n\nwe derive two versions for each application,\n\n * one using cuda managed memory\n * one using system-allocated memory.\n\nfor this purpose, we first identify candidate memory allocations to replace, by locating explicit host-to-device data movements in the code.\nwe replace the destination and source buffers in those data transfers by a single buffer, allocated using one of the two unified memory allocators, either the system-level allocator (malloc) or cuda managed memory allocator (cudamallocmanaged).\ngpu-only buffers, which are never meant be accessed by the cpu, and are typically only used for storing intermediary results on the gpu, are still allocated with cudamalloc.\n\nphases:\ngpu context initialization and argument parsing, allocation, cpu-side buffers initialization, computation, and de-allocation.\n\n\n# overview\n\ncategorized into two classes.\n\n * in some applications, the system memory version outperforms the managed memory version.\n   the managed memory will trigger page faults when the gpu accesses data that is not in gpu memory, and start on-demand page migration.\n   as pointed in multiple existing works [2, 9], the page fault handling can cause higher overhead than the data migration itself.\n   the new cache-coherent nvlink-c2c enables direct data access to cpu memory at cacheline level without involving the expensive page fault mechanism, attributing to the observed speedup.\n   the system memory version even outperforms the original explicit version. the significant difference in the allocation and de-allocation time depending on the type of memory management in use.\n * in contrast, for srad and quantum volume simulations of 21-23 qubits, the managed memory version outperforms the system memory version.\n\nour in-depth analysis in section 5 identifies the main factors coming from\n\n * the data structures that are initialized on gpu\n * the different sizes of the integrated system pages and gpu-exclusive pages.\n\nwe also identified a difference in behavior for the gpu context initialization.\nin the traditional explicit version and managed memory version, memory allocations, and data transfer are done through specific cuda apis before kernel launches, which implicitly initialize gpu context.\nhowever, in the system memory version, due to the absence of explicit cuda memory allocation and data copy api calls, gpu context initialization occurs within the first kernel launch, apparently prolonging the computation time.\n\n\n\ncuda managed mode: once in the computation phase, gpu access to data triggers page migration, and a steep decrease in system memory and a sharp increase in gpu memory usage is observed. hotspot represents a typical class of existing gpu applications, where data structures used in gpu computation are initialized on cpu.\n\n\n\nin this application, the end-to-end execution is significantly prolonged in the system memory version, compared to the managed memory version.\nhowever, we also notice that the main difference is only constrained in the initialization phase,\nwhere the gpu memory usage slowly ramps up in the system memory version (orange) but quickly reaches the peak in the managed memory version (blue).\nin fact, the computation phase in both versions are similar.\n\n\n# cpu-gpu integrated system page table\n\nthe following two factor will affect integrated page table.\nsystem memory uses a first-touch placement policy and pages always reside in the system page table.\nmanaged memory also uses a first-touch placement policy but pages may reside in either the system page table or gpu page table, depending on its physical location.\n\n# cpu-side initialization\n\ncommon hpc perform data initialization, often including pre-processing, on the cpu before offloading data onto the gpu for computation.\n\nin such a pattern, the first-touch policy will cause pages to be placed on the cpu during initialization.\nwhen the computation phase starts, in managed memory, data is migrated on demand to the gpu memory often with additional pages from speculative prefetching, which will result in both traffic on the nvlink-c2c and increased gpu memory utilization.\ninstead, in the system memory, data will not be migrated on access but deferred, which will result in only traffic on the nvlink-c2c link and no immediately increased gpu memory utilization. using system memory (left) and cuda managed memory (right).\n\nin short:\n\n * managed, introduce additional prefetch\n * system, access-counter-based migration\n\n# gpu-side initialization\n\n> please notice that malloc and cuda can both allocate memory, then initialized by gpu, which is far more different.\n\nwith cuda managed memory, the initialization is much shorter than that in the system memory version, and no page migration is performed during the computation phase, as the first touch by gpu has directly mapped data to gpu memory.\n\nwith system memory, the gpu first-touch policy triggers a replayable page fault, as the page being first-touched is neither present in the gmmu page table, nor through address translation.\nthe cpu then handles the page fault and populates the system page table, therefore slowing down the initialization time on the gpu.\n\nin short:\n\n * managed, no page migration\n * system, cpu intervene the process, handle page fault and populate the system page table.\n\n\n\nsystem-allocated memory performs better in cases of cpu-side initialization as the page faults are both triggered and handled on the cpu side.\nin the gpu-side initialization, page table initialization on the cpu-side significantly slows down the execution. in the latter cases, we observed that cuda managed memory performed better.\n\n\n# system page size\n\nall pages in a system allocation use the system page size, while only pages resident on cpu memory in the managed memory uses the system page size.\nthe system page sizes mainly impacts the page initialization overhead that often occurs in application initialization phase, and migration performance between cpu and gpu memory that often occurs in the computation phase.\n\nwe run each application in the system memory version by configuring the system pages in 4 kb and 64 kb.\n\na noticeable difference between 4 kb and 64 kb pages lies in the de-allocation time, which is significantly higher in 4 kb system pages, for all applications.\n\nrodinia applications, with the exception of srad, exhibit lower compute time for 4 kb pages compared to 64 kb pages (1.1×-2.1×).\n\n\n\none possible reason for the lower performance in 64 kb pages pages is the granularity of migrated pages may cause amplification, resulting in unused data being migrated.\nthis performance loss could also partially be attributed to the automatic migrations that might incur temporary latency increase when the computation accesses on pages that are being migrated, reducing performance.\nin rodinia applications, this is particularly noticeable as applications have a short computation time, where migrated data may not be sufficiently reused.\n\n\n\nwith an increasing problem size, the speedup in the managed memory version is decreasing while the speedup in the system memory version is increasing.\n\nin cuda managed memory, when using 64 kb pages, the execution time is 10% lower than with 4 kb pages. this limited impact of the system page size is expected, as qiskit has gpu-side data initialization, and cuda managed memory uses the gpu page table for gpu-resident data, with a constant 2 mb page size, independent of system page size.\n\nwhile the computation time remains stable between page sizes, the initialization time is drastically reduced with 64 kb pages, with a 5× improvement.\nthis difference highlights the cost of gpu-side page initialization, where memory pages are first-touched on the gpu-side, and page table initialization is performed on the cpu-side, representing a notable bottleneck in the application.\n\n\n# page migration\n\nwe compare the new automatic access-counter-based strategy in system-allocated memory on grace hopper with the on-demand page migration strategy in cuda managed memory.\n\n * access-counter-based an application needs to have access patterns that can clearly expose hot pages to exploit the access-counter-based strategy in system-allocated memory.\n   we examined all the test applications and choose srad as this application uses an iterative algorithm in its computation phase.\n   therefore, with a sufficient number of iterations, the access-counter-based page migration should migrate pages repeatedly accessed during computation iterations into gpu memory. \\\n * on-demand migration the on-demand page migration in the managed system version should migrate all accessed pages on their first access.\n\nfor the managed memory version, due to page migration in the first iteration, the execution time of this iteration is significantly higher than the other iterations.\n\nin the system memory version, from a performance standpoint, the computational phase consists of three sub-phases, as separated by dashed line on figure 10.\n\n * the first phase corresponds to the first iteration, with high execution time, primarily caused by the overhead of gpu first-touch on system-allocated data, as memory pages must be initialized on the cpu-side.\n * the second phase (iteration 2-4), exhibits a decreasing iteration time but still slower than that of the managed memory version.\n * in the final phase (iteration 5 and above), the iteration time stabilizes and outperform the managed memory version.\n\n\n\nin the managed memory version, all reads are performed from gpu memory, even for the first iteration, where pages are being migrated, and exhibit non-zero reads over nvlink-c2c.\nthis is because in managed memory, pages are first migrated, and then read from local gpu memory.\nin the system memory version, we observe that memory reads over nvlink-c2c decreases as reads from gpu memory increases gradually in iteration 1-4.\nthis observation confirms that the ccess-pattern-based automatic migrations are being triggered in this stage, which hinders performance in this period.\nafter the entire working set has been migrated to gpu memory, that is, for iterations 5-12, memory reads over nvlink-c2c remain nearly zero while reads from gpu memory stabilize at 1.5 gb per iteration.\nconsequently, the performance in iterations 5-12 improves to outperform that of the managed memory version.\n\n\n# memory oversubscription\n\n * first, pages can be evicted from gpu memory, and the required pages can be migrated into gpu. this is the expected behavior for cuda managed memory.\n * in addition, as grace hopper supports direct memory access over nvlink-c2c, data in cpu memory can be remotely accessed without migration.\n\n\n\nthe system memory version of rodinia applications, bfs, hotspot, needle, pathfinder, are less affected by oversubscription than the managed versions, as indicated by the increased speedup at increased over-subscription.\nthis trend is because that the system-memory version always places data on cpu memory, and performs accesses over nvlink-c2c link.\nhowever, in the managed memory version, data is being migrated to the gpu, and evicted when the gpu memory has been exhausted.\nthis eviction and migration process significantly impacts the performance.\n\nfor the 34-qubits quantum volume simulation (about 130% gpu memory oversubscription), a significant slowdown with managed memory is observed compared to the explicit copy version. further analysis reveals that no page is migrated and all data is accessed over nvlink-c2c at a low bandwidth.\nwe optimize the managed memory version using cuda managed memory prefetching to transform the majority of data access to be read locally from gpu memory.\n\nin previous in-memory scenarios, cuda managed memory in both 4 kb and 64 kb pages exhibits similar execution times.\nhowever, in oversubscription scenarios, the system page size shows a high impact on execution time.\n\n\n\nin the 34-qubit quantum simulation, switching from 4 kb to 64 kb system pages shortens initialization and accelerates page migration by 58%.\ninterestingly, the 30-qubit simulation shows a different preference on the system page size, nearly 3× slower computation when using 64 kb system pages as shown in figure 13.\\ this is unexpected, as the page size for gpu-resident memory is 2 mb in managed memory, and is not modified by the system page size. we suggest that this difference is due to some pages being evicted to cpu memory where the system page size is used. in the case of 64 kb pages, when those pages are migrated back to the gpu, the amount of migrated memory at a time is higher than 4 kb, affecting performance.",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (1)",frontmatter:{title:"Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (1)",date:"2024-08-12T00:00:00.000Z",permalink:"/pages/458725/"},regularPath:"/03.gpu/12.gpgpusim.html",relativePath:"03.gpu/12.gpgpusim.md",key:"v-5474106a",path:"/pages/458725/",headers:[{level:3,title:"How did GPGPU-sim get instruction from CUDA?",slug:"how-did-gpgpu-sim-get-instruction-from-cuda",normalizedTitle:"how did gpgpu-sim get instruction from cuda?",charIndex:3278}],headersStr:"How did GPGPU-sim get instruction from CUDA?",content:" * libcuda\n   * cuda_runtime_api.cc\n * src\n   * abstract_hardware_model.h/cpp\n   * gpgpusim_entrypoint.h/cpp\n   * stream_manager.h/cpp\n   * cuda-sim\n     \n     * cuda-sim.h/cc 🐝\n       \n       Code\n       \n       \n       \n       void function_info::ptx_assemble() {\n        for ( i=m_instructions.begin(); i != m_instructions.end(); i++ ) {\n         // map pc to instruction\n         g_pc_to_finfo[PC] = this;\n         // This is a uniform array, each entry is one instruction\n         m_instr_mem[n] = pI; \n         s_g_pc_to_insn.push_back(pI);\n         ssert(pI == s_g_pc_to_insn[PC]);\n         pI->set_m_instr_mem_index(n);\n         pI->set_PC(PC);\n        }\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       \n     \n     * memory.h/cc\n     \n     * opcode.h/def\n     \n     * ptx_loader.h/cc\n     \n     * ptx.y\n       It is a Yacc/Bison grammar file used to parse PTX (Parallel Thread Execution) assembly code, which is a low-level intermediate representation used by NVIDIA GPUs.\n       It defines the grammar rules for PTX assembly code and specifies how different components of PTX code should be interpreted and processed.\n       This includes recognizing various PTX instructions, operands, directives, and control structures, and translating them into an internal representation that the simulator can work with.\n       \n       When it meets instruction statement it will call add_instruction.\n       \n       Code\n       \n       \n       \n       statement_list: directive_statement { add_directive(); }\n       | instruction_statement { add_instruction();}\n       ...\n       \n       \n       1\n       2\n       3\n       \n     \n     * ptx_parser.h/cc the add_instruction used in ptx.y will call the following instruction.\n       \n       Code\n       \n       \n       \n       void add_instruction() \n       {\n       ptx_instruction *i = new ptx_instruction(**);\n       g_instructions.push_back(i);\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       \n       in the end of function it will add all the instructins into function infomation Code\n       \n       void end_function()\n       {\n       ...\n       g_func_info->add_inst( g_instructions );\n       ...\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       \n     \n     * ptx_ir.h/cc\n       \n       Code\n       \n       \n       \n       //@@@@@@ ptx_ir.h\n       ...\n       std::vector<const symbol*> m_args;\n       // end_function will put function into this list\n       std::list<ptx_instruction*> m_instructions;\n       std::vector<basic_block_t*> m_basic_blocks;\n       \n       //@@@@@@ ptx_ir.cc\n       void gpgpu_ptx_assemble( std::string kname, void *kinfo ) {\n        function_info *func_info = (function_info *)kinfo;\n        // This will call cuda_sim ptx_assemble function\n        func_info->ptx_assemble();\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       \n     \n     * ptx_sim.h/cc\n   \n   * gpgpu-sim\n     * gpgpu-sim.h/cc\n     * shader.h/shader.cc\n     * mem_fetch.h/cc\n     * stack.h/cc\n     * addrdec.h/cc\n     * dram.h/cc\n     * traffic_breakdown.h/cc\n\n\n# How did GPGPU-sim get instruction from CUDA?\n\n🐝 show how Yacc/Bison grammar file is used to add instruction in function_info. Now we will describe how GPGPU-sim execute each instructin.\n\n# Function Mode\n\nabstract_hardware_model.cc implement this function, if you provde a warpId, it can return warp_instruction.\nThus, if we know next pc, we can use ptx_fetch_inst to get instruction.\n\nCode\n\n\n\n// @@@@@@ abstract_hardware_model.cc\n//! Get the warp to be executed using the data taken form the SIMT stack\nwarp_inst_t core_t::getExecuteWarp(unsigned warpId)\n{\n    unsigned pc,rpc;\n    m_simt_stack[warpId]->get_pdom_stack_top_info(&pc,&rpc);\n    warp_inst_t wi= *ptx_fetch_inst(pc);\n    wi.set_active(m_simt_stack[warpId]->get_active_mask());\n    return wi;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n      gpgpu*_sim_main_function\n                 |\n    gpgpu_cuda_ptx_sim_main_func\n                 |\n              execute\n                 |\n            executeWarp\n                 |\n   getExecuteWarp execute_warp_inst_t\n\n\nCode\n\n// @@@@@@ gpgpusim_entrypoint.cc\nint gpgpu_opencl_ptx_sim_main_func( kernel_info_t *grid )\n{\n    //calling the CUDA PTX simulator, sending the kernel by reference and a flag set to true,\n    //the flag used by the function to distinguish OpenCL calls from the CUDA simulation calls which\n    //it is needed by the called function to not register the exit the exit of OpenCL kernel as it doesn't register entering in the first place as the CUDA kernels does\n   gpgpu_cuda_ptx_sim_main_func( *grid, true );\n   return 0;\n}\n\n// @@@@@@ cuda-sim.cc\n/*!\nThis function simulates the CUDA code functionally, it takes a kernel_info_t parameter \nwhich holds the data for the CUDA kernel to be executed\n!*/\nvoid gpgpu_cuda_ptx_sim_main_func( kernel_info_t &kernel, bool openCL ) {\n  while(!kernel.no_more_ctas_to_run()){\n    functionalCoreSim cta(&kernel,g_the_gpu,\n    g_the_gpu->getShaderCoreConfig()->warp_size\n    );\n    cta.execute();\n }\n}\n\n\nvoid functionalCoreSim::execute()\n {\n    ...\n    //start executing the CTA\n    while(true){\n        ...\n        for(unsigned i=0;i<m_warp_count;i++){\n            executeWarp(i,allAtBarrier,someOneLive);\n        }\n        ...\n    }\n }\n\nvoid functionalCoreSim::executeWarp(unsigned i, bool &allAtBarrier, bool & someOneLive)\n{\n ...\n warp_inst_t inst =getExecuteWarp(i);\n //!!!!! Attention !!!!!!!!\n execute_warp_inst_t(inst,i);\n ...\n updateSIMTStack( i, &inst );\n}\n\nconst warp_inst_t *ptx_fetch_inst( address_type pc )\n{\n    return function_info::pc_to_instruction(pc);\n}\n\n// @@@@@@ ptx_ir.h\nstatic const ptx_instruction* pc_to_instruction(unsigned pc) \n{\n  if( pc < s_g_pc_to_insn.size() )\n      return s_g_pc_to_insn[pc];\n  else\n      return NULL;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n# Timing Mode\n\n * shader.cc decode() fill instruction into ibuffer\n * shader.h filled into m_ibuffer\n * shader.cc cycle issue warp\n\nCode\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::decode()\n{\n    if( m_inst_fetch_buffer.m_valid ) {\n        // decode 1 or 2 instructions and place them into ibuffer\n        address_type pc = m_inst_fetch_buffer.m_pc;\n        const warp_inst_t* pI1 = ptx_fetch_inst(pc);\n        m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(0,pI1);\n        m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();\n        ...\n    }\n}\n\n// @@@@@@ shader.h\n    void ibuffer_fill( unsigned slot, const warp_inst_t *pI )\n    {\n       m_ibuffer[slot].m_inst=pI;\n       m_ibuffer[slot].m_valid=true;\n    }\n\n    const warp_inst_t *ibuffer_next_inst() { return m_ibuffer[m_next].m_inst; }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nEvery cycle, if current warp is done, it will pick form the m_next_cycle_prioritized_warps to schedule next warp.\nThe instruction is obtained from m_ibuffer.\n\nCode\n\nvoid scheduler_unit::cycle()\n{\n    SCHED_DPRINTF( \"scheduler_unit::cycle()\\n\" );\n    bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn't require flush due to control hazard)\n    bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes\n    bool issued_inst = false; // of these we issued one\n\n    order_warps();\n    for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();\n          iter != m_next_cycle_prioritized_warps.end();\n          iter++ ) {\n        // Don't consider warps that are not yet valid\n        if ( (*iter) == NULL || (*iter)->done_exit() ) {\n            continue;\n        }\n        while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() && (checked < max_issue) && (checked <= issued) && (issued < max_issue) ) {\n            const warp_inst_t *pI = warp(warp_id).ibuffer_next_inst();\n            if( pI ) {\n            ...\n              if ( (pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP) ) {\n                m_shader->issue_warp(*m_mem_out,pI,active_mask,warp_id);\n                issued++;\n                issued_inst=true;\n                warp_inst_issued = true;\n              } else if ( (pI->op == SFU_OP) || (pI->op == ALU_SFU_OP) ) {\n                m_shader->issue_warp(*m_sfu_out,pI,active_mask,warp_id);\n              }\n            }\n        }\n    }\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n",normalizedContent:" * libcuda\n   * cuda_runtime_api.cc\n * src\n   * abstract_hardware_model.h/cpp\n   * gpgpusim_entrypoint.h/cpp\n   * stream_manager.h/cpp\n   * cuda-sim\n     \n     * cuda-sim.h/cc 🐝\n       \n       code\n       \n       \n       \n       void function_info::ptx_assemble() {\n        for ( i=m_instructions.begin(); i != m_instructions.end(); i++ ) {\n         // map pc to instruction\n         g_pc_to_finfo[pc] = this;\n         // this is a uniform array, each entry is one instruction\n         m_instr_mem[n] = pi; \n         s_g_pc_to_insn.push_back(pi);\n         ssert(pi == s_g_pc_to_insn[pc]);\n         pi->set_m_instr_mem_index(n);\n         pi->set_pc(pc);\n        }\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       \n     \n     * memory.h/cc\n     \n     * opcode.h/def\n     \n     * ptx_loader.h/cc\n     \n     * ptx.y\n       it is a yacc/bison grammar file used to parse ptx (parallel thread execution) assembly code, which is a low-level intermediate representation used by nvidia gpus.\n       it defines the grammar rules for ptx assembly code and specifies how different components of ptx code should be interpreted and processed.\n       this includes recognizing various ptx instructions, operands, directives, and control structures, and translating them into an internal representation that the simulator can work with.\n       \n       when it meets instruction statement it will call add_instruction.\n       \n       code\n       \n       \n       \n       statement_list: directive_statement { add_directive(); }\n       | instruction_statement { add_instruction();}\n       ...\n       \n       \n       1\n       2\n       3\n       \n     \n     * ptx_parser.h/cc the add_instruction used in ptx.y will call the following instruction.\n       \n       code\n       \n       \n       \n       void add_instruction() \n       {\n       ptx_instruction *i = new ptx_instruction(**);\n       g_instructions.push_back(i);\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       \n       in the end of function it will add all the instructins into function infomation code\n       \n       void end_function()\n       {\n       ...\n       g_func_info->add_inst( g_instructions );\n       ...\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       \n     \n     * ptx_ir.h/cc\n       \n       code\n       \n       \n       \n       //@@@@@@ ptx_ir.h\n       ...\n       std::vector<const symbol*> m_args;\n       // end_function will put function into this list\n       std::list<ptx_instruction*> m_instructions;\n       std::vector<basic_block_t*> m_basic_blocks;\n       \n       //@@@@@@ ptx_ir.cc\n       void gpgpu_ptx_assemble( std::string kname, void *kinfo ) {\n        function_info *func_info = (function_info *)kinfo;\n        // this will call cuda_sim ptx_assemble function\n        func_info->ptx_assemble();\n       }\n       \n       \n       1\n       2\n       3\n       4\n       5\n       6\n       7\n       8\n       9\n       10\n       11\n       12\n       13\n       \n     \n     * ptx_sim.h/cc\n   \n   * gpgpu-sim\n     * gpgpu-sim.h/cc\n     * shader.h/shader.cc\n     * mem_fetch.h/cc\n     * stack.h/cc\n     * addrdec.h/cc\n     * dram.h/cc\n     * traffic_breakdown.h/cc\n\n\n# how did gpgpu-sim get instruction from cuda?\n\n🐝 show how yacc/bison grammar file is used to add instruction in function_info. now we will describe how gpgpu-sim execute each instructin.\n\n# function mode\n\nabstract_hardware_model.cc implement this function, if you provde a warpid, it can return warp_instruction.\nthus, if we know next pc, we can use ptx_fetch_inst to get instruction.\n\ncode\n\n\n\n// @@@@@@ abstract_hardware_model.cc\n//! get the warp to be executed using the data taken form the simt stack\nwarp_inst_t core_t::getexecutewarp(unsigned warpid)\n{\n    unsigned pc,rpc;\n    m_simt_stack[warpid]->get_pdom_stack_top_info(&pc,&rpc);\n    warp_inst_t wi= *ptx_fetch_inst(pc);\n    wi.set_active(m_simt_stack[warpid]->get_active_mask());\n    return wi;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n      gpgpu*_sim_main_function\n                 |\n    gpgpu_cuda_ptx_sim_main_func\n                 |\n              execute\n                 |\n            executewarp\n                 |\n   getexecutewarp execute_warp_inst_t\n\n\ncode\n\n// @@@@@@ gpgpusim_entrypoint.cc\nint gpgpu_opencl_ptx_sim_main_func( kernel_info_t *grid )\n{\n    //calling the cuda ptx simulator, sending the kernel by reference and a flag set to true,\n    //the flag used by the function to distinguish opencl calls from the cuda simulation calls which\n    //it is needed by the called function to not register the exit the exit of opencl kernel as it doesn't register entering in the first place as the cuda kernels does\n   gpgpu_cuda_ptx_sim_main_func( *grid, true );\n   return 0;\n}\n\n// @@@@@@ cuda-sim.cc\n/*!\nthis function simulates the cuda code functionally, it takes a kernel_info_t parameter \nwhich holds the data for the cuda kernel to be executed\n!*/\nvoid gpgpu_cuda_ptx_sim_main_func( kernel_info_t &kernel, bool opencl ) {\n  while(!kernel.no_more_ctas_to_run()){\n    functionalcoresim cta(&kernel,g_the_gpu,\n    g_the_gpu->getshadercoreconfig()->warp_size\n    );\n    cta.execute();\n }\n}\n\n\nvoid functionalcoresim::execute()\n {\n    ...\n    //start executing the cta\n    while(true){\n        ...\n        for(unsigned i=0;i<m_warp_count;i++){\n            executewarp(i,allatbarrier,someonelive);\n        }\n        ...\n    }\n }\n\nvoid functionalcoresim::executewarp(unsigned i, bool &allatbarrier, bool & someonelive)\n{\n ...\n warp_inst_t inst =getexecutewarp(i);\n //!!!!! attention !!!!!!!!\n execute_warp_inst_t(inst,i);\n ...\n updatesimtstack( i, &inst );\n}\n\nconst warp_inst_t *ptx_fetch_inst( address_type pc )\n{\n    return function_info::pc_to_instruction(pc);\n}\n\n// @@@@@@ ptx_ir.h\nstatic const ptx_instruction* pc_to_instruction(unsigned pc) \n{\n  if( pc < s_g_pc_to_insn.size() )\n      return s_g_pc_to_insn[pc];\n  else\n      return null;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n# timing mode\n\n * shader.cc decode() fill instruction into ibuffer\n * shader.h filled into m_ibuffer\n * shader.cc cycle issue warp\n\ncode\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::decode()\n{\n    if( m_inst_fetch_buffer.m_valid ) {\n        // decode 1 or 2 instructions and place them into ibuffer\n        address_type pc = m_inst_fetch_buffer.m_pc;\n        const warp_inst_t* pi1 = ptx_fetch_inst(pc);\n        m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(0,pi1);\n        m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();\n        ...\n    }\n}\n\n// @@@@@@ shader.h\n    void ibuffer_fill( unsigned slot, const warp_inst_t *pi )\n    {\n       m_ibuffer[slot].m_inst=pi;\n       m_ibuffer[slot].m_valid=true;\n    }\n\n    const warp_inst_t *ibuffer_next_inst() { return m_ibuffer[m_next].m_inst; }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\nevery cycle, if current warp is done, it will pick form the m_next_cycle_prioritized_warps to schedule next warp.\nthe instruction is obtained from m_ibuffer.\n\ncode\n\nvoid scheduler_unit::cycle()\n{\n    sched_dprintf( \"scheduler_unit::cycle()\\n\" );\n    bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn't require flush due to control hazard)\n    bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes\n    bool issued_inst = false; // of these we issued one\n\n    order_warps();\n    for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();\n          iter != m_next_cycle_prioritized_warps.end();\n          iter++ ) {\n        // don't consider warps that are not yet valid\n        if ( (*iter) == null || (*iter)->done_exit() ) {\n            continue;\n        }\n        while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() && (checked < max_issue) && (checked <= issued) && (issued < max_issue) ) {\n            const warp_inst_t *pi = warp(warp_id).ibuffer_next_inst();\n            if( pi ) {\n            ...\n              if ( (pi->op == load_op) || (pi->op == store_op) || (pi->op == memory_barrier_op) ) {\n                m_shader->issue_warp(*m_mem_out,pi,active_mask,warp_id);\n                issued++;\n                issued_inst=true;\n                warp_inst_issued = true;\n              } else if ( (pi->op == sfu_op) || (pi->op == alu_sfu_op) ) {\n                m_shader->issue_warp(*m_sfu_out,pi,active_mask,warp_id);\n              }\n            }\n        }\n    }\n  }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n",charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (2)",frontmatter:{title:"Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (2)",date:"2024-08-13T00:00:00.000Z",permalink:"/pages/458726/"},regularPath:"/03.gpu/13.gpgpusim.html",relativePath:"03.gpu/13.gpgpusim.md",key:"v-4349358d",path:"/pages/458726/",headers:[{level:3,title:"How do CUDA instructions get executed ?",slug:"how-do-cuda-instructions-get-executed",normalizedTitle:"how do cuda instructions get executed ?",charIndex:482}],headersStr:"How do CUDA instructions get executed ?",content:' * libcuda\n   * cuda_runtime_api.cc\n * src\n   * abstract_hardware_model.h/cpp\n   * gpgpusim_entrypoint.h/cpp\n   * stream_manager.h/cpp\n   * cuda-sim\n     * cuda-sim.h/cc\n     * memory.h/cc\n     * opcode.h/def\n     * ptx_loader.h/cc\n     * ptx.y\\\n     * ptx_parser.h/cc\n     * ptx_ir.h/cc\n     * ptx_sim.h/cc\n   * gpgpu-sim\n     * gpgpu-sim.h/cc\n     * shader.h/shader.cc\n     * mem_fetch.h/cc\n     * stack.h/cc\n     * addrdec.h/cc\n     * dram.h/cc\n     * traffic_breakdown.h/cc\n\n\n# How do CUDA instructions get executed ?\n\n# Instruction Level\n\nIn opcodes.def, it defines hook for each type of instruction.\n\nIn instruction.cc, it implements the detail of each function. If this is a Load instruction, mem->read() is exectued.\n\nCode\n\n// @@@@@@ opcodes.def\nOP_DEF(LD_OP,ld_impl,"ld",1,5)\nOP_DEF(ST_OP,st_impl,"st",0,5)\n\n// @@@@@@ instructions.cc\nvoid ld_exec( const ptx_instruction *pI, ptx_thread_info *thread ) \n{ \n   const operand_info &dst = pI->dst();\n   const operand_info &src1 = pI->src1();\n   ...\n   mem->read(addr,size/8,&data.s64);\n}\n\nvoid ld_impl( const ptx_instruction *pI, ptx_thread_info *thread ) \n{\n   ld_exec(pI,thread);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# Abstract Level\n\nThe abstract core calls execute_warp_inst_t, which will execute each thread in the warp.\n\nAs to each instruction, it will call a "DEFINE" macro, which will invoke function, described in previous Section.\n\nCode\n\n// @@@@@@ abstract_hardware_model.cc\n\nvoid core_t::execute_warp_inst_t(warp_inst_t &inst, unsigned warpId)\n{\n    for ( unsigned t=0; t < m_warp_size; t++ ) {\n        if( inst.active(t) ) {\n            if(warpId==(unsigned (-1)))\n                warpId = inst.warp_id();\n            unsigned tid=m_warp_size*warpId+t;\n            m_thread[tid]->ptx_exec_inst(inst,t);\n            \n            //virtual function\n            checkExecutionStatusAndUpdate(inst,t,tid);\n        }\n    } \n}\n\n// @@@@@@ cuda-sim.cc\nvoid ptx_thread_info::ptx_exec_inst( warp_inst_t &inst, unsigned lane_id)\n{\n      ...\n      switch ( pI->get_opcode() ) {\n      #define OP_DEF(OP,FUNC,STR,DST,CLASSIFICATION) case OP: FUNC(pI,this); op_classification = CLASSIFICATION; break;\n      ...\n      #include "opcodes.def"\n      #undef OP_DEF\n      default: printf( "Execution error: Invalid opcode (0x%x)\\n", pI->get_opcode() ); break;\n      }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n# Detail GPU Mode\n\nIn detail GPU, programmed in shader.cc, instruction is executed at issue time.\n\nThis is reasonable, as long as the latency and bandwidth is modeled correctly, it is accurate.\n\nCode\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )\n{\n    ...\n    func_exec_inst( **pipe_reg );\n}\n\nvoid shader_core_ctx::func_exec_inst( warp_inst_t &inst )\n{\n    execute_warp_inst_t(inst);\n    // !!!!!! Notice that as to memory access instruction, it will generate memory access\n    if( inst.is_load() || inst.is_store() )\n        inst.generate_mem_accesses();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nThen we can go further, how is issue_warp called?\n\nIn shader.cc, for each cycle, it will check is the status of for each warp.\n\nAs to each warp, it will check whether the first instruction in the instruction buffer, after checking scoreboard, if this is no hazard and issue width is not saturated, the warp could be issued.\n\nCode\n\n// @@@@@@ shader.cc\nvoid scheduler_unit::cycle()\n{\n    SCHED_DPRINTF( "scheduler_unit::cycle()\\n" );\n    bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn\'t require flush due to control hazard)\n    bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes\n    bool issued_inst = false; // of these we issued one\n\n    for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();\n          iter != m_next_cycle_prioritized_warps.end();\n          iter++ ) {\n        // Don\'t consider warps that are not yet valid\n        if ( (*iter) == NULL || (*iter)->done_exit() ) {\n            continue;\n        }\n        while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() ... && (checked <= issued) && (issued < max_issue) ) {\n         const warp_inst_t *pI = warp(warp_id).ibuffer_next_inst();\n         ...\n         if ( (pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP) ) {\n             if( m_mem_out->has_free() ) {\n                 m_shader->issue_warp(*m_mem_out,pI,active_mask,warp_id);\n             }\n         } else {\n             \n         }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n# Register Hazard\n\nIt seems like gpgpu-sim does not use register dependency wakeup, like ROB, which contains a register ID that if dependency instruction has executed, the result transfer through bypassnet work and wakeup pending instructions.\n\nIn gpgpu-sim, in the writeback stage, it will release its target registers in scoreboard.\n\nCode\n\n\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::writeback()\n{\n    warp_inst_t** preg = m_pipeline_reg[EX_WB].get_ready();\n    warp_inst_t* pipe_reg = (preg==NULL)? NULL:*preg;\n    while( preg and !pipe_reg->empty()) {\n        m_scoreboard->releaseRegisters( pipe_reg );\n        warp_inst_complete(*pipe_reg);\n    }\n}\n\n// @@@@@@ scoreboard.cc\n// Release target registers for an instruction\nvoid Scoreboard::releaseRegisters(const class warp_inst_t *inst) \n{\n    for( unsigned r=0; r < 4; r++) {\n        if(inst->out[r] > 0) {\n            releaseRegister(inst->warp_id(), inst->out[r]);\n        }\n    }\n}\n\n// It can be seen that in this function, there is no ready or pending status. Only bookkeeping.\n// Unmark register as write-pending\nvoid Scoreboard::releaseRegister(unsigned wid, unsigned regnum) \n{\n      reg_table[wid].erase(regnum);\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\nSo the scoreboard will maintains all pending registers. Once it is written back, the register will be released.\n\nIf another instruction is about to issue, it will check whether the register it reads or writes matches with any of the registes.\n\nCode\n\n\n\n/** \n * Checks to see if registers used by an instruction are reserved in the scoreboard\n *  \n * @return \n * true if WAW or RAW hazard (no WAR since in-order issue)\n **/ \nbool Scoreboard::checkCollision( unsigned wid, const class inst_t *inst ) const\n{\n\t// Get list of all input and output registers\n\tstd::set<int> inst_regs;\n\n\t// from 0 to 3\n\tif(inst->out[0] > 0) inst_regs.insert(inst->out[0]);\n\n\t// from 0 to 3\n\tif(inst->in[0] > 0) inst_regs.insert(inst->in[0]);\n\n\tif(inst->pred > 0) inst_regs.insert(inst->pred);\n\tif(inst->ar1 > 0) inst_regs.insert(inst->ar1);\n\tif(inst->ar2 > 0) inst_regs.insert(inst->ar2);\n\n\t// Check for collision, get the intersection of reserved registers and instruction registers\n\tstd::set<int>::const_iterator it2;\n\tfor ( it2=inst_regs.begin() ; it2 != inst_regs.end(); it2++ )\n\t\tif(reg_table[wid].find(*it2) != reg_table[wid].end()) {\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n# Function GPU Mode\n\nfunction mode is described in previous blog.',normalizedContent:' * libcuda\n   * cuda_runtime_api.cc\n * src\n   * abstract_hardware_model.h/cpp\n   * gpgpusim_entrypoint.h/cpp\n   * stream_manager.h/cpp\n   * cuda-sim\n     * cuda-sim.h/cc\n     * memory.h/cc\n     * opcode.h/def\n     * ptx_loader.h/cc\n     * ptx.y\\\n     * ptx_parser.h/cc\n     * ptx_ir.h/cc\n     * ptx_sim.h/cc\n   * gpgpu-sim\n     * gpgpu-sim.h/cc\n     * shader.h/shader.cc\n     * mem_fetch.h/cc\n     * stack.h/cc\n     * addrdec.h/cc\n     * dram.h/cc\n     * traffic_breakdown.h/cc\n\n\n# how do cuda instructions get executed ?\n\n# instruction level\n\nin opcodes.def, it defines hook for each type of instruction.\n\nin instruction.cc, it implements the detail of each function. if this is a load instruction, mem->read() is exectued.\n\ncode\n\n// @@@@@@ opcodes.def\nop_def(ld_op,ld_impl,"ld",1,5)\nop_def(st_op,st_impl,"st",0,5)\n\n// @@@@@@ instructions.cc\nvoid ld_exec( const ptx_instruction *pi, ptx_thread_info *thread ) \n{ \n   const operand_info &dst = pi->dst();\n   const operand_info &src1 = pi->src1();\n   ...\n   mem->read(addr,size/8,&data.s64);\n}\n\nvoid ld_impl( const ptx_instruction *pi, ptx_thread_info *thread ) \n{\n   ld_exec(pi,thread);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n# abstract level\n\nthe abstract core calls execute_warp_inst_t, which will execute each thread in the warp.\n\nas to each instruction, it will call a "define" macro, which will invoke function, described in previous section.\n\ncode\n\n// @@@@@@ abstract_hardware_model.cc\n\nvoid core_t::execute_warp_inst_t(warp_inst_t &inst, unsigned warpid)\n{\n    for ( unsigned t=0; t < m_warp_size; t++ ) {\n        if( inst.active(t) ) {\n            if(warpid==(unsigned (-1)))\n                warpid = inst.warp_id();\n            unsigned tid=m_warp_size*warpid+t;\n            m_thread[tid]->ptx_exec_inst(inst,t);\n            \n            //virtual function\n            checkexecutionstatusandupdate(inst,t,tid);\n        }\n    } \n}\n\n// @@@@@@ cuda-sim.cc\nvoid ptx_thread_info::ptx_exec_inst( warp_inst_t &inst, unsigned lane_id)\n{\n      ...\n      switch ( pi->get_opcode() ) {\n      #define op_def(op,func,str,dst,classification) case op: func(pi,this); op_classification = classification; break;\n      ...\n      #include "opcodes.def"\n      #undef op_def\n      default: printf( "execution error: invalid opcode (0x%x)\\n", pi->get_opcode() ); break;\n      }\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n# detail gpu mode\n\nin detail gpu, programmed in shader.cc, instruction is executed at issue time.\n\nthis is reasonable, as long as the latency and bandwidth is modeled correctly, it is accurate.\n\ncode\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )\n{\n    ...\n    func_exec_inst( **pipe_reg );\n}\n\nvoid shader_core_ctx::func_exec_inst( warp_inst_t &inst )\n{\n    execute_warp_inst_t(inst);\n    // !!!!!! notice that as to memory access instruction, it will generate memory access\n    if( inst.is_load() || inst.is_store() )\n        inst.generate_mem_accesses();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\nthen we can go further, how is issue_warp called?\n\nin shader.cc, for each cycle, it will check is the status of for each warp.\n\nas to each warp, it will check whether the first instruction in the instruction buffer, after checking scoreboard, if this is no hazard and issue width is not saturated, the warp could be issued.\n\ncode\n\n// @@@@@@ shader.cc\nvoid scheduler_unit::cycle()\n{\n    sched_dprintf( "scheduler_unit::cycle()\\n" );\n    bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn\'t require flush due to control hazard)\n    bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes\n    bool issued_inst = false; // of these we issued one\n\n    for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();\n          iter != m_next_cycle_prioritized_warps.end();\n          iter++ ) {\n        // don\'t consider warps that are not yet valid\n        if ( (*iter) == null || (*iter)->done_exit() ) {\n            continue;\n        }\n        while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() ... && (checked <= issued) && (issued < max_issue) ) {\n         const warp_inst_t *pi = warp(warp_id).ibuffer_next_inst();\n         ...\n         if ( (pi->op == load_op) || (pi->op == store_op) || (pi->op == memory_barrier_op) ) {\n             if( m_mem_out->has_free() ) {\n                 m_shader->issue_warp(*m_mem_out,pi,active_mask,warp_id);\n             }\n         } else {\n             \n         }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n\n\n# register hazard\n\nit seems like gpgpu-sim does not use register dependency wakeup, like rob, which contains a register id that if dependency instruction has executed, the result transfer through bypassnet work and wakeup pending instructions.\n\nin gpgpu-sim, in the writeback stage, it will release its target registers in scoreboard.\n\ncode\n\n\n\n// @@@@@@ shader.cc\nvoid shader_core_ctx::writeback()\n{\n    warp_inst_t** preg = m_pipeline_reg[ex_wb].get_ready();\n    warp_inst_t* pipe_reg = (preg==null)? null:*preg;\n    while( preg and !pipe_reg->empty()) {\n        m_scoreboard->releaseregisters( pipe_reg );\n        warp_inst_complete(*pipe_reg);\n    }\n}\n\n// @@@@@@ scoreboard.cc\n// release target registers for an instruction\nvoid scoreboard::releaseregisters(const class warp_inst_t *inst) \n{\n    for( unsigned r=0; r < 4; r++) {\n        if(inst->out[r] > 0) {\n            releaseregister(inst->warp_id(), inst->out[r]);\n        }\n    }\n}\n\n// it can be seen that in this function, there is no ready or pending status. only bookkeeping.\n// unmark register as write-pending\nvoid scoreboard::releaseregister(unsigned wid, unsigned regnum) \n{\n      reg_table[wid].erase(regnum);\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\nso the scoreboard will maintains all pending registers. once it is written back, the register will be released.\n\nif another instruction is about to issue, it will check whether the register it reads or writes matches with any of the registes.\n\ncode\n\n\n\n/** \n * checks to see if registers used by an instruction are reserved in the scoreboard\n *  \n * @return \n * true if waw or raw hazard (no war since in-order issue)\n **/ \nbool scoreboard::checkcollision( unsigned wid, const class inst_t *inst ) const\n{\n\t// get list of all input and output registers\n\tstd::set<int> inst_regs;\n\n\t// from 0 to 3\n\tif(inst->out[0] > 0) inst_regs.insert(inst->out[0]);\n\n\t// from 0 to 3\n\tif(inst->in[0] > 0) inst_regs.insert(inst->in[0]);\n\n\tif(inst->pred > 0) inst_regs.insert(inst->pred);\n\tif(inst->ar1 > 0) inst_regs.insert(inst->ar1);\n\tif(inst->ar2 > 0) inst_regs.insert(inst->ar2);\n\n\t// check for collision, get the intersection of reserved registers and instruction registers\n\tstd::set<int>::const_iterator it2;\n\tfor ( it2=inst_regs.begin() ; it2 != inst_regs.end(); it2++ )\n\t\tif(reg_table[wid].find(*it2) != reg_table[wid].end()) {\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n\n\n# function gpu mode\n\nfunction mode is described in previous blog.',charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"TO READ",frontmatter:{title:"TO READ",date:"2023-11-21T00:00:00.000Z",permalink:"/pages/47871e/"},regularPath:"/03.gpu/1234.TODO.html",relativePath:"03.gpu/1234.TODO.md",key:"v-02aef12e",path:"/pages/47871e/",headersStr:null,content:" 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity",normalizedContent:" 1. a survey of architectural approaches for improving gpgpu performance, programmability and heterogeneity",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (3)",frontmatter:{title:"Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (3)",date:"2024-08-13T00:00:00.000Z",permalink:"/pages/458727/"},regularPath:"/03.gpu/14.gpgpusim.html",relativePath:"03.gpu/14.gpgpusim.md",key:"v-9e671962",path:"/pages/458727/",headers:[{level:3,title:"How is the simulation started?",slug:"how-is-the-simulation-started",normalizedTitle:"how is the simulation started?",charIndex:2}],headersStr:"How is the simulation started?",content:'# How is the simulation started?\n\n# From Binary File\n\nCode\n\n   0x000000000040171f <+13>:    callq  0x400ad0 <__cudaRegisterFatBinary@plt>\n=> 0x0000000000401724 <+18>:    mov    %rax,0x2029f5(%rip)        # 0x604120 <_ZL20__cudaFatCubinHandle>\n\n\n1\n2\n\nCode\n\n// @@@@@@ cuda_runtime_api.cc\nvoid** CUDARTAPI __cudaRegisterFatBinary( void *fatCubin ) {\n\tCUctx_st *context = GPGPUSim_Context();\n}\n\nstatic CUctx_st* GPGPUSim_Context()\n{\n\tstatic CUctx_st *the_context = NULL;\n\tif( the_context == NULL ) {\n\t\t_cuda_device_id *the_gpu = GPGPUSim_Init();\n\t\tthe_context = new CUctx_st(the_gpu);\n\t}\n\treturn the_context;\n}\n\nclass _cuda_device_id *GPGPUSim_Init()\n{\n\tstatic _cuda_device_id *the_device = NULL;\n\tif( !the_device ) {\n\t\tgpgpu_sim *the_gpu = gpgpu_ptx_sim_init_perf();\n\n\t\tcudaDeviceProp *prop = (cudaDeviceProp *) calloc(sizeof(cudaDeviceProp),1);\n\t\t...\n\t\tthe_gpu->set_prop(prop);\n\t\tthe_device = new _cuda_device_id(the_gpu);\n\t}\n\tstart_sim_thread(1);\n\treturn the_device;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\nGPGPUSim_Init() in cuda_runtime will call function from gpgpusim_entrypoint.cc.\n\nIt conductthe initialization and create gpgpu_sim and also stream_manager\n\n * gpu config\n * opcode latency config\n\nThis is the creation time of gpu simulation.\n\nNotice that function start_sim_thread. It starts the simulation thread.\n\nCode\n\n// @@@@@@ gpgpusim_entrypoint.cc gpgpu_ptx_sim_init_perf\ngpgpu_sim *gpgpu_ptx_sim_init_perf()\n{\n\n   read_sim_environment_variables();\n   read_parser_environment_variables();\n   option_parser_t opp = option_parser_create();\n\n   icnt_reg_options(opp);\n   g_the_gpu_config.reg_options(opp); // register GPU microrachitecture options\n   ptx_reg_options(opp);\n   ptx_opcocde_latency_options(opp);\n   option_parser_cmdline(opp, sg_argc, sg_argv); // parse configuration options\n   g_the_gpu_config.convert_byte_string();\n   fprintf(stdout, "GPGPU-Sim: Configuration options:\\n\\n");\n   option_parser_print(opp, stdout);\n\n   g_the_gpu_config.init();\n   g_the_gpu = new gpgpu_sim(g_the_gpu_config);\n   g_stream_manager = new stream_manager(g_the_gpu,g_cuda_launch_blocking);\n\n   g_simulation_starttime = time((time_t *)NULL);\n\n   sem_init(&g_sim_signal_start,0,0);\n   sem_init(&g_sim_signal_finish,0,0);\n   sem_init(&g_sim_signal_exit,0,0);\n\n   return g_the_gpu;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\ncuda_runtime_api.cc\n\nCode\n\n// @@@@@@ gpgpusim_entrypoint.cc\nvoid start_sim_thread(int api)\n{\n    if( g_sim_done ) {\n        g_sim_done = false;\n\t...\n\tpthread_create(&g_simulation_thread,NULL,gpgpu_sim_thread_concurrent,NULL);\n\n    }\n}\n\n\nvoid *gpgpu_sim_thread_concurrent(void*)\n{\n    // concurrent kernel execution simulation thread\n    do {\n\t...\n        while( g_stream_manager->empty() && !g_sim_done )\n            ;\n\t...\n        pthread_mutex_lock(&g_sim_lock);\n        g_sim_active = true;\n        pthread_mutex_unlock(&g_sim_lock);\n        bool active = false;\n        bool sim_cycles = false;\n        g_the_gpu->init();\n        do {\n            // check if a kernel has completed\n            // launch operation on device if one is pending and can be run\n\n            // Need to break this loop when a kernel completes. This was a\n            // source of non-deterministic behaviour in GPGPU-Sim (bug 147).\n            // If another stream operation is available, g_the_gpu remains active,\n            // causing this loop to not break. If the next operation happens to be\n            // another kernel, the gpu is not re-initialized and the inter-kernel\n            // behaviour may be incorrect. Check that a kernel has finished and\n            // no other kernel is currently running.\n            // !!!!!! This will check whether the operation is done and whether gpu is active\n            if(g_stream_manager->operation(&sim_cycles) && !g_the_gpu->active())\n                break;\n\n            //functional simulation\n            if( g_the_gpu->is_functional_sim()) {\n                kernel_info_t * kernel = g_the_gpu->get_functional_kernel();\n                gpgpu_cuda_ptx_sim_main_func(*kernel);\n                g_the_gpu->finish_functional_sim(kernel);\n            }\n\n            // !!!!!! This is the most essential part of event-driven function\n            // The gpu event is cycled().\n            //performance simulation\n            if( g_the_gpu->active() ) {\n                g_the_gpu->cycle();\n                sim_cycles = true;\n                g_the_gpu->deadlock_check();\n            }else {\n\t\tg_the_gpu->cycle();\n                if(g_the_gpu->cycle_insn_cta_max_hit()){\n                    g_stream_manager->stop_all_running_kernels();\n                    g_sim_done = true;\n                    break_limit = true;\n                }\n            }\n\n            active=g_the_gpu->active() || !g_stream_manager->empty_protected();\n\n        } while( active && !g_sim_done);\n\t...\n        pthread_mutex_lock(&g_sim_lock);\n        g_sim_active = false;\n        pthread_mutex_unlock(&g_sim_lock);\n    } while( !g_sim_done );\n    printf("GPGPU-Sim: *** simulation thread exiting ***\\n");\n    fflush(stdout);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n\n\nThe stream_manager operation function used above is not for drive each cycle of the simulation.\nInstead, it will call stream_operation do_operation to launch kernel on GPU.\nBased on different type of operation, if just copy from * to * it will copy, if it is kernel launch.\nIt will launch according to function mode or detail mode.\nBut, as to launch, it just put kenerl onto gpu-sim m_running_kernels queue.\nIt does not run it.\n\ng_the_gpu->cycle() drives the gpu simulation.\n\nCode\n\n// @@@@@@ stream_manger.cc\nbool stream_manager::operation( bool * sim)\n{\n    bool check=check_finished_kernel();\n    ...\n    stream_operation op =front();\n    if(!op.do_operation( m_gpu )) //not ready to execute\n    {\n       ...\n    }\n    ...\n    return check;\n}\n\nbool stream_operation::do_operation( gpgpu_sim *gpu )\n{\n    case stream_prefetch_host_to_device:\n    ...\n    case stream_memcpy_device_to_device:\n    ...\n    case stream_kernel_launch:\n        if( m_sim_mode ) { //Functional Sim\n            gpu->set_cache_config(m_kernel->name());\n            gpu->functional_launch( m_kernel );\n        }\n        else { //Performance Sim\n            if( gpu->can_start_kernel() && m_kernel->m_launch_latency == 0) {\n                gpu->set_cache_config(m_kernel->name());\n                gpu->launch( m_kernel );\n\t        gpu->getGmmu()->log_kernel_info(m_kernel->get_uid(), gpu_sim_cycle + gpu_tot_sim_cycle, false);\n                if(sim_prof_enable) {\n\t           kernel_stats* k_s = new kernel_stats(cur_cycle, m_stream->get_uid(), m_kernel->get_uid());\n\t           sim_prof[cur_cycle].push_back(k_s);\n\t        }\t\t\t\n            }\n            else {\n                return false;    \n            }\n        }\n        break;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\nCode\n\n// @@@@@@ gpu-sim.h/cc\nvoid gpgpu_sim::launch( kernel_info_t *kinfo )\n{\n   unsigned cta_size = kinfo->threads_per_cta();\n   if ( cta_size > m_shader_config->n_thread_per_shader ) {\n      abort();\n   }\n\n   unsigned n=0;\n   for(n=0; n < m_running_kernels.size(); n++ ) {\n\t// If previous kernel is already done or not empty yet, replace\n\t// !! There might be bug that if earlist kenerl is done, it will insert kernel to the earilist position\n\t// !! which might break first-in-first-out priority\n       if( (NULL==m_running_kernels[n]) || m_running_kernels[n]->done() ) {\n           m_running_kernels[n] = kinfo;\n           break;\n       }\n   }\n}\n\nvoid functional_launch(kernel_info_t * k) {\n     m_functional_sim = true;\n     m_functional_sim_kernel = k;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n',normalizedContent:'# how is the simulation started?\n\n# from binary file\n\ncode\n\n   0x000000000040171f <+13>:    callq  0x400ad0 <__cudaregisterfatbinary@plt>\n=> 0x0000000000401724 <+18>:    mov    %rax,0x2029f5(%rip)        # 0x604120 <_zl20__cudafatcubinhandle>\n\n\n1\n2\n\ncode\n\n// @@@@@@ cuda_runtime_api.cc\nvoid** cudartapi __cudaregisterfatbinary( void *fatcubin ) {\n\tcuctx_st *context = gpgpusim_context();\n}\n\nstatic cuctx_st* gpgpusim_context()\n{\n\tstatic cuctx_st *the_context = null;\n\tif( the_context == null ) {\n\t\t_cuda_device_id *the_gpu = gpgpusim_init();\n\t\tthe_context = new cuctx_st(the_gpu);\n\t}\n\treturn the_context;\n}\n\nclass _cuda_device_id *gpgpusim_init()\n{\n\tstatic _cuda_device_id *the_device = null;\n\tif( !the_device ) {\n\t\tgpgpu_sim *the_gpu = gpgpu_ptx_sim_init_perf();\n\n\t\tcudadeviceprop *prop = (cudadeviceprop *) calloc(sizeof(cudadeviceprop),1);\n\t\t...\n\t\tthe_gpu->set_prop(prop);\n\t\tthe_device = new _cuda_device_id(the_gpu);\n\t}\n\tstart_sim_thread(1);\n\treturn the_device;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\ngpgpusim_init() in cuda_runtime will call function from gpgpusim_entrypoint.cc.\n\nit conductthe initialization and create gpgpu_sim and also stream_manager\n\n * gpu config\n * opcode latency config\n\nthis is the creation time of gpu simulation.\n\nnotice that function start_sim_thread. it starts the simulation thread.\n\ncode\n\n// @@@@@@ gpgpusim_entrypoint.cc gpgpu_ptx_sim_init_perf\ngpgpu_sim *gpgpu_ptx_sim_init_perf()\n{\n\n   read_sim_environment_variables();\n   read_parser_environment_variables();\n   option_parser_t opp = option_parser_create();\n\n   icnt_reg_options(opp);\n   g_the_gpu_config.reg_options(opp); // register gpu microrachitecture options\n   ptx_reg_options(opp);\n   ptx_opcocde_latency_options(opp);\n   option_parser_cmdline(opp, sg_argc, sg_argv); // parse configuration options\n   g_the_gpu_config.convert_byte_string();\n   fprintf(stdout, "gpgpu-sim: configuration options:\\n\\n");\n   option_parser_print(opp, stdout);\n\n   g_the_gpu_config.init();\n   g_the_gpu = new gpgpu_sim(g_the_gpu_config);\n   g_stream_manager = new stream_manager(g_the_gpu,g_cuda_launch_blocking);\n\n   g_simulation_starttime = time((time_t *)null);\n\n   sem_init(&g_sim_signal_start,0,0);\n   sem_init(&g_sim_signal_finish,0,0);\n   sem_init(&g_sim_signal_exit,0,0);\n\n   return g_the_gpu;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\ncuda_runtime_api.cc\n\ncode\n\n// @@@@@@ gpgpusim_entrypoint.cc\nvoid start_sim_thread(int api)\n{\n    if( g_sim_done ) {\n        g_sim_done = false;\n\t...\n\tpthread_create(&g_simulation_thread,null,gpgpu_sim_thread_concurrent,null);\n\n    }\n}\n\n\nvoid *gpgpu_sim_thread_concurrent(void*)\n{\n    // concurrent kernel execution simulation thread\n    do {\n\t...\n        while( g_stream_manager->empty() && !g_sim_done )\n            ;\n\t...\n        pthread_mutex_lock(&g_sim_lock);\n        g_sim_active = true;\n        pthread_mutex_unlock(&g_sim_lock);\n        bool active = false;\n        bool sim_cycles = false;\n        g_the_gpu->init();\n        do {\n            // check if a kernel has completed\n            // launch operation on device if one is pending and can be run\n\n            // need to break this loop when a kernel completes. this was a\n            // source of non-deterministic behaviour in gpgpu-sim (bug 147).\n            // if another stream operation is available, g_the_gpu remains active,\n            // causing this loop to not break. if the next operation happens to be\n            // another kernel, the gpu is not re-initialized and the inter-kernel\n            // behaviour may be incorrect. check that a kernel has finished and\n            // no other kernel is currently running.\n            // !!!!!! this will check whether the operation is done and whether gpu is active\n            if(g_stream_manager->operation(&sim_cycles) && !g_the_gpu->active())\n                break;\n\n            //functional simulation\n            if( g_the_gpu->is_functional_sim()) {\n                kernel_info_t * kernel = g_the_gpu->get_functional_kernel();\n                gpgpu_cuda_ptx_sim_main_func(*kernel);\n                g_the_gpu->finish_functional_sim(kernel);\n            }\n\n            // !!!!!! this is the most essential part of event-driven function\n            // the gpu event is cycled().\n            //performance simulation\n            if( g_the_gpu->active() ) {\n                g_the_gpu->cycle();\n                sim_cycles = true;\n                g_the_gpu->deadlock_check();\n            }else {\n\t\tg_the_gpu->cycle();\n                if(g_the_gpu->cycle_insn_cta_max_hit()){\n                    g_stream_manager->stop_all_running_kernels();\n                    g_sim_done = true;\n                    break_limit = true;\n                }\n            }\n\n            active=g_the_gpu->active() || !g_stream_manager->empty_protected();\n\n        } while( active && !g_sim_done);\n\t...\n        pthread_mutex_lock(&g_sim_lock);\n        g_sim_active = false;\n        pthread_mutex_unlock(&g_sim_lock);\n    } while( !g_sim_done );\n    printf("gpgpu-sim: *** simulation thread exiting ***\\n");\n    fflush(stdout);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n\n\nthe stream_manager operation function used above is not for drive each cycle of the simulation.\ninstead, it will call stream_operation do_operation to launch kernel on gpu.\nbased on different type of operation, if just copy from * to * it will copy, if it is kernel launch.\nit will launch according to function mode or detail mode.\nbut, as to launch, it just put kenerl onto gpu-sim m_running_kernels queue.\nit does not run it.\n\ng_the_gpu->cycle() drives the gpu simulation.\n\ncode\n\n// @@@@@@ stream_manger.cc\nbool stream_manager::operation( bool * sim)\n{\n    bool check=check_finished_kernel();\n    ...\n    stream_operation op =front();\n    if(!op.do_operation( m_gpu )) //not ready to execute\n    {\n       ...\n    }\n    ...\n    return check;\n}\n\nbool stream_operation::do_operation( gpgpu_sim *gpu )\n{\n    case stream_prefetch_host_to_device:\n    ...\n    case stream_memcpy_device_to_device:\n    ...\n    case stream_kernel_launch:\n        if( m_sim_mode ) { //functional sim\n            gpu->set_cache_config(m_kernel->name());\n            gpu->functional_launch( m_kernel );\n        }\n        else { //performance sim\n            if( gpu->can_start_kernel() && m_kernel->m_launch_latency == 0) {\n                gpu->set_cache_config(m_kernel->name());\n                gpu->launch( m_kernel );\n\t        gpu->getgmmu()->log_kernel_info(m_kernel->get_uid(), gpu_sim_cycle + gpu_tot_sim_cycle, false);\n                if(sim_prof_enable) {\n\t           kernel_stats* k_s = new kernel_stats(cur_cycle, m_stream->get_uid(), m_kernel->get_uid());\n\t           sim_prof[cur_cycle].push_back(k_s);\n\t        }\t\t\t\n            }\n            else {\n                return false;    \n            }\n        }\n        break;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\ncode\n\n// @@@@@@ gpu-sim.h/cc\nvoid gpgpu_sim::launch( kernel_info_t *kinfo )\n{\n   unsigned cta_size = kinfo->threads_per_cta();\n   if ( cta_size > m_shader_config->n_thread_per_shader ) {\n      abort();\n   }\n\n   unsigned n=0;\n   for(n=0; n < m_running_kernels.size(); n++ ) {\n\t// if previous kernel is already done or not empty yet, replace\n\t// !! there might be bug that if earlist kenerl is done, it will insert kernel to the earilist position\n\t// !! which might break first-in-first-out priority\n       if( (null==m_running_kernels[n]) || m_running_kernels[n]->done() ) {\n           m_running_kernels[n] = kinfo;\n           break;\n       }\n   }\n}\n\nvoid functional_launch(kernel_info_t * k) {\n     m_functional_sim = true;\n     m_functional_sim_kernel = k;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n',charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (4)",frontmatter:{title:"Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (4)",date:"2024-08-14T00:00:00.000Z",permalink:"/pages/45872/"},regularPath:"/03.gpu/15.gpgpusim.html",relativePath:"03.gpu/15.gpgpusim.md",key:"v-1e4fb111",path:"/pages/45872/",headers:[{level:3,title:"What is the microarchitecture of GPGPU-SIM?",slug:"what-is-the-microarchitecture-of-gpgpu-sim",normalizedTitle:"what is the microarchitecture of gpgpu-sim?",charIndex:2},{level:3,title:"configuration file",slug:"configuration-file",normalizedTitle:"configuration file",charIndex:5670}],headersStr:"What is the microarchitecture of GPGPU-SIM? configuration file",content:"# What is the microarchitecture of GPGPU-SIM?\n\n\n\n# shader_core_ctx\n\nThis should be the streaming multiprocessors that exeutes at warp level and shares the same L1 cache.\n\nCode\n\n// @@@@@@ shader.cc\nshader_core_ctx::shader_core_ctx( class gpgpu_sim *gpu, \n                                  class simt_core_cluster *cluster,\n                                  unsigned shader_id,\n                                  unsigned tpc_id,\n                                  const struct shader_core_config *config,\n                                  const struct memory_config *mem_config,\n                                  shader_core_stats *stats,\n\t\t\t\t  class gpgpu_new_stats *new_stats )\n   : core_t( gpu, NULL, config->warp_size, config->n_thread_per_shader ),\n     m_barriers( this, config->max_warps_per_shader, config->max_cta_per_core, config->max_barriers_per_cta, config->warp_size ),\n     m_dynamic_warp_id(0)\n{\n    ...\n    m_L1I = new read_only_cache( name,m_config->m_L1I_config,m_sid,get_shader_instruction_cache_id(),m_icnt,IN_L1I_MISS_QUEUE);\n    ...\n    m_warp.resize(m_config->max_warps_per_shader, shd_warp_t(this, warp_size));\n    m_scoreboard = new Scoreboard(m_sid, m_config->max_warps_per_shader);\n}\n\n// different stages:\n{\n    void decode();\n    \n    void issue();\n    friend class scheduler_unit; //this is needed to use private issue warp.\n    friend class TwoLevelScheduler;\n    friend class LooseRoundRobbinScheduler;\n    void issue_warp( register_set& warp, const warp_inst_t *pI, const active_mask_t &active_mask, unsigned warp_id );\n    void func_exec_inst( warp_inst_t &inst );\n\n     // Returns numbers of addresses in translated_addrs\n    unsigned translate_local_memaddr( address_type localaddr, unsigned tid, unsigned num_shader, unsigned datasize, new_addr_type* translated_addrs );\n\n    void read_operands();\n    \n    void execute();\n    \n    void writeback();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n# simt_core_cluster\n\nThis should be at GPU level.\n\nCode\n\n// @@@@@@ shader.cc\nclass simt_core_cluster {\npublic:\n    simt_core_cluster( class gpgpu_sim *gpu, \n                       unsigned cluster_id, \n                       const struct shader_core_config *config, \n                       const struct memory_config *mem_config,\n                       shader_core_stats *stats,\n                       memory_stats_t *mstats,\n\t\t       class gpgpu_new_stats *new_stats ) {\n\n  }\n}\n\nsimt_core_cluster::simt_core_cluster( class gpgpu_sim *gpu, \n                                      unsigned cluster_id, \n                                      const struct shader_core_config *config, \n                                      const struct memory_config *mem_config,\n                                      shader_core_stats *stats, \n                                      class memory_stats_t *mstats,\n\t\t\t\t      class gpgpu_new_stats *new_stats )\n{\n    m_config = config;\n    m_cta_issue_next_core=m_config->n_simt_cores_per_cluster-1; // this causes first launch to use hw cta 0\n    m_cluster_id=cluster_id;\n    m_gpu = gpu;\n    m_stats = stats;\n    m_memory_stats = mstats;\n    \n    m_new_stats = new_stats;\n\n    m_core = new shader_core_ctx*[ config->n_simt_cores_per_cluster ];\n    for( unsigned i=0; i < config->n_simt_cores_per_cluster; i++ ) {\n        unsigned sid = m_config->cid_to_sid(i,m_cluster_id);\n        m_core[i] = new shader_core_ctx(gpu,this,sid,m_cluster_id,config,mem_config,stats, new_stats);\n        m_core_sim_order.push_back(i); \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n# how thread block is issued to stream multiprocessor?\n\ngpgpu-sim cycle() function\n\nCode\n\n// @@@@@@ gpgpu_sim\nvoid gpgpu_sim::cycle()\n{\n    ...\n    m_cluster[i]->icnt_cycle();\n    ...\n    m_memory_partition_unit[i]->dram_cycle();\n    ...\n    m_memory_sub_partition[i]->cache_cycle(gpu_sim_cycle+gpu_tot_sim_cycle);\n    ...\n    icnt_transfer();\n    ...\n    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {\n        if (m_cluster[i]->get_not_completed() || get_more_cta_left() ) {\n            m_cluster[i]->core_cycle();\n            *active_sms+=m_cluster[i]->get_n_active_sms();\n         }\n    }\n    issue_block2core();\n}\n\nvoid gpgpu_sim::issue_block2core()\n{\n    unsigned last_issued = m_last_cluster_issue; \n    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {\n        unsigned idx = (i + last_issued + 1) % m_shader_config->n_simt_clusters;\n        unsigned num = m_cluster[idx]->issue_block2core();\n        if( num ) {\n            m_last_cluster_issue=idx;\n            m_total_cta_launched += num;\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\nsimt_core_cluster issue block to stream multiprocessr.\n\nCode\n\n// @@@@@@ shader.cc\nunsigned simt_core_cluster::issue_block2core()\n{\n    unsigned num_blocks_issued=0;\n    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {\n        unsigned core = (i+m_cta_issue_next_core+1)%m_config->n_simt_cores_per_cluster;\n\n        kernel_info_t * kernel;\n\n        if(m_config->gpgpu_concurrent_kernel_sm) {//concurrent kernel on sm \n            //always select latest issued kernel\n            kernel_info_t *k = m_gpu->select_kernel();\n            kernel = k;\n        }\n\t...\n\n        if( m_gpu->kernel_more_cta_left(kernel) && \n            m_core[core]->can_issue_1block(*kernel)) {\n            m_core[core]->issue_block2core(*kernel);\n            ...\n        }\n    }\n    return num_blocks_issued;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# configuration file\n\n./config/GeForceGTX1080Ti/gpgpusim.config\n\ngpgpu_n_clusters means the number of stream multiprocessing cores\n\nCode\n\n# high level architecture configuration\n-gpgpu_n_clusters 28\n\n\n\n1\n2\n3\n\n\nHow does gpgpu-sim configure the number of cores inside the SM? I didn't find any configuration for that.",normalizedContent:"# what is the microarchitecture of gpgpu-sim?\n\n\n\n# shader_core_ctx\n\nthis should be the streaming multiprocessors that exeutes at warp level and shares the same l1 cache.\n\ncode\n\n// @@@@@@ shader.cc\nshader_core_ctx::shader_core_ctx( class gpgpu_sim *gpu, \n                                  class simt_core_cluster *cluster,\n                                  unsigned shader_id,\n                                  unsigned tpc_id,\n                                  const struct shader_core_config *config,\n                                  const struct memory_config *mem_config,\n                                  shader_core_stats *stats,\n\t\t\t\t  class gpgpu_new_stats *new_stats )\n   : core_t( gpu, null, config->warp_size, config->n_thread_per_shader ),\n     m_barriers( this, config->max_warps_per_shader, config->max_cta_per_core, config->max_barriers_per_cta, config->warp_size ),\n     m_dynamic_warp_id(0)\n{\n    ...\n    m_l1i = new read_only_cache( name,m_config->m_l1i_config,m_sid,get_shader_instruction_cache_id(),m_icnt,in_l1i_miss_queue);\n    ...\n    m_warp.resize(m_config->max_warps_per_shader, shd_warp_t(this, warp_size));\n    m_scoreboard = new scoreboard(m_sid, m_config->max_warps_per_shader);\n}\n\n// different stages:\n{\n    void decode();\n    \n    void issue();\n    friend class scheduler_unit; //this is needed to use private issue warp.\n    friend class twolevelscheduler;\n    friend class looseroundrobbinscheduler;\n    void issue_warp( register_set& warp, const warp_inst_t *pi, const active_mask_t &active_mask, unsigned warp_id );\n    void func_exec_inst( warp_inst_t &inst );\n\n     // returns numbers of addresses in translated_addrs\n    unsigned translate_local_memaddr( address_type localaddr, unsigned tid, unsigned num_shader, unsigned datasize, new_addr_type* translated_addrs );\n\n    void read_operands();\n    \n    void execute();\n    \n    void writeback();\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\n# simt_core_cluster\n\nthis should be at gpu level.\n\ncode\n\n// @@@@@@ shader.cc\nclass simt_core_cluster {\npublic:\n    simt_core_cluster( class gpgpu_sim *gpu, \n                       unsigned cluster_id, \n                       const struct shader_core_config *config, \n                       const struct memory_config *mem_config,\n                       shader_core_stats *stats,\n                       memory_stats_t *mstats,\n\t\t       class gpgpu_new_stats *new_stats ) {\n\n  }\n}\n\nsimt_core_cluster::simt_core_cluster( class gpgpu_sim *gpu, \n                                      unsigned cluster_id, \n                                      const struct shader_core_config *config, \n                                      const struct memory_config *mem_config,\n                                      shader_core_stats *stats, \n                                      class memory_stats_t *mstats,\n\t\t\t\t      class gpgpu_new_stats *new_stats )\n{\n    m_config = config;\n    m_cta_issue_next_core=m_config->n_simt_cores_per_cluster-1; // this causes first launch to use hw cta 0\n    m_cluster_id=cluster_id;\n    m_gpu = gpu;\n    m_stats = stats;\n    m_memory_stats = mstats;\n    \n    m_new_stats = new_stats;\n\n    m_core = new shader_core_ctx*[ config->n_simt_cores_per_cluster ];\n    for( unsigned i=0; i < config->n_simt_cores_per_cluster; i++ ) {\n        unsigned sid = m_config->cid_to_sid(i,m_cluster_id);\n        m_core[i] = new shader_core_ctx(gpu,this,sid,m_cluster_id,config,mem_config,stats, new_stats);\n        m_core_sim_order.push_back(i); \n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n\n\n# how thread block is issued to stream multiprocessor?\n\ngpgpu-sim cycle() function\n\ncode\n\n// @@@@@@ gpgpu_sim\nvoid gpgpu_sim::cycle()\n{\n    ...\n    m_cluster[i]->icnt_cycle();\n    ...\n    m_memory_partition_unit[i]->dram_cycle();\n    ...\n    m_memory_sub_partition[i]->cache_cycle(gpu_sim_cycle+gpu_tot_sim_cycle);\n    ...\n    icnt_transfer();\n    ...\n    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {\n        if (m_cluster[i]->get_not_completed() || get_more_cta_left() ) {\n            m_cluster[i]->core_cycle();\n            *active_sms+=m_cluster[i]->get_n_active_sms();\n         }\n    }\n    issue_block2core();\n}\n\nvoid gpgpu_sim::issue_block2core()\n{\n    unsigned last_issued = m_last_cluster_issue; \n    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {\n        unsigned idx = (i + last_issued + 1) % m_shader_config->n_simt_clusters;\n        unsigned num = m_cluster[idx]->issue_block2core();\n        if( num ) {\n            m_last_cluster_issue=idx;\n            m_total_cta_launched += num;\n        }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\nsimt_core_cluster issue block to stream multiprocessr.\n\ncode\n\n// @@@@@@ shader.cc\nunsigned simt_core_cluster::issue_block2core()\n{\n    unsigned num_blocks_issued=0;\n    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {\n        unsigned core = (i+m_cta_issue_next_core+1)%m_config->n_simt_cores_per_cluster;\n\n        kernel_info_t * kernel;\n\n        if(m_config->gpgpu_concurrent_kernel_sm) {//concurrent kernel on sm \n            //always select latest issued kernel\n            kernel_info_t *k = m_gpu->select_kernel();\n            kernel = k;\n        }\n\t...\n\n        if( m_gpu->kernel_more_cta_left(kernel) && \n            m_core[core]->can_issue_1block(*kernel)) {\n            m_core[core]->issue_block2core(*kernel);\n            ...\n        }\n    }\n    return num_blocks_issued;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n\n\n\n# configuration file\n\n./config/geforcegtx1080ti/gpgpusim.config\n\ngpgpu_n_clusters means the number of stream multiprocessing cores\n\ncode\n\n# high level architecture configuration\n-gpgpu_n_clusters 28\n\n\n\n1\n2\n3\n\n\nhow does gpgpu-sim configure the number of cores inside the sm? i didn't find any configuration for that.",charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (5)",frontmatter:{title:"Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (5)",date:"2024-08-15T00:00:00.000Z",permalink:"/pages/45874/"},regularPath:"/03.gpu/16.gpgpusim.html",relativePath:"03.gpu/16.gpgpusim.md",key:"v-e85a225a",path:"/pages/45874/",headers:[{level:3,title:"GPGPU-sim Memory Interface",slug:"gpgpu-sim-memory-interface",normalizedTitle:"gpgpu-sim memory interface",charIndex:2}],headersStr:"GPGPU-sim Memory Interface",content:"# GPGPU-sim Memory Interface\n\n# Response Phase from Interconnect to SM\n\nsimt_core_cluster is at GPU level, it can fetch response into m_response_fifo.\n\nicnt_pop, if return non null ptr, it is a memory fetch repsonse, push it into m_response_fifo.\n\nCode\n\n// @@@@@@ shader.cc\n\nvoid simt_core_cluster::icnt_cycle()\n{\n    // pop from upward queue (GMMU to CU) of cluster and push it to the one in core (SM/CU)\n    if ( !m_gmmu_cu_queue.empty() ) {\n      mem_fetch *mf = m_gmmu_cu_queue.front();\n      ...\n      m_core[cid]->accept_access_response(mf);\n    } \n    \n    // pop it from the downward queue (CU to GMMU) of the core (SM/CU) and push it to the one in cluster (TPC)\n    for (unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++) {\n       if (!m_core[i]->empty_cu_gmmu_queue()){\n          mem_fetch *mf = m_core[i]->front_cu_gmmu_queue();\n          ...\n          m_cu_gmmu_queue.push_front(mf);\n       }\n    }\n\n    // Forward response from GPU response fifo into shader core (SM) response fifo\n    if( !m_response_fifo.empty() ) {\n        mem_fetch *mf = m_response_fifo.front();\n        unsigned cid = m_config->sid_to_cid(mf->get_sid());\n        ...\n            // data response\n            if( !m_core[cid]->ldst_unit_response_buffer_full() ) {\n                m_response_fifo.pop_front();\n                // GPU ---\x3e SM\n                m_core[cid]->accept_ldst_unit_response(mf);\n            }\n        }\n    }\n\n    // Accept Response from Interconnect Network\n    if( m_response_fifo.size() < m_config->n_simt_ejection_buffer_size ) {\n        mem_fetch *mf = (mem_fetch*) ::icnt_pop(m_cluster_id);\n        if (!mf) \n            return;\n\n        // The packet size varies depending on the type of request: \n        // - For read request and atomic request, the packet contains the data \n        // - For write-ack, the packet only has control metadata\n        ...\n        m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size); \n        mf->set_status(IN_CLUSTER_TO_SHADER_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);\n        ...\n        // Interconnect to GPU\n        m_response_fifo.push_back(mf);\n        m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);\n    } \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\nThe flow below is from SM to ldst unit and then to L1 cache.\n\nThe L1 cache is included in ldst unit.\n\nCode\n\n\nvoid shader_core_ctx::accept_ldst_unit_response(mem_fetch * mf) \n{\n   m_ldst_unit->fill(mf);\n}\n\nvoid ldst_unit::fill( mem_fetch *mf )\n{\n    mf->set_status(IN_SHADER_LDST_RESPONSE_FIFO,gpu_sim_cycle+gpu_tot_sim_cycle);\n    m_response_fifo.push_back(mf);\n}\n\nvoid ldst_unit::cycle()\n{\n   writeback();\n   ...\n   if( !m_response_fifo.empty() ) {\n       mem_fetch *mf = m_response_fifo.front();\n       ...\n    \t   if( mf->get_type() == WRITE_ACK || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {\n               m_core->store_ack(mf);\n               m_response_fifo.pop_front();\n\n               if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) {\n                    m_gpu->getGmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());\n               }\n               ...\n           } else {\n              ...\n              if (m_L1D->fill_port_free()) {\n                   m_L1D->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);\n                   m_response_fifo.pop_front();\n              }\n           }\n       }\n   }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\nAt this time the response is fed into L1 Cache, which is shared in SM.\n\nBut how is it responsed to core?\n\n# Response Phase From L1 Cache to Wrap execution\n\nCode\n\n/// @@@@@@ gpu-cache.cc\n/// Interface for response from lower memory level (model bandwidth restictions in caller)\nvoid baseline_cache::fill(mem_fetch *mf, unsigned time){\n    ...\n    if ( m_config.m_alloc_policy == ON_MISS )\n        m_tag_array->fill(e->second.m_cache_index,time);\n    else if ( m_config.m_alloc_policy == ON_FILL )\n        m_tag_array->fill(e->second.m_block_addr,time);\n\n    m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);\n    ...\n}\n\n/// Accept a new cache fill response: mark entry ready for processing\nvoid mshr_table::mark_ready( new_addr_type block_addr, bool &has_atomic ){\n    ...\n    m_current_response.push_back( block_addr );\n    ...\n}\n\n/// Returns next ready access\nmem_fetch *mshr_table::next_access(){\n    ...\n    new_addr_type block_addr = m_current_response.front();\n    ...\n    mem_fetch *result = m_data[block_addr].m_list.front();\n    return result;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\ncache wrapped the next_access, when cache->next_access() is called, it will call mshr next_access().\n\nwriteback() function()\n\n 1. m_next_wb store next_writeback_function, it could be hit in cache or just get response from interconnect\\\n    * scoreboard relase register\n    * m_core->warp_inst_complete\n 2. update m_next_wb from MSHR in L1Cache\n\nCode\n\nvoid ldst_unit::writeback()\n{\n    // process next instruction that is going to writeback\n    if( !m_next_wb.empty() ) {\n        if( m_operand_collector->writeback(m_next_wb) ) {\n            bool insn_completed = false; \n            for( unsigned r=0; r < 4; r++ ) {\n                if( m_next_wb.out[r] > 0 ) {\n\t\t    ...\n\t\t    else { // shared \n                        m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );\n                        insn_completed = true; \n                    }\n                }\n            }\n            if( insn_completed ) {\n                m_core->warp_inst_complete(m_next_wb);\n            }\n            m_next_wb.clear();\n            m_last_inst_gpu_sim_cycle = gpu_sim_cycle;\n            m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;\n        }\n    }\n\n\n    for( unsigned c = 0; m_next_wb.empty() && (c < m_num_writeback_clients); c++ ) {\n        case 4: \n            if( m_L1D && m_L1D->access_ready() ) {\n                mem_fetch *mf = m_L1D->next_access();\n                m_next_wb = mf->get_inst();\n\n                if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) { \n                    m_gpu->getGmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());\n                }\n\t\t \n                delete mf;\n                serviced_client = next_client; \n            }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\nm_core->warp_inst_complete is simple:\n\nCode\n\nvoid shader_core_ctx::warp_inst_complete(const warp_inst_t &inst)\n{\n  ...\n  m_gpu->gpu_sim_insn += inst.active_count();\n  inst.completed(gpu_tot_sim_cycle + gpu_sim_cycle);\n  ...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# How is memory request generated from warp inst?\n\n 1. warp_inst will generate memory access, for address of each thread in the warp, it will be push into access_q.\n    When the warp is issued, it will call generate_mem_access to generate this m_access_q.\n\nCode\n\n// @@@@@@ abstract_hardware_mode.cc\nvoid warp_inst_t::generate_mem_accesses()\n{\n    if( cache_block_size ) {\n\t...\n        for( unsigned thread=0; thread < m_config->warp_size; thread++ ) {\n            new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[0];\n            unsigned block_address = line_size_based_tag_func(addr,cache_block_size);\n            accesses[block_address].set(thread);\n        }\n        for( a=accesses.begin(); a != accesses.end(); ++a ) \n            m_accessq.push_back( mem_access_t(access_type,a->first,cache_block_size,is_write,a->second,byte_mask) );\n}\n\n// The above function is called by:\nvoid shader_core_ctx::func_exec_inst( warp_inst_t &inst )\n{\n    execute_warp_inst_t(inst);\n    if( inst.is_load() || inst.is_store() )\n        inst.generate_mem_accesses();\n}\n\nvoid shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )\n{\n    warp_inst_t** pipe_reg = pipe_reg_set.get_free();\n    ...\n    m_warp[warp_id].ibuffer_free();\n    ...\n    **pipe_reg = *next_inst; // static instruction information\n    (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id() ); // dynamic instruction information\n    m_stats->shader_cycle_distro[2+(*pipe_reg)->active_count()]++;\n    func_exec_inst( **pipe_reg );\n    ...\n    updateSIMTStack(warp_id,*pipe_reg);\n    m_scoreboard->reserveRegisters(*pipe_reg);\n    m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n 2. ldst_unit::memory_cycle\n    In this memory_cycle, it will invoke each access request inside inst.\n    The access will be converted into memory request, sending out downstream to L1 cache.\n    \n    This means that if each thread access a different block in cache, step 1) will generate 32 request inside the m_access_q.\n    Then memory_cycle will at least needs 32 cycle to allocate mem_fetch request for each req.\n\nCode\n\n// shader.cc\nbool ldst_unit::memory_cycle( warp_inst_t &inst, mem_stage_stall_type &stall_reason, mem_stage_access_type &access_type )\n{\n   if ( !inst.accessq_empty() ) {\n       \tconst mem_access_t &access = inst.accessq_front();\n        if( bypassL1D ) {\n           // bypass L1 cache\n           unsigned control_size = inst.is_store() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE;\n           unsigned size = access.get_size() + control_size;\n           if( m_icnt->full(size, inst.is_store() || inst.isatomic()) ) {\n               stall_cond = ICNT_RC_FAIL;\n           } else {\n               mem_fetch *mf = m_mf_allocator->alloc(inst,access);\n               m_icnt->push(mf);\n\n\t       inst.accessq_pop_front();\n               //inst.clear_active( access.get_warp_mask() );\n               if( inst.is_load() ) { \n                  for( unsigned r=0; r < 4; r++) \n                      if(inst.out[r] > 0) \n                          assert( m_pending_writes[inst.warp_id()][inst.out[r]] > 0 );\n               } else if( inst.is_store() ) \n                  m_core->inc_store_req( inst.warp_id() );\n           }\n       } else {\n           stall_cond = process_memory_access_queue(m_L1D,inst);\n       }\n   }\n}\n\nmem_stage_stall_type ldst_unit::process_memory_access_queue( cache_t *cache, warp_inst_t &inst )\n{\n    ...\n    mem_fetch *mf = m_mf_allocator->alloc(inst,inst.accessq_front());\n    enum cache_request_status status = cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);\n    return process_cache_access( cache, mf->get_addr(), inst, events, mf, status );\n}\n\nmem_fetch *alloc( new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const \n{\n    mem_access_t access( type, addr, size, wr );\n    mem_fetch *mf = new mem_fetch( access, \n    \t\t       NULL,\n    \t\t       wr?WRITE_PACKET_SIZE:READ_PACKET_SIZE, \n    \t\t       -1, \n    \t\t       m_core_id, \n    \t\t       m_cluster_id,\n    \t\t       m_memory_config );\n    \treturn mf;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n",normalizedContent:"# gpgpu-sim memory interface\n\n# response phase from interconnect to sm\n\nsimt_core_cluster is at gpu level, it can fetch response into m_response_fifo.\n\nicnt_pop, if return non null ptr, it is a memory fetch repsonse, push it into m_response_fifo.\n\ncode\n\n// @@@@@@ shader.cc\n\nvoid simt_core_cluster::icnt_cycle()\n{\n    // pop from upward queue (gmmu to cu) of cluster and push it to the one in core (sm/cu)\n    if ( !m_gmmu_cu_queue.empty() ) {\n      mem_fetch *mf = m_gmmu_cu_queue.front();\n      ...\n      m_core[cid]->accept_access_response(mf);\n    } \n    \n    // pop it from the downward queue (cu to gmmu) of the core (sm/cu) and push it to the one in cluster (tpc)\n    for (unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++) {\n       if (!m_core[i]->empty_cu_gmmu_queue()){\n          mem_fetch *mf = m_core[i]->front_cu_gmmu_queue();\n          ...\n          m_cu_gmmu_queue.push_front(mf);\n       }\n    }\n\n    // forward response from gpu response fifo into shader core (sm) response fifo\n    if( !m_response_fifo.empty() ) {\n        mem_fetch *mf = m_response_fifo.front();\n        unsigned cid = m_config->sid_to_cid(mf->get_sid());\n        ...\n            // data response\n            if( !m_core[cid]->ldst_unit_response_buffer_full() ) {\n                m_response_fifo.pop_front();\n                // gpu ---\x3e sm\n                m_core[cid]->accept_ldst_unit_response(mf);\n            }\n        }\n    }\n\n    // accept response from interconnect network\n    if( m_response_fifo.size() < m_config->n_simt_ejection_buffer_size ) {\n        mem_fetch *mf = (mem_fetch*) ::icnt_pop(m_cluster_id);\n        if (!mf) \n            return;\n\n        // the packet size varies depending on the type of request: \n        // - for read request and atomic request, the packet contains the data \n        // - for write-ack, the packet only has control metadata\n        ...\n        m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size); \n        mf->set_status(in_cluster_to_shader_queue,gpu_sim_cycle+gpu_tot_sim_cycle);\n        ...\n        // interconnect to gpu\n        m_response_fifo.push_back(mf);\n        m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);\n    } \n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n\n\nthe flow below is from sm to ldst unit and then to l1 cache.\n\nthe l1 cache is included in ldst unit.\n\ncode\n\n\nvoid shader_core_ctx::accept_ldst_unit_response(mem_fetch * mf) \n{\n   m_ldst_unit->fill(mf);\n}\n\nvoid ldst_unit::fill( mem_fetch *mf )\n{\n    mf->set_status(in_shader_ldst_response_fifo,gpu_sim_cycle+gpu_tot_sim_cycle);\n    m_response_fifo.push_back(mf);\n}\n\nvoid ldst_unit::cycle()\n{\n   writeback();\n   ...\n   if( !m_response_fifo.empty() ) {\n       mem_fetch *mf = m_response_fifo.front();\n       ...\n    \t   if( mf->get_type() == write_ack || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {\n               m_core->store_ack(mf);\n               m_response_fifo.pop_front();\n\n               if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) {\n                    m_gpu->getgmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());\n               }\n               ...\n           } else {\n              ...\n              if (m_l1d->fill_port_free()) {\n                   m_l1d->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);\n                   m_response_fifo.pop_front();\n              }\n           }\n       }\n   }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\nat this time the response is fed into l1 cache, which is shared in sm.\n\nbut how is it responsed to core?\n\n# response phase from l1 cache to wrap execution\n\ncode\n\n/// @@@@@@ gpu-cache.cc\n/// interface for response from lower memory level (model bandwidth restictions in caller)\nvoid baseline_cache::fill(mem_fetch *mf, unsigned time){\n    ...\n    if ( m_config.m_alloc_policy == on_miss )\n        m_tag_array->fill(e->second.m_cache_index,time);\n    else if ( m_config.m_alloc_policy == on_fill )\n        m_tag_array->fill(e->second.m_block_addr,time);\n\n    m_mshrs.mark_ready(e->second.m_block_addr, has_atomic);\n    ...\n}\n\n/// accept a new cache fill response: mark entry ready for processing\nvoid mshr_table::mark_ready( new_addr_type block_addr, bool &has_atomic ){\n    ...\n    m_current_response.push_back( block_addr );\n    ...\n}\n\n/// returns next ready access\nmem_fetch *mshr_table::next_access(){\n    ...\n    new_addr_type block_addr = m_current_response.front();\n    ...\n    mem_fetch *result = m_data[block_addr].m_list.front();\n    return result;\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n\n\ncache wrapped the next_access, when cache->next_access() is called, it will call mshr next_access().\n\nwriteback() function()\n\n 1. m_next_wb store next_writeback_function, it could be hit in cache or just get response from interconnect\\\n    * scoreboard relase register\n    * m_core->warp_inst_complete\n 2. update m_next_wb from mshr in l1cache\n\ncode\n\nvoid ldst_unit::writeback()\n{\n    // process next instruction that is going to writeback\n    if( !m_next_wb.empty() ) {\n        if( m_operand_collector->writeback(m_next_wb) ) {\n            bool insn_completed = false; \n            for( unsigned r=0; r < 4; r++ ) {\n                if( m_next_wb.out[r] > 0 ) {\n\t\t    ...\n\t\t    else { // shared \n                        m_scoreboard->releaseregister( m_next_wb.warp_id(), m_next_wb.out[r] );\n                        insn_completed = true; \n                    }\n                }\n            }\n            if( insn_completed ) {\n                m_core->warp_inst_complete(m_next_wb);\n            }\n            m_next_wb.clear();\n            m_last_inst_gpu_sim_cycle = gpu_sim_cycle;\n            m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;\n        }\n    }\n\n\n    for( unsigned c = 0; m_next_wb.empty() && (c < m_num_writeback_clients); c++ ) {\n        case 4: \n            if( m_l1d && m_l1d->access_ready() ) {\n                mem_fetch *mf = m_l1d->next_access();\n                m_next_wb = mf->get_inst();\n\n                if ( m_gpu->get_global_memory()->is_page_managed(mf->get_mem_access().get_addr(), mf->get_mem_access().get_size()) ) { \n                    m_gpu->getgmmu()->reserve_pages_remove(mf->get_mem_access().get_addr(), mf->get_mem_access().get_uid());\n                }\n\t\t \n                delete mf;\n                serviced_client = next_client; \n            }\n    }\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n\n\nm_core->warp_inst_complete is simple:\n\ncode\n\nvoid shader_core_ctx::warp_inst_complete(const warp_inst_t &inst)\n{\n  ...\n  m_gpu->gpu_sim_insn += inst.active_count();\n  inst.completed(gpu_tot_sim_cycle + gpu_sim_cycle);\n  ...\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# how is memory request generated from warp inst?\n\n 1. warp_inst will generate memory access, for address of each thread in the warp, it will be push into access_q.\n    when the warp is issued, it will call generate_mem_access to generate this m_access_q.\n\ncode\n\n// @@@@@@ abstract_hardware_mode.cc\nvoid warp_inst_t::generate_mem_accesses()\n{\n    if( cache_block_size ) {\n\t...\n        for( unsigned thread=0; thread < m_config->warp_size; thread++ ) {\n            new_addr_type addr = m_per_scalar_thread[thread].memreqaddr[0];\n            unsigned block_address = line_size_based_tag_func(addr,cache_block_size);\n            accesses[block_address].set(thread);\n        }\n        for( a=accesses.begin(); a != accesses.end(); ++a ) \n            m_accessq.push_back( mem_access_t(access_type,a->first,cache_block_size,is_write,a->second,byte_mask) );\n}\n\n// the above function is called by:\nvoid shader_core_ctx::func_exec_inst( warp_inst_t &inst )\n{\n    execute_warp_inst_t(inst);\n    if( inst.is_load() || inst.is_store() )\n        inst.generate_mem_accesses();\n}\n\nvoid shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )\n{\n    warp_inst_t** pipe_reg = pipe_reg_set.get_free();\n    ...\n    m_warp[warp_id].ibuffer_free();\n    ...\n    **pipe_reg = *next_inst; // static instruction information\n    (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id() ); // dynamic instruction information\n    m_stats->shader_cycle_distro[2+(*pipe_reg)->active_count()]++;\n    func_exec_inst( **pipe_reg );\n    ...\n    updatesimtstack(warp_id,*pipe_reg);\n    m_scoreboard->reserveregisters(*pipe_reg);\n    m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n\n\n 2. ldst_unit::memory_cycle\n    in this memory_cycle, it will invoke each access request inside inst.\n    the access will be converted into memory request, sending out downstream to l1 cache.\n    \n    this means that if each thread access a different block in cache, step 1) will generate 32 request inside the m_access_q.\n    then memory_cycle will at least needs 32 cycle to allocate mem_fetch request for each req.\n\ncode\n\n// shader.cc\nbool ldst_unit::memory_cycle( warp_inst_t &inst, mem_stage_stall_type &stall_reason, mem_stage_access_type &access_type )\n{\n   if ( !inst.accessq_empty() ) {\n       \tconst mem_access_t &access = inst.accessq_front();\n        if( bypassl1d ) {\n           // bypass l1 cache\n           unsigned control_size = inst.is_store() ? write_packet_size : read_packet_size;\n           unsigned size = access.get_size() + control_size;\n           if( m_icnt->full(size, inst.is_store() || inst.isatomic()) ) {\n               stall_cond = icnt_rc_fail;\n           } else {\n               mem_fetch *mf = m_mf_allocator->alloc(inst,access);\n               m_icnt->push(mf);\n\n\t       inst.accessq_pop_front();\n               //inst.clear_active( access.get_warp_mask() );\n               if( inst.is_load() ) { \n                  for( unsigned r=0; r < 4; r++) \n                      if(inst.out[r] > 0) \n                          assert( m_pending_writes[inst.warp_id()][inst.out[r]] > 0 );\n               } else if( inst.is_store() ) \n                  m_core->inc_store_req( inst.warp_id() );\n           }\n       } else {\n           stall_cond = process_memory_access_queue(m_l1d,inst);\n       }\n   }\n}\n\nmem_stage_stall_type ldst_unit::process_memory_access_queue( cache_t *cache, warp_inst_t &inst )\n{\n    ...\n    mem_fetch *mf = m_mf_allocator->alloc(inst,inst.accessq_front());\n    enum cache_request_status status = cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);\n    return process_cache_access( cache, mf->get_addr(), inst, events, mf, status );\n}\n\nmem_fetch *alloc( new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const \n{\n    mem_access_t access( type, addr, size, wr );\n    mem_fetch *mf = new mem_fetch( access, \n    \t\t       null,\n    \t\t       wr?write_packet_size:read_packet_size, \n    \t\t       -1, \n    \t\t       m_core_id, \n    \t\t       m_cluster_id,\n    \t\t       m_memory_config );\n    \treturn mf;\n}\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n",charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"Warp Related Memory Optimization",frontmatter:{title:"Warp Related Memory Optimization",date:"2024-08-15T00:00:00.000Z",permalink:"/pages/45873/"},regularPath:"/03.gpu/17.warp_mem.html",relativePath:"03.gpu/17.warp_mem.md",key:"v-c25059fe",path:"/pages/45873/",headers:[{level:3,title:"1. [215] CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization",slug:"_1-215-cudadma-optimizing-gpu-memory-bandwidth-via-warp-specialization",normalizedTitle:"1. [215] cudadma: optimizing gpu memory bandwidth via warp specialization",charIndex:1}],headersStr:"1. [215] CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization",content:" 1. [215] CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization\n 2. \n\n----------------------------------------\n\n\n# 1. [215] CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization\n\nAPI:\n\n * One synchronization point corresponds to the data having been consumed and the buffer standing empty awaiting the next transfer.\n   The compute threads indicate this status using a non-blocking call to start_async_dma()\n * The DMA threads wait to begin this transfer using the blocking call wait_for_dma_start()\n * DMA warps indicate that a transfer is complete using a non-blocking call to finish_async_dma()\n * The compute warps wait for a transfer to complete using a blocking call to wait_for_dma_finish()\n * The DMA-side calls are usually abstracted behind execute_dma()",normalizedContent:" 1. [215] cudadma: optimizing gpu memory bandwidth via warp specialization\n 2. \n\n----------------------------------------\n\n\n# 1. [215] cudadma: optimizing gpu memory bandwidth via warp specialization\n\napi:\n\n * one synchronization point corresponds to the data having been consumed and the buffer standing empty awaiting the next transfer.\n   the compute threads indicate this status using a non-blocking call to start_async_dma()\n * the dma threads wait to begin this transfer using the blocking call wait_for_dma_start()\n * dma warps indicate that a transfer is complete using a non-blocking call to finish_async_dma()\n * the compute warps wait for a transfer to complete using a blocking call to wait_for_dma_finish()\n * the dma-side calls are usually abstracted behind execute_dma()",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"GPU Cache & Memory Hirerarchy",frontmatter:{title:"GPU Cache & Memory Hirerarchy",date:"2024-08-25T00:00:00.000Z",permalink:"/pages/45876/"},regularPath:"/03.gpu/19.gpu_cache_mem.html",relativePath:"03.gpu/19.gpu_cache_mem.md",key:"v-53bdf925",path:"/pages/45876/",headers:[{level:3,title:"1. Dissecting GPU Memory Hierarchy through Microbenchmarking",slug:"_1-dissecting-gpu-memory-hierarchy-through-microbenchmarking",normalizedTitle:"1. dissecting gpu memory hierarchy through microbenchmarking",charIndex:436},{level:3,title:"Shared Memory",slug:"shared-memory",normalizedTitle:"shared memory",charIndex:367},{level:3,title:"4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking",slug:"_4-dissecting-the-nvidia-turing-t4-gpu-via-microbenchmarking",normalizedTitle:"4. dissecting the nvidia turing t4 gpu via microbenchmarking",charIndex:9912},{level:3,title:"4. Benchmarking the GPU memory at the warp level",slug:"_4-benchmarking-the-gpu-memory-at-the-warp-level",normalizedTitle:"4. benchmarking the gpu memory at the warp level",charIndex:10033},{level:3,title:"5. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling",slug:"_5-exploring-modern-gpu-memory-system-design-challenges-through-accurate-modeling",normalizedTitle:"5. exploring modern gpu memory system design challenges through accurate modeling",charIndex:12554},{level:3,title:"6. OSM: Off-Chip Shared Memory for GPUs",slug:"_6-osm-off-chip-shared-memory-for-gpus",normalizedTitle:"6. osm: off-chip shared memory for gpus",charIndex:15213}],headersStr:"1. Dissecting GPU Memory Hierarchy through Microbenchmarking Shared Memory 4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking 4. Benchmarking the GPU memory at the warp level 5. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling 6. OSM: Off-Chip Shared Memory for GPUs",content:" 1. [248] Dissecting GPU Memory Hierarchy through Microbenchmarking\n 2. [75] Benchmarking the Memory Hierarchy of Modern GPUs\n 3. [18] Benchmarking the GPU memory at the warp level\n 4. [90] Dissecting the NVidia Turing T4 GPU via Microbenchmarking\n 5. [38] Exploring Modern GPU Memory System Design Challenges through Accurate Modeling 👍 👍 👍\n 6. [9] OSM: Off-Chip Shared Memory for GPUs\n\n----------------------------------------\n\n\n# 1. Dissecting GPU Memory Hierarchy through Microbenchmarking\n\nA paper in 2015, profile memory in Fermi, Kepler and Maxwell\n\n\n\n# Parameter\n\n\n\n\n\n\n\n# L1 Data Cache\n\nOn the Fermi and Kepler devices, the L1 data cache and shared memory are physically implemented together.\nOn the Maxwell devices, the L1 data cache is unified with the texture cache.\n\nThe 16 KB L1 cache has 128 cache lines mapped onto four cache ways.\nFor each cache way, 32 cache sets are divided into 8 major sets. Each major set contains 16 cache lines.\n\nThe data mapping is also unconventional.\nThe 12-13th bits in the memory address define the cache way, the 9-11th bits define the major set, and the 0-6th bits define the memory offset inside the cache line.\n\nOne distinctive feature of the Fermi L1 cache is that its replacement policy is not LRU, as pointed out by Meltzer et.al. Among the four cache ways, cache way 2 is three times more likely to be replaced than the other three cache ways.\n\nAnother paper[4] We found that when the L1 data cache saturates, Turing randomly evicts 4 consecutive cache lines (128 B).\nWe observed that once a block of cache lines are evicted, the second scan will cause more cache lines from the same set to be evicted.\n\n\n\n# L2 Data Cache\n\n * The replacement policy of the L2 cache is not LRU\n * The L2 cache line size is 32 bytes by observing the memory access pattern of overflowing the cache and visiting array element one by one.\n * The data mapping is sophisticated and not conventional bits-defined\n * a hardware-level pre-fetching mechanism from the DRAM to the L2 data cache on all three platforms.\n   The pre-fetching size is about 2/3 of the L2 cache size and the prefetching is sequential. This is deduced from that if we load an array smaller than 2/3 of the L2 data cache size, there is no cold cache miss patterns.\n   🙋(Maybe they can cover the gap just by prefetching sequential line.)\n\n# Global Memory\n\nglobal memory access involves accessing the DRAM, L1 and L2 data caches, TLBs and page tables.\n\n# Global Memory Throughput\n\nThe theoretical bandwidth is calculated as fmem * bus width * DDR factor.\n\n\n\nthe throughput of a larger ILP saturates faster.\n\nThe GTX780 has the highest throughput as it benefits from the highest bus width,\nbut its convergence speed is the slowest, i.e., it requires the most memory requests to hide the pipeline latency.\n\nThis could be part of the reason that NVIDIA reduced the bus width back to 256 bits in Maxwell devices.\n\n# Global Memory Latency\n\nThe global memory access latency is the whole time accessing a data located in DRAM/L2 or L1 cache, including the latency of page table look-ups.\n\n * very large s1 = 32 MB to construct the TLB/page table miss and cache miss (P5&P6)\n * set s2 = 1 MB to construct the L1 TLB hit but cache miss (P4)\n * After a total of 65 data accesses, 65 data lines are loaded into the cache.\n   We then visit the cached data lines with s1 again for several times, to construct cache hit but TLB miss (P2&P3).\n * set s3 = 1 element and repeatedly load the data in a cache line so that every memory access is a cache hit (P1).\n\n\n\n\n\n\n\n * The Maxwell and Kepler devices have a unique memory access pattern (P6) for page table context switching.\n   When a kernel is launched, only memory page entries of 512 MB are activated.\n   If the thread visits an inactivate page entry, the hardware needs a rather long time to switch between page tables.\n   This phenomena is also reported in [22] as page table “miss”.\n * The Maxwell L1 data cache addressing does not go through the TLBs or page tables.\n   On the GTX980, there is no TLB miss pattern (i.e., P2 and P3) when the L1 data cache is hit.\n   Once the L1 cache is missed, the access latency increases from tens of cycles to hundreds or even thousands of cycles. My comments: But if we look at GTX560Ti in P2, the latency is different with P1. So does this means that in Fermi, the memory request has to go through TLB first, and then access L1 DataCache? This might be the reason that the latency is longer. But this will degrade the performance....\n * The TLBs are off-chip. we infer that the physical memory locations of the L1 TLB and L2 data cache are close.\n   The physical memory locations of the L1 TLB and L2 TLB are also close, which means that the L1/L2 TLB and L2 data cache are shared off-chip by all SMs.\n * The GTX780 generally has the shortest global memory latencies, almost half that of the Fermi, with an access pattern of P2-P5.\n   The page table context switching of the GTX980 is also much more expensive than that of the GTX780.\n\nTo summarize, the Maxwell device has long global memory access latencies for cold cache misses and page table context switching.\nExcept for these rare access patterns, its access latency cycles are close to those of the Kepler device.\nbecause the GTX980 has higher fmem than the GTX780, it actually offers the shortest global memory access time (P2-P4).\n\n\n\n\n# Shared Memory\n\nIn CUDA programming, different CTAs assigned to the same SM have to share the same physical memory space.\nOn the Fermi and Kepler platforms, the shared memory is physically integrated with the L1 cache.\nOn the Maxwell platform, it occupies a separate memory space. Note that the shared memory and L1 cache are separated since Maxwell architecture.\n\nProgrammers move the data into and out of shared memory from global memory before and after arithmetic execution,\nto avoid the frequent occurrence of long global memory access latencies.\n\nWe report a dramatic improvement in performance for the Maxwell device.\n\n# Shared Memory Throughput\n\nthe shared memory is organized as 32 memory banks [15].\nThe bank width of the Fermi and Maxwell devices is 4 bytes, while that of the Kepler device is 8 bytes. The theoretical peak throughput of each SM (WSM) is calculated as fcore ∗ Wbank ∗ 32.\n\n\n\nThe achieved throughput per SM is calculated as 2 * fcore * sizeof(int) * (number of active threads per SM) * ILP / (total latency of each SM). Usually a large value of ILP results in less active warps per SM.\nThe peak throughput W0SM denotes the respective maximum throughput of the abovecombinations.\nTwo key factors that affect the throughput are the number of active warps per SM and the ILP level.\n\nThe GTX980 reaches its peak throughput when the CTA size = 256, CTAs per SM = 2 and ILP = 8, i.e., 16 active warps per SM. The peak throughput is 137.41 GB/s, about 83.9% of the theoretical bandwidth. The Maxwell device shows the best use of its shared memory bandwidth, and the Kepler device shows the worst.\n\nGTX980 exhibits similar behavior as GTX780: high ILP is required to achieve high throughput for high SM occupancy.\n\nAccording to Little’s Law, we roughly have: number of active warps * ILP = latency cycles * throughput.\n\nGTX780 sucks in ILP = 1, since its limited 64 warps at most to be scheduled concurrently.\nWe consider this to be the main reason the achieved throughput of the GTX780 is poor compared with its designed value.\n\n# Shared Memory Latency\n\nThe shared memory latencies on Fermi, Kepler and Maxwell devices are 50, 47 and 28 cycles, respectively.\n\nFermi and Maxwell devices have the same number of potential bank conflicts because they have the same architecture.\n\nThe shared memory space is divided into 32 banks.\nSuccessive words are allocated to successive banks.\nIf two threads in the same warp access memory spaces in the same bank, a 2-way bank conflict occurs.\n\n\n\n\n\n\n\nFor the Fermi and Kepler devices, where there is a 32-way bank conflict, it takes much longer to access shared memory than regular global memory (TLB hit, cache miss).\nSurprisingly, the effect of a bank conflict on shared memory access latency on the Maxwell device is mild.\nEven the longest shared memory access latency is still at the same level as L1 data cache latency.\n\nIn summary, although the shared memory has very short access latency, it can be rather long if there are many ways of bank conflicts.\nThis is most obvious on the Fermi hardware.\nThe Kepler device tries to solve it by doubling the bank width of shared memory.\nCompared with the Fermi, the Kepler’s 4-byte mode shared memory halves the chance of bank conflict, and the 8-byte mode reduces it further.\n\nHowever, we also find that the Kepler’s shared memory is inefficient in terms of throughput.\nThe Maxwell device has the best shared memory performance.\nWith the same architecture as the Fermi device, the Maxwell hardware shows a 2x size, 2x memory access speedup and achieves the highest throughput.\nMost importantly, the Maxwell device’s shared memory has been optimized to avoid the long latency caused by bank conflicts.\n\n# Conclusion\n\nThe memory capacity is significantly enhanced in both Kepler and Maxwell as compared with Fermi.\nThe Kepler device is performance-oriented and incorporates several aggressive elements in its design, such as increasing the bus width of DRAM and doubling the bank width of shared memory.\nThese designs have some side-effects.\nThe theoretical bandwidths of both global memory and shared memory are difficult to saturate, and hardware resources are imbalanced with a low utilization rate.\nThe Maxwell device has a more efficient and conservative design.\nIt has a reduced bus width and bank width, and the on-chip cache architectures are adjusted, including doubling the shared memory size and the read-only data cache size.\nFurthermore, it sharply decreases the shared memory latency caused under bank conflicts.\n\n\n# 4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking\n\n# Result\n\n\n\n\n\n# Shared Memory Latency\n\n\n\n# Bandwidth\n\n\n\n\n# 4. Benchmarking the GPU memory at the warp level\n\nIn this work, we investigate the data accessing capability of a warp of threads: broadcasting and parallel accessing.\\\n\n * Broadcasting occurs when multiple threads access the same data element, i.e., multiple threads request a single data element (MTSD).\n * We refer the case of multiple threads accessing multiple distinct data elements (MTMD) as parallel accessing.\n\n# Local Memory\n\n * For the simple memory access patterns, we should allocate a sufficient small array to guarantee that it is located in registers.\n * For the complex memory access patterns, we should simplify codes to exploit registers. For example, we merge a three-level loop into an one-level loop so that a larger temporal vector can be allocated in registers.\n * \n\n# Shared Memory\n\n * Bank conflicts must be avoided by the ways of e.g., data padding.\n * Shared memory supports both broadcasting and parallel accessing.\n * Neither consecutively accessing nor aligned accessing is a must.\n * The latency decreases when the number of threads increase, and thus we should use a sufficiently large thread block.\n * Replacing global memory with shared memory, because the latency of shared memory is smaller than that of global memory.\n * Using shared memory bares an overhead (i.e., buffer allocation and data movement) and reusing data in it is a must for improved performance.\n\n# Constant Memory\n\nBut constant memory does not support parallel accessing.\nThat is, constant memory can only be accessed serially when requesting different data elements.\nOn the one hand, constant memory is used to store a small amount of read-only data, which is not sensitive to bandwidth.\nSo parallel accessing is not a must for constant memory.\n\n * Constant memory supports the accessing capability of broadcasting.\n * Constant memory does not support parallel accessing, and satisfies parallel memory requests in a serial manner.\n\n# Shared Memory\n\n * Global memory supports both broadcasting and parallel accessing.\n * The data types of 4 or 8 bytes can obtain the near upper-bounded bandwidth of global memory, while the data types cannot.\n   So the char data should be coalesced into the char4 type for improved bandwidth.\n * Global memory accesses should be consecutive, but aligned accessing is not necessary for global memory.\n * When memory accessing is non-consecutive, the latency changes with the number of threads, but not with the number of blocks. So we should configure the thread dimensionality.\n\n\n# 5. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling\n\n👍 👍 👍\n\n# Memory Coalescer\n\nthe eviction granularity of the cache is 128B, indicating that the L1 cache has 128B lines with 32B sectors.\nFurthermore, the coalescer operates across eight threads, i.e. the coalescer tries to coalesce each group of eight threads separately to generate sectored accesses.\n\n\n\nWhen the stride=32, the memory access is converged, and all the threads within the same warp will access the same cache line,\nhowever we receive four read accesses at L1 cache.\n\n8 Thread register 32bit == 32Byte.\n\n# L2 Cache\n\nL2 cache applies something similar to write-validate not fetch on write.\\ 😱 However, all the reads received by L2 caches from the coalescer are 32-byte sectored accesses.\nThus, the read access granularity (32 bytes) is different from the write access granularity (one byte).\nTo handle this, the L2 cache applies a different write allocation policy, which we named lazy fetch-on-read, that is a compromise between write-validate and fetch-on-write.\n\nWhen a sector read request is received to a modified sector, it first checks if the sector write-mask is complete, i.e. all the bytes have been written to and the line is fully readable.\nIf so, it reads the sector, otherwise, similar to fetch-on-write, it generates a read request for this sector and merges it with the modified bytes.\n\n# Streaming Throughput-oriented L1 Cache\n\n\n\nThe L1 cache in Volta is what NVIDIA is calling a streaming cache [33].\nIt is streaming because the documentation states that it allows unlimited cache misses to be in flight regardless the number of cache lines per cache set [10].\n\nindependent of the number of L1 configured size, the number of MSHRs available are the same, even if more of the on-chip SRAM storage is devoted to shared memory.\n\nWe believe that unified cache is a plain SRAM where sectored data blocks are shared between the L1D and the CUDA shared memory.\nIt can be configured adaptively by the driver as we discussed earlier.\nWe assume that the L1D’s TAG and MSHR merging functionality are combined together in a separate table structure (TAG-MSHR table).\nSince, the filling policy is now ON FILL, we can have more TAG entries and outstanding requests than the assigned L1D cache lines.\n\nIf it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps).\nWhen the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table.\nThen, the merged warps access the sector, on a cycle-by-cyle basis.\n\n\n# 6. OSM: Off-Chip Shared Memory for GPUs\n\n\n\nL1-D cache and shared memory use the same 32-bank memory structure (4 KB capacity per bank) as shown in Fig. 4;\nhowever, they have some differences.\nWe can access 32-bit shared memory arrays via a thread-index directly, while for accessing L1-D cache, we should read 128B (four 32B sectors) of the cache block.\nIn addition, L1 cache requires an extra hardware for managing tags and implementing LRU replacement policy.",normalizedContent:" 1. [248] dissecting gpu memory hierarchy through microbenchmarking\n 2. [75] benchmarking the memory hierarchy of modern gpus\n 3. [18] benchmarking the gpu memory at the warp level\n 4. [90] dissecting the nvidia turing t4 gpu via microbenchmarking\n 5. [38] exploring modern gpu memory system design challenges through accurate modeling 👍 👍 👍\n 6. [9] osm: off-chip shared memory for gpus\n\n----------------------------------------\n\n\n# 1. dissecting gpu memory hierarchy through microbenchmarking\n\na paper in 2015, profile memory in fermi, kepler and maxwell\n\n\n\n# parameter\n\n\n\n\n\n\n\n# l1 data cache\n\non the fermi and kepler devices, the l1 data cache and shared memory are physically implemented together.\non the maxwell devices, the l1 data cache is unified with the texture cache.\n\nthe 16 kb l1 cache has 128 cache lines mapped onto four cache ways.\nfor each cache way, 32 cache sets are divided into 8 major sets. each major set contains 16 cache lines.\n\nthe data mapping is also unconventional.\nthe 12-13th bits in the memory address define the cache way, the 9-11th bits define the major set, and the 0-6th bits define the memory offset inside the cache line.\n\none distinctive feature of the fermi l1 cache is that its replacement policy is not lru, as pointed out by meltzer et.al. among the four cache ways, cache way 2 is three times more likely to be replaced than the other three cache ways.\n\nanother paper[4] we found that when the l1 data cache saturates, turing randomly evicts 4 consecutive cache lines (128 b).\nwe observed that once a block of cache lines are evicted, the second scan will cause more cache lines from the same set to be evicted.\n\n\n\n# l2 data cache\n\n * the replacement policy of the l2 cache is not lru\n * the l2 cache line size is 32 bytes by observing the memory access pattern of overflowing the cache and visiting array element one by one.\n * the data mapping is sophisticated and not conventional bits-defined\n * a hardware-level pre-fetching mechanism from the dram to the l2 data cache on all three platforms.\n   the pre-fetching size is about 2/3 of the l2 cache size and the prefetching is sequential. this is deduced from that if we load an array smaller than 2/3 of the l2 data cache size, there is no cold cache miss patterns.\n   🙋(maybe they can cover the gap just by prefetching sequential line.)\n\n# global memory\n\nglobal memory access involves accessing the dram, l1 and l2 data caches, tlbs and page tables.\n\n# global memory throughput\n\nthe theoretical bandwidth is calculated as fmem * bus width * ddr factor.\n\n\n\nthe throughput of a larger ilp saturates faster.\n\nthe gtx780 has the highest throughput as it benefits from the highest bus width,\nbut its convergence speed is the slowest, i.e., it requires the most memory requests to hide the pipeline latency.\n\nthis could be part of the reason that nvidia reduced the bus width back to 256 bits in maxwell devices.\n\n# global memory latency\n\nthe global memory access latency is the whole time accessing a data located in dram/l2 or l1 cache, including the latency of page table look-ups.\n\n * very large s1 = 32 mb to construct the tlb/page table miss and cache miss (p5&p6)\n * set s2 = 1 mb to construct the l1 tlb hit but cache miss (p4)\n * after a total of 65 data accesses, 65 data lines are loaded into the cache.\n   we then visit the cached data lines with s1 again for several times, to construct cache hit but tlb miss (p2&p3).\n * set s3 = 1 element and repeatedly load the data in a cache line so that every memory access is a cache hit (p1).\n\n\n\n\n\n\n\n * the maxwell and kepler devices have a unique memory access pattern (p6) for page table context switching.\n   when a kernel is launched, only memory page entries of 512 mb are activated.\n   if the thread visits an inactivate page entry, the hardware needs a rather long time to switch between page tables.\n   this phenomena is also reported in [22] as page table “miss”.\n * the maxwell l1 data cache addressing does not go through the tlbs or page tables.\n   on the gtx980, there is no tlb miss pattern (i.e., p2 and p3) when the l1 data cache is hit.\n   once the l1 cache is missed, the access latency increases from tens of cycles to hundreds or even thousands of cycles. my comments: but if we look at gtx560ti in p2, the latency is different with p1. so does this means that in fermi, the memory request has to go through tlb first, and then access l1 datacache? this might be the reason that the latency is longer. but this will degrade the performance....\n * the tlbs are off-chip. we infer that the physical memory locations of the l1 tlb and l2 data cache are close.\n   the physical memory locations of the l1 tlb and l2 tlb are also close, which means that the l1/l2 tlb and l2 data cache are shared off-chip by all sms.\n * the gtx780 generally has the shortest global memory latencies, almost half that of the fermi, with an access pattern of p2-p5.\n   the page table context switching of the gtx980 is also much more expensive than that of the gtx780.\n\nto summarize, the maxwell device has long global memory access latencies for cold cache misses and page table context switching.\nexcept for these rare access patterns, its access latency cycles are close to those of the kepler device.\nbecause the gtx980 has higher fmem than the gtx780, it actually offers the shortest global memory access time (p2-p4).\n\n\n\n\n# shared memory\n\nin cuda programming, different ctas assigned to the same sm have to share the same physical memory space.\non the fermi and kepler platforms, the shared memory is physically integrated with the l1 cache.\non the maxwell platform, it occupies a separate memory space. note that the shared memory and l1 cache are separated since maxwell architecture.\n\nprogrammers move the data into and out of shared memory from global memory before and after arithmetic execution,\nto avoid the frequent occurrence of long global memory access latencies.\n\nwe report a dramatic improvement in performance for the maxwell device.\n\n# shared memory throughput\n\nthe shared memory is organized as 32 memory banks [15].\nthe bank width of the fermi and maxwell devices is 4 bytes, while that of the kepler device is 8 bytes. the theoretical peak throughput of each sm (wsm) is calculated as fcore ∗ wbank ∗ 32.\n\n\n\nthe achieved throughput per sm is calculated as 2 * fcore * sizeof(int) * (number of active threads per sm) * ilp / (total latency of each sm). usually a large value of ilp results in less active warps per sm.\nthe peak throughput w0sm denotes the respective maximum throughput of the abovecombinations.\ntwo key factors that affect the throughput are the number of active warps per sm and the ilp level.\n\nthe gtx980 reaches its peak throughput when the cta size = 256, ctas per sm = 2 and ilp = 8, i.e., 16 active warps per sm. the peak throughput is 137.41 gb/s, about 83.9% of the theoretical bandwidth. the maxwell device shows the best use of its shared memory bandwidth, and the kepler device shows the worst.\n\ngtx980 exhibits similar behavior as gtx780: high ilp is required to achieve high throughput for high sm occupancy.\n\naccording to little’s law, we roughly have: number of active warps * ilp = latency cycles * throughput.\n\ngtx780 sucks in ilp = 1, since its limited 64 warps at most to be scheduled concurrently.\nwe consider this to be the main reason the achieved throughput of the gtx780 is poor compared with its designed value.\n\n# shared memory latency\n\nthe shared memory latencies on fermi, kepler and maxwell devices are 50, 47 and 28 cycles, respectively.\n\nfermi and maxwell devices have the same number of potential bank conflicts because they have the same architecture.\n\nthe shared memory space is divided into 32 banks.\nsuccessive words are allocated to successive banks.\nif two threads in the same warp access memory spaces in the same bank, a 2-way bank conflict occurs.\n\n\n\n\n\n\n\nfor the fermi and kepler devices, where there is a 32-way bank conflict, it takes much longer to access shared memory than regular global memory (tlb hit, cache miss).\nsurprisingly, the effect of a bank conflict on shared memory access latency on the maxwell device is mild.\neven the longest shared memory access latency is still at the same level as l1 data cache latency.\n\nin summary, although the shared memory has very short access latency, it can be rather long if there are many ways of bank conflicts.\nthis is most obvious on the fermi hardware.\nthe kepler device tries to solve it by doubling the bank width of shared memory.\ncompared with the fermi, the kepler’s 4-byte mode shared memory halves the chance of bank conflict, and the 8-byte mode reduces it further.\n\nhowever, we also find that the kepler’s shared memory is inefficient in terms of throughput.\nthe maxwell device has the best shared memory performance.\nwith the same architecture as the fermi device, the maxwell hardware shows a 2x size, 2x memory access speedup and achieves the highest throughput.\nmost importantly, the maxwell device’s shared memory has been optimized to avoid the long latency caused by bank conflicts.\n\n# conclusion\n\nthe memory capacity is significantly enhanced in both kepler and maxwell as compared with fermi.\nthe kepler device is performance-oriented and incorporates several aggressive elements in its design, such as increasing the bus width of dram and doubling the bank width of shared memory.\nthese designs have some side-effects.\nthe theoretical bandwidths of both global memory and shared memory are difficult to saturate, and hardware resources are imbalanced with a low utilization rate.\nthe maxwell device has a more efficient and conservative design.\nit has a reduced bus width and bank width, and the on-chip cache architectures are adjusted, including doubling the shared memory size and the read-only data cache size.\nfurthermore, it sharply decreases the shared memory latency caused under bank conflicts.\n\n\n# 4. dissecting the nvidia turing t4 gpu via microbenchmarking\n\n# result\n\n\n\n\n\n# shared memory latency\n\n\n\n# bandwidth\n\n\n\n\n# 4. benchmarking the gpu memory at the warp level\n\nin this work, we investigate the data accessing capability of a warp of threads: broadcasting and parallel accessing.\\\n\n * broadcasting occurs when multiple threads access the same data element, i.e., multiple threads request a single data element (mtsd).\n * we refer the case of multiple threads accessing multiple distinct data elements (mtmd) as parallel accessing.\n\n# local memory\n\n * for the simple memory access patterns, we should allocate a sufficient small array to guarantee that it is located in registers.\n * for the complex memory access patterns, we should simplify codes to exploit registers. for example, we merge a three-level loop into an one-level loop so that a larger temporal vector can be allocated in registers.\n * \n\n# shared memory\n\n * bank conflicts must be avoided by the ways of e.g., data padding.\n * shared memory supports both broadcasting and parallel accessing.\n * neither consecutively accessing nor aligned accessing is a must.\n * the latency decreases when the number of threads increase, and thus we should use a sufficiently large thread block.\n * replacing global memory with shared memory, because the latency of shared memory is smaller than that of global memory.\n * using shared memory bares an overhead (i.e., buffer allocation and data movement) and reusing data in it is a must for improved performance.\n\n# constant memory\n\nbut constant memory does not support parallel accessing.\nthat is, constant memory can only be accessed serially when requesting different data elements.\non the one hand, constant memory is used to store a small amount of read-only data, which is not sensitive to bandwidth.\nso parallel accessing is not a must for constant memory.\n\n * constant memory supports the accessing capability of broadcasting.\n * constant memory does not support parallel accessing, and satisfies parallel memory requests in a serial manner.\n\n# shared memory\n\n * global memory supports both broadcasting and parallel accessing.\n * the data types of 4 or 8 bytes can obtain the near upper-bounded bandwidth of global memory, while the data types cannot.\n   so the char data should be coalesced into the char4 type for improved bandwidth.\n * global memory accesses should be consecutive, but aligned accessing is not necessary for global memory.\n * when memory accessing is non-consecutive, the latency changes with the number of threads, but not with the number of blocks. so we should configure the thread dimensionality.\n\n\n# 5. exploring modern gpu memory system design challenges through accurate modeling\n\n👍 👍 👍\n\n# memory coalescer\n\nthe eviction granularity of the cache is 128b, indicating that the l1 cache has 128b lines with 32b sectors.\nfurthermore, the coalescer operates across eight threads, i.e. the coalescer tries to coalesce each group of eight threads separately to generate sectored accesses.\n\n\n\nwhen the stride=32, the memory access is converged, and all the threads within the same warp will access the same cache line,\nhowever we receive four read accesses at l1 cache.\n\n8 thread register 32bit == 32byte.\n\n# l2 cache\n\nl2 cache applies something similar to write-validate not fetch on write.\\ 😱 however, all the reads received by l2 caches from the coalescer are 32-byte sectored accesses.\nthus, the read access granularity (32 bytes) is different from the write access granularity (one byte).\nto handle this, the l2 cache applies a different write allocation policy, which we named lazy fetch-on-read, that is a compromise between write-validate and fetch-on-write.\n\nwhen a sector read request is received to a modified sector, it first checks if the sector write-mask is complete, i.e. all the bytes have been written to and the line is fully readable.\nif so, it reads the sector, otherwise, similar to fetch-on-write, it generates a read request for this sector and merges it with the modified bytes.\n\n# streaming throughput-oriented l1 cache\n\n\n\nthe l1 cache in volta is what nvidia is calling a streaming cache [33].\nit is streaming because the documentation states that it allows unlimited cache misses to be in flight regardless the number of cache lines per cache set [10].\n\nindependent of the number of l1 configured size, the number of mshrs available are the same, even if more of the on-chip sram storage is devoted to shared memory.\n\nwe believe that unified cache is a plain sram where sectored data blocks are shared between the l1d and the cuda shared memory.\nit can be configured adaptively by the driver as we discussed earlier.\nwe assume that the l1d’s tag and mshr merging functionality are combined together in a separate table structure (tag-mshr table).\nsince, the filling policy is now on fill, we can have more tag entries and outstanding requests than the assigned l1d cache lines.\n\nif it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps).\nwhen the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table.\nthen, the merged warps access the sector, on a cycle-by-cyle basis.\n\n\n# 6. osm: off-chip shared memory for gpus\n\n\n\nl1-d cache and shared memory use the same 32-bank memory structure (4 kb capacity per bank) as shown in fig. 4;\nhowever, they have some differences.\nwe can access 32-bit shared memory arrays via a thread-index directly, while for accessing l1-d cache, we should read 128b (four 32b sectors) of the cache block.\nin addition, l1 cache requires an extra hardware for managing tags and implementing lru replacement policy.",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"GPU Cache Coherency",frontmatter:{title:"GPU Cache Coherency",date:"2024-08-21T00:00:00.000Z",permalink:"/pages/45875/"},regularPath:"/03.gpu/18.gpucoherency.html",relativePath:"03.gpu/18.gpucoherency.md",key:"v-798f0c72",path:"/pages/45875/",headers:[{level:3,title:"1. [213] Cache Coherence for GPU Architectures",slug:"_1-213-cache-coherence-for-gpu-architectures",normalizedTitle:"1. [213] cache coherence for gpu architectures",charIndex:1}],headersStr:"1. [213] Cache Coherence for GPU Architectures",content:" 1. [213] Cache Coherence for GPU Architectures\n\n----------------------------------------\n\n\n# 1. [213] Cache Coherence for GPU Architectures\n\n# Another Design in CPU LLC\n\nLibrary Cache Coherence (LCC), that implements sequential consistency on CMPs by stalling writes to cache blocks until they have been self-invalidated by all sharers.\n\nLibrary Cache Coherence (LCC) [34, 54] is a time-based hardware coherence proposal that stores timestamps in a directory structure and delays stores to unexpired blocks to enforce sequential consistency on CMPs.\nThe TC-Strong implementation of the TC framework is similar to LCC as both enforce write atomicity by stalling writes at the shared last level cache.\n\n\n\n# Background\n\n\n\nWe propose TC-Weak and a novel time-based memory fence mechanism to eliminate all write-stalling, improve performance, and reduce interconnect traffic compared to TC-Strong.\n\n * We find that the stalling of writes in TC-Strong causes poor performance on a GPU.\n * We also show that unlike for CPU applications [34, 54], the fixed timestamp prediction proposed by LCC is not suited for GPU applications.\n\n# GPU Memory System\n\nBoth thread-private and global memory are stored in off-chip GDDR DRAM and cached in the multi-level cache hierarchy, however only global memory requires coherence.\n\nMemory accesses to the same cache block from different threads within a wavefront are merged into a single wide access by the Coalescing Unit.\nA memory instruction generates one memory access for every unique cache line accessed by the wavefront.\nAll requests are handled in FIFO order by the in-order memory stage of a GPU core.\n🙇 Writes to the same word by multiple scalar threads in a single wavefront do not have a defined behaviour [46]; only one write will succeed.\n\n# GPU Cache Hierarchy\n\nThe GPU cache hierarchy consists of per-core private L1 data caches and a shared L2 cache.\nThe L1 caches are not coherent.\nThey follow a write-evict [46] (write-purge [24]), write no-allocate caching policy.\nThe L2 caches are writeback with write-allocate.\n\nMemory accesses generated by the coalescing unit in each GPU core are passed, one per cycle, to the per-core MSHR table.\nThe MSHR table combines read accesses to the same cache line from different wavefronts to ensure only a single read access per-cache line per-GPU core is outstanding.\n\nWrites are not combined and, since they write-through, any number of write requests to the same cache line from a GPU core may be outstanding.\n\nPoint-to-point ordering in the interconnection network, L2 cache controllers and off-chip DRAM channels ensures that multiple outstanding writes from the same wavefront to the same address complete in program order.\n\nThis is another situation different from 🙇. It is different code in a program that write to the same address.\n\n# Atomic Operation\n\nAtomic Operation. Read-modify-write atomic operations are performed at each memory partition by an Atomic Operation Unit.\n\n# Consistency and Coherence\n\nA cache coherence protocol performs the following three duties [3].\n\n * It propagates newly written values to all privately cached copies.\n * It informs the writing thread or processor when a write has been completed and is visible to all threads and processors.\n * Lastly, a coherence protocol may ensure write atomicity [3], i.e., a value from a write is logically seen by all threads at once.\n\nWrite atomicity is commonly enforced in write-invalidate coherence protocols by requiring that all other copies of a cache block are invalidated before a write is completed.\n\nMemory consistency models may [4, 19, 57, 59] or may not [2, 19, 53] require write atomicity.\n\n# Directory Protocols\n\n# MESI\n\nfour-state coherence protocol with writeback L1 and L2 caches The write-allocate policy at L1 requires that write data be buffered until proper coherence permission has been obtained.\nThis requires the addition of area and complexity to buffer stores in each GPU core.\n\n# GPU-VI\n\ntwo-state coherence protocol GPU-VI implements write-through, no write-allocate L1 caches.\nIt requires that any write completing at the L2 invalidate all L1 copies.\nA write to a shared cache line cannot complete until the L2 controller has sent invalidation requests and received acknowledgments from all sharers.\n\nTwo Optimizations:\n\n * First, it writes data directly to the L1 cache on a write hit before receiving an acknowledgement, eliminating the area and complexity overheads of buffering stores.\n * Second, it treats loads to L1 blocks with pending writes as misses. This reduces stalling at the cache controller while maintaining write atomicity.\n\n# Challenges\n\n# Coherence Traffic\n\nThese overheads consist of recall traffic due to:\n\n * directory evictions,\n * false sharing invalidation traffic,\n * invalidation traffic due to inter-kernel communication.\n\nAn effective way to reduce coherence traffic is to selectively disable coherence for data regions that do not require it.\n\n# Storage Requirements\n\nIn a CPU-like coherence implementation [18] with enough storage to handle the worst case number of memory accesses (one memory request per thread), a directory protocol would require an impractical on-chip buffer as large as 28% of the total GPU L2 cache for tracking coherence requests.\n\n# Protocal Complexity\n\nstable states & Transient coherent states\n\n# Temporal Coherence\n\n\n\n\n\n# TC-strong Coherence\n\n\n\n# TC-weak Coherence\n\nTC-Weak relaxes the write atomicity of TC-Strong. As we show in Section 8.3, doing so improves performance by 28% and lowers interconnect traffic by 26% compared to TC-Strong.\n\nTC-Strong and LCC enforce coherence across all data by stalling writes.\nTC-Weak uses the insight that GPU applications may contain large amounts of data which does not require coherence and is unnecessarily penalized by write-stalling.\nBy relaxing write-atomicity, TC-Weak eliminates write-stalling and shifts any potential stalling to explicit memory fence operations.\n\nMajor two benefits：\n\n * First, it eliminates expensive stalling at the shared L2 cache controllers, which affects all cores and wavefronts, and shifts it to scheduling of individual wavefronts at memory fences.\n   A wavefront descheduled due to a memory fence does not affect the performance of other wavefronts.\n * Second, it enforces coherence only when required and specified by the program through memory fences. It implements the RCpc [19] consistency model; a detailed discussion on this is available elsewhere [56].\n\n\n\n# TC Strong:\n\nF1 defers scheduling the wavefront because the wavefront has an outstanding store request.\nWhen S1’s store request reaches the L2 ( 3 ), the L2 stalls it because data’s global timestamp will not expire until time T=30.\nAt T=30, C2 self-invalidates data ( 4 ), and the L2 processes S1’s store ( 5 ).\nThe fence instruction completes when C1 receives the acknowledgment for S1’s request ( 6 ).\n\n# TC Weak:\n\nThe write response returns with the global timestamp of the L2 cache line at the time of the write. The returned global timestamp is the guaranteed time by which the write will become visible to all cores in the system.\nThis is because by this time all cores will have invalidated their privately cached stale copies.\n\nIn this case, the value returned is 30 and corresponds to C2’s initially cached copy.\nThe L2 does not stall the write and sends back an acknowledgment with the GWCT, which updates the C1’s GWCT entry for this wavefront.\nAfter C1 receives the acknowledgment ( 4’ ), no memory requests are outstanding.\n\nComparing Figure 6(c) to 6(b) shows that TC-Weak performs better than TC-Strong because it only stalls at explicit memory fence operations.\nThis ensures that writes to data that does not require coherence has minimal impact.\n\nTC-Weak tracks the global timestamps returned by writes, called Global Write Completion Times (GWCT), for each wave-front.\nA memory fence operation uses this information to deschedule the wavefront sufficiently long enough to guar=antee that all previous writes from the wavefront have become globally visible.\n\n# Lifetime Prediction\n\n * we show that a single lifetime value for all accesses performs well.\n * Moreover, this value is application dependent.\n\na single lifetime prediction value at each L2 cache bank, and adjusts it based on application behaviour. A load obtains its lifetime prediction at the L2 bank.\n\nThe lifetime estimation is based on events local to L2 bank\n\n * L2 Block with unexpired timestamp evicted\n * load request miss in L1 due to expired\n * L2 receive a load request to a valid block with an expired global timestamp\n * store operation writes to an unexpired block at L2",normalizedContent:" 1. [213] cache coherence for gpu architectures\n\n----------------------------------------\n\n\n# 1. [213] cache coherence for gpu architectures\n\n# another design in cpu llc\n\nlibrary cache coherence (lcc), that implements sequential consistency on cmps by stalling writes to cache blocks until they have been self-invalidated by all sharers.\n\nlibrary cache coherence (lcc) [34, 54] is a time-based hardware coherence proposal that stores timestamps in a directory structure and delays stores to unexpired blocks to enforce sequential consistency on cmps.\nthe tc-strong implementation of the tc framework is similar to lcc as both enforce write atomicity by stalling writes at the shared last level cache.\n\n\n\n# background\n\n\n\nwe propose tc-weak and a novel time-based memory fence mechanism to eliminate all write-stalling, improve performance, and reduce interconnect traffic compared to tc-strong.\n\n * we find that the stalling of writes in tc-strong causes poor performance on a gpu.\n * we also show that unlike for cpu applications [34, 54], the fixed timestamp prediction proposed by lcc is not suited for gpu applications.\n\n# gpu memory system\n\nboth thread-private and global memory are stored in off-chip gddr dram and cached in the multi-level cache hierarchy, however only global memory requires coherence.\n\nmemory accesses to the same cache block from different threads within a wavefront are merged into a single wide access by the coalescing unit.\na memory instruction generates one memory access for every unique cache line accessed by the wavefront.\nall requests are handled in fifo order by the in-order memory stage of a gpu core.\n🙇 writes to the same word by multiple scalar threads in a single wavefront do not have a defined behaviour [46]; only one write will succeed.\n\n# gpu cache hierarchy\n\nthe gpu cache hierarchy consists of per-core private l1 data caches and a shared l2 cache.\nthe l1 caches are not coherent.\nthey follow a write-evict [46] (write-purge [24]), write no-allocate caching policy.\nthe l2 caches are writeback with write-allocate.\n\nmemory accesses generated by the coalescing unit in each gpu core are passed, one per cycle, to the per-core mshr table.\nthe mshr table combines read accesses to the same cache line from different wavefronts to ensure only a single read access per-cache line per-gpu core is outstanding.\n\nwrites are not combined and, since they write-through, any number of write requests to the same cache line from a gpu core may be outstanding.\n\npoint-to-point ordering in the interconnection network, l2 cache controllers and off-chip dram channels ensures that multiple outstanding writes from the same wavefront to the same address complete in program order.\n\nthis is another situation different from 🙇. it is different code in a program that write to the same address.\n\n# atomic operation\n\natomic operation. read-modify-write atomic operations are performed at each memory partition by an atomic operation unit.\n\n# consistency and coherence\n\na cache coherence protocol performs the following three duties [3].\n\n * it propagates newly written values to all privately cached copies.\n * it informs the writing thread or processor when a write has been completed and is visible to all threads and processors.\n * lastly, a coherence protocol may ensure write atomicity [3], i.e., a value from a write is logically seen by all threads at once.\n\nwrite atomicity is commonly enforced in write-invalidate coherence protocols by requiring that all other copies of a cache block are invalidated before a write is completed.\n\nmemory consistency models may [4, 19, 57, 59] or may not [2, 19, 53] require write atomicity.\n\n# directory protocols\n\n# mesi\n\nfour-state coherence protocol with writeback l1 and l2 caches the write-allocate policy at l1 requires that write data be buffered until proper coherence permission has been obtained.\nthis requires the addition of area and complexity to buffer stores in each gpu core.\n\n# gpu-vi\n\ntwo-state coherence protocol gpu-vi implements write-through, no write-allocate l1 caches.\nit requires that any write completing at the l2 invalidate all l1 copies.\na write to a shared cache line cannot complete until the l2 controller has sent invalidation requests and received acknowledgments from all sharers.\n\ntwo optimizations:\n\n * first, it writes data directly to the l1 cache on a write hit before receiving an acknowledgement, eliminating the area and complexity overheads of buffering stores.\n * second, it treats loads to l1 blocks with pending writes as misses. this reduces stalling at the cache controller while maintaining write atomicity.\n\n# challenges\n\n# coherence traffic\n\nthese overheads consist of recall traffic due to:\n\n * directory evictions,\n * false sharing invalidation traffic,\n * invalidation traffic due to inter-kernel communication.\n\nan effective way to reduce coherence traffic is to selectively disable coherence for data regions that do not require it.\n\n# storage requirements\n\nin a cpu-like coherence implementation [18] with enough storage to handle the worst case number of memory accesses (one memory request per thread), a directory protocol would require an impractical on-chip buffer as large as 28% of the total gpu l2 cache for tracking coherence requests.\n\n# protocal complexity\n\nstable states & transient coherent states\n\n# temporal coherence\n\n\n\n\n\n# tc-strong coherence\n\n\n\n# tc-weak coherence\n\ntc-weak relaxes the write atomicity of tc-strong. as we show in section 8.3, doing so improves performance by 28% and lowers interconnect traffic by 26% compared to tc-strong.\n\ntc-strong and lcc enforce coherence across all data by stalling writes.\ntc-weak uses the insight that gpu applications may contain large amounts of data which does not require coherence and is unnecessarily penalized by write-stalling.\nby relaxing write-atomicity, tc-weak eliminates write-stalling and shifts any potential stalling to explicit memory fence operations.\n\nmajor two benefits：\n\n * first, it eliminates expensive stalling at the shared l2 cache controllers, which affects all cores and wavefronts, and shifts it to scheduling of individual wavefronts at memory fences.\n   a wavefront descheduled due to a memory fence does not affect the performance of other wavefronts.\n * second, it enforces coherence only when required and specified by the program through memory fences. it implements the rcpc [19] consistency model; a detailed discussion on this is available elsewhere [56].\n\n\n\n# tc strong:\n\nf1 defers scheduling the wavefront because the wavefront has an outstanding store request.\nwhen s1’s store request reaches the l2 ( 3 ), the l2 stalls it because data’s global timestamp will not expire until time t=30.\nat t=30, c2 self-invalidates data ( 4 ), and the l2 processes s1’s store ( 5 ).\nthe fence instruction completes when c1 receives the acknowledgment for s1’s request ( 6 ).\n\n# tc weak:\n\nthe write response returns with the global timestamp of the l2 cache line at the time of the write. the returned global timestamp is the guaranteed time by which the write will become visible to all cores in the system.\nthis is because by this time all cores will have invalidated their privately cached stale copies.\n\nin this case, the value returned is 30 and corresponds to c2’s initially cached copy.\nthe l2 does not stall the write and sends back an acknowledgment with the gwct, which updates the c1’s gwct entry for this wavefront.\nafter c1 receives the acknowledgment ( 4’ ), no memory requests are outstanding.\n\ncomparing figure 6(c) to 6(b) shows that tc-weak performs better than tc-strong because it only stalls at explicit memory fence operations.\nthis ensures that writes to data that does not require coherence has minimal impact.\n\ntc-weak tracks the global timestamps returned by writes, called global write completion times (gwct), for each wave-front.\na memory fence operation uses this information to deschedule the wavefront sufficiently long enough to guar=antee that all previous writes from the wavefront have become globally visible.\n\n# lifetime prediction\n\n * we show that a single lifetime value for all accesses performs well.\n * moreover, this value is application dependent.\n\na single lifetime prediction value at each l2 cache bank, and adjusts it based on application behaviour. a load obtains its lifetime prediction at the l2 bank.\n\nthe lifetime estimation is based on events local to l2 bank\n\n * l2 block with unexpired timestamp evicted\n * load request miss in l1 due to expired\n * l2 receive a load request to a valid block with an expired global timestamp\n * store operation writes to an unexpired block at l2",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"GPU TLB",frontmatter:{title:"GPU TLB",date:"2024-08-26T00:00:00.000Z",permalink:"/pages/45877/"},regularPath:"/03.gpu/20.gpu_tlb_ptw.html",relativePath:"03.gpu/20.gpu_tlb_ptw.md",key:"v-c2e69736",path:"/pages/45877/",headers:[{level:3,title:"1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring",slug:"_1-dissecting-the-nvidia-volta-gpu-architecture-via-microbenchmaring",normalizedTitle:"1. dissecting the nvidia volta gpu architecture via microbenchmaring",charIndex:202},{level:3,title:"2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs",slug:"_2-snakebyte-a-tlb-design-with-adaptive-and-recursive-page-merging-in-gpus",normalizedTitle:"2. snakebyte: a tlb design with adaptive and recursive page merging in gpus",charIndex:438}],headersStr:"1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring 2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs",content:" 1. [90] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring\n 2. [6] SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs\n\n----------------------------------------\n\n\n# 1. Dissecting the NVIDIA Volta GPU Architecture via Microbenchmaring\n\nOn Volta and on all other architectures we examined:\n\n * the L1 data cache is indexed by virtual addresses;\n * the L2 data cache is indexed by physical addresses\n\n\n# 2. SnakeByte: A TLB Design with Adaptive and Recursive Page Merging in GPUs\n\n# Idea\n\nSnakeByte allows multiple equal-sized pages coalescing into a page table entry (PTE).\nIt records the validity of pages to be merged using a bit vector, and few bits are annexed to indicate the size of merged pages.\n\n# TLB & PTW & GMMU\n\nDeparting from conventional paging schemes of CPUs that heavily rely on operating systems, hardware-based GPU memory management units (GMMUs) are essential to effectively separate device memory management from host CPUs.\nOtherwise, GPUs require the frequent intervention of OS to handle page table walks (PTWs) and TLB misses, which significantly penalize the GPU performance.\n\nObservations:\n\n * GPU workloads demand a large number of TLB entries (e.g., 32K to 256K entries) to handle sizable working sets, but conventional TLBs cannot provide sufficient coverage.\n * GPU workloads have variable ranges of page contiguity.\n\n\n\n# Address Translation\n\nAn L1 TLB is private to a streaming multiprocessor (SM), and an L2 TLB is shared among SMs [41], [42].\nOn a last-level TLB miss, a request is sent to a centralized GMMU [18], [41], [42] to walk through page tables, and the GMMU concurrently handles multiple PTW requests (e.g., 8-16 PTWs).\nTo amortize the latency cost of PTWs, GPUs employ page walk caches that store recently used translations at different levels of page tables.\nImportantly, the GMMU execution has to be independent of host-side operations unlike the conventional paging schemes of CPUs that heavily rely on operating systems.\nOtherwise, GPUs involve frequent OS interventions, which significantly penalize the GPU performance [44], [54].\n\nThis observation is the primary motivation of SnakeByte that can flexibly manage variable-sized page groups and maximize TLB reach.\n\n\n\n\n\nWhen eight 4KB pages are allocated with contiguity, the page group is promoted to be coalesced into the next level of page group.\n\nAt the new page allocation, SnakeByte checks the contiguity of the new PTE with others in the page group.\n\n# Simulation\n\n * By recursively coalescing PTEs, SnakeByte inevitably loses fine-grained controls on the A/D bits for individual pages.\n   SnakeByte adds 8-bit access and dirty fields to a TLB entry to trace A/D states within a page group.\n * GPUs have long shootdown delays (4.2us).\n * The TLB hierarchy consists of a private L1 TLB per SM, a shared L2 TLB, and miss status holding registers (MSHRs).\n   An MSHR in an L1 TLB merges up to 16 misses.\n * 16 page table walkers can concurrently access four-level page tables, and a page walk cache per page table level stores up to 16 recently used translations.\n * When a new page is allocated, a sequential page prefetcher allocates 16 consecutive pages (total 64KB) at a time.\n * To analyze the effect of page migration latency [9], [55], we add a 20us latency overhead for each 4KB page fault [55] with 8.48GB/s bandwidth for a 64KB prefetcher [18].",normalizedContent:" 1. [90] dissecting the nvidia volta gpu architecture via microbenchmaring\n 2. [6] snakebyte: a tlb design with adaptive and recursive page merging in gpus\n\n----------------------------------------\n\n\n# 1. dissecting the nvidia volta gpu architecture via microbenchmaring\n\non volta and on all other architectures we examined:\n\n * the l1 data cache is indexed by virtual addresses;\n * the l2 data cache is indexed by physical addresses\n\n\n# 2. snakebyte: a tlb design with adaptive and recursive page merging in gpus\n\n# idea\n\nsnakebyte allows multiple equal-sized pages coalescing into a page table entry (pte).\nit records the validity of pages to be merged using a bit vector, and few bits are annexed to indicate the size of merged pages.\n\n# tlb & ptw & gmmu\n\ndeparting from conventional paging schemes of cpus that heavily rely on operating systems, hardware-based gpu memory management units (gmmus) are essential to effectively separate device memory management from host cpus.\notherwise, gpus require the frequent intervention of os to handle page table walks (ptws) and tlb misses, which significantly penalize the gpu performance.\n\nobservations:\n\n * gpu workloads demand a large number of tlb entries (e.g., 32k to 256k entries) to handle sizable working sets, but conventional tlbs cannot provide sufficient coverage.\n * gpu workloads have variable ranges of page contiguity.\n\n\n\n# address translation\n\nan l1 tlb is private to a streaming multiprocessor (sm), and an l2 tlb is shared among sms [41], [42].\non a last-level tlb miss, a request is sent to a centralized gmmu [18], [41], [42] to walk through page tables, and the gmmu concurrently handles multiple ptw requests (e.g., 8-16 ptws).\nto amortize the latency cost of ptws, gpus employ page walk caches that store recently used translations at different levels of page tables.\nimportantly, the gmmu execution has to be independent of host-side operations unlike the conventional paging schemes of cpus that heavily rely on operating systems.\notherwise, gpus involve frequent os interventions, which significantly penalize the gpu performance [44], [54].\n\nthis observation is the primary motivation of snakebyte that can flexibly manage variable-sized page groups and maximize tlb reach.\n\n\n\n\n\nwhen eight 4kb pages are allocated with contiguity, the page group is promoted to be coalesced into the next level of page group.\n\nat the new page allocation, snakebyte checks the contiguity of the new pte with others in the page group.\n\n# simulation\n\n * by recursively coalescing ptes, snakebyte inevitably loses fine-grained controls on the a/d bits for individual pages.\n   snakebyte adds 8-bit access and dirty fields to a tlb entry to trace a/d states within a page group.\n * gpus have long shootdown delays (4.2us).\n * the tlb hierarchy consists of a private l1 tlb per sm, a shared l2 tlb, and miss status holding registers (mshrs).\n   an mshr in an l1 tlb merges up to 16 misses.\n * 16 page table walkers can concurrently access four-level page tables, and a page walk cache per page table level stores up to 16 recently used translations.\n * when a new page is allocated, a sequential page prefetcher allocates 16 consecutive pages (total 64kb) at a time.\n * to analyze the effect of page migration latency [9], [55], we add a 20us latency overhead for each 4kb page fault [55] with 8.48gb/s bandwidth for a 64kb prefetcher [18].",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"checkpoint",frontmatter:{title:"checkpoint",date:"2023-11-09T15:32:49.000Z",permalink:"/pages/cc7035/"},regularPath:"/04.cpu/01.checkpoint.html",relativePath:"04.cpu/01.checkpoint.md",key:"v-121d90a9",path:"/pages/cc7035/",headersStr:null,content:"An Analysis of a Resource Efficient Checkpoint Architecture [Intel]\n\nThe main part that I like is the discussion\n\n(1) Using map table checkpoints. Map table checkpoints are created periodically either at every branch or every few cycles [Leibholz and Razdan 1997; Yeager 1996]. On a misprediction, the checkpoint corresponding to the mispredicted branch is restored. The number of checkpoints limits the number of unresolved branches allowed in the instruction window.\n\n(2) Using the retirement map table (RMAP). In this scheme, a retirement map table [Hinton et al. 2001] is used in addition to the frontend map table. Each ROB entry also has the rename map for its corresponding instruction. Once a misprediction is resolved, the mispredicted branch is allowed to reach the head of the ROB at which time the retirement map table will have the correct map table corresponding to the mispredicted branch. At this point, the retirement map table is copied to the frontend map table, after which renaming can start. Since all instructions prior to the mispredicted branch must be retired before renaming can start, this scheme can lead to significant delays if long latency operations prior to the mispredicted branch stall retirement.\n\n(3) Using the retirement map table and the ROB (RMAP+WALK). This scheme is an optimization on the scheme above. Instead of waiting for the mispredicted branch to reach the head of the ROB, we start with the current retirement map table and pro-actively walk from the head of the ROB toward the mispredicted branch, incorporating the rename information of each ROB entry. This allows renaming of correct path instructions to commence without waiting for all instructions prior to the mispredicted branch to retire. (4) Using the frontend map table and a history buffer (HBMAP+WALK). In this scheme, a history buffer is used to store overwritten maps of each instruction. On a branch misprediction, we start with the current frontend map table. We pro-actively walk from the current tail of the ROB (i.e., the most recently allocated instruction) toward the mispredicted branch, incorporating the overwritten maps of each instruction. Depending on whether the mispredicted branch is closer to the ROB head or ROB tail, RMAP + WALK, or HBMAP + WALK will perform better.\n\nIn short:\n\n1. checkpoint generated at the moment of decoding branch instruction. Recover at detection of missprediction.\n\n2. use retire map table(RMAP). wait the commit of missprediction instruction, then rewrite the RAT(remap alias table) with RMAP\n\n3. RMAP + WALK. Restart from the moment that miss prediction is detected, copy the RMAP into RAT and then modify RMAP with ROB remapping, until we get to the miss predicted intruction.\n\n4. HBMAP + WALK. Start from frontend RAT(FRAT), use history buffer to recover the overwritten register relation.\n\nRMAP+WALK utilize commited RMAP, thus it walks from the head of ROB (oldest) to the branch instruction. HBMAP+WALK utilize frontend RAT, thus it walks from the end of ROB (youngest) to the branch instruction.",normalizedContent:"an analysis of a resource efficient checkpoint architecture [intel]\n\nthe main part that i like is the discussion\n\n(1) using map table checkpoints. map table checkpoints are created periodically either at every branch or every few cycles [leibholz and razdan 1997; yeager 1996]. on a misprediction, the checkpoint corresponding to the mispredicted branch is restored. the number of checkpoints limits the number of unresolved branches allowed in the instruction window.\n\n(2) using the retirement map table (rmap). in this scheme, a retirement map table [hinton et al. 2001] is used in addition to the frontend map table. each rob entry also has the rename map for its corresponding instruction. once a misprediction is resolved, the mispredicted branch is allowed to reach the head of the rob at which time the retirement map table will have the correct map table corresponding to the mispredicted branch. at this point, the retirement map table is copied to the frontend map table, after which renaming can start. since all instructions prior to the mispredicted branch must be retired before renaming can start, this scheme can lead to significant delays if long latency operations prior to the mispredicted branch stall retirement.\n\n(3) using the retirement map table and the rob (rmap+walk). this scheme is an optimization on the scheme above. instead of waiting for the mispredicted branch to reach the head of the rob, we start with the current retirement map table and pro-actively walk from the head of the rob toward the mispredicted branch, incorporating the rename information of each rob entry. this allows renaming of correct path instructions to commence without waiting for all instructions prior to the mispredicted branch to retire. (4) using the frontend map table and a history buffer (hbmap+walk). in this scheme, a history buffer is used to store overwritten maps of each instruction. on a branch misprediction, we start with the current frontend map table. we pro-actively walk from the current tail of the rob (i.e., the most recently allocated instruction) toward the mispredicted branch, incorporating the overwritten maps of each instruction. depending on whether the mispredicted branch is closer to the rob head or rob tail, rmap + walk, or hbmap + walk will perform better.\n\nin short:\n\n1. checkpoint generated at the moment of decoding branch instruction. recover at detection of missprediction.\n\n2. use retire map table(rmap). wait the commit of missprediction instruction, then rewrite the rat(remap alias table) with rmap\n\n3. rmap + walk. restart from the moment that miss prediction is detected, copy the rmap into rat and then modify rmap with rob remapping, until we get to the miss predicted intruction.\n\n4. hbmap + walk. start from frontend rat(frat), use history buffer to recover the overwritten register relation.\n\nrmap+walk utilize commited rmap, thus it walks from the head of rob (oldest) to the branch instruction. hbmap+walk utilize frontend rat, thus it walks from the end of rob (youngest) to the branch instruction.",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"topdown analysis",frontmatter:{title:"topdown analysis",date:"2024-07-15T15:32:49.000Z",permalink:"/pages/cc7036/"},regularPath:"/04.cpu/02.topdown.html",relativePath:"04.cpu/02.topdown.md",key:"v-3e3c1a05",path:"/pages/cc7036/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"load store unit",frontmatter:{title:"load store unit",date:"2024-07-15T15:32:49.000Z",permalink:"/pages/cc7037/"},regularPath:"/04.cpu/03.loadstore.html",relativePath:"04.cpu/03.loadstore.md",key:"v-891a4db6",path:"/pages/cc7037/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"cache & bank structure",frontmatter:{title:"cache & bank structure",date:"2024-07-16T15:32:49.000Z",permalink:"/pages/cc7038/"},regularPath:"/04.cpu/05.cache%20structure.html",relativePath:"04.cpu/05.cache structure.md",key:"v-64b7ef30",path:"/pages/cc7038/",headers:[{level:3,title:"Cache",slug:"cache",normalizedTitle:"cache",charIndex:2},{level:3,title:"Timing",slug:"timing",normalizedTitle:"timing",charIndex:4764},{level:3,title:"References",slug:"references",normalizedTitle:"references",charIndex:5151}],headersStr:"Cache Timing References",content:"# Cache\n\n# Structure\n\n[1]\n\n[1]\n\n * Bank - A memory structure that consists of a data and a tag array. A cache is typically split into multiple banks and CACTI assumes enough bandwidth so that these banks can be accessed simultaneously. The network topology that interconnects these banks can vary depending on the cache model.\n * Sub-arrays - A data or tag array is divided into a number of sub-arrays to reduce the delay due to wordline and bitline. Unlike banks, at any given time, these sub-arrays support only one single access. The total number of sub-arrays in a cache is equal to the product of Ndwl and Ndbl.\n * Mat - A group of four sub-arrays (2x2) that share a common central predecoder. CACTI’s exhaustive search starts from a minimum of at least one mat.\n * Sub-bank - In a typical cache, a cache block is scattered across multiple sub-arrays to improve the reliability of a cache. Irrespective of the cache organization, CACTI assumes that every cache block in a cache is distributed across an entire row of mats and the row number corresponding to a particular block is determined based on the block address. Each row (of mats) in an array is referred to as a sub-bank.\n\n# why do we needs bank\n\n[2] Support for more than one access to a memory structure at the same time can be provided by adding more ports to each memory cell.\n\nMultiporting a cache makes the cache capable of handling as many read or write requests as ports of that type.\n\nBut multiporting a cache makes each bit cell much bigger and so the overall area of the memory array can increase enormously for large number of ports. The extra length spanned by the cache also adds directly to the access time and power consumed by the cache.\n\nAnother way of supporting simultaneous multiple accesses to a cache is by banking with fully independent banks.\n\nEach bank does not share address and data.\n\nBanking also adds the decoding overhead of routing the appropriate address to the right bank and detecting collisions.\n\n[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n# How to supply 64Byte at each time\n\nEach sub-array outputs 128 bits, all 4 sub-array in the mat output 512bit together. [3]\n\n[4]\n\nA bank consists of Nsubbanks of identical sub-banks that are activated sequentially with each access. In turn, each sub-bank contains a number of identical mats.\n\nA mat is a self-contained, compact memory array composed of four identical sub-arrays, with Nrows rows and Ncols columns, and accompanying predecoding logic, with each sub-array being a two-dimensional matrix of memory cells and associated peripheral circuitry.\n\nEach mat holds a portion of a word in one of its four sub-arrays; during cache access, all mats in a sub-bank are activated to form the whole word.\n\nBy whole word, he means a cacheline.\n\nH-tree distribution networks are often used to deliver address and data to mats.\n\nIn the following figure, each sub-array outputs 8 bits(1 byte), all mats in a subbank makesup 64 bytes.\n\n\n\n[5]\n\n\n\nwhy is there 10 bit in the above figure?\n\nParity and Error Correcting Codes(ECC).\n\n\n\n[5] discussed the same flow.\n\nConsidering a number of input ports and a different/same number of banks, the controller detects the desired bank accesses, arbitrates eventual bank conflicts, and generates the required crossbar switches selection signals.\n\nAt the highest level the address space is split across identical banks, four in this example, with each bank having its own address and data bus, thus allowing for concurrent bank accesses.\n\nEach bank is composed of identical sub-banks, again four in this example, with only one being active per access.\n\nFurther, each sub-bank is partitioned into multiple mats that simultaneously provide parts of the required data (cache block in a cache data array).\n\nFinally, each mat is composed of four identical sub-arrays that share predecoding/decoding logic and peripheral circuitry, and which again deliver together the requested data.\n\nAn H-tree routing distribution network is used to drive addresses and data to/from the banks, and also to/from every mat inside a bank.\n\n# How to index into bank\n\n[6]In particular, the cache index, consisting of line number (LN) and line offset (LO), is divided into two portions: bank-internal address (BA) and bank index (BI). BA selects a cache word or tag within a 1-port-memory-cell bank, and BI selects the respective bank within data/instruction or tag memory. BI uses the lower rank bits to assure that consecutive cache-lines and words within the same line are interleaved and located in different banks.\n\n\n\n[7]\n\n\n\n\n\n# conclusion\n\nIn summary, mat is the mininum unit to provide data. and all mat in a subbank provides whole cache line.\n\nBank can only be accessed serially. Multi bank can provide mutiple request parallely.\n\n\n# Timing\n\n[1] CACTI models the delay/power/area of eight major cache components: decoder, wordline, bitline, senseamp, comparator, multiplexor, output driver, and inter-bank wires. The wordline and bitline delays are two of the most significant components of the access time. The wordline and bitline delays are quadratic functions of the width and height of each array, respectively.\n\n\n# References\n\n[1] CACTI 6.0: A Tool to Understand Large Caches\n\n[2] CACTI 3.0: An Integrated Cache Timing, Power, and Area Model\n\n[3] Flexicache: Highly Reliable and Low Power Cache\n\n[4] Best Memory Architecture Exploration under Parameters Variations accelerated with Machine Learning\n\n[5] A Shared Polyhedral Cache for 3D Wide-I/O Multi-core Computing Platforms\n\n[6] 4-Port Unified Data/Instruction Cache Design with Distributed Crossbar and Interleaved Cache-Line Words\n\n[7] Unified Data/Instruction Cache with Bank-Based Multi-Port Architecture",normalizedContent:"# cache\n\n# structure\n\n[1]\n\n[1]\n\n * bank - a memory structure that consists of a data and a tag array. a cache is typically split into multiple banks and cacti assumes enough bandwidth so that these banks can be accessed simultaneously. the network topology that interconnects these banks can vary depending on the cache model.\n * sub-arrays - a data or tag array is divided into a number of sub-arrays to reduce the delay due to wordline and bitline. unlike banks, at any given time, these sub-arrays support only one single access. the total number of sub-arrays in a cache is equal to the product of ndwl and ndbl.\n * mat - a group of four sub-arrays (2x2) that share a common central predecoder. cacti’s exhaustive search starts from a minimum of at least one mat.\n * sub-bank - in a typical cache, a cache block is scattered across multiple sub-arrays to improve the reliability of a cache. irrespective of the cache organization, cacti assumes that every cache block in a cache is distributed across an entire row of mats and the row number corresponding to a particular block is determined based on the block address. each row (of mats) in an array is referred to as a sub-bank.\n\n# why do we needs bank\n\n[2] support for more than one access to a memory structure at the same time can be provided by adding more ports to each memory cell.\n\nmultiporting a cache makes the cache capable of handling as many read or write requests as ports of that type.\n\nbut multiporting a cache makes each bit cell much bigger and so the overall area of the memory array can increase enormously for large number of ports. the extra length spanned by the cache also adds directly to the access time and power consumed by the cache.\n\nanother way of supporting simultaneous multiple accesses to a cache is by banking with fully independent banks.\n\neach bank does not share address and data.\n\nbanking also adds the decoding overhead of routing the appropriate address to the right bank and detecting collisions.\n\n[2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n# how to supply 64byte at each time\n\neach sub-array outputs 128 bits, all 4 sub-array in the mat output 512bit together. [3]\n\n[4]\n\na bank consists of nsubbanks of identical sub-banks that are activated sequentially with each access. in turn, each sub-bank contains a number of identical mats.\n\na mat is a self-contained, compact memory array composed of four identical sub-arrays, with nrows rows and ncols columns, and accompanying predecoding logic, with each sub-array being a two-dimensional matrix of memory cells and associated peripheral circuitry.\n\neach mat holds a portion of a word in one of its four sub-arrays; during cache access, all mats in a sub-bank are activated to form the whole word.\n\nby whole word, he means a cacheline.\n\nh-tree distribution networks are often used to deliver address and data to mats.\n\nin the following figure, each sub-array outputs 8 bits(1 byte), all mats in a subbank makesup 64 bytes.\n\n\n\n[5]\n\n\n\nwhy is there 10 bit in the above figure?\n\nparity and error correcting codes(ecc).\n\n\n\n[5] discussed the same flow.\n\nconsidering a number of input ports and a different/same number of banks, the controller detects the desired bank accesses, arbitrates eventual bank conflicts, and generates the required crossbar switches selection signals.\n\nat the highest level the address space is split across identical banks, four in this example, with each bank having its own address and data bus, thus allowing for concurrent bank accesses.\n\neach bank is composed of identical sub-banks, again four in this example, with only one being active per access.\n\nfurther, each sub-bank is partitioned into multiple mats that simultaneously provide parts of the required data (cache block in a cache data array).\n\nfinally, each mat is composed of four identical sub-arrays that share predecoding/decoding logic and peripheral circuitry, and which again deliver together the requested data.\n\nan h-tree routing distribution network is used to drive addresses and data to/from the banks, and also to/from every mat inside a bank.\n\n# how to index into bank\n\n[6]in particular, the cache index, consisting of line number (ln) and line offset (lo), is divided into two portions: bank-internal address (ba) and bank index (bi). ba selects a cache word or tag within a 1-port-memory-cell bank, and bi selects the respective bank within data/instruction or tag memory. bi uses the lower rank bits to assure that consecutive cache-lines and words within the same line are interleaved and located in different banks.\n\n\n\n[7]\n\n\n\n\n\n# conclusion\n\nin summary, mat is the mininum unit to provide data. and all mat in a subbank provides whole cache line.\n\nbank can only be accessed serially. multi bank can provide mutiple request parallely.\n\n\n# timing\n\n[1] cacti models the delay/power/area of eight major cache components: decoder, wordline, bitline, senseamp, comparator, multiplexor, output driver, and inter-bank wires. the wordline and bitline delays are two of the most significant components of the access time. the wordline and bitline delays are quadratic functions of the width and height of each array, respectively.\n\n\n# references\n\n[1] cacti 6.0: a tool to understand large caches\n\n[2] cacti 3.0: an integrated cache timing, power, and area model\n\n[3] flexicache: highly reliable and low power cache\n\n[4] best memory architecture exploration under parameters variations accelerated with machine learning\n\n[5] a shared polyhedral cache for 3d wide-i/o multi-core computing platforms\n\n[6] 4-port unified data/instruction cache design with distributed crossbar and interleaved cache-line words\n\n[7] unified data/instruction cache with bank-based multi-port architecture",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"cache timing",frontmatter:{title:"cache timing",date:"2024-07-16T15:32:49.000Z",permalink:"/pages/cc7039/"},regularPath:"/04.cpu/06.cache%20timing.html",relativePath:"04.cpu/06.cache timing.md",key:"v-90037b60",path:"/pages/cc7039/",headers:[{level:3,title:"Analysis",slug:"analysis",normalizedTitle:"analysis",charIndex:2618},{level:3,title:"An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches [[2]](#2)",slug:"an-adaptive-non-uniform-cache-structure-for-wire-delay-dominated-on-chip-caches-2",normalizedTitle:'an adaptive, non-uniform cache structure for wire-delay dominated on-chip caches <a href="#2">[2]</a>',charIndex:null},{level:3,title:"References",slug:"references",normalizedTitle:"references",charIndex:6629}],headersStr:"Analysis An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches [[2]](#2) References",content:"Non-Uniform Cache Architecture (NUCA). The idea is to split the cache into a large number of banks and employ an on-chip network for communication. Now the access time of the cache is a function of distance between the bank and cache controller rather than the latency of the slowest subbank.\n\n * The access latency of a cache depends on delays in the decoders, word-lines, bit-lines, and drivers.\n * Decoder and driver delay components of a cache go up with increase in the number of sub-banks.\n * On the other hand, word line or bit line delay components reduce with decrease in subbank size in the vertical and horizontal directions, respectively.\n * large caches in future processors will have an additional overhead of network delay. Due to the growing disparity between wire and transistor delay, this factor will continue to dominate with technology improvements. T\n\n[1]\n\nCACTI divides the total access time of the cache into seven main components (decoder, wordline, bitline, senseamp, comparator, multiplexer-driver, and output driver).\n\n# senseamp and comparator delay\n\nOf these, senseamp and comparator delay is almost constant across different cache sizes and its contribution to the total access time reduces with increase in cache size.\n\n# multiplexer\n\nThe mux-driver delay component consists of delay due to multiplexer logic (to select the appropriate line) and driver delay to route the control signal to the output driver.\n\nThe latter part is proportional to the size of the cache.\n\n# Decoder\n\nThe decoder part of the cache consists of a single centralized pre-decoder and a separate decoder for each subarray.\n\nThe output from the pre-decoder is fed to the final decoder stage to drive the appropriate wordline.\n\nThus, the decoder delay component is the sum of time to route address bits to the central predecoder, time to route the output of the pre-decoder to the finalstage decoder, and the logic delay associated with predecoder, decoder, and driver circuits. Thus, decoder delay depends on both cache size and subarray count.\n\n# wordline bit delay\n\nThe wordline delay of data/tag array is proportional to the length of the array and the bit line delay is proportional to the height of the array.\n\nThese two delay values can be tweaked by adjusting the aspect ratio of the sub-array.\n\nTo bring down the delay values of both these components the cache is split into a number of sub-arrays.\n\nBut, with an increased number of sub-arrays, the latency to send signals to and from the central pre-decoder increases.\n\nThus, there exists a trade-off between the sub-array size and the wire length.\n\n\n# Analysis\n\n\n\nbank access latency increases exponentially with a decrease in bank count value. This is because as the size of the cache increases, the decoder delay component (that includes wiring delay) increases significantly. Also, the bank access time saturates beyond a bank count value of 512 (bank size of 64KB).\n\nBeyond this point, the latency is primarily due to logic delay associated with each stage, which is constant across different cache sizes.\n\nThe average (uncontended) network latency plotted in the graph is obtained by calculating the access time to each indvidual bank and averaging them against the total bank count value. This value depends on both the bank size and total number of banks.\n\n> Please notice, in the following statement, hop latency means the hop latency for each hop. There is another variable hop counts. The network latency is determined by hop latency * hop count.\n\nIt can be observed that the average latency first goes down with an increase in bank count and then starts increasing for large bank count values. If the bank size is extremely large, the hop latency dominates the total access time and hence the network latency is very high.\n\nIdeally, the network latency should go down with an increase in bank count. But, dividing the bank into half only reduces the area of data and tag arrays.\n\nOther constant area overheads associated with each bank will remain the same and hence the reduction in hop latency is less than half its original value.\n\nFor very large bank count values, the reduction in hop latency is usually less than the increase in hop count to reach a destination, leading to high average network latencies. The only exception is a change in bank count value from 1024 to 2048 – because hop latencies are rounded up to the next integer value, a doubling in bank count results in halving the vertical hop latency.\n\nThus, finding the optimal bank count value is critical to achieving the least possible access latency.\n\n\n# An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches [2]\n\nCitation over 1000.\n\nData residing in the part of a large cache close to the processor could be accessed much faster than data that reside physically farther from the processor. For example, the closest bank in a 16-megabyte, on-chip L2 cache built in a 50-nanometer process technology could be accessed in 4 cycles, while an access to the farthest bank might take 47 cycles.\n\n\n\n\n\n\n\n\n\n * Simple mapping: each column of banks in the cache becomes a bank set, and all banks within that column comprise the set-associative ways. Thus, the cache may be searched for a line by first selecting the bank column, selecting the set within the column, and finally performing a tag match on banks within that column of the cache. The two drawbacks of this scheme are that\n   \n   * the number of rows may not correspond to the number of desired ways in each bank set\n   * latencies to access all bank sets are not the same; some bank sets will be faster than others, since some rows are closer to the cache controller than others.\n\n * fair mapping policy, which addresses both problems of the simple mapping policy at the cost of additional complexity.\n\n * shared mapping policy, attempts to provide fastest-bank access to all bank sets by sharing the closest banks among multiple bank sets.\n\nThe policies we explore for D-NUCA consist of four major components: (1) Mapping: simple or shared.\n\n(2) Search: multicast, incremental, or combination. We restrict the combined policies such that a block set is partitioned into just two groups, which may then each vary in size (number of blocks) and the method of access (incremental or multicast).\n\n(3) Promotion: described by promotion distance, measured in cache banks, and promotion trigger, measured in number of hits to a bank before a promotion occurs.\n\n(4) Insertion: identifies the location to place an incoming block and what to do with the block it replaces (zero copy or one copy policies).\n\n\n# References\n\n[1] The Effect of Interconnect Design on the Performance of Large L2 Caches\n\n[2] An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches",normalizedContent:"non-uniform cache architecture (nuca). the idea is to split the cache into a large number of banks and employ an on-chip network for communication. now the access time of the cache is a function of distance between the bank and cache controller rather than the latency of the slowest subbank.\n\n * the access latency of a cache depends on delays in the decoders, word-lines, bit-lines, and drivers.\n * decoder and driver delay components of a cache go up with increase in the number of sub-banks.\n * on the other hand, word line or bit line delay components reduce with decrease in subbank size in the vertical and horizontal directions, respectively.\n * large caches in future processors will have an additional overhead of network delay. due to the growing disparity between wire and transistor delay, this factor will continue to dominate with technology improvements. t\n\n[1]\n\ncacti divides the total access time of the cache into seven main components (decoder, wordline, bitline, senseamp, comparator, multiplexer-driver, and output driver).\n\n# senseamp and comparator delay\n\nof these, senseamp and comparator delay is almost constant across different cache sizes and its contribution to the total access time reduces with increase in cache size.\n\n# multiplexer\n\nthe mux-driver delay component consists of delay due to multiplexer logic (to select the appropriate line) and driver delay to route the control signal to the output driver.\n\nthe latter part is proportional to the size of the cache.\n\n# decoder\n\nthe decoder part of the cache consists of a single centralized pre-decoder and a separate decoder for each subarray.\n\nthe output from the pre-decoder is fed to the final decoder stage to drive the appropriate wordline.\n\nthus, the decoder delay component is the sum of time to route address bits to the central predecoder, time to route the output of the pre-decoder to the finalstage decoder, and the logic delay associated with predecoder, decoder, and driver circuits. thus, decoder delay depends on both cache size and subarray count.\n\n# wordline bit delay\n\nthe wordline delay of data/tag array is proportional to the length of the array and the bit line delay is proportional to the height of the array.\n\nthese two delay values can be tweaked by adjusting the aspect ratio of the sub-array.\n\nto bring down the delay values of both these components the cache is split into a number of sub-arrays.\n\nbut, with an increased number of sub-arrays, the latency to send signals to and from the central pre-decoder increases.\n\nthus, there exists a trade-off between the sub-array size and the wire length.\n\n\n# analysis\n\n\n\nbank access latency increases exponentially with a decrease in bank count value. this is because as the size of the cache increases, the decoder delay component (that includes wiring delay) increases significantly. also, the bank access time saturates beyond a bank count value of 512 (bank size of 64kb).\n\nbeyond this point, the latency is primarily due to logic delay associated with each stage, which is constant across different cache sizes.\n\nthe average (uncontended) network latency plotted in the graph is obtained by calculating the access time to each indvidual bank and averaging them against the total bank count value. this value depends on both the bank size and total number of banks.\n\n> please notice, in the following statement, hop latency means the hop latency for each hop. there is another variable hop counts. the network latency is determined by hop latency * hop count.\n\nit can be observed that the average latency first goes down with an increase in bank count and then starts increasing for large bank count values. if the bank size is extremely large, the hop latency dominates the total access time and hence the network latency is very high.\n\nideally, the network latency should go down with an increase in bank count. but, dividing the bank into half only reduces the area of data and tag arrays.\n\nother constant area overheads associated with each bank will remain the same and hence the reduction in hop latency is less than half its original value.\n\nfor very large bank count values, the reduction in hop latency is usually less than the increase in hop count to reach a destination, leading to high average network latencies. the only exception is a change in bank count value from 1024 to 2048 – because hop latencies are rounded up to the next integer value, a doubling in bank count results in halving the vertical hop latency.\n\nthus, finding the optimal bank count value is critical to achieving the least possible access latency.\n\n\n# an adaptive, non-uniform cache structure for wire-delay dominated on-chip caches [2]\n\ncitation over 1000.\n\ndata residing in the part of a large cache close to the processor could be accessed much faster than data that reside physically farther from the processor. for example, the closest bank in a 16-megabyte, on-chip l2 cache built in a 50-nanometer process technology could be accessed in 4 cycles, while an access to the farthest bank might take 47 cycles.\n\n\n\n\n\n\n\n\n\n * simple mapping: each column of banks in the cache becomes a bank set, and all banks within that column comprise the set-associative ways. thus, the cache may be searched for a line by first selecting the bank column, selecting the set within the column, and finally performing a tag match on banks within that column of the cache. the two drawbacks of this scheme are that\n   \n   * the number of rows may not correspond to the number of desired ways in each bank set\n   * latencies to access all bank sets are not the same; some bank sets will be faster than others, since some rows are closer to the cache controller than others.\n\n * fair mapping policy, which addresses both problems of the simple mapping policy at the cost of additional complexity.\n\n * shared mapping policy, attempts to provide fastest-bank access to all bank sets by sharing the closest banks among multiple bank sets.\n\nthe policies we explore for d-nuca consist of four major components: (1) mapping: simple or shared.\n\n(2) search: multicast, incremental, or combination. we restrict the combined policies such that a block set is partitioned into just two groups, which may then each vary in size (number of blocks) and the method of access (incremental or multicast).\n\n(3) promotion: described by promotion distance, measured in cache banks, and promotion trigger, measured in number of hits to a bank before a promotion occurs.\n\n(4) insertion: identifies the location to place an incoming block and what to do with the block it replaces (zero copy or one copy policies).\n\n\n# references\n\n[1] the effect of interconnect design on the performance of large l2 caches\n\n[2] an adaptive, non-uniform cache structure for wire-delay dominated on-chip caches",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"cache timing",frontmatter:{title:"cache timing",date:"2024-07-20T15:32:49.000Z",permalink:"/pages/cc7040/"},regularPath:"/04.cpu/07.register%20file.html",relativePath:"04.cpu/07.register file.md",key:"v-7ddc0f42",path:"/pages/cc7040/",headers:[{level:3,title:"1. Revisiting Wide Superscalar Microarchitecture",slug:"_1-revisiting-wide-superscalar-microarchitecture",normalizedTitle:"1. revisiting wide superscalar microarchitecture",charIndex:191},{level:3,title:"2. Banked Multiported Register Files for High-Frequency Superscalar Microprocessors",slug:"_2-banked-multiported-register-files-for-high-frequency-superscalar-microprocessors",normalizedTitle:"2. banked multiported register files for high-frequency superscalar microprocessors",charIndex:1521}],headersStr:"1. Revisiting Wide Superscalar Microarchitecture 2. Banked Multiported Register Files for High-Frequency Superscalar Microprocessors",content:" 1. [4] Revisiting Wide Superscalar Microarchitecture\n 2. [196] Banked Multiported Register Files for High-Frequency Superscalar Microprocessors\n\n----------------------------------------\n\n\n# 1. Revisiting Wide Superscalar Microarchitecture\n\nMust Read\n\nImpact 👍 👍 👍 👍\n\nUnderstand ☺️ ☺️\n\nThis paper discussed the chanllening issue of wide superscalar microarchitecture might trigger.\n\nAlso from circuit perspective.\n\nContributions\n\n * This study shows that considering wide issue instead of narrow issue clusters has a dramatic impact on the performance of Mod-N, one of the simplest steering policy.\n * We find that, with wide issue clusters, if the instruction window is large enough and considering a realistic intercluster delay, the optimal value of N is much larger than three, typically several tens.\n * We argue that the instruction window and the issue width can be augmented by combining clustering and register write specialization.\n * we propose two independent and orthogonal energy optimizations exploiting loops.\n   * The first optimization we propose is a mechanism to detect redundant microops producing the same result on every iteration and to remove these micro-ops from the execution core.\n   * The second optimization focuses on saving energy consumed by load microops.\n * We also test this mechanism with a configuration which emulates what happens for very large instruction footprint applications.\n\nChapter 2 state of the art is so well-written.\n\n\n\n----------------------------------------\n\n\n# 2. Banked Multiported Register Files for High-Frequency Superscalar Microprocessors\n\nImpact 👍 👍 👍\n\nUnderstand ☺️\n\nContribution\n\n * banked register file with much simpler and faster control logic while only slightly increasing the number of ports per bank\n * We present area, delay, and energy numbers extracted from layouts of the banked register file\n\n\n\n\n\n\n\n\n\n\n\n\n\n#banks/#reads/#writes/bypass/sharing\n\n\n\nConclusion\n\nArea\n\n * As the number of local ports per bank is reduced, area drops dramatically\n\n * Compared to the baseline design, the designs with four banks are around one quarter the size, and the designs with eight banks are around one third the size.\n\n * Apart from the reduction in storage cell size, designs with smaller numbers of ports per bank have significantly less address decoder area than the highly multiported designs. Each bank has fewer decoders with narrower addresses.\n\n * Designs with two read ports per bank are not much larger than designs with a single read port per bank given that the single read port must connect to all global read ports whereas each of the two read ports only connects to half of the global read ports.\n\n * For the fully ported storage cell designs, using hierarchical bitlines cuts delay by 8–17%.\n\n * The lesser-ported bank designs have a slightly greater reduction in delay, at around 25% faster for the two read, two write port case.\n\n * we found the (8/2/2/y/y) configuration to perform well for this design point, and we chose this as our center point in perturbing other parameters.\n\n * Reducing the number of banks to four (4/2/2/y/y), lowers performance by another 3–4%.\n\n * We can also see that moving from 1 to 2 write ports (8/2/1/y/y, 8/2/2/y/y) improves performance by more than 4% but having more than 2 write ports per bank (8/2/4/y/y and 8/2/8/y/y) adds only another 0.3%.\n\n * This is expected given that average IPCs are rarely above 2, and some instructions do not write registers.\n\nRemarks\n\nThis paper discussed in detail the circuit design and tradeoff of multi-bank register file and bypass network.\n\nThe area break down of multi bank and multi port.",normalizedContent:" 1. [4] revisiting wide superscalar microarchitecture\n 2. [196] banked multiported register files for high-frequency superscalar microprocessors\n\n----------------------------------------\n\n\n# 1. revisiting wide superscalar microarchitecture\n\nmust read\n\nimpact 👍 👍 👍 👍\n\nunderstand ☺️ ☺️\n\nthis paper discussed the chanllening issue of wide superscalar microarchitecture might trigger.\n\nalso from circuit perspective.\n\ncontributions\n\n * this study shows that considering wide issue instead of narrow issue clusters has a dramatic impact on the performance of mod-n, one of the simplest steering policy.\n * we find that, with wide issue clusters, if the instruction window is large enough and considering a realistic intercluster delay, the optimal value of n is much larger than three, typically several tens.\n * we argue that the instruction window and the issue width can be augmented by combining clustering and register write specialization.\n * we propose two independent and orthogonal energy optimizations exploiting loops.\n   * the first optimization we propose is a mechanism to detect redundant microops producing the same result on every iteration and to remove these micro-ops from the execution core.\n   * the second optimization focuses on saving energy consumed by load microops.\n * we also test this mechanism with a configuration which emulates what happens for very large instruction footprint applications.\n\nchapter 2 state of the art is so well-written.\n\n\n\n----------------------------------------\n\n\n# 2. banked multiported register files for high-frequency superscalar microprocessors\n\nimpact 👍 👍 👍\n\nunderstand ☺️\n\ncontribution\n\n * banked register file with much simpler and faster control logic while only slightly increasing the number of ports per bank\n * we present area, delay, and energy numbers extracted from layouts of the banked register file\n\n\n\n\n\n\n\n\n\n\n\n\n\n#banks/#reads/#writes/bypass/sharing\n\n\n\nconclusion\n\narea\n\n * as the number of local ports per bank is reduced, area drops dramatically\n\n * compared to the baseline design, the designs with four banks are around one quarter the size, and the designs with eight banks are around one third the size.\n\n * apart from the reduction in storage cell size, designs with smaller numbers of ports per bank have significantly less address decoder area than the highly multiported designs. each bank has fewer decoders with narrower addresses.\n\n * designs with two read ports per bank are not much larger than designs with a single read port per bank given that the single read port must connect to all global read ports whereas each of the two read ports only connects to half of the global read ports.\n\n * for the fully ported storage cell designs, using hierarchical bitlines cuts delay by 8–17%.\n\n * the lesser-ported bank designs have a slightly greater reduction in delay, at around 25% faster for the two read, two write port case.\n\n * we found the (8/2/2/y/y) configuration to perform well for this design point, and we chose this as our center point in perturbing other parameters.\n\n * reducing the number of banks to four (4/2/2/y/y), lowers performance by another 3–4%.\n\n * we can also see that moving from 1 to 2 write ports (8/2/1/y/y, 8/2/2/y/y) improves performance by more than 4% but having more than 2 write ports per bank (8/2/4/y/y and 8/2/8/y/y) adds only another 0.3%.\n\n * this is expected given that average ipcs are rarely above 2, and some instructions do not write registers.\n\nremarks\n\nthis paper discussed in detail the circuit design and tradeoff of multi-bank register file and bypass network.\n\nthe area break down of multi bank and multi port.",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"two-test-1",frontmatter:{title:"two-test-1",date:"2022-07-18T17:23:23.000Z",permalink:"/pages/f07697/"},regularPath:"/04.cpu/1234.markdown.html",relativePath:"04.cpu/1234.markdown.md",key:"v-c1b2e1c6",path:"/pages/f07697/",headers:[{level:2,title:"为什么要使用 Markdown?",slug:"为什么要使用-markdown",normalizedTitle:"为什么要使用 markdown?",charIndex:429},{level:2,title:"Markdown 相关软件推荐",slug:"markdown-相关软件推荐",normalizedTitle:"markdown 相关软件推荐",charIndex:1004},{level:2,title:"1. 标题&目录",slug:"_1-标题-目录",normalizedTitle:"1. 标题&amp;目录",charIndex:null},{level:3,title:"1.1 标题",slug:"_1-1-标题",normalizedTitle:"1.1 标题",charIndex:1269},{level:3,title:"1.2 目录",slug:"_1-2-目录",normalizedTitle:"1.2 目录",charIndex:1470},{level:2,title:"2. 斜体&粗体",slug:"_2-斜体-粗体",normalizedTitle:"2. 斜体&amp;粗体",charIndex:null},{level:3,title:"2.1 斜体",slug:"_2-1-斜体",normalizedTitle:"2.1 斜体",charIndex:1662},{level:3,title:"2.2 粗体",slug:"_2-2-粗体",normalizedTitle:"2.2 粗体",charIndex:1844},{level:3,title:"2.3 粗斜体 (斜粗体)",slug:"_2-3-粗斜体-斜粗体",normalizedTitle:"2.3 粗斜体 (斜粗体)",charIndex:2046},{level:3,title:"2.4 斜体包含粗体",slug:"_2-4-斜体包含粗体",normalizedTitle:"2.4 斜体包含粗体",charIndex:2434},{level:3,title:"2.5 粗体包含斜体",slug:"_2-5-粗体包含斜体",normalizedTitle:"2.5 粗体包含斜体",charIndex:2973},{level:2,title:"3. 线",slug:"_3-线",normalizedTitle:"3. 线",charIndex:3505},{level:3,title:"3.1 水平分割线",slug:"_3-1-水平分割线",normalizedTitle:"3.1 水平分割线",charIndex:3515},{level:3,title:"3.2 文本删除线",slug:"_3-2-文本删除线",normalizedTitle:"3.2 文本删除线",charIndex:3676},{level:3,title:"3.3 文本下划线",slug:"_3-3-文本下划线",normalizedTitle:"3.3 文本下划线",charIndex:3779},{level:2,title:"4. 列表&引用",slug:"_4-列表-引用",normalizedTitle:"4. 列表&amp;引用",charIndex:null},{level:3,title:"4.1 有序列表",slug:"_4-1-有序列表",normalizedTitle:"4.1 有序列表",charIndex:3903},{level:3,title:"4.2 无序列表",slug:"_4-2-无序列表",normalizedTitle:"4.2 无序列表",charIndex:4966},{level:3,title:"4.3 引用",slug:"_4-3-引用",normalizedTitle:"4.3 引用",charIndex:5496},{level:3,title:"4.4 缩进&退格",slug:"_4-4-缩进-退格",normalizedTitle:"4.4 缩进&amp;退格",charIndex:null},{level:2,title:"5. 网页链接与图像",slug:"_5-网页链接与图像",normalizedTitle:"5. 网页链接与图像",charIndex:8422},{level:3,title:"5.1 网页链接",slug:"_5-1-网页链接",normalizedTitle:"5.1 网页链接",charIndex:8438},{level:3,title:"5.2 图像",slug:"_5-2-图像",normalizedTitle:"5.2 图像",charIndex:9013},{level:2,title:"6. 表格",slug:"_6-表格",normalizedTitle:"6. 表格",charIndex:9834},{level:3,title:"6.1 表格中文本内容的换行",slug:"_6-1-表格中文本内容的换行",normalizedTitle:"6.1 表格中文本内容的换行",charIndex:10206},{level:2,title:"7. 代码域",slug:"_7-代码域",normalizedTitle:"7. 代码域",charIndex:10549},{level:3,title:"7.1 行内代码",slug:"_7-1-行内代码",normalizedTitle:"7.1 行内代码",charIndex:10561},{level:3,title:"7.2 代码块",slug:"_7-2-代码块",normalizedTitle:"7.2 代码块",charIndex:10969},{level:3,title:"7.3 如何在行内代码里显示反引号",slug:"_7-3-如何在行内代码里显示反引号",normalizedTitle:"7.3 如何在行内代码里显示反引号",charIndex:14992},{level:2,title:"8. 任务列表（待办）",slug:"_8-任务列表-待办",normalizedTitle:"8. 任务列表（待办）",charIndex:15142},{level:3,title:"示范",slug:"示范-22",normalizedTitle:"示范",charIndex:1805},{level:3,title:"示范",slug:"示范-23",normalizedTitle:"示范",charIndex:1805},{level:2,title:"9. 注释",slug:"_9-注释",normalizedTitle:"9. 注释",charIndex:15992},{level:3,title:"示范",slug:"示范-只有切换至-编辑模式-才能看到喔",normalizedTitle:"示范",charIndex:1805},{level:2,title:"10. 变量",slug:"_10-变量",normalizedTitle:"10. 变量",charIndex:16421},{level:3,title:"10.1 网页链接变量",slug:"_10-1-网页链接变量",normalizedTitle:"10.1 网页链接变量",charIndex:16433},{level:3,title:"10.2 脚注",slug:"_10-2-脚注",normalizedTitle:"10.2 脚注",charIndex:16756},{level:2,title:"11. 拓展文本格式标记",slug:"_11-拓展文本格式标记",normalizedTitle:"11. 拓展文本格式标记",charIndex:17028},{level:3,title:"11.1 键盘文本",slug:"_11-1-键盘文本",normalizedTitle:"11.1 键盘文本",charIndex:17130},{level:3,title:"11.2 放大文本",slug:"_11-2-放大文本",normalizedTitle:"11.2 放大文本",charIndex:17465},{level:3,title:"11.3 缩小文本",slug:"_11-3-缩小文本",normalizedTitle:"11.3 缩小文本",charIndex:17707},{level:3,title:"11.4 多彩文本",slug:"_11-4-多彩文本",normalizedTitle:"11.4 多彩文本",charIndex:17949},{level:2,title:"12. 拓展文本显示效果",slug:"_12-拓展文本显示效果",normalizedTitle:"12. 拓展文本显示效果",charIndex:18979},{level:3,title:"12.1 文本高亮",slug:"_12-1-文本高亮",normalizedTitle:"12.1 文本高亮",charIndex:19120},{level:3,title:"12.2 上标",slug:"_12-2-上标",normalizedTitle:"12.2 上标",charIndex:19194},{level:3,title:"12.3 下标",slug:"_12-3-下标",normalizedTitle:"12.3 下标",charIndex:19368},{level:3,title:"12.4 Emoji 符号",slug:"_12-4-emoji-符号",normalizedTitle:"12.4 emoji 符号",charIndex:19547},{level:2,title:"13. 转义字符",slug:"_13-转义字符",normalizedTitle:"13. 转义字符",charIndex:19898},{level:3,title:"例1 以普通字符显示星号",slug:"例1-以普通字符显示星号",normalizedTitle:"例1 以普通字符显示星号",charIndex:20457},{level:3,title:"例2 表格内 单元格中的竖杠",slug:"例2-表格内-单元格中的竖杠",normalizedTitle:"例2 表格内 单元格中的竖杠",charIndex:20722},{level:3,title:"例3 不会变成代码的反引号",slug:"例3-不会变成代码的反引号",normalizedTitle:"例3 不会变成代码的反引号",charIndex:21280},{level:3,title:"例4 链接中的中括号",slug:"例4-链接中的中括号",normalizedTitle:"例4 链接中的中括号",charIndex:21554},{level:3,title:"例5 不是列表的连接符(横杠)",slug:"例5-不是列表的连接符-横杠",normalizedTitle:"例5 不是列表的连接符(横杠)",charIndex:21719},{level:3,title:"例6 不是标题的 \\#",slug:"例6-不是标题的",normalizedTitle:"例6 不是标题的 #",charIndex:22158},{level:3,title:"例7 不会注释的 \\%",slug:"例7-不会注释的",normalizedTitle:"例7 不会注释的 %",charIndex:22235},{level:3,title:"例8 木有链接的双链",slug:"例8-木有链接的双链",normalizedTitle:"例8 木有链接的双链",charIndex:22364},{level:3,title:"例9 页链接里 显示文本内的 中括号",slug:"例9-页链接里-显示文本内的-中括号",normalizedTitle:"例9 页链接里 显示文本内的 中括号",charIndex:22501},{level:3,title:"特殊情况 文本修饰的中括号",slug:"特殊情况-文本修饰的中括号",normalizedTitle:"特殊情况 文本修饰的中括号",charIndex:22675},{level:2,title:"14. 空格&换行&强制删除",slug:"_14-空格-换行-强制删除",normalizedTitle:"14. 空格&amp;换行&amp;强制删除",charIndex:null},{level:3,title:"14.1 空格",slug:"_14-1-空格",normalizedTitle:"14.1 空格",charIndex:22819},{level:3,title:"14.2 换行",slug:"_14-2-换行",normalizedTitle:"14.2 换行",charIndex:23070},{level:3,title:"14.3 强制删除",slug:"_14-3-强制删除",normalizedTitle:"14.3 强制删除",charIndex:23728},{level:2,title:"15. 嵌入",slug:"_15-嵌入",normalizedTitle:"15. 嵌入",charIndex:23900},{level:3,title:"15.1 嵌入音频",slug:"_15-1-嵌入音频",normalizedTitle:"15.1 嵌入音频",charIndex:24080},{level:3,title:"15.2 嵌入视频",slug:"_15-2-嵌入视频",normalizedTitle:"15.2 嵌入视频",charIndex:24341},{level:3,title:"15.3 嵌入页面",slug:"_15-3-嵌入页面",normalizedTitle:"15.3 嵌入页面",charIndex:24721},{level:2,title:"16. Latex 数学公式",slug:"_16-latex-数学公式",normalizedTitle:"16. latex 数学公式",charIndex:25471},{level:3,title:"16.1 行内公式",slug:"_16-1-行内公式",normalizedTitle:"16.1 行内公式",charIndex:25516},{level:3,title:"16.2 公式块",slug:"_16-2-公式块",normalizedTitle:"16.2 公式块",charIndex:25823},{level:2,title:"17. Mermaid",slug:"_17-mermaid",normalizedTitle:"17. mermaid",charIndex:27121},{level:3,title:"17.1 流程图",slug:"_17-1-流程图",normalizedTitle:"17.1 流程图",charIndex:27284},{level:3,title:"17.2 饼图",slug:"_17-2-饼图",normalizedTitle:"17.2 饼图",charIndex:28498},{level:3,title:"17.3 序列图 (时序图)",slug:"_17-3-序列图-时序图",normalizedTitle:"17.3 序列图 (时序图)",charIndex:28739},{level:3,title:"17.4 甘特图",slug:"_17-4-甘特图",normalizedTitle:"17.4 甘特图",charIndex:32416},{level:3,title:"17.5 类图",slug:"_17-5-类图",normalizedTitle:"17.5 类图",charIndex:32999},{level:2,title:"18. 标签 (Tag)",slug:"_18-标签-tag",normalizedTitle:"18. 标签 (tag)",charIndex:33887},{level:3,title:"关于空格",slug:"关于空格",normalizedTitle:"关于空格",charIndex:33986},{level:3,title:"关于数字",slug:"关于数字",normalizedTitle:"关于数字",charIndex:34197},{level:3,title:"标签的嵌套",slug:"标签的嵌套",normalizedTitle:"标签的嵌套",charIndex:34299},{level:3,title:"能被使用的符号",slug:"能被使用的符号",normalizedTitle:"能被使用的符号",charIndex:34521},{level:3,title:"如何让 \\# 不被识别",slug:"如何让-不被识别",normalizedTitle:"如何让 # 不被识别",charIndex:34585},{level:2,title:"19. 避免标识符的滥用",slug:"_19-避免标识符的滥用",normalizedTitle:"19. 避免标识符的滥用",charIndex:34678}],headersStr:"为什么要使用 Markdown? Markdown 相关软件推荐 1. 标题&目录 1.1 标题 1.2 目录 2. 斜体&粗体 2.1 斜体 2.2 粗体 2.3 粗斜体 (斜粗体) 2.4 斜体包含粗体 2.5 粗体包含斜体 3. 线 3.1 水平分割线 3.2 文本删除线 3.3 文本下划线 4. 列表&引用 4.1 有序列表 4.2 无序列表 4.3 引用 4.4 缩进&退格 5. 网页链接与图像 5.1 网页链接 5.2 图像 6. 表格 6.1 表格中文本内容的换行 7. 代码域 7.1 行内代码 7.2 代码块 7.3 如何在行内代码里显示反引号 8. 任务列表（待办） 示范 示范 9. 注释 示范 10. 变量 10.1 网页链接变量 10.2 脚注 11. 拓展文本格式标记 11.1 键盘文本 11.2 放大文本 11.3 缩小文本 11.4 多彩文本 12. 拓展文本显示效果 12.1 文本高亮 12.2 上标 12.3 下标 12.4 Emoji 符号 13. 转义字符 例1 以普通字符显示星号 例2 表格内 单元格中的竖杠 例3 不会变成代码的反引号 例4 链接中的中括号 例5 不是列表的连接符(横杠) 例6 不是标题的 \\# 例7 不会注释的 \\% 例8 木有链接的双链 例9 页链接里 显示文本内的 中括号 特殊情况 文本修饰的中括号 14. 空格&换行&强制删除 14.1 空格 14.2 换行 14.3 强制删除 15. 嵌入 15.1 嵌入音频 15.2 嵌入视频 15.3 嵌入页面 16. Latex 数学公式 16.1 行内公式 16.2 公式块 17. Mermaid 17.1 流程图 17.2 饼图 17.3 序列图 (时序图) 17.4 甘特图 17.5 类图 18. 标签 (Tag) 关于空格 关于数字 标签的嵌套 能被使用的符号 如何让 \\# 不被识别 19. 避免标识符的滥用",content:'这里是 two-test-1 的内容。\n\n\n\n以下Markdown内容转载自：Markdown超级教程 Obsidian版\n\n这里仅作为展示Vuepress解析Markdown效果的一个展示。\n\n\n# 什么是 Markdown?\n\n 1. Markdown 是一款轻量级标记语言，不同于HTML (Hypertext Markup Language)，Markdown 的语法非常简单，且容易上手\n 2. Markdown 以 纯文本格式 编写文档，依赖键盘而非鼠标，专注于写作本身，感受书写的魅力\n 3. Markdown 的通过添加一些简单的 标识符，让文本具有恰到好处的格式\n 4. Markdown 核心特征就是 删繁剪芜， 简扼 + 精炼\n 5. Markdown 是 笔记 与 网页文章 的最佳载体\n 6. Down 的核心：坐 下 来，就能把思维写 下 来\n    * 牛津高阶英汉双解词典第九版 中，关于 down 的释义：\n\n\n\n\n\n\n# 为什么要使用 Markdown?\n\n有朋友问我 ，Markdown 的效果 用Word 完全可以复现，甚至功能更多，那为何要用 Markdown 呢？\n\n答：\n\n * 功能多，不一定是好事\n   * 功能一多，选择就会变多，然后你会开始纠结……\n     * 这个字号是不是该大一点呢？\n     * 这个颜色好像有点不太搭呢？\n     * 这个粗体，是不是该再加点颜色呢？\n     * 这个图片的位置看起来有点不大对劲呢？\n   * 结果，写了半天，就憋出一点点东西\n     * 写出来的内容...好像...也不咋滴\n\nMD的优势：\n\n 1. Markdown 让我们免于 被繁杂臃肿的功能晃花了眼 的困扰\n 2. Markdown 让我们回归内容本身，拥抱笔记的内核，而非浮于表象的样式，写出高效精练的笔记！\n\n用 Markdown 写东西，记住一个原则\n\n> 能用10个字搞定的，绝不用11个字\n\n经常使用 Markdown 书写的朋友，也许会有一种奇妙的感触\n\n * 书写，会==倒逼==思维的跃进。像是有东西拽着你的思绪往前冲\n   * 倒逼：逆向逼迫，反向推动\n\n关于标识符的滥用\n\n这个其实是写在最后的，之所以放在这里，是因为它很重要！\n\n如果你有一定的MD语法基础，可以直接[[#19 避免标识符的滥用|点击跳转]]\n\n\n\n# Markdown 相关软件推荐\n\n * Markdown 书写软件 推荐：Typora 优秀的 MD网页文章 书写软件\n   * 点击跳转下载地址\n     * #提示 以前是免费的，现在收费了，不过是买断制\n * Markdown 笔记软件 推荐：Obsidian 银河系最强 MD+双向链 笔记软件\n   * 点击跳转下载地址\n\n\n\n\n\n\n# Markdown 语法\n\n * 提示1： 本教程推荐使用 Obsidian 打开阅读\n * 提示2： 下文提到的所有标识符都是 英文状态 的 ！\n\n\n# 1. 标题&目录\n\n\n\n# 1.1 标题\n\n * Markdown标题共有 六级，和 HTML 一样\n * 区分 一级标题 → 六级标题\n   * 标题 的格式：\n     * # × 标题级数 + 空格 + 文本内容\n\n这是一段普通的文本\n\n# 这是一级标题\n## 这是二级标题\n### 这是三级标题\n#### 这是四级标题\n##### 这是五级标题\n###### 这是六级标题\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n# 1.2 目录\n\n * 目录的 格式：\n   * 在文档的顶部 输入 [toc] ，会根据 标题 自动生成目录 ( Table of Content )\n * 不是所有 MD编辑器 都支持目录生成\n   * Obsidian 就不支持，不过 OB 是自带大纲的，就是目录的效果\n\n输入下方内容会生成一个目录：\n\n[toc]\n\n\n1\n2\n3\n\n\n\n\n\n\n\n# 2. 斜体&粗体\n\n\n\n# 2.1 斜体\n\n * 斜体 的格式：\n   1. * + 文本内容 + *\n   2. _ + 文本内容 + _ ( 下划线 )\n * 说明：\n   * 斜体文本，首尾只有 单个 标识符\n\n这是一段普通文本\n\n*这里是一段斜体文本*\n_这也是一段斜体文本_\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体文本 这也是一段斜体文本\n\n\n\n# 2.2 粗体\n\n * 粗体 的格式：\n   \n   1. ** + 文本内容 + **\n   2. __ + 文本内容 + __ (这里是两个 _ )\n\n * 说明：\n   \n   * 粗体文本，首尾各有 两个 标识符\n\n这是一段普通文本\n\n**这里是一段加粗文本**\n__这也是一段加粗文本__\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段加粗文本 这也是一段加粗文本\n\n\n\n# 2.3 粗斜体 (斜粗体)\n\n * 粗斜体 的格式：\n   \n   1. *** + 文本内容 + ***\n   2. ___ + 文本内容 + ___ （ 这里是3个 _ )\n   3. **_ + 文本内容 + _**\n   4. __* + 文本内容 + *__\n   5. *__ + 文本内容 + __*\n   6. _** + 文本内容 + **_\n\n * 说明：\n   \n   * 粗斜体文本，首尾各有 三个 标识符\n\n这是一段普通文本\n\n***粗斜体文本1***\n___粗斜体文本2___\n**_粗斜体文本3_**\n__*粗斜体文本4*__\n*__粗斜体文本5__*\n_**粗斜体文本6**_\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n这是一段普通文本\n\n粗斜体文本1 粗斜体文本2 粗斜体文本3 粗斜体文本4 粗斜体文本5 粗斜体文本6\n\n\n\n# 2.4 斜体包含粗体\n\n * 斜体中包含粗体 的格式：\n   \n   1. * + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + *\n   2. _ + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + _ （ 这里是两个 _ )\n   3. * + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + *\n   4. _ + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + _\n\n * 说明：\n   \n   * 斜体 中包含 粗体，其实就是嵌套的关系，外层 是 斜体，内层 是 粗体\n   * 外层是斜体，标识符是单个；内层是粗体，标识符是两个\n   * 因为 粗体 是被包裹在 斜体 中的，所以显示效果为 斜粗体\n\n这是一段普通文本\n\n*这里是一段斜体中**包含粗体**的文字*\n_这也是一段斜体中**包含粗体**的文字_\n*这又是一段斜体中__包含粗体__的文字*\n_这还是一段斜体中**包含粗体**的文字_\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体中包含粗体的文字 这也是一段斜体中包含粗体的文字 这又是一段斜体中__包含粗体__的文字 这还是一段斜体中包含粗体的文字\n\n\n\n# 2.5 粗体包含斜体\n\n * 粗体中包含斜体 的格式：\n   1. ** + 粗体文本 + * + 斜体文本 + * + 粗体文本 + **\n   2. __ + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + __ （ 这里是两个 _ )\n   3. ** + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + **\n   4. __ + 粗体文本 + * + 斜体文本 + * + 粗体文本 + __\n * 说明：\n   * 粗体 中包含 斜体，也就是嵌套的关系，外层 是 粗体，内层 是 斜体\n   * 外层是粗体，标识符是两个；内层是斜体，标识符是单个\n   * 因为 斜体 是被包裹在 粗体 中的，所以显示效果为 粗斜体\n\n这是一段普通文本\n\n**这里是一段粗体中*包含斜体*的文字**\n__这也是一段粗体中_包含斜体_的文字__\n**这又是一段粗体中_包含斜体_的文字**\n__这还是一段粗体中*包含斜体*的文字__\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段粗体中包含斜体的文字 这也是一段粗体中_包含斜体_的文字 这又是一段粗体中_包含斜体_的文字 这还是一段粗体中包含斜体的文字\n\n\n\n\n\n\n# 3. 线\n\n\n\n# 3.1 水平分割线\n\n * 水平分割线由至少 3 个 * 或 - 组成\n\n下面是一条水平分割线：\n---\n***\n\n\n1\n2\n3\n\n\n# 示范\n\n----------------------------------------\n\n----------------------------------------\n\n\n\n# 3.2 文本删除线\n\n * 删除线 的格式：\n   * ~~ + 文本内容 +~~ 首尾各加两个 ~ 波浪号\n\n~~这是一段加了删除线的文本~~\n\n\n1\n\n\n# 示范\n\n这是一段加了删除线的文本\n\n\n\n# 3.3 文本下划线\n\n * 下划线的格式，和 HTML 是一样的\n   * <u> + 文本内容 + </u>\n\n<u>这是一段加了下划线的文本</u>\n\n\n1\n\n\n# 示范\n\n这是一段加了下划线的文本\n\n\n\n\n\n\n# 4. 列表&引用\n\n\n\n# 4.1 有序列表\n\n * 有序列表 的格式：\n   \n   * 1. + 空格 + 文本内容\n\n * 说明：\n   \n   * 输入文本内容后，敲击 Enter 自动补全格式，并进入 下个 有序列表\n   * 若需要在同个列表内，增加 换行显示 的内容 (但不进入下个列表) 敲击 Shift + Enter ，即可另起一行输入文本\n   * 在有序列表的中间，插入一个新的列表，后面列表的 数字序号 会自动 递进 一层\n   * 即便在源代码模式中修改了数字序号，渲染界面依然是 依照顺序 显示的\n\n1. 这是第一个有序列表 \x3c!-- (Enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (Enter) --\x3e\n3. 这是第三个有序列表\n\n\n1. 这是第一个有序列表 \x3c!-- (Shift + Enter) --\x3e\n   这是同个列表下，另起一行的文本内容 \x3c!-- (Enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (Shift + Enter) --\x3e\n   这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 这是第一个有序列表\n\n 2. 这是第二个有序列表\n\n 3. 这是第三个有序列表\n\n 4. 这是第一个有序列表 这是同个列表下，另起一行的文本内容\n\n 5. 这是第二个有序列表 这是同个列表下，另起一行的文本内容\n\n# 补充\n\n * 由于有序列表存在强制排序性，它的数字序号必然是逐一递进的 若你希望内容前的数字，不依照递进顺序排序，或者以 整百，整十数 排序\n * 可以配合无序列表，在无序列表中输入：\n   * 数字 + . + 内容 #注意 点号 与 内容 之间，没有空格 (其实有空格也行，就是会感觉有点奇怪)\n\n- 10.这是无序列表下，整十数排列的内容\n- 20.这是无序列表下，整十数排列的内容\n- 30.这是无序列表下，整十数排列的内容\n\n\n- 100.这是无序列表下，整百数排列的内容\n- 200.这是无序列表下，整百数排列的内容\n- 300.这是无序列表下，整百数排列的内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n效果：\n\n * 10.这是无序列表下，整十数排列的内容\n * 20.这是无序列表下，整十数排列的内容\n * 30.这是无序列表下，整十数排列的内容\n\n\n * 100.这是无序列表下，整百数排列的内容\n * 200.这是无序列表下，整百数排列的内容\n * 300.这是无序列表下，整百数排列的内容\n\n\n\n# 4.2 无序列表\n\n * 无序列表 的格式：\n * - + 空格 + 文本内容\n * 说明：\n   * 输入文本内容后，敲击 Enter 自动补全格式，并进入 下个 无序列表\n   * 若需要在同个列表内，增加换行显示的内容 (但不进入下个列表) 敲击 Shift + Enter ，即可另起一行输入文本\n * 补充：\n   * 在Obsidian中，按下 Ctrl + Enter\n   * 即可快速生成一个无序列表\n\n- 这是第1个无序列表 \x3c!-- (Enter) --\x3e\n- 这是第2个无序列表 \x3c!-- (Enter) --\x3e\n- 这是第3个无序列表\n\n- 这是第一个无序列表 \x3c!-- (Shift + Enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n- 这是第二个无序列表 \x3c!-- (Shift + Enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n * 这是第1个无序列表\n * 这是第2个无序列表\n * 这是第3个无序列表\n\n\n * 这是第一个无序列表 这是同个列表下，另起一行的文本内容\n * 这是第二个无序列表 这是同个列表下，另起一行的文本内容\n\n\n\n# 4.3 引用\n\n * 引用 的格式：\n   * > + 文本内容 （不需要空格)\n * 说明：\n   * 同个引用段落内的换行直接敲击 Enter 即可\n   * 若需添加 第二个独立引用段落 ，连续敲击 两下 Enter 即可\n\n>这是第一段引用文本的第1行 \x3c!-- (Enter) --\x3e\n>这是第一段引用文本的第2行 \x3c!-- (Enter) --\x3e\n\x3c!-- (Enter) --\x3e\n>这是第二段引用文本的第1行 \x3c!-- (Enter) --\x3e\n>这是第二段引用文本内第2行\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n> 这是第一段引用文本的第1行 这是第一段引用文本的第2行\n\n> 这是第二段引用文本的第1行 这是第二段引用文本的第2行\n\n\n\n# 4.4 缩进&退格\n\n在列表和引用的书写过程中，我们需要利用 ==缩进== 与 ==退格== ，让文章肌理分明，更具层级\n\n * 缩进：\n   \n   1. Tab\n   2. Ctrl + [   (左中括号)\n\n * 退格：\n   \n   1. Shift + Tab\n   2. Ctrl + ] （右中括号）\n\n\n# 4.4.1 有序列表的缩&退\n\n1. 第一级有序列表1 \x3c!-- (Enter) --\x3e\n\t1. 第二级有序列表1    \x3c!-- 写文本之前，先( Tab 或 Ctrl + ] ) ；写完文本后，再(Enter) --\x3e\n\t2. 第二级有序列表2 \x3c!-- (Enter) --\x3e\n2. 第一级有序列表2    \x3c!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n * 补充说明：\n   * 有序列表的数字序号，即便你在源代码模式里 强行改掉 数字，它仍然会 依照顺序 显示\n\n# 示范\n\n 1. 第一级有序列表1\n    1. 第二级有序列表1\n    2. 第二级有序列表2\n 2. 第一级有序列表2\n\n\n# 4.4.2 无序列表的缩&退\n\n- 第一级无序列表1 \x3c!-- (Enter) --\x3e\n\t- 第二级无序列表1  \x3c!-- 写文本前，先( Tab 或 Ctrl + ] ) ；写完后，再(Enter) --\x3e\n\t- 第二级无序列表2 \x3c!-- (Enter) --\x3e\n- 第一级无序列表2  \x3c!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n * 第一级无序列表1\n   * 第二级无序列表1\n   * 第二级无序列表2\n * 第一级无序列表2\n\n\n# 4.4.3 引用的缩&退\n\n * 引用的 缩进 和列表 不同\n   * 引用需另起一行，并额外多打一个 > 来完成 缩进\n * 引用的 退格 与列表 相同\n   1. Shift + Tab\n   2. Ctrl + ] （右中括号）\n\n>第一级引用1 \x3c!-- (enter) --\x3e\n>>第二级引用1 \x3c!-- 先打1个 > (这里的第一个 > 是会自动补充的，只需额外增补1个即可) ，再(enter) --\x3e\n>>第二级引用2 \x3c!-- (enter) --\x3e\n>第一级引用2   \x3c!-- 写文本前，先 ( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n> 第一级引用1\n> \n> > 第二级引用1 第二级引用2\n> \n> 第一级引用2\n\n\n * 补充： 在 Obsidian 中，引用的退格是不太一样的\n * **Obsidian **中，如果想让已经缩进的引用 退回一层\n   * 得使用 Shift + Enter ，配合方向键，在多个 > 之间灵活断行 并在下一行 根据需要 选择性补充 >\n * 这个用文字比较难以描述，这里选择用2个带键位的 Gif图 来描述\n\nGif演示1：\n\n\n\n\n\n * 效果1：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> > \n> > 444\n> \n> 555\n\n\nGif演示2：\n\n\n\n\n\n * 效果2：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> \n> > 444\n> > \n> > > 555\n> \n> 666\n\n777\n\n\n# 4.4.4 有序&无序&引用 连续套娃\n\n * 有序列表、无序列表、引用 三者之间，可以相互嵌套\n * 核心键 ： Shift + Enter & Enter & Shift + Tab ( 或 Ctrl + [ )\n   * Shift + Enter 在切换格式的嵌套中，是 自带一层 缩进 效果的\n\n1. 第一级 有序列表1 \x3c!-- (Shift + Enter) --\x3e\n\t- 第二级 无序列表1 \x3c!-- (Shift + Enter) --\x3e\n\t\t>第三级 引用1  \x3c!-- (Enter) --\x3e\n\t\t\t- 第四级 无序列表2 \x3c!-- (Shift + Enter) --\x3e\n            \t1. 第五级 有序列表2 \x3c!-- (Enter) --\x3e\n            - 第四级 无序列表3   \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) ；写完后再 (Enter) --\x3e\n        >第三级 引用2  \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) ；写完后再 (Enter × 2) --\x3e\n    - 第二级 无序列表4  \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) --\x3e\n2. 第一级 有序列表3  \x3c!-- 写文本前，先( Shift + Tab 或 Ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 第一级 有序列表1\n    \n    * 第二级 无序列表1\n      \n      > 第三级 引用1\n      > \n      >  * 第四级 无序列表2\n      >    1. 第五级 有序列表2\n      >  * 第四级 无序列表3\n      > \n      > 第三级 引用2\n    \n    * 第二级 无序列表4\n\n 2. 第一级 有序列表3\n\n# 4.4.5 Obsidian 的一些缩退问题\n\n * Obsidian 在列表首行使用缩进的时候，后续的列表会出现一些问题\n   * Tab 和 Shift + tab 会无法 缩进 退格\n     * 可以使用 Ctrl + ] 与 Ctrl + [ 来解决问题\n\n- - 这是第一段就被缩进的列表\n\t- 这是第二段被再次缩进的列表  \x3c!-- 这里需按两次 Ctrl + ] ,Tab键是无效的 --\x3e\n  - 这是第三段列表  \x3c!-- Ctrl + [ --\x3e\n\n\n1\n2\n3\n\n * * 这是第一段就被缩进的列表 - 这是第二段被再次缩进的列表\n     * 这是第三段列表\n\n\n\n\n\n\n# 5. 网页链接与图像\n\n\n\n# 5.1 网页链接\n\n * 网页链接的 格式：\n   * [ + 显示文本内容 + ] + ( + 链接地址 + 空格 + " + 提示信息文本 + " + )\n * 说明：\n   * 显示文本内容，是在渲染界面实际 可见 的文本，用以 说明 链接\n   * 提示信息文本，需鼠标悬停于 显示文本内容 方可触发，用于增加额外提示信息\n     * #注意 "提示信息文本" 是可选项，一般不会填\n     * 一般来讲，需按住 Ctrl + 鼠标左键点击 才可跳转链接，不过也有 直接鼠标点击 就能跳转的\n\n[显示文本内容](链接地址 "提示信息文本")\n\n[百度一下，你就知道](http://www.baidu.com "按住Ctrl点击跳转百度")\n\n\n1\n2\n3\n\n\n示范：\n\n百度一下，你就知道\n\n\n# 5.1.1链接的加粗\n\n * 格式有两种：\n   \n   1. 把一对 ** 加在 ==显示文本内容==的首尾\n      \n      * 格式1：[**显示文本内容**](链接地址)\n      * 效果： 百度一下，你就知道\n   \n   2. 把一对 ** 加在 链接格式==整体== 的首尾\n      \n      * 格式2：**[显示文本内容](链接地址)**\n      * 效果： 百度一下，你就知道\n\n\n\n\n\n\n# 5.2 图像\n\n * 图像格式：\n   * 图像格式，就是在网页链接前面加个 ! (英文格式的)，! 代表 可见\n   * 图片的提示信息，和网页链接一样，写在 " " 内\n   * [ ] 方括号里的文字信息在 Markdown 没啥实质的作用，只是方便在源代码模式下，知道这个图片是什么，在渲染界面是不会显示的。有点类似于HTML img标签 里的 alt属性。\n\n![文字信息](图片链接 "提示文本信息")\n\n![湘湖1](https://z3.ax1x.com/2021/08/06/fuNkXq.jpg "湘湖一角")\n\n\n1\n2\n3\n\n\n * 补充：\n   \n   * 图像链接可以是本地的，也可以是在线的\n     * 本地图像直接 Ctrl + C 黏贴，Ctrl + V 复制 就可以\n     * 在线图像推荐使用 图床\n   * 调整图像的大小需要使用 HTML 和 CSS，在 Typora编辑器 中右键可以直接缩放图片 本质是转成了HTML的格式，最后会有一个 style="zoom: %;" ，这里数值可以自己修改\n   * 如果有使用 Obsidian 的朋友，在线图片链接是通用的。不过，因为 Obsidian 是双向链笔记 它的本地图片格式不太一样\n     * ![[图片名]]\n       * Obsidian 中的图片是以双链的格式引用在目标笔记中，用 ! 使它可见\n       * Obsidian的图片设置大小是用 | 分隔，后面写宽度数值，单位是px。 设定好宽度，高度会自动等比例调整\n         * ![[图片名|宽度数值]] - 若想自主调整图片宽高，则用： - ![[图片名|宽度数值x高度数值]] - #提示 这里的 x 是 英文字母x\n     * 如果是在线图床，需要调整图片大小：\n       * ![图床|宽度数值](链接地址)\n\n# 示范\n\n\n\n\n\n\n\n\n# 6. 表格\n\n * Markdown的表格，比HTML简单很多\n   * | 是构成表格的主要 框架\n   * - 区分 表头 和 表格主体\n   * : 控制 表格内 文本内容 的 对齐方式\n   * **Typora编辑器中 ** 输入 Ctrl + T 即可快速插入表格，自由定义样式\n\n|这里是表头1|这里是表头2|这里是表头3|\n|:-|:-:|-:|    \x3c!--区分表头和表格主体，:代表文本对齐方式，分别是左对齐，居中对齐，右对齐--\x3e\n|单元格数据1|单元格数据2|单元格数据3|\n|单元格数据4|单元格数据5|单元格数据6|\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这里是表头1   这里是表头2   这里是表头3\n单元格数据1   单元格数据2   单元格数据3\n单元格数据4   单元格数据5   单元格数据6\n\n\n\n# 6.1 表格中文本内容的换行\n\n * Mardown中表格，它的宽高是由 单元格数据内的文本内容 撑开 的\n * 当我们输入一段很长很长的文本，它所在的单元格会变得过宽\n\n如下图所示：\n\n表头1                                   表头2\n这是一段很长很长很长很长很长很长很长很长很长很长很长很长很长很长的文本   普通文本\n\n * 若想对一段长文本进行换行，可以在 中间 插入一个 <br> （ 换行标签 )\n\n| 表头1 |  表头2 |\n|:-:|:-:|\n|这是第一行文本<br>这是另起一行的文本|普通文本|\n\n\n1\n2\n3\n\n\n# 示范\n\n表头1         表头2\n这是第一行文本     普通文本\n这是另起一行的文本\n\n\n\n\n\n\n# 7. 代码域\n\n\n\n# 7.1 行内代码\n\n * 行内代码 的格式：\n   * 输入两个 ` 反引号 ，在中间写代码内容\n * 补充：\n   * 行内代码不一定非得写代码，也可以作为**着重标记**，突出显示内容\n   * 行内代码中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 所谓行内代码： 只要你的屏幕足够宽，它就不会换行\n\n`这是一段行内代码`\n\n`<table border="1" cellspacing="0" width="500" height="500">`\n\n`print("Hello, World!")`\n\n`这是一行突出显示的文本内容`\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 示范\n\n<table border="1" cellspacing="0" width="500" height="500">\n\n\nprint("Hello, World!")\n\n\n这是一行突出显示的文本内容\n\n\n\n# 7.2 代码块\n\n * 代码块 的格式：\n   1. 在首行和末行各加 三个 ` 反引号\n   * ``` + 语言种类 代码内容 ```\n   2. 在首行和末行各加 三个 ~ 波浪号\n      * ~~~ + 语言种类 代码内容 ~~~\n * 补充：\n   * 在代码块也不一定要写代码，可以写一段突出的文本内容，语言类型可以填写 txt 或者 干脆不写\n   * 代码块中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 在 Typora编辑器 ，用键盘按键脱离代码块区域，需输入： Ctrl + Enter\n\n```语言种类\n代码内容\n代码内容\n代码内容\n```\n\n下面是HTML代码块\n\n```html\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n```\n\n下面是CSS代码块\n\n```css\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%,red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n```\n\n下面是JavaScript代码块\n\n```js\n    // 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n        sum += arr.shift();\n        count++;\n        if (count % 5 === 0) {\n            newarr.push(sum / 5);\n            sum =  0;\n        }\n    }\n    console.log(newarr);\n\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, len = arr.length; i < len / 5; i++) {\n        let subarr = arr.splice(0, 5)\n        for (let j = 0, sum = 0; j < subarr.length; j++) {\n            sum += subarr[j];\n        }\n        newarr.push(sum / 5);\n    }\n    console.log(newarr);\n```\n\n\n下面是Python代码块\n\n```python\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "Good bye!"\n```\n\n下面是一块突出显示的文本\n\n```txt\n这是一段\n突出显示的\n文本内容\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n# 示范\n\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%, red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n// 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n\tsum += arr.shift();\n\tcount++;\n\tif (count % 5 === 0) {\n\t\tnewarr.push(sum / 5);\n\t\tsum =  0;\n\t}\n}\nconsole.log(newarr);\n\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, len = arr.length; i < len / 5; i++) {\n\tlet subarr = arr.splice(0, 5)\n\tfor (let j = 0, sum = 0; j < subarr.length; j++) {\n\t\tsum += subarr[j];\n\t}\n\tnewarr.push(sum / 5);\n}\nconsole.log(newarr);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "Good bye!"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n这是一段\n突出显示的\n文本内容\n\n\n1\n2\n3\n\n\n\n# 7.2.1 代码块的嵌套\n\n\n格式：\n\n * 使用4个 ` 包裹 3个 `\n\n# 示范\n\n````txt\n```js\n// 3. 输出 100以内(不包括100) 所有偶数的和\n// 这类求和问题的核心 ： 利用循环  (总和 = 旧数的和 + 新数)\n\nlet sum = 0;\n\nfor (let i = 1, sum = 0; i < 100; i++) {\n if (i % 2 == 0) {\n // 筛选偶数\n sum += i; // sum = sum + i // 累加偶数并赋值给sum\n // sum为(旧的，已经进入循环的数)的和，i 为新进入循环的数。当加到(最后一个新数i)时，sum就是最后的 总和\n }\n}\n\nconsole.log(sum); // 打印总和\n```\n````\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n如果要再套一层，就在最外层 加 5个 ` ，以此类推……\n\n\n\n# 7.3 如何在行内代码里显示反引号\n\n首尾各用 两个反引号`+ 空格 包裹\n\n格式：\n\n``+空格+带`的内容+空格+``  \x3c!-- 不要忘记前后的两个空格 --\x3e\n\n`` 这是一段能显示`反引号`的行内代码 ``\n\n\n1\n2\n3\n\n\n效果：\n\n这是一段能显示`反引号`的行内代码\n\n\n\n\n\n\n# 8. 任务列表（待办）\n\n * 任务列表 的格式：\n   \n   * - + 空格 +[ ] +空格 + 任务列表内容 ( 中括号[ ] 里面必须有个空格)\n   * 给待办任务列表打 √ ，变成 已办\n     1. 在渲染界面，直接鼠标左键点击框框\n     2. 在源代码界面，在中括号内输入 英文字母x\n        * 部分编辑器，在 中括号内 输入任意字符都可以打 √ ( 例如 Obsidian )\n\n * 补充：\n   \n   * 大部分 MD编辑器 支持输入第一个任务列表后，按下 Enter 进入下一行会 自动补全待办格式\n   * 在Obsidian中，连续输入两次 Ctrl + Enter ，即可生成一个待办列表\n     * 再输入一次 Ctrl + Enter ，会在待办列表 打 √\n\n * 格式：\n\n- [ ] 待办任务列表1\n- [ ] 待办任务列表2\n- [x] 已办任务列表1    \x3c!-- 英文字母X --\x3e\n- [x] 已办任务列表2\n\n\n1\n2\n3\n4\n\n\n\n# 示范\n\n * [ ] 待办任务列表1\n * [ ] 待办任务列表2\n * [x] 已办任务列表1\n * [x] 已办任务列表2\n\n\n * 在 Obsidian 中，可以利用 Ctrl + Enter ，快速生成任务列表\n   1. - + 空格 + Ctrl + Enter +待办文本内容\n   2. 待办文本内容 + Ctrl + Enter ×2   ( 输入文本后，连续2次 Ctrl + enter )\n\n\n * 任务列表也是可以缩进+退格的，操作跟 无序、有序列表一样\n\n\n# 示范\n\n * [ ] 第一级待办列表1\n   * [ ] 第二级待办列表1 另起一行的第二级待办列表1\n     * [x] 第三级已办列表1\n     * [x] 第三级已办列表2\n   * [ ] 第二级待办列表2 另起一行的第二级待办列表2\n * [ ] 第一级待办列表2\n\n\n\n\n\n\n# 9. 注释\n\nMarkdown 的 注释 和 HMTL 一样，注释的内容在 渲染界面 不可见 （部分编辑器可见)\n\n * 注释 的格式：\n   * \x3c!-- 这里是注释的内容 --\x3e\n     * 注释可以是单行，也可以是多行\n   * 如果有在使用 Obsidian 的，它的注释格式是不一样的\n     * %%这是Obsidian的注释内容%%\n\n\x3c!-- 这里是一行注释 --\x3e\n\n\x3c!--\n这里是\n一段\n假装有\n很多行的\n注释\n--\x3e\n\n%%这是一行Obsidian里的注释%%\n\n%%\n这里是\n一段\n假装有\n很多行的\nObsidian里的\n注释\n%%\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 示范 (只有切换至 编辑模式 才能看到喔)\n\n%%这是一行Obsidian里的注释%%\n\n%% 这里是 一段 假装有 很多行的 Obsidian里的 注释 %%\n\n\n\n\n\n\n# 10. 变量\n\n\n\n# 10.1 网页链接变量\n\n * 网页链接变量 的格式：\n   1. 首先输入\n      * [显示文本内容] + [变量名]\n        * 变量名可以自己取，没啥限制，任意字符都可以\n   2. 在文档任意一个区域，输入：\n      * [变量名] + : + 空格 + 链接地址 （这个**空格** 不打也没事)\n\n[百度一下，你就知道][度娘]\n[知乎-有问题，就会有答案][知乎]\n\n\x3c!-- 这里是变量区域 --\x3e\n[度娘]: http://www.baidu.com\n[知乎]: https://www.zhihu.com\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n百度一下，你就知道\n\n知乎-有问题，就会有答案\n\n\n\n# 10.2 脚注\n\n * 脚注 的格式：\n   * 在需要脚注的地方，输入：\n     * [^脚注代号] ( 脚注代号会直接显示在渲染界面 )\n       * 脚注代号可以随便命名，不过推荐使用 数字序号\n   * 在其他区域，输入：\n     * [^脚注代号] + : + 空格 + 脚注内容 （这个 空格 不打也没事)\n\n鲁迅原名是什么[^1] ，浙江哪里人[^2]\n\n\x3c!-- 这里是变量区域 --\x3e\n[^1]: 周树人\n[^2]: 绍兴人\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n鲁迅原名是什么^1，浙江哪里人^2\n\n\n\n\n\n\n# 11. 拓展文本格式标记\n\n * Markdown 想实现更多的文本显示效果，只能依赖HTML标记实现\n * 个人不是很推荐在 MD 中使用 HTML，不过一些简单的标记还是可以 轻度使用 的\n\n\n\n# 11.1 键盘文本\n\n * 键盘文本的 格式：\n   \n   * <kbd>键盘文本</kbd>\n   * <kbd>Ctrl</kbd> + <kbd>X</kbd>\n\n * 效果：\n   \n   * 键盘文本\n   * Ctrl + X ( 剪切 )\n\n * 说明：\n   \n   * 键盘文本也不一定非得是键盘按键，也可以作为着重文本突出显示\n     * 效果： 这也算一种着重文本的方式\n\n# 11.1.1 加粗键盘文本\n\n * 加粗键盘文本的格式有两种：\n   \n   * <kbd>**键盘文本**</kbd>\n   * **<kbd>ctrl + x</kbd>**\n\n * 效果：\n   \n   1. 键盘文本\n   2. ctrl + x\n\n\n\n# 11.2 放大文本\n\n * 放大文本 的格式：\n   \n   * 这是一段普通文本 <big>这是一段放大文本</big>\n\n * 效果：\n   \n   * 这是一段普通文本 这是一段放大文本\n\n# 11.2.1 放大粗体文本\n\n * 放大加粗文本的格式有两种：\n   1. **<big>这是一段放大粗体文本</big>**\n   2. <big>**这是一段放大粗体文本**</big>\n * 效果：\n   1. 这是一段放大粗体文本\n   2. 这是一段放大粗体文本\n\n\n\n# 11.3 缩小文本\n\n * 缩小文本 的格式：\n   * 这是一段普通文本 <small>这是一段缩小文本</small>\n * 效果：\n   * 这是一段普通文本 这是一段缩小文本\n\n# 11.3.1 缩小斜体文本\n\n * 斜体缩小文本 的格式有两种：\n   1. <small>*这是一段缩小斜体文本*</small>\n   2. *<small>这是一段缩小斜体文本</small>*\n * 效果：\n   1. 这是一段缩小斜体文本\n   2. 这是一段缩小斜体文本\n\n\n\n# 11.4 多彩文本\n\n * 多彩文本 的格式：\n   * <font color=orange>这是一段橘色文本</font>\n * 效果：\n   * 这是一段橘色文本\n     * color 里的颜色支持 英文单词，16进制，rgb，rgba\n\n\n# 11.4.1 多彩粗体文本\n\n * 只需要在上面示例的基础上，加上 加粗标识符，有两种格式：\n   1. 格式1： **<font color=teal>这是一段加粗的水鸭色文本</font>**\n      * 效果： 这是一段加粗的水鸭色文本\n   2. 格式2： <font color=teal>**这是一段加粗的水鸭色文本**</font>\n      * 效果： 这是一段加粗的水鸭色文本\n * 若上述混搭方法的样式失效 ，可以使用 纯HTML标记\n   * 格式： <strong style="color:teal;">这是一段加粗的水鸭色文本</strong> (标记略复杂，不是很推荐)\n   * 效果： 这是一段加粗的水鸭色文本\n\n\n# 11.4.2 多彩斜体文本\n\n * 跟多彩加粗文本完全一样，只需把首尾的 ** 换成 * 即可\n\n 1. 格式1： *<font color=teal>This is an italic teal text</font>*\n    * 效果： This is an italic teal text\n 2. 格式2： <font color=teal>*This is an italic teal text*</font>\n    * 效果： This is an italic teal text\n\n\n# 11.4.2 多彩粗斜体文本\n\n * 首尾换成 ***\n\n 1. 格式1： ***<font color=teal>This is a bold italic teal text</font>***\n    * 效果： This is a bold italic teal text\n 2. 格式2： <font color=teal>***This is a bold italic teal text***</font>\n    * 效果： This is a bold italic teal text\n\n\n#注意 多彩文本尽量慎用，Markdown 的核心就是 简洁精炼，注重 实质内容，而非花哨的 颜色样式\n\n\n\n\n\n\n# 12. 拓展文本显示效果\n\n * 拓展显示效果既不是原生 Markdown语法 支持的，也非 HTML标记，而是部分编辑器 提供的 额外标识符，属于拓展语法，旨在为 Markdown使用者 提供更多样式选择\n * 不同编辑器，支持不一样，这里以 Typora编辑器 为例\n\n\n\n# 12.1 文本高亮\n\n * 文本高亮 的格式：\n   * ==这里是一段高亮文本==\n * 效果：\n   * ==这里是一段高亮文本==\n\n\n\n# 12.2 上标\n\n * 用一对 ^ 包裹 (Shift+ 6)\n   * 格式： x^2^\n   * 效果： x^2^\n * Obsidian 没效果的，可以用后面会讲的 Latex\n * 或者，也可以使用 HTML标记\n   * <sup>这里是上标内容</sup>\n   * X<sup>2</sup>\n * 效果：\n   * X2\n\n\n\n# 12.3 下标\n\n * 用一对 ~ 包裹 (Shift + `)\n   * 格式： H~2~O\n   * 效果： H~2~O\n * Obsidian 没效果的，可以用后面会讲的 Latex\n * 或者，也可以使用 HTML标记\n   * <sub>这里是下标内容</sub>\n   * H<sub>2</sub>O\n * 效果：\n   * H2O\n\n\n\n# 12.4 Emoji 符号\n\n用一对 : 包裹，里面是 Emoji 符号的 语义化文本 ( Typora编辑器 中，输入 : 就会带提示器 )\n\n * 示例：\n   * :smile: :sweat: :cat: :woman_cartwheeling:\n * 效果：\n   * 😄 😓 🐱 🤸‍♀\n\n\n * 补充：\n   * 不支持上述方式的 MD编辑器或笔记软件，直接用 输入法 输入也是可以的\n   * Windows系统 用户 win + . 就可以输入 Emoji 了\n   * Obsidian 用户可以安装第三方插件来支持 Emoji 的输入，推荐两个\n     1. ==Emoji Shortcodes==\n     2. ==Emoji Toolbar==\n\n\n\n\n\n\n# 13. 转义字符\n\n * 在 Markdown 中，我们 通过 标识符 改变 文本显示效果\n * 现在我们希望它不作为标识符，而是 作为字符本身呈现出来 （不具备改变文本显示效果的功能，只是一个普通字符)\n   * 首先我们可以用前面介绍的 代码域 ，因为代码模式的显示效果就是源代码完全一致的\n   * 还有一种方法，可以利用转义字符，在这些标识符 前面 加上 反斜线 \\ ( 反斜线要紧贴在标识符前面，不能 有 空格 )\n     * 原理：\n       * \\ 的作用是让标识符 转义 变为一个普通字符，完成这个效果后，反斜线会自动隐藏\n       * 隐藏后的反斜线仅在源代码界面可见，在渲染界面不可见\n       * 反斜线只争对标识符起作用，其他字符添加 \\，\\ 不会自动隐藏\n     * 补充：\n       * 如果想给已经被加在标识符前面，会自动隐藏的 \\ 显示出来，可以在反斜线前面再加一个 \\ ，用它自己来转义自己\n         * 示例： 这里紧跟在标识符前面的反斜线\\\\*会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体*\n         * **效果： ** 这里紧跟在标识符前面的 反斜线\\会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体\n\n\n\n# 例1 以普通字符显示星号\n\n * 如何让被一对或多对 * 号 包裹的文本内容，能够正常显示 * ，且文本不改变格式\n   * \\*这段文本被一对星号包裹，但不会倾斜\\*\n     * 效果： *这段文本被1对星号包裹，但不会倾斜*\n   * \\*\\*这段文本被2对星号包裹，但不会加粗\\*\\*\n     * 效果： **这段文本被2对星号包裹，但不会加粗**\n   * \\*\\*\\*这段文本被3对星号包裹，但它既不倾斜也不加粗\\*\\*\\*\n     * 效果： ***这段文本被3对星号包裹，但它既不倾斜也不加粗***\n\n\n\n# 例2 表格内 单元格中的竖杠\n\n * 在表格中，使用 | 作为单元格的内容，但不会被识别为表格的结构，不会增加额外的单元格\n\n|表头1|表头2|\n|-|-|\n|这里的文本被\\|分隔|这里的文本也被\\|分隔|\n\n\n1\n2\n3\n\n * 效果：\n\n表头1         表头2\n这里的文本被|分隔   这里的文本也被|分隔\n\n\n#补充 该技巧可用于 Obsidian 表格内 双链的文本修饰\n\n文本修饰：\n\n在 双链[[ ]]内 以 | 引导的内容\n\n * 格式： [[链接的内容|文本修饰]]\n * 说明： 文本修饰是渲染界面实际显示的文本，便于更好地融入语境\n\n表格内的格式：\n\n在 | 前面加上 \\\n\n * [[表格内的链接内容\\|文本修饰]]\n\n示例：\n\n|                  表头1                  |                        表头2                        |\n|:---------------------------------------:|:---------------------------------------------------:|\n| [[#例2 表格内 单元格中的竖杠\\|单元格中的竖杠]] | [[#例3 不会变成代码的反引号\\|不会变成代码的反引号]] |\n\n\n1\n2\n3\n\n\n效果：\n\n表头1                           表头2\n[[#例2 表格内 单元格中的竖杠|单元格中的竖杠]]   [[#例3 不会变成代码的反引号|不会变成代码的反引号]]\n\n\n\n# 例3 不会变成代码的反引号\n\n使用 转义符号\\ 让 反引号` 变成普通字符，不再具有[[#7 1 行内代码|行内代码]]的标识符功能\n\n格式：\n\n\\`这段被反引号包裹的内容不会变成行内代码\\`\n\n效果：\n\n`这段被反引号包裹的内容不会变成行内代码`\n\n\n\n# 例4 链接中的中括号\n\n在 网页链接 的 显示文本内容 中，使用 中括号 [ ]\n\n * 在显示文本内容中，在其中一个中括号前面，加上转义符号 反斜杠 \\\n   * 格式： [链接里的 \\[中括号\\] 能被正常显示](https://www.runoob.com)\n   * 效果： 链接里的 [中括号] 能被正常显示\n\n\n\n# 例5 不是列表的连接符(横杠)\n\n * 引用一段话，一般会在换行之后，加上 - 出处\n * 因为 - 是标识符，会变成一个无序列表\n\n如下所示：\n\n> The Web, the Tree, and the String. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n> \n>  * 史蒂芬·平克\n\n * 解决方法：\n   \n   * 在 - 前面加上 转义符号 \\\n   \n   >The Web, the Tree, and the String.\n   >写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n   >\\- 史蒂芬·平克   \x3c!-- 加上转义符号 \\ , 不会变成无序列表 --\x3e\n   \n   \n   1\n   2\n   3\n   \n\n * 效果：\n\n> The Web, the Tree, and the String. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。 - 史蒂芬·平克\n\n\n\n# 例6 不是标题的 #\n\n让 # 不被识别为标题标识符\n\n格式：\n\n\\# 这里的内容不会被识别为标题\n\n效果：\n\n# 这里的内容不会被识别为标题\n\n\n\n# 例7 不会注释的 %\n\n在 Obsidian 中 注释是前后各两个 % 号\n\n使用 转义符号\\，让 %% 作为普通字符显示出来，不具备注释的功能\n\n * 格式： \\%\\%这里的内容可以被显示喔\\%\\%\n * 效果： %%这里的内容可以被显示喔%%\n\n\n\n# 例8 木有链接的双链\n\nObsidian 的双向链格式是2个方括号 [[ ]] (双方)，使用 转义符号\\，让 [ ] 不再具有 双链功能\n\n格式：\n\n\\[\\[这段文本被双方包裹，但不是一个双向链\\]\\]\n\n效果：\n\n[[这段文本被双方包裹，但不是一个双向链]]\n\n\n\n# 例9 页链接里 显示文本内的 中括号\n\n使用转义符号\\，让中括号可以作为显示文本 在[[#5 1 网页链接|网页链接]]中显示出来\n\n格式：\n\n[\\[这是一个带中括号的网页链接显示文本，点击会跳转至百度\\]](https://www.baidu.com/)\n\n\n1\n\n\n效果：\n\n[这是一个带中括号的网页链接显示文本，点击会跳转至百度]\n\n\n\n# 特殊情况 文本修饰的中括号\n\n文本修饰的 中括号[ ] 不需要使用 转义符号\\\n\n示范：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n效果：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n\n\n\n\n\n# 14. 空格&换行&强制删除\n\n\n\n# 14.1 空格\n\n * 在一些编辑器或者支持MD的笔记软件里，无论你打多少个空格，它只会显示单个 空格 的距离\n   * 可以使用 HTML中 空格 的 字符实体 —— &nbsp;\n   * 若要添加 多个 空格，就输入多个 —— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n * 格式：\n   * 这里有&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6个空格分隔\n * 效果：\n   * 这里有      6个空格分隔\n\n\n\n# 14.2 换行\n\n场景1：\n\n * 在一些编辑器或者支持MD的笔记软件里，无论你打多少个 回车，它只会显示单个 回车 的空行间距\n   * 可以使用之前表格里提到的 <br> 标签，在 单独一行 中使用，增加额外的空行间距\n   * 如果要增加 多个，就输入 多个 —— <br><br><br><br><br>\n   * #注意 当单独一行使用 <br> 标签的时候，如果前后有标题标识符或者列表标识符，确保 br元素 前后两行都是空白行\n\n格式：\n\n这里是第一段文本\n\n<br><br><br><br><br>     \x3c!-- 这里插入了5个空行间距 --\x3e\n\n这里是第二段文本\n\n\n1\n2\n3\n4\n5\n\n\n效果：\n\n这里是第一段文本\n\n\n\n\n\n\n\n\n这里是第二段文本\n\n\n\n\n\n场景2：\n\n * 在列表中也可以插入换行符\n\n- 这是一段无序列表\n  <br>     \x3c!-- 插入一个空行间距，需单独一行，上下不用预留空格 --\x3e\n  这是同一段无序列表中，空一行距离显示的内容\n- 这是第二段无序列表\n\n\n1\n2\n3\n4\n\n\n效果：\n\n * 这里是第一段无序列表\n   这里是同一段无序列表中，空一行距离显示的内容\n * 这里是第二段无序列表\n\n\n * 补充：\n   * 有一些MD编辑器或笔记软件，严格遵循MD的换行规则，你敲一个回车是没法换行的，必须在 行末 敲 2个空格，再按回车键\n     * 格式：\n       * 这里是一段想换行的文本空格 空格 Enter 这是换行后的文本\n\n\n\n# 14.3 强制删除\n\n * 很多编辑器都有英文标点自动补全功能，自动生成一对，光标落在中间 只想删除前面1个，却会把 一整对 都删掉\n * 在多个列表的嵌套中，也许会遇到一些 无法被删除 的 列表标识符\n * 解决方法： 使用 Shift + Backspace 即可强制删除\n   * Bcakspace   ( 退格键 )\n\n\n\n\n\n\n# 15. 嵌入\n\n * 嵌入都是依赖 HTML标签 实现的，嵌入的都是在线链接格式\n   * 如果是本地的，Obsidian 中音频是有自带的可录制的录音机插件的，其他的 音频、视频 直接复制黏贴就可以了，也可以直接拖拽到OB的笔记界面\n     * 其他的媒体文件在 Obsidian 也和图片一样，以双链的格式引用在目标笔记中，使用 ! 使它可见\n\n\n\n# 15.1 嵌入音频\n\n * 格式：\n   \n   * <audio controls="controls" preload="none" src="音频链接地址"></audio>\n\n * 示例：\n\n<audio controls="controls" preload="none" src="https://www.ldoceonline.com/media/english/exaProns/p008-001803372.mp3?version=1.2.37"></audio>\n\n\n1\n\n * 效果：\n\n\n\n\n\n# 15.2 嵌入视频\n\n * 格式：\n\n<video width="600" height="420" controls>\n  <source src="movie.mp4" type="video/mp4">\n  <source src="movie.ogg" type="video/ogg">\n  <source src="movie.webm" type="video/webm">\n</video>\n\n\n1\n2\n3\n4\n5\n\n * 说明：\n   * width ( 宽度 ) height ( 高度 ) ，可以自己设置，直接输入数字即可，单位默认是 px(像素) 也可以使用 百分比 width=100% 代表水平撑满整个窗口 height=50% 代表垂直撑满半个窗口\n   * Video标签 支持的视频格式 ：MP4 ogg webm\n\n\n\n# 15.3 嵌入页面\n\n * 格式： <iframe width=600 height=400 src="页面链接地址" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n<iframe width=600 height=400 src="https://www.runoob.com/html/html-tutorial.html" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 效果：\n\n\n * iframe标签 除了嵌入页面，也可以嵌入在线视频，主流的视频网站都会提供嵌入代码\n   \n   * 具体可以看这个 iframe视频嵌入教程\n   * B站 的视频，得在 // 前面补充 http:\n   * 不是所有的 编辑器和笔记软件 都支持这个\n\n * 示例：\n\n<iframe width=600 height=400 src="http://player.bilibili.com/player.html?aid=20190823&bvid=BV1yW411s7og&cid=32964980&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 宽高设置和前面的 video 一样\n\n\n * 效果：\n\n\n\n\n\n\n# 16. Latex 数学公式\n\n * 主要用于 数学公式 与 化学公式 的书写\n\n\n\n# 16.1 行内公式\n\n * 格式：\n   \n   * $ + 行内公式 + $\n\n\n * 示例：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $\\ce{CO2 + C -> 2 CO}$\n   * $\\ce{CO2 + C -> 2 CO}$\n   * $\\ce{2Mg + O2 ->[燃烧] 2 MgO}$\n\n\n * 效果：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $e^{i\\pi} + 1 = 0$\n   * $\\ce{CO2 + C -> 2 CO}$\n   * $\\ce{2Mg + O2 ->[燃烧] 2 MgO}$\n\n\n\n# 16.2 公式块\n\n * 格式：\n   * $$ 公式块 $$\n\n\n * 示例：\n\n% 化学公式\n$$\n\\ce{Zn^2+  <=>[+ 2OH-][+ 2H+]  $\\underset{\\text{amphoteres Hydroxid}}{\\ce{Zn(OH)2 v}}$  <=>[+ 2OH-][+ 2H+]  $\\underset{\\text{Hydroxozikat}}{\\ce{[Zn(OH)4]^2-}}$}\n$$\n\n\n1\n2\n3\n4\n\n\n% 麦克斯韦方程组\n$$\n\\begin{array}{lll}\n\\nabla\\times E &=& -\\;\\frac{\\partial{B}}{\\partial{t}}\n\\ \\nabla\\times H &=& \\frac{\\partial{D}}{\\partial{t}}+J\n\\ \\nabla\\cdot D &=& \\rho\n\\ \\nabla\\cdot B &=& 0\n\\ \\end{array}\n$$\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n% 薛定谔方程\n$$\ni\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + V \\psi\n$$\n\n\n1\n2\n3\n4\n\n\n * 效果：\n\n$$ % 化学公式 \\ce{Zn^2+ <=>[+ 2OH-][+ 2H+] $\\underset{\\text{amphoteres Hydroxid}}{\\ce{Zn(OH)2 v}}$ <=>[+ 2OH-][+ 2H+] $\\underset{\\text{Hydroxozikat}}{\\ce{[Zn(OH)4]^2-}}$} $$\n\n\n$$ % 麦克斯韦方程组 \\begin{array}{lll} \\nabla\\times E &=& -;\\frac{\\partial{B}}{\\partial{t}} \\ \\nabla\\times H &=& \\frac{\\partial{D}}{\\partial{t}}+J \\ \\nabla\\cdot D &=& \\rho \\ \\nabla\\cdot B &=& 0 \\ \\end{array} $$\n\n\n$$ i\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + V \\psi $$\n\n * 补充：\n   * 需要详细教程的，可戳下方链接\n   * Latex详细教程\n\n\n\n\n\n\n# 17. Mermaid\n\n * 一些 MD编辑器 和 笔记软件 支持通过 Mermaid 及其所提供的 编译器 来为用户提供图表的绘制功能\n\n * 这里只提供一些演示的图表，具体教程可戳下方\n   \n   * [[MOC Mermiad 教程 Obsidian版| Mermiad 超级教程 Obsidian版]]\n\n\n\n# 17.1 流程图\n\n\n源码1：\n\n```mermaid\ngraph TB\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点2\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点1\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n渲染1：\n\ngraph TB\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点1\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点2\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n源码2：\n\n```mermaid\ngraph LR\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n渲染2：\n\ngraph LR\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n# 17.2 饼图\n\n\n源码：\n\n```mermaid\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n渲染：\n\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n# 17.3 序列图 (时序图)\n\n\n源码：\n\n```mermaid\nsequenceDiagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant A as Aly\n        participant B as Bob\n        participant C as CofCai\n        %% 便签说明\n        Note left of A: 只复习了一部分\n        Note right of B: 没复习\n        Note over A,B: are contacting\n\n        A->>B: 明天是要考试吗？\n        B--\x3e>A: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par askA\n                C --\x3e> A:你复习好了吗？\n            and askB\n                C --\x3e> B:你复习好了吗？\n            and self\n                C ->>C:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/Every min\n            %% <br/>可以换行\n            C ->> C:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                A ->> C:复习了一点\n            else is common\n                B ->> C:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt Extra response\n                C ->> C:你们怎么不回答我\n            end\n        endsequenceDiagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant A as Aly\n        participant B as Bob\n        participant C as CofCai\n        %% 便签说明\n        Note left of A: 只复习了一部分\n        Note right of B: 没复习\n        Note over A,B: are contacting\n\n        A->>B: 明天是要考试吗？\n        B--\x3e>A: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par askA\n                C --\x3e> A:你复习好了吗？\n            and askB\n                C --\x3e> B:你复习好了吗？\n            and self\n                C ->>C:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/Every min\n            %% <br/>可以换行\n            C ->> C:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                A ->> C:复习了一点\n            else is common\n                B ->> C:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt Extra response\n                C ->> C:你们怎么不回答我\n            end\n        end\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\n\n渲染：\n\nsequenceDiagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant A as Aly\n        participant B as Bob\n        participant C as CofCai\n        %% 便签说明\n        Note left of A: 只复习了一部分\n        Note right of B: 没复习\n        Note over A,B: are contacting\n\n        A->>B: 明天是要考试吗？\n        B--\x3e>A: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par askA\n                C --\x3e> A:你复习好了吗？\n            and askB\n                C --\x3e> B:你复习好了吗？\n            and self\n                C ->>C:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/Every min\n            %% <br/>可以换行\n            C ->> C:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                A ->> C:复习了一点\n            else is common\n                B ->> C:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt Extra response\n                C ->> C:你们怎么不回答我\n            end\n        end\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n\n# 17.4 甘特图\n\n\n源码：\n\n```mermaid\ngantt\n    title A Gantt Diagram\n    dateFormat  YYYY-MM-DD\n    section Section\n    A task           :a1, 2014-01-01, 30d\n    Another task     :after a1  , 20d\n    section Another\n    Task in sec      :2014-01-12  , 12d\n    another task      : 24d\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n渲染：\n\ngantt\n    title A Gantt Diagram\n    dateFormat  YYYY-MM-DD\n    section Section\n    A task           :a1, 2014-01-01, 30d\n    Another task     :after a1  , 20d\n    section Another\n    Task in sec      :2014-01-12  , 12d\n    another task      : 24d\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 17.5 类图\n\n\n源码：\n\n```mermaid\nclassDiagram\n    Animal <|-- Duck\n    Animal <|-- Fish\n    Animal <|-- Zebra\n    Animal : +int age\n    Animal : +String gender\n    Animal: +isMammal()\n    Animal: +mate()\n    class Duck{\n      +String beakColor\n      +swim()\n      +quack()\n    }\n    class Fish{\n      -int sizeInFeet\n      -canEat()\n    }\n    class Zebra{\n      +bool is_wild\n      +run()\n    }\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n渲染：\n\nclassDiagram\n    Animal <|-- Duck\n    Animal <|-- Fish\n    Animal <|-- Zebra\n    Animal : +int age\n    Animal : +String gender\n    Animal: +isMammal()\n    Animal: +mate()\n    class Duck{\n      +String beakColor\n      +swim()\n      +quack()\n    }\n    class Fish{\n      -int sizeInFeet\n      -canEat()\n    }\n    class Zebra{\n      +bool is_wild\n      +run()\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n\n\n# 18. 标签 (Tag)\n\n * 标签是 Obsidian 特有的一个功能，标签可以通过点击唤起快速搜索 (搜索包含该标签的所有笔记)\n\n格式：\n\n * # + 标签名\n   * #标签名\n\n\n# 关于空格\n\n * 在一段正文文本的后面添加 Tag， # 的前面 需要有个空格\n   * 空格 + # + 标签名\n\n\n * # 与 标签名 之间，不能有空格，否则就变成 一级标题 了\n\n\n * 标签名的内部，不允许使用空格，若想区分标签中的词语，可使用以下三种方法：\n   1. 驼峰式大小写： #BlueTopaz\n   2. 下划线： #blue_topaz\n   3. 连字符： #blue-topaz\n\n\n\n# 关于数字\n\n * 标签内允许使用数字，但不能完全由数字组成\n   * #1984 ❌\n   * #1984Date ⭕\n   * #da_1984_te ⭕\n   * #date-1984 ⭕\n\n\n\n# 标签的嵌套\n\n在标签名内，使用 / 斜杠 可以实现标签的嵌套\n\n格式：\n\n * #主标签/子标签1\n * #主标签/子标签2\n * #主标签/子标签3\n\n嵌套标签可以像普通标签一样通过点击来唤起搜索，嵌套标签允许你选择搜索的层次。例如：\n\n * 搜索 #主标签 ，即可找到包含任意一个子标签的所有笔记\n   * 返回的结果会是上述的三个例子\n * 当你在一个主分类下设置了多个子分类，想找到这个主分类包含的所有内容时，该功能会很实用\n\n\n\n# 能被使用的符号\n\n综上所述，标签内能被使用的符号共有三种\n\n 1. _ 下划线\n 2. - 连字符\n 3. / 斜杠\n\n\n\n# 如何让 # 不被识别\n\n可以使用前面提到的转义符号 \\ 反斜杠，与上述的 转义标题 类似\n\n格式：\n\n\\#这里的内容不会被识别为标签\n\n效果：\n\n#这里的内容不会被识别为标签\n\n\n\n# 19. 避免标识符的滥用\n\n即使在 Markdown 中，也要尽量避免标识符的滥用\n\n比如我的这篇教程，就存在一定程度的滥用\n\n * 其实是因为我这篇是教学性质的，不太一样，有些不能避免\n   * (好吧，我就是在甩锅)\n\n标识符的本质是突出显示，代表重点\n\n * 一篇笔记里的某段文本，使用各式各样的的标识符，会造成重点不清晰\n\n有三种标识，慎用！\n\n 1. 词中对单个汉字的标识\n    1. 卧==虎==藏==龙==\n 2. 短语中对单个英语单词的标识\n    1. get a ==bang== out of\n 3. 标识符的多层嵌套\n    1. 我感觉快要==原地起飞==了\n\n原因：\n\n * 词义的割裂\n * 视觉的混乱\n * 不利于搜索\n   * 卧==虎==藏==龙==\n     * 搜 卧虎 -- 搜不到\n     * 搜 藏龙 -- 搜不到',normalizedContent:'这里是 two-test-1 的内容。\n\n\n\n以下markdown内容转载自：markdown超级教程 obsidian版\n\n这里仅作为展示vuepress解析markdown效果的一个展示。\n\n\n# 什么是 markdown?\n\n 1. markdown 是一款轻量级标记语言，不同于html (hypertext markup language)，markdown 的语法非常简单，且容易上手\n 2. markdown 以 纯文本格式 编写文档，依赖键盘而非鼠标，专注于写作本身，感受书写的魅力\n 3. markdown 的通过添加一些简单的 标识符，让文本具有恰到好处的格式\n 4. markdown 核心特征就是 删繁剪芜， 简扼 + 精炼\n 5. markdown 是 笔记 与 网页文章 的最佳载体\n 6. down 的核心：坐 下 来，就能把思维写 下 来\n    * 牛津高阶英汉双解词典第九版 中，关于 down 的释义：\n\n\n\n\n\n\n# 为什么要使用 markdown?\n\n有朋友问我 ，markdown 的效果 用word 完全可以复现，甚至功能更多，那为何要用 markdown 呢？\n\n答：\n\n * 功能多，不一定是好事\n   * 功能一多，选择就会变多，然后你会开始纠结……\n     * 这个字号是不是该大一点呢？\n     * 这个颜色好像有点不太搭呢？\n     * 这个粗体，是不是该再加点颜色呢？\n     * 这个图片的位置看起来有点不大对劲呢？\n   * 结果，写了半天，就憋出一点点东西\n     * 写出来的内容...好像...也不咋滴\n\nmd的优势：\n\n 1. markdown 让我们免于 被繁杂臃肿的功能晃花了眼 的困扰\n 2. markdown 让我们回归内容本身，拥抱笔记的内核，而非浮于表象的样式，写出高效精练的笔记！\n\n用 markdown 写东西，记住一个原则\n\n> 能用10个字搞定的，绝不用11个字\n\n经常使用 markdown 书写的朋友，也许会有一种奇妙的感触\n\n * 书写，会==倒逼==思维的跃进。像是有东西拽着你的思绪往前冲\n   * 倒逼：逆向逼迫，反向推动\n\n关于标识符的滥用\n\n这个其实是写在最后的，之所以放在这里，是因为它很重要！\n\n如果你有一定的md语法基础，可以直接[[#19 避免标识符的滥用|点击跳转]]\n\n\n\n# markdown 相关软件推荐\n\n * markdown 书写软件 推荐：typora 优秀的 md网页文章 书写软件\n   * 点击跳转下载地址\n     * #提示 以前是免费的，现在收费了，不过是买断制\n * markdown 笔记软件 推荐：obsidian 银河系最强 md+双向链 笔记软件\n   * 点击跳转下载地址\n\n\n\n\n\n\n# markdown 语法\n\n * 提示1： 本教程推荐使用 obsidian 打开阅读\n * 提示2： 下文提到的所有标识符都是 英文状态 的 ！\n\n\n# 1. 标题&目录\n\n\n\n# 1.1 标题\n\n * markdown标题共有 六级，和 html 一样\n * 区分 一级标题 → 六级标题\n   * 标题 的格式：\n     * # × 标题级数 + 空格 + 文本内容\n\n这是一段普通的文本\n\n# 这是一级标题\n## 这是二级标题\n### 这是三级标题\n#### 这是四级标题\n##### 这是五级标题\n###### 这是六级标题\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n# 1.2 目录\n\n * 目录的 格式：\n   * 在文档的顶部 输入 [toc] ，会根据 标题 自动生成目录 ( table of content )\n * 不是所有 md编辑器 都支持目录生成\n   * obsidian 就不支持，不过 ob 是自带大纲的，就是目录的效果\n\n输入下方内容会生成一个目录：\n\n[toc]\n\n\n1\n2\n3\n\n\n\n\n\n\n\n# 2. 斜体&粗体\n\n\n\n# 2.1 斜体\n\n * 斜体 的格式：\n   1. * + 文本内容 + *\n   2. _ + 文本内容 + _ ( 下划线 )\n * 说明：\n   * 斜体文本，首尾只有 单个 标识符\n\n这是一段普通文本\n\n*这里是一段斜体文本*\n_这也是一段斜体文本_\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体文本 这也是一段斜体文本\n\n\n\n# 2.2 粗体\n\n * 粗体 的格式：\n   \n   1. ** + 文本内容 + **\n   2. __ + 文本内容 + __ (这里是两个 _ )\n\n * 说明：\n   \n   * 粗体文本，首尾各有 两个 标识符\n\n这是一段普通文本\n\n**这里是一段加粗文本**\n__这也是一段加粗文本__\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段加粗文本 这也是一段加粗文本\n\n\n\n# 2.3 粗斜体 (斜粗体)\n\n * 粗斜体 的格式：\n   \n   1. *** + 文本内容 + ***\n   2. ___ + 文本内容 + ___ （ 这里是3个 _ )\n   3. **_ + 文本内容 + _**\n   4. __* + 文本内容 + *__\n   5. *__ + 文本内容 + __*\n   6. _** + 文本内容 + **_\n\n * 说明：\n   \n   * 粗斜体文本，首尾各有 三个 标识符\n\n这是一段普通文本\n\n***粗斜体文本1***\n___粗斜体文本2___\n**_粗斜体文本3_**\n__*粗斜体文本4*__\n*__粗斜体文本5__*\n_**粗斜体文本6**_\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n这是一段普通文本\n\n粗斜体文本1 粗斜体文本2 粗斜体文本3 粗斜体文本4 粗斜体文本5 粗斜体文本6\n\n\n\n# 2.4 斜体包含粗体\n\n * 斜体中包含粗体 的格式：\n   \n   1. * + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + *\n   2. _ + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + _ （ 这里是两个 _ )\n   3. * + 斜体文本 + __ + 粗体文本 + __ + 斜体文本 + *\n   4. _ + 斜体文本 + ** + 粗体文本 + ** + 斜体文本 + _\n\n * 说明：\n   \n   * 斜体 中包含 粗体，其实就是嵌套的关系，外层 是 斜体，内层 是 粗体\n   * 外层是斜体，标识符是单个；内层是粗体，标识符是两个\n   * 因为 粗体 是被包裹在 斜体 中的，所以显示效果为 斜粗体\n\n这是一段普通文本\n\n*这里是一段斜体中**包含粗体**的文字*\n_这也是一段斜体中**包含粗体**的文字_\n*这又是一段斜体中__包含粗体__的文字*\n_这还是一段斜体中**包含粗体**的文字_\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段斜体中包含粗体的文字 这也是一段斜体中包含粗体的文字 这又是一段斜体中__包含粗体__的文字 这还是一段斜体中包含粗体的文字\n\n\n\n# 2.5 粗体包含斜体\n\n * 粗体中包含斜体 的格式：\n   1. ** + 粗体文本 + * + 斜体文本 + * + 粗体文本 + **\n   2. __ + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + __ （ 这里是两个 _ )\n   3. ** + 粗体文本 + _ + 斜体文本 + _ + 粗体文本 + **\n   4. __ + 粗体文本 + * + 斜体文本 + * + 粗体文本 + __\n * 说明：\n   * 粗体 中包含 斜体，也就是嵌套的关系，外层 是 粗体，内层 是 斜体\n   * 外层是粗体，标识符是两个；内层是斜体，标识符是单个\n   * 因为 斜体 是被包裹在 粗体 中的，所以显示效果为 粗斜体\n\n这是一段普通文本\n\n**这里是一段粗体中*包含斜体*的文字**\n__这也是一段粗体中_包含斜体_的文字__\n**这又是一段粗体中_包含斜体_的文字**\n__这还是一段粗体中*包含斜体*的文字__\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n这是一段普通文本\n\n这里是一段粗体中包含斜体的文字 这也是一段粗体中_包含斜体_的文字 这又是一段粗体中_包含斜体_的文字 这还是一段粗体中包含斜体的文字\n\n\n\n\n\n\n# 3. 线\n\n\n\n# 3.1 水平分割线\n\n * 水平分割线由至少 3 个 * 或 - 组成\n\n下面是一条水平分割线：\n---\n***\n\n\n1\n2\n3\n\n\n# 示范\n\n----------------------------------------\n\n----------------------------------------\n\n\n\n# 3.2 文本删除线\n\n * 删除线 的格式：\n   * ~~ + 文本内容 +~~ 首尾各加两个 ~ 波浪号\n\n~~这是一段加了删除线的文本~~\n\n\n1\n\n\n# 示范\n\n这是一段加了删除线的文本\n\n\n\n# 3.3 文本下划线\n\n * 下划线的格式，和 html 是一样的\n   * <u> + 文本内容 + </u>\n\n<u>这是一段加了下划线的文本</u>\n\n\n1\n\n\n# 示范\n\n这是一段加了下划线的文本\n\n\n\n\n\n\n# 4. 列表&引用\n\n\n\n# 4.1 有序列表\n\n * 有序列表 的格式：\n   \n   * 1. + 空格 + 文本内容\n\n * 说明：\n   \n   * 输入文本内容后，敲击 enter 自动补全格式，并进入 下个 有序列表\n   * 若需要在同个列表内，增加 换行显示 的内容 (但不进入下个列表) 敲击 shift + enter ，即可另起一行输入文本\n   * 在有序列表的中间，插入一个新的列表，后面列表的 数字序号 会自动 递进 一层\n   * 即便在源代码模式中修改了数字序号，渲染界面依然是 依照顺序 显示的\n\n1. 这是第一个有序列表 \x3c!-- (enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (enter) --\x3e\n3. 这是第三个有序列表\n\n\n1. 这是第一个有序列表 \x3c!-- (shift + enter) --\x3e\n   这是同个列表下，另起一行的文本内容 \x3c!-- (enter) --\x3e\n2. 这是第二个有序列表 \x3c!-- (shift + enter) --\x3e\n   这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 这是第一个有序列表\n\n 2. 这是第二个有序列表\n\n 3. 这是第三个有序列表\n\n 4. 这是第一个有序列表 这是同个列表下，另起一行的文本内容\n\n 5. 这是第二个有序列表 这是同个列表下，另起一行的文本内容\n\n# 补充\n\n * 由于有序列表存在强制排序性，它的数字序号必然是逐一递进的 若你希望内容前的数字，不依照递进顺序排序，或者以 整百，整十数 排序\n * 可以配合无序列表，在无序列表中输入：\n   * 数字 + . + 内容 #注意 点号 与 内容 之间，没有空格 (其实有空格也行，就是会感觉有点奇怪)\n\n- 10.这是无序列表下，整十数排列的内容\n- 20.这是无序列表下，整十数排列的内容\n- 30.这是无序列表下，整十数排列的内容\n\n\n- 100.这是无序列表下，整百数排列的内容\n- 200.这是无序列表下，整百数排列的内容\n- 300.这是无序列表下，整百数排列的内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n效果：\n\n * 10.这是无序列表下，整十数排列的内容\n * 20.这是无序列表下，整十数排列的内容\n * 30.这是无序列表下，整十数排列的内容\n\n\n * 100.这是无序列表下，整百数排列的内容\n * 200.这是无序列表下，整百数排列的内容\n * 300.这是无序列表下，整百数排列的内容\n\n\n\n# 4.2 无序列表\n\n * 无序列表 的格式：\n * - + 空格 + 文本内容\n * 说明：\n   * 输入文本内容后，敲击 enter 自动补全格式，并进入 下个 无序列表\n   * 若需要在同个列表内，增加换行显示的内容 (但不进入下个列表) 敲击 shift + enter ，即可另起一行输入文本\n * 补充：\n   * 在obsidian中，按下 ctrl + enter\n   * 即可快速生成一个无序列表\n\n- 这是第1个无序列表 \x3c!-- (enter) --\x3e\n- 这是第2个无序列表 \x3c!-- (enter) --\x3e\n- 这是第3个无序列表\n\n- 这是第一个无序列表 \x3c!-- (shift + enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n- 这是第二个无序列表 \x3c!-- (shift + enter) --\x3e\n  这是同个列表下，另起一行的文本内容\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n# 示范\n\n * 这是第1个无序列表\n * 这是第2个无序列表\n * 这是第3个无序列表\n\n\n * 这是第一个无序列表 这是同个列表下，另起一行的文本内容\n * 这是第二个无序列表 这是同个列表下，另起一行的文本内容\n\n\n\n# 4.3 引用\n\n * 引用 的格式：\n   * > + 文本内容 （不需要空格)\n * 说明：\n   * 同个引用段落内的换行直接敲击 enter 即可\n   * 若需添加 第二个独立引用段落 ，连续敲击 两下 enter 即可\n\n>这是第一段引用文本的第1行 \x3c!-- (enter) --\x3e\n>这是第一段引用文本的第2行 \x3c!-- (enter) --\x3e\n\x3c!-- (enter) --\x3e\n>这是第二段引用文本的第1行 \x3c!-- (enter) --\x3e\n>这是第二段引用文本内第2行\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n> 这是第一段引用文本的第1行 这是第一段引用文本的第2行\n\n> 这是第二段引用文本的第1行 这是第二段引用文本的第2行\n\n\n\n# 4.4 缩进&退格\n\n在列表和引用的书写过程中，我们需要利用 ==缩进== 与 ==退格== ，让文章肌理分明，更具层级\n\n * 缩进：\n   \n   1. tab\n   2. ctrl + [   (左中括号)\n\n * 退格：\n   \n   1. shift + tab\n   2. ctrl + ] （右中括号）\n\n\n# 4.4.1 有序列表的缩&退\n\n1. 第一级有序列表1 \x3c!-- (enter) --\x3e\n\t1. 第二级有序列表1    \x3c!-- 写文本之前，先( tab 或 ctrl + ] ) ；写完文本后，再(enter) --\x3e\n\t2. 第二级有序列表2 \x3c!-- (enter) --\x3e\n2. 第一级有序列表2    \x3c!-- 写文本前，先 ( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n * 补充说明：\n   * 有序列表的数字序号，即便你在源代码模式里 强行改掉 数字，它仍然会 依照顺序 显示\n\n# 示范\n\n 1. 第一级有序列表1\n    1. 第二级有序列表1\n    2. 第二级有序列表2\n 2. 第一级有序列表2\n\n\n# 4.4.2 无序列表的缩&退\n\n- 第一级无序列表1 \x3c!-- (enter) --\x3e\n\t- 第二级无序列表1  \x3c!-- 写文本前，先( tab 或 ctrl + ] ) ；写完后，再(enter) --\x3e\n\t- 第二级无序列表2 \x3c!-- (enter) --\x3e\n- 第一级无序列表2  \x3c!-- 写文本前，先 ( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n * 第一级无序列表1\n   * 第二级无序列表1\n   * 第二级无序列表2\n * 第一级无序列表2\n\n\n# 4.4.3 引用的缩&退\n\n * 引用的 缩进 和列表 不同\n   * 引用需另起一行，并额外多打一个 > 来完成 缩进\n * 引用的 退格 与列表 相同\n   1. shift + tab\n   2. ctrl + ] （右中括号）\n\n>第一级引用1 \x3c!-- (enter) --\x3e\n>>第二级引用1 \x3c!-- 先打1个 > (这里的第一个 > 是会自动补充的，只需额外增补1个即可) ，再(enter) --\x3e\n>>第二级引用2 \x3c!-- (enter) --\x3e\n>第一级引用2   \x3c!-- 写文本前，先 ( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n> 第一级引用1\n> \n> > 第二级引用1 第二级引用2\n> \n> 第一级引用2\n\n\n * 补充： 在 obsidian 中，引用的退格是不太一样的\n * **obsidian **中，如果想让已经缩进的引用 退回一层\n   * 得使用 shift + enter ，配合方向键，在多个 > 之间灵活断行 并在下一行 根据需要 选择性补充 >\n * 这个用文字比较难以描述，这里选择用2个带键位的 gif图 来描述\n\ngif演示1：\n\n\n\n\n\n * 效果1：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> > \n> > 444\n> \n> 555\n\n\ngif演示2：\n\n\n\n\n\n * 效果2：\n\n> 111\n> \n> > 222\n> > \n> > > 333\n> \n> > 444\n> > \n> > > 555\n> \n> 666\n\n777\n\n\n# 4.4.4 有序&无序&引用 连续套娃\n\n * 有序列表、无序列表、引用 三者之间，可以相互嵌套\n * 核心键 ： shift + enter & enter & shift + tab ( 或 ctrl + [ )\n   * shift + enter 在切换格式的嵌套中，是 自带一层 缩进 效果的\n\n1. 第一级 有序列表1 \x3c!-- (shift + enter) --\x3e\n\t- 第二级 无序列表1 \x3c!-- (shift + enter) --\x3e\n\t\t>第三级 引用1  \x3c!-- (enter) --\x3e\n\t\t\t- 第四级 无序列表2 \x3c!-- (shift + enter) --\x3e\n            \t1. 第五级 有序列表2 \x3c!-- (enter) --\x3e\n            - 第四级 无序列表3   \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) ；写完后再 (enter) --\x3e\n        >第三级 引用2  \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) ；写完后再 (enter × 2) --\x3e\n    - 第二级 无序列表4  \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) --\x3e\n2. 第一级 有序列表3  \x3c!-- 写文本前，先( shift + tab 或 ctrl + [ ) --\x3e\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n# 示范\n\n 1. 第一级 有序列表1\n    \n    * 第二级 无序列表1\n      \n      > 第三级 引用1\n      > \n      >  * 第四级 无序列表2\n      >    1. 第五级 有序列表2\n      >  * 第四级 无序列表3\n      > \n      > 第三级 引用2\n    \n    * 第二级 无序列表4\n\n 2. 第一级 有序列表3\n\n# 4.4.5 obsidian 的一些缩退问题\n\n * obsidian 在列表首行使用缩进的时候，后续的列表会出现一些问题\n   * tab 和 shift + tab 会无法 缩进 退格\n     * 可以使用 ctrl + ] 与 ctrl + [ 来解决问题\n\n- - 这是第一段就被缩进的列表\n\t- 这是第二段被再次缩进的列表  \x3c!-- 这里需按两次 ctrl + ] ,tab键是无效的 --\x3e\n  - 这是第三段列表  \x3c!-- ctrl + [ --\x3e\n\n\n1\n2\n3\n\n * * 这是第一段就被缩进的列表 - 这是第二段被再次缩进的列表\n     * 这是第三段列表\n\n\n\n\n\n\n# 5. 网页链接与图像\n\n\n\n# 5.1 网页链接\n\n * 网页链接的 格式：\n   * [ + 显示文本内容 + ] + ( + 链接地址 + 空格 + " + 提示信息文本 + " + )\n * 说明：\n   * 显示文本内容，是在渲染界面实际 可见 的文本，用以 说明 链接\n   * 提示信息文本，需鼠标悬停于 显示文本内容 方可触发，用于增加额外提示信息\n     * #注意 "提示信息文本" 是可选项，一般不会填\n     * 一般来讲，需按住 ctrl + 鼠标左键点击 才可跳转链接，不过也有 直接鼠标点击 就能跳转的\n\n[显示文本内容](链接地址 "提示信息文本")\n\n[百度一下，你就知道](http://www.baidu.com "按住ctrl点击跳转百度")\n\n\n1\n2\n3\n\n\n示范：\n\n百度一下，你就知道\n\n\n# 5.1.1链接的加粗\n\n * 格式有两种：\n   \n   1. 把一对 ** 加在 ==显示文本内容==的首尾\n      \n      * 格式1：[**显示文本内容**](链接地址)\n      * 效果： 百度一下，你就知道\n   \n   2. 把一对 ** 加在 链接格式==整体== 的首尾\n      \n      * 格式2：**[显示文本内容](链接地址)**\n      * 效果： 百度一下，你就知道\n\n\n\n\n\n\n# 5.2 图像\n\n * 图像格式：\n   * 图像格式，就是在网页链接前面加个 ! (英文格式的)，! 代表 可见\n   * 图片的提示信息，和网页链接一样，写在 " " 内\n   * [ ] 方括号里的文字信息在 markdown 没啥实质的作用，只是方便在源代码模式下，知道这个图片是什么，在渲染界面是不会显示的。有点类似于html img标签 里的 alt属性。\n\n![文字信息](图片链接 "提示文本信息")\n\n![湘湖1](https://z3.ax1x.com/2021/08/06/funkxq.jpg "湘湖一角")\n\n\n1\n2\n3\n\n\n * 补充：\n   \n   * 图像链接可以是本地的，也可以是在线的\n     * 本地图像直接 ctrl + c 黏贴，ctrl + v 复制 就可以\n     * 在线图像推荐使用 图床\n   * 调整图像的大小需要使用 html 和 css，在 typora编辑器 中右键可以直接缩放图片 本质是转成了html的格式，最后会有一个 style="zoom: %;" ，这里数值可以自己修改\n   * 如果有使用 obsidian 的朋友，在线图片链接是通用的。不过，因为 obsidian 是双向链笔记 它的本地图片格式不太一样\n     * ![[图片名]]\n       * obsidian 中的图片是以双链的格式引用在目标笔记中，用 ! 使它可见\n       * obsidian的图片设置大小是用 | 分隔，后面写宽度数值，单位是px。 设定好宽度，高度会自动等比例调整\n         * ![[图片名|宽度数值]] - 若想自主调整图片宽高，则用： - ![[图片名|宽度数值x高度数值]] - #提示 这里的 x 是 英文字母x\n     * 如果是在线图床，需要调整图片大小：\n       * ![图床|宽度数值](链接地址)\n\n# 示范\n\n\n\n\n\n\n\n\n# 6. 表格\n\n * markdown的表格，比html简单很多\n   * | 是构成表格的主要 框架\n   * - 区分 表头 和 表格主体\n   * : 控制 表格内 文本内容 的 对齐方式\n   * **typora编辑器中 ** 输入 ctrl + t 即可快速插入表格，自由定义样式\n\n|这里是表头1|这里是表头2|这里是表头3|\n|:-|:-:|-:|    \x3c!--区分表头和表格主体，:代表文本对齐方式，分别是左对齐，居中对齐，右对齐--\x3e\n|单元格数据1|单元格数据2|单元格数据3|\n|单元格数据4|单元格数据5|单元格数据6|\n\n\n1\n2\n3\n4\n\n\n# 示范\n\n这里是表头1   这里是表头2   这里是表头3\n单元格数据1   单元格数据2   单元格数据3\n单元格数据4   单元格数据5   单元格数据6\n\n\n\n# 6.1 表格中文本内容的换行\n\n * mardown中表格，它的宽高是由 单元格数据内的文本内容 撑开 的\n * 当我们输入一段很长很长的文本，它所在的单元格会变得过宽\n\n如下图所示：\n\n表头1                                   表头2\n这是一段很长很长很长很长很长很长很长很长很长很长很长很长很长很长的文本   普通文本\n\n * 若想对一段长文本进行换行，可以在 中间 插入一个 <br> （ 换行标签 )\n\n| 表头1 |  表头2 |\n|:-:|:-:|\n|这是第一行文本<br>这是另起一行的文本|普通文本|\n\n\n1\n2\n3\n\n\n# 示范\n\n表头1         表头2\n这是第一行文本     普通文本\n这是另起一行的文本\n\n\n\n\n\n\n# 7. 代码域\n\n\n\n# 7.1 行内代码\n\n * 行内代码 的格式：\n   * 输入两个 ` 反引号 ，在中间写代码内容\n * 补充：\n   * 行内代码不一定非得写代码，也可以作为**着重标记**，突出显示内容\n   * 行内代码中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 所谓行内代码： 只要你的屏幕足够宽，它就不会换行\n\n`这是一段行内代码`\n\n`<table border="1" cellspacing="0" width="500" height="500">`\n\n`print("hello, world!")`\n\n`这是一行突出显示的文本内容`\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n# 示范\n\n<table border="1" cellspacing="0" width="500" height="500">\n\n\nprint("hello, world!")\n\n\n这是一行突出显示的文本内容\n\n\n\n# 7.2 代码块\n\n * 代码块 的格式：\n   1. 在首行和末行各加 三个 ` 反引号\n   * ``` + 语言种类 代码内容 ```\n   2. 在首行和末行各加 三个 ~ 波浪号\n      * ~~~ + 语言种类 代码内容 ~~~\n * 补充：\n   * 在代码块也不一定要写代码，可以写一段突出的文本内容，语言类型可以填写 txt 或者 干脆不写\n   * 代码块中，源代码界面和渲染界面是完全一致的，标识符会失效\n   * 在 typora编辑器 ，用键盘按键脱离代码块区域，需输入： ctrl + enter\n\n```语言种类\n代码内容\n代码内容\n代码内容\n```\n\n下面是html代码块\n\n```html\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n```\n\n下面是css代码块\n\n```css\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%,red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n```\n\n下面是javascript代码块\n\n```js\n    // 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n        sum += arr.shift();\n        count++;\n        if (count % 5 === 0) {\n            newarr.push(sum / 5);\n            sum =  0;\n        }\n    }\n    console.log(newarr);\n\n    let arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\n    let newarr = [];\n    for (let i = 0, len = arr.length; i < len / 5; i++) {\n        let subarr = arr.splice(0, 5)\n        for (let j = 0, sum = 0; j < subarr.length; j++) {\n            sum += subarr[j];\n        }\n        newarr.push(sum / 5);\n    }\n    console.log(newarr);\n```\n\n\n下面是python代码块\n\n```python\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "good bye!"\n```\n\n下面是一块突出显示的文本\n\n```txt\n这是一段\n突出显示的\n文本内容\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n\n\n# 示范\n\n<table border="1">\n    <tr>\n        <td>row 1, cell 1</td>\n        <td>row 1, cell 2</td>\n    </tr>\n    <tr>\n        <td>row 2, cell 1</td>\n        <td>row 2, cell 2</td>\n    </tr>\n</table>\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n.box {\n\twidth: 600px;\n\theight: 400px;\n\tmargin: 100px auto;\n\tbackground-image: linear-gradient(black 33.3%, red 33.3%, red 66.6%, yellow 66.6%, yellow);\n}\n\n\n1\n2\n3\n4\n5\n6\n\n\n// 定义一个30个整数的数组，按顺序分别赋予从2开始的偶数；然后按顺序每五个数求出一个平均值，放在另一个数组中并输出。试编程\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, count = 0, sum = 0, len = arr.length; i < len; i++) {\n\tsum += arr.shift();\n\tcount++;\n\tif (count % 5 === 0) {\n\t\tnewarr.push(sum / 5);\n\t\tsum =  0;\n\t}\n}\nconsole.log(newarr);\n\nlet arr = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]\nlet newarr = [];\nfor (let i = 0, len = arr.length; i < len / 5; i++) {\n\tlet subarr = arr.splice(0, 5)\n\tfor (let j = 0, sum = 0; j < subarr.length; j++) {\n\t\tsum += subarr[j];\n\t}\n\tnewarr.push(sum / 5);\n}\nconsole.log(newarr);\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\ni = 2\nwhile(i < 100):\n   j = 2\n   while(j <= (i/j)):\n      if not(i%j): break\n      j = j + 1\n   if (j > i/j) : print i, " 是素数"\n   i = i + 1\n\nprint "good bye!"\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n这是一段\n突出显示的\n文本内容\n\n\n1\n2\n3\n\n\n\n# 7.2.1 代码块的嵌套\n\n\n格式：\n\n * 使用4个 ` 包裹 3个 `\n\n# 示范\n\n````txt\n```js\n// 3. 输出 100以内(不包括100) 所有偶数的和\n// 这类求和问题的核心 ： 利用循环  (总和 = 旧数的和 + 新数)\n\nlet sum = 0;\n\nfor (let i = 1, sum = 0; i < 100; i++) {\n if (i % 2 == 0) {\n // 筛选偶数\n sum += i; // sum = sum + i // 累加偶数并赋值给sum\n // sum为(旧的，已经进入循环的数)的和，i 为新进入循环的数。当加到(最后一个新数i)时，sum就是最后的 总和\n }\n}\n\nconsole.log(sum); // 打印总和\n```\n````\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n\n\n\n如果要再套一层，就在最外层 加 5个 ` ，以此类推……\n\n\n\n# 7.3 如何在行内代码里显示反引号\n\n首尾各用 两个反引号`+ 空格 包裹\n\n格式：\n\n``+空格+带`的内容+空格+``  \x3c!-- 不要忘记前后的两个空格 --\x3e\n\n`` 这是一段能显示`反引号`的行内代码 ``\n\n\n1\n2\n3\n\n\n效果：\n\n这是一段能显示`反引号`的行内代码\n\n\n\n\n\n\n# 8. 任务列表（待办）\n\n * 任务列表 的格式：\n   \n   * - + 空格 +[ ] +空格 + 任务列表内容 ( 中括号[ ] 里面必须有个空格)\n   * 给待办任务列表打 √ ，变成 已办\n     1. 在渲染界面，直接鼠标左键点击框框\n     2. 在源代码界面，在中括号内输入 英文字母x\n        * 部分编辑器，在 中括号内 输入任意字符都可以打 √ ( 例如 obsidian )\n\n * 补充：\n   \n   * 大部分 md编辑器 支持输入第一个任务列表后，按下 enter 进入下一行会 自动补全待办格式\n   * 在obsidian中，连续输入两次 ctrl + enter ，即可生成一个待办列表\n     * 再输入一次 ctrl + enter ，会在待办列表 打 √\n\n * 格式：\n\n- [ ] 待办任务列表1\n- [ ] 待办任务列表2\n- [x] 已办任务列表1    \x3c!-- 英文字母x --\x3e\n- [x] 已办任务列表2\n\n\n1\n2\n3\n4\n\n\n\n# 示范\n\n * [ ] 待办任务列表1\n * [ ] 待办任务列表2\n * [x] 已办任务列表1\n * [x] 已办任务列表2\n\n\n * 在 obsidian 中，可以利用 ctrl + enter ，快速生成任务列表\n   1. - + 空格 + ctrl + enter +待办文本内容\n   2. 待办文本内容 + ctrl + enter ×2   ( 输入文本后，连续2次 ctrl + enter )\n\n\n * 任务列表也是可以缩进+退格的，操作跟 无序、有序列表一样\n\n\n# 示范\n\n * [ ] 第一级待办列表1\n   * [ ] 第二级待办列表1 另起一行的第二级待办列表1\n     * [x] 第三级已办列表1\n     * [x] 第三级已办列表2\n   * [ ] 第二级待办列表2 另起一行的第二级待办列表2\n * [ ] 第一级待办列表2\n\n\n\n\n\n\n# 9. 注释\n\nmarkdown 的 注释 和 hmtl 一样，注释的内容在 渲染界面 不可见 （部分编辑器可见)\n\n * 注释 的格式：\n   * \x3c!-- 这里是注释的内容 --\x3e\n     * 注释可以是单行，也可以是多行\n   * 如果有在使用 obsidian 的，它的注释格式是不一样的\n     * %%这是obsidian的注释内容%%\n\n\x3c!-- 这里是一行注释 --\x3e\n\n\x3c!--\n这里是\n一段\n假装有\n很多行的\n注释\n--\x3e\n\n%%这是一行obsidian里的注释%%\n\n%%\n这里是\n一段\n假装有\n很多行的\nobsidian里的\n注释\n%%\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n# 示范 (只有切换至 编辑模式 才能看到喔)\n\n%%这是一行obsidian里的注释%%\n\n%% 这里是 一段 假装有 很多行的 obsidian里的 注释 %%\n\n\n\n\n\n\n# 10. 变量\n\n\n\n# 10.1 网页链接变量\n\n * 网页链接变量 的格式：\n   1. 首先输入\n      * [显示文本内容] + [变量名]\n        * 变量名可以自己取，没啥限制，任意字符都可以\n   2. 在文档任意一个区域，输入：\n      * [变量名] + : + 空格 + 链接地址 （这个**空格** 不打也没事)\n\n[百度一下，你就知道][度娘]\n[知乎-有问题，就会有答案][知乎]\n\n\x3c!-- 这里是变量区域 --\x3e\n[度娘]: http://www.baidu.com\n[知乎]: https://www.zhihu.com\n\n\n1\n2\n3\n4\n5\n6\n\n\n# 示范\n\n百度一下，你就知道\n\n知乎-有问题，就会有答案\n\n\n\n# 10.2 脚注\n\n * 脚注 的格式：\n   * 在需要脚注的地方，输入：\n     * [^脚注代号] ( 脚注代号会直接显示在渲染界面 )\n       * 脚注代号可以随便命名，不过推荐使用 数字序号\n   * 在其他区域，输入：\n     * [^脚注代号] + : + 空格 + 脚注内容 （这个 空格 不打也没事)\n\n鲁迅原名是什么[^1] ，浙江哪里人[^2]\n\n\x3c!-- 这里是变量区域 --\x3e\n[^1]: 周树人\n[^2]: 绍兴人\n\n\n1\n2\n3\n4\n5\n\n\n# 示范\n\n鲁迅原名是什么^1，浙江哪里人^2\n\n\n\n\n\n\n# 11. 拓展文本格式标记\n\n * markdown 想实现更多的文本显示效果，只能依赖html标记实现\n * 个人不是很推荐在 md 中使用 html，不过一些简单的标记还是可以 轻度使用 的\n\n\n\n# 11.1 键盘文本\n\n * 键盘文本的 格式：\n   \n   * <kbd>键盘文本</kbd>\n   * <kbd>ctrl</kbd> + <kbd>x</kbd>\n\n * 效果：\n   \n   * 键盘文本\n   * ctrl + x ( 剪切 )\n\n * 说明：\n   \n   * 键盘文本也不一定非得是键盘按键，也可以作为着重文本突出显示\n     * 效果： 这也算一种着重文本的方式\n\n# 11.1.1 加粗键盘文本\n\n * 加粗键盘文本的格式有两种：\n   \n   * <kbd>**键盘文本**</kbd>\n   * **<kbd>ctrl + x</kbd>**\n\n * 效果：\n   \n   1. 键盘文本\n   2. ctrl + x\n\n\n\n# 11.2 放大文本\n\n * 放大文本 的格式：\n   \n   * 这是一段普通文本 <big>这是一段放大文本</big>\n\n * 效果：\n   \n   * 这是一段普通文本 这是一段放大文本\n\n# 11.2.1 放大粗体文本\n\n * 放大加粗文本的格式有两种：\n   1. **<big>这是一段放大粗体文本</big>**\n   2. <big>**这是一段放大粗体文本**</big>\n * 效果：\n   1. 这是一段放大粗体文本\n   2. 这是一段放大粗体文本\n\n\n\n# 11.3 缩小文本\n\n * 缩小文本 的格式：\n   * 这是一段普通文本 <small>这是一段缩小文本</small>\n * 效果：\n   * 这是一段普通文本 这是一段缩小文本\n\n# 11.3.1 缩小斜体文本\n\n * 斜体缩小文本 的格式有两种：\n   1. <small>*这是一段缩小斜体文本*</small>\n   2. *<small>这是一段缩小斜体文本</small>*\n * 效果：\n   1. 这是一段缩小斜体文本\n   2. 这是一段缩小斜体文本\n\n\n\n# 11.4 多彩文本\n\n * 多彩文本 的格式：\n   * <font color=orange>这是一段橘色文本</font>\n * 效果：\n   * 这是一段橘色文本\n     * color 里的颜色支持 英文单词，16进制，rgb，rgba\n\n\n# 11.4.1 多彩粗体文本\n\n * 只需要在上面示例的基础上，加上 加粗标识符，有两种格式：\n   1. 格式1： **<font color=teal>这是一段加粗的水鸭色文本</font>**\n      * 效果： 这是一段加粗的水鸭色文本\n   2. 格式2： <font color=teal>**这是一段加粗的水鸭色文本**</font>\n      * 效果： 这是一段加粗的水鸭色文本\n * 若上述混搭方法的样式失效 ，可以使用 纯html标记\n   * 格式： <strong style="color:teal;">这是一段加粗的水鸭色文本</strong> (标记略复杂，不是很推荐)\n   * 效果： 这是一段加粗的水鸭色文本\n\n\n# 11.4.2 多彩斜体文本\n\n * 跟多彩加粗文本完全一样，只需把首尾的 ** 换成 * 即可\n\n 1. 格式1： *<font color=teal>this is an italic teal text</font>*\n    * 效果： this is an italic teal text\n 2. 格式2： <font color=teal>*this is an italic teal text*</font>\n    * 效果： this is an italic teal text\n\n\n# 11.4.2 多彩粗斜体文本\n\n * 首尾换成 ***\n\n 1. 格式1： ***<font color=teal>this is a bold italic teal text</font>***\n    * 效果： this is a bold italic teal text\n 2. 格式2： <font color=teal>***this is a bold italic teal text***</font>\n    * 效果： this is a bold italic teal text\n\n\n#注意 多彩文本尽量慎用，markdown 的核心就是 简洁精炼，注重 实质内容，而非花哨的 颜色样式\n\n\n\n\n\n\n# 12. 拓展文本显示效果\n\n * 拓展显示效果既不是原生 markdown语法 支持的，也非 html标记，而是部分编辑器 提供的 额外标识符，属于拓展语法，旨在为 markdown使用者 提供更多样式选择\n * 不同编辑器，支持不一样，这里以 typora编辑器 为例\n\n\n\n# 12.1 文本高亮\n\n * 文本高亮 的格式：\n   * ==这里是一段高亮文本==\n * 效果：\n   * ==这里是一段高亮文本==\n\n\n\n# 12.2 上标\n\n * 用一对 ^ 包裹 (shift+ 6)\n   * 格式： x^2^\n   * 效果： x^2^\n * obsidian 没效果的，可以用后面会讲的 latex\n * 或者，也可以使用 html标记\n   * <sup>这里是上标内容</sup>\n   * x<sup>2</sup>\n * 效果：\n   * x2\n\n\n\n# 12.3 下标\n\n * 用一对 ~ 包裹 (shift + `)\n   * 格式： h~2~o\n   * 效果： h~2~o\n * obsidian 没效果的，可以用后面会讲的 latex\n * 或者，也可以使用 html标记\n   * <sub>这里是下标内容</sub>\n   * h<sub>2</sub>o\n * 效果：\n   * h2o\n\n\n\n# 12.4 emoji 符号\n\n用一对 : 包裹，里面是 emoji 符号的 语义化文本 ( typora编辑器 中，输入 : 就会带提示器 )\n\n * 示例：\n   * :smile: :sweat: :cat: :woman_cartwheeling:\n * 效果：\n   * 😄 😓 🐱 🤸‍♀\n\n\n * 补充：\n   * 不支持上述方式的 md编辑器或笔记软件，直接用 输入法 输入也是可以的\n   * windows系统 用户 win + . 就可以输入 emoji 了\n   * obsidian 用户可以安装第三方插件来支持 emoji 的输入，推荐两个\n     1. ==emoji shortcodes==\n     2. ==emoji toolbar==\n\n\n\n\n\n\n# 13. 转义字符\n\n * 在 markdown 中，我们 通过 标识符 改变 文本显示效果\n * 现在我们希望它不作为标识符，而是 作为字符本身呈现出来 （不具备改变文本显示效果的功能，只是一个普通字符)\n   * 首先我们可以用前面介绍的 代码域 ，因为代码模式的显示效果就是源代码完全一致的\n   * 还有一种方法，可以利用转义字符，在这些标识符 前面 加上 反斜线 \\ ( 反斜线要紧贴在标识符前面，不能 有 空格 )\n     * 原理：\n       * \\ 的作用是让标识符 转义 变为一个普通字符，完成这个效果后，反斜线会自动隐藏\n       * 隐藏后的反斜线仅在源代码界面可见，在渲染界面不可见\n       * 反斜线只争对标识符起作用，其他字符添加 \\，\\ 不会自动隐藏\n     * 补充：\n       * 如果想给已经被加在标识符前面，会自动隐藏的 \\ 显示出来，可以在反斜线前面再加一个 \\ ，用它自己来转义自己\n         * 示例： 这里紧跟在标识符前面的反斜线\\\\*会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体*\n         * **效果： ** 这里紧跟在标识符前面的 反斜线\\会被转义成普通字符显示出来，不会自动隐藏，且这段文件会是斜体\n\n\n\n# 例1 以普通字符显示星号\n\n * 如何让被一对或多对 * 号 包裹的文本内容，能够正常显示 * ，且文本不改变格式\n   * \\*这段文本被一对星号包裹，但不会倾斜\\*\n     * 效果： *这段文本被1对星号包裹，但不会倾斜*\n   * \\*\\*这段文本被2对星号包裹，但不会加粗\\*\\*\n     * 效果： **这段文本被2对星号包裹，但不会加粗**\n   * \\*\\*\\*这段文本被3对星号包裹，但它既不倾斜也不加粗\\*\\*\\*\n     * 效果： ***这段文本被3对星号包裹，但它既不倾斜也不加粗***\n\n\n\n# 例2 表格内 单元格中的竖杠\n\n * 在表格中，使用 | 作为单元格的内容，但不会被识别为表格的结构，不会增加额外的单元格\n\n|表头1|表头2|\n|-|-|\n|这里的文本被\\|分隔|这里的文本也被\\|分隔|\n\n\n1\n2\n3\n\n * 效果：\n\n表头1         表头2\n这里的文本被|分隔   这里的文本也被|分隔\n\n\n#补充 该技巧可用于 obsidian 表格内 双链的文本修饰\n\n文本修饰：\n\n在 双链[[ ]]内 以 | 引导的内容\n\n * 格式： [[链接的内容|文本修饰]]\n * 说明： 文本修饰是渲染界面实际显示的文本，便于更好地融入语境\n\n表格内的格式：\n\n在 | 前面加上 \\\n\n * [[表格内的链接内容\\|文本修饰]]\n\n示例：\n\n|                  表头1                  |                        表头2                        |\n|:---------------------------------------:|:---------------------------------------------------:|\n| [[#例2 表格内 单元格中的竖杠\\|单元格中的竖杠]] | [[#例3 不会变成代码的反引号\\|不会变成代码的反引号]] |\n\n\n1\n2\n3\n\n\n效果：\n\n表头1                           表头2\n[[#例2 表格内 单元格中的竖杠|单元格中的竖杠]]   [[#例3 不会变成代码的反引号|不会变成代码的反引号]]\n\n\n\n# 例3 不会变成代码的反引号\n\n使用 转义符号\\ 让 反引号` 变成普通字符，不再具有[[#7 1 行内代码|行内代码]]的标识符功能\n\n格式：\n\n\\`这段被反引号包裹的内容不会变成行内代码\\`\n\n效果：\n\n`这段被反引号包裹的内容不会变成行内代码`\n\n\n\n# 例4 链接中的中括号\n\n在 网页链接 的 显示文本内容 中，使用 中括号 [ ]\n\n * 在显示文本内容中，在其中一个中括号前面，加上转义符号 反斜杠 \\\n   * 格式： [链接里的 \\[中括号\\] 能被正常显示](https://www.runoob.com)\n   * 效果： 链接里的 [中括号] 能被正常显示\n\n\n\n# 例5 不是列表的连接符(横杠)\n\n * 引用一段话，一般会在换行之后，加上 - 出处\n * 因为 - 是标识符，会变成一个无序列表\n\n如下所示：\n\n> the web, the tree, and the string. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n> \n>  * 史蒂芬·平克\n\n * 解决方法：\n   \n   * 在 - 前面加上 转义符号 \\\n   \n   >the web, the tree, and the string.\n   >写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。\n   >\\- 史蒂芬·平克   \x3c!-- 加上转义符号 \\ , 不会变成无序列表 --\x3e\n   \n   \n   1\n   2\n   3\n   \n\n * 效果：\n\n> the web, the tree, and the string. 写作之难，在于把网状的思考，用树状结构，体现在线性展开的语句里。 - 史蒂芬·平克\n\n\n\n# 例6 不是标题的 #\n\n让 # 不被识别为标题标识符\n\n格式：\n\n\\# 这里的内容不会被识别为标题\n\n效果：\n\n# 这里的内容不会被识别为标题\n\n\n\n# 例7 不会注释的 %\n\n在 obsidian 中 注释是前后各两个 % 号\n\n使用 转义符号\\，让 %% 作为普通字符显示出来，不具备注释的功能\n\n * 格式： \\%\\%这里的内容可以被显示喔\\%\\%\n * 效果： %%这里的内容可以被显示喔%%\n\n\n\n# 例8 木有链接的双链\n\nobsidian 的双向链格式是2个方括号 [[ ]] (双方)，使用 转义符号\\，让 [ ] 不再具有 双链功能\n\n格式：\n\n\\[\\[这段文本被双方包裹，但不是一个双向链\\]\\]\n\n效果：\n\n[[这段文本被双方包裹，但不是一个双向链]]\n\n\n\n# 例9 页链接里 显示文本内的 中括号\n\n使用转义符号\\，让中括号可以作为显示文本 在[[#5 1 网页链接|网页链接]]中显示出来\n\n格式：\n\n[\\[这是一个带中括号的网页链接显示文本，点击会跳转至百度\\]](https://www.baidu.com/)\n\n\n1\n\n\n效果：\n\n[这是一个带中括号的网页链接显示文本，点击会跳转至百度]\n\n\n\n# 特殊情况 文本修饰的中括号\n\n文本修饰的 中括号[ ] 不需要使用 转义符号\\\n\n示范：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n效果：\n\n[[#例8 木有链接的双链|[这是一个带中括号的文本修饰]]]\n\n\n\n\n\n\n# 14. 空格&换行&强制删除\n\n\n\n# 14.1 空格\n\n * 在一些编辑器或者支持md的笔记软件里，无论你打多少个空格，它只会显示单个 空格 的距离\n   * 可以使用 html中 空格 的 字符实体 —— &nbsp;\n   * 若要添加 多个 空格，就输入多个 —— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n * 格式：\n   * 这里有&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6个空格分隔\n * 效果：\n   * 这里有      6个空格分隔\n\n\n\n# 14.2 换行\n\n场景1：\n\n * 在一些编辑器或者支持md的笔记软件里，无论你打多少个 回车，它只会显示单个 回车 的空行间距\n   * 可以使用之前表格里提到的 <br> 标签，在 单独一行 中使用，增加额外的空行间距\n   * 如果要增加 多个，就输入 多个 —— <br><br><br><br><br>\n   * #注意 当单独一行使用 <br> 标签的时候，如果前后有标题标识符或者列表标识符，确保 br元素 前后两行都是空白行\n\n格式：\n\n这里是第一段文本\n\n<br><br><br><br><br>     \x3c!-- 这里插入了5个空行间距 --\x3e\n\n这里是第二段文本\n\n\n1\n2\n3\n4\n5\n\n\n效果：\n\n这里是第一段文本\n\n\n\n\n\n\n\n\n这里是第二段文本\n\n\n\n\n\n场景2：\n\n * 在列表中也可以插入换行符\n\n- 这是一段无序列表\n  <br>     \x3c!-- 插入一个空行间距，需单独一行，上下不用预留空格 --\x3e\n  这是同一段无序列表中，空一行距离显示的内容\n- 这是第二段无序列表\n\n\n1\n2\n3\n4\n\n\n效果：\n\n * 这里是第一段无序列表\n   这里是同一段无序列表中，空一行距离显示的内容\n * 这里是第二段无序列表\n\n\n * 补充：\n   * 有一些md编辑器或笔记软件，严格遵循md的换行规则，你敲一个回车是没法换行的，必须在 行末 敲 2个空格，再按回车键\n     * 格式：\n       * 这里是一段想换行的文本空格 空格 enter 这是换行后的文本\n\n\n\n# 14.3 强制删除\n\n * 很多编辑器都有英文标点自动补全功能，自动生成一对，光标落在中间 只想删除前面1个，却会把 一整对 都删掉\n * 在多个列表的嵌套中，也许会遇到一些 无法被删除 的 列表标识符\n * 解决方法： 使用 shift + backspace 即可强制删除\n   * bcakspace   ( 退格键 )\n\n\n\n\n\n\n# 15. 嵌入\n\n * 嵌入都是依赖 html标签 实现的，嵌入的都是在线链接格式\n   * 如果是本地的，obsidian 中音频是有自带的可录制的录音机插件的，其他的 音频、视频 直接复制黏贴就可以了，也可以直接拖拽到ob的笔记界面\n     * 其他的媒体文件在 obsidian 也和图片一样，以双链的格式引用在目标笔记中，使用 ! 使它可见\n\n\n\n# 15.1 嵌入音频\n\n * 格式：\n   \n   * <audio controls="controls" preload="none" src="音频链接地址"></audio>\n\n * 示例：\n\n<audio controls="controls" preload="none" src="https://www.ldoceonline.com/media/english/exaprons/p008-001803372.mp3?version=1.2.37"></audio>\n\n\n1\n\n * 效果：\n\n\n\n\n\n# 15.2 嵌入视频\n\n * 格式：\n\n<video width="600" height="420" controls>\n  <source src="movie.mp4" type="video/mp4">\n  <source src="movie.ogg" type="video/ogg">\n  <source src="movie.webm" type="video/webm">\n</video>\n\n\n1\n2\n3\n4\n5\n\n * 说明：\n   * width ( 宽度 ) height ( 高度 ) ，可以自己设置，直接输入数字即可，单位默认是 px(像素) 也可以使用 百分比 width=100% 代表水平撑满整个窗口 height=50% 代表垂直撑满半个窗口\n   * video标签 支持的视频格式 ：mp4 ogg webm\n\n\n\n# 15.3 嵌入页面\n\n * 格式： <iframe width=600 height=400 src="页面链接地址" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n<iframe width=600 height=400 src="https://www.runoob.com/html/html-tutorial.html" scrolling="auto" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 效果：\n\n\n * iframe标签 除了嵌入页面，也可以嵌入在线视频，主流的视频网站都会提供嵌入代码\n   \n   * 具体可以看这个 iframe视频嵌入教程\n   * b站 的视频，得在 // 前面补充 http:\n   * 不是所有的 编辑器和笔记软件 都支持这个\n\n * 示例：\n\n<iframe width=600 height=400 src="http://player.bilibili.com/player.html?aid=20190823&bvid=bv1yw411s7og&cid=32964980&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>\n\n\n1\n\n * 宽高设置和前面的 video 一样\n\n\n * 效果：\n\n\n\n\n\n\n# 16. latex 数学公式\n\n * 主要用于 数学公式 与 化学公式 的书写\n\n\n\n# 16.1 行内公式\n\n * 格式：\n   \n   * $ + 行内公式 + $\n\n\n * 示例：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $\\ce{co2 + c -> 2 co}$\n   * $\\ce{co2 + c -> 2 co}$\n   * $\\ce{2mg + o2 ->[燃烧] 2 mgo}$\n\n\n * 效果：\n   * $x^2 + 2x + 5 + \\sqrt x = 0$\n   * $e^{i\\pi} + 1 = 0$\n   * $\\ce{co2 + c -> 2 co}$\n   * $\\ce{2mg + o2 ->[燃烧] 2 mgo}$\n\n\n\n# 16.2 公式块\n\n * 格式：\n   * $$ 公式块 $$\n\n\n * 示例：\n\n% 化学公式\n$$\n\\ce{zn^2+  <=>[+ 2oh-][+ 2h+]  $\\underset{\\text{amphoteres hydroxid}}{\\ce{zn(oh)2 v}}$  <=>[+ 2oh-][+ 2h+]  $\\underset{\\text{hydroxozikat}}{\\ce{[zn(oh)4]^2-}}$}\n$$\n\n\n1\n2\n3\n4\n\n\n% 麦克斯韦方程组\n$$\n\\begin{array}{lll}\n\\nabla\\times e &=& -\\;\\frac{\\partial{b}}{\\partial{t}}\n\\ \\nabla\\times h &=& \\frac{\\partial{d}}{\\partial{t}}+j\n\\ \\nabla\\cdot d &=& \\rho\n\\ \\nabla\\cdot b &=& 0\n\\ \\end{array}\n$$\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n% 薛定谔方程\n$$\ni\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + v \\psi\n$$\n\n\n1\n2\n3\n4\n\n\n * 效果：\n\n$$ % 化学公式 \\ce{zn^2+ <=>[+ 2oh-][+ 2h+] $\\underset{\\text{amphoteres hydroxid}}{\\ce{zn(oh)2 v}}$ <=>[+ 2oh-][+ 2h+] $\\underset{\\text{hydroxozikat}}{\\ce{[zn(oh)4]^2-}}$} $$\n\n\n$$ % 麦克斯韦方程组 \\begin{array}{lll} \\nabla\\times e &=& -;\\frac{\\partial{b}}{\\partial{t}} \\ \\nabla\\times h &=& \\frac{\\partial{d}}{\\partial{t}}+j \\ \\nabla\\cdot d &=& \\rho \\ \\nabla\\cdot b &=& 0 \\ \\end{array} $$\n\n\n$$ i\\hbar\\frac{\\partial \\psi}{\\partial t} = \\frac{-\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2} \\right) \\psi + v \\psi $$\n\n * 补充：\n   * 需要详细教程的，可戳下方链接\n   * latex详细教程\n\n\n\n\n\n\n# 17. mermaid\n\n * 一些 md编辑器 和 笔记软件 支持通过 mermaid 及其所提供的 编译器 来为用户提供图表的绘制功能\n\n * 这里只提供一些演示的图表，具体教程可戳下方\n   \n   * [[moc mermiad 教程 obsidian版| mermiad 超级教程 obsidian版]]\n\n\n\n# 17.1 流程图\n\n\n源码1：\n\n```mermaid\ngraph tb\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点2\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点1\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n渲染1：\n\ngraph tb\n\t%% s=start  e=end  f=fork  n=normal\n\n\ts([开始])--\x3ef1{{if条件}};\n\n\t%% 分支点1\n\tf1--true--\x3en1[if语句块]--\x3ee([结束]);\n\tf1--false--\x3ef2{{else if条件}};\n\n\t%% 分支点2\n\tf2--true--\x3en2[else if语句块]--\x3ee;\n\tf2--false--\x3en3[else语句块]--\x3ee;\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n源码2：\n\n```mermaid\ngraph lr\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n渲染2：\n\ngraph lr\n\t%% s=start  e=end  f= fork n=normal\n\n\t%% 虚线\n\ts[朱百六]-.->|子|n1[朱四九]-.->|子|n2[朱五四]-.->|子|f1_帝((朱八八))\n\n\t%% 分支点 朱八八\n\tf1_帝--\x3e|长子|f2[朱标]\n\tf1_帝--\x3e|次子|n3[朱樉]\n\tf1_帝--\x3e|三子|n4[朱棢]\n\tf1_帝--\x3e|四子|n5_帝((朱棣))\n\n\t%% 分支点 朱标\n\tf2--\x3e|长子|e1[朱雄英]\n\tf2--\x3e|次子|e2_帝((朱允炆))\n\n\tn5_帝--\x3e|长子|e3[朱高炽]\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n# 17.2 饼图\n\n\n源码：\n\n```mermaid\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n渲染：\n\npie\n    title 为什么总是宅在家里？\n    "喜欢宅" : 45\n    "天气太热" : 70\n    "穷" : 500\n\t"关你屁事" : 95\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n# 17.3 序列图 (时序图)\n\n\n源码：\n\n```mermaid\nsequencediagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant a as aly\n        participant b as bob\n        participant c as cofcai\n        %% 便签说明\n        note left of a: 只复习了一部分\n        note right of b: 没复习\n        note over a,b: are contacting\n\n        a->>b: 明天是要考试吗？\n        b--\x3e>a: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par aska\n                c --\x3e> a:你复习好了吗？\n            and askb\n                c --\x3e> b:你复习好了吗？\n            and self\n                c ->>c:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/every min\n            %% <br/>可以换行\n            c ->> c:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                a ->> c:复习了一点\n            else is common\n                b ->> c:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt extra response\n                c ->> c:你们怎么不回答我\n            end\n        endsequencediagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant a as aly\n        participant b as bob\n        participant c as cofcai\n        %% 便签说明\n        note left of a: 只复习了一部分\n        note right of b: 没复习\n        note over a,b: are contacting\n\n        a->>b: 明天是要考试吗？\n        b--\x3e>a: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par aska\n                c --\x3e> a:你复习好了吗？\n            and askb\n                c --\x3e> b:你复习好了吗？\n            and self\n                c ->>c:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/every min\n            %% <br/>可以换行\n            c ->> c:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                a ->> c:复习了一点\n            else is common\n                b ->> c:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt extra response\n                c ->> c:你们怎么不回答我\n            end\n        end\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\n\n渲染：\n\nsequencediagram\n\t%% 自动编号\n\tautonumber\n\t%% 定义参与者并取别名，aliases：别名\n        participant a as aly\n        participant b as bob\n        participant c as cofcai\n        %% 便签说明\n        note left of a: 只复习了一部分\n        note right of b: 没复习\n        note over a,b: are contacting\n\n        a->>b: 明天是要考试吗？\n        b--\x3e>a: 好像是的！\n\n        %% 显示并行发生的动作，parallel：平行\n        %% par [action1]\n        rect rgb(0, 25, 155)\n            par aska\n                c --\x3e> a:你复习好了吗？\n            and askb\n                c --\x3e> b:你复习好了吗？\n            and self\n                c ->>c:我还没准备复习......\n            end\n        end\n\n        %% 背景高亮，提供一个有颜色的背景矩形\n        rect rgb(25, 55, 0)\n            loop 自问/every min\n            %% <br/>可以换行\n            c ->> c:我什么时候<br/>开始复习呢？\n            end\n        end\n\n        %% 可选择路径\n        rect rgb(153, 83, 60)\n            alt is good\n                a ->> c:复习了一点\n            else is common\n                b ->> c:我也是\n            end\n            %% 没有else时可以提供默认的opt\n            opt extra response\n                c ->> c:你们怎么不回答我\n            end\n        end\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n\n\n\n\n# 17.4 甘特图\n\n\n源码：\n\n```mermaid\ngantt\n    title a gantt diagram\n    dateformat  yyyy-mm-dd\n    section section\n    a task           :a1, 2014-01-01, 30d\n    another task     :after a1  , 20d\n    section another\n    task in sec      :2014-01-12  , 12d\n    another task      : 24d\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n渲染：\n\ngantt\n    title a gantt diagram\n    dateformat  yyyy-mm-dd\n    section section\n    a task           :a1, 2014-01-01, 30d\n    another task     :after a1  , 20d\n    section another\n    task in sec      :2014-01-12  , 12d\n    another task      : 24d\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n# 17.5 类图\n\n\n源码：\n\n```mermaid\nclassdiagram\n    animal <|-- duck\n    animal <|-- fish\n    animal <|-- zebra\n    animal : +int age\n    animal : +string gender\n    animal: +ismammal()\n    animal: +mate()\n    class duck{\n      +string beakcolor\n      +swim()\n      +quack()\n    }\n    class fish{\n      -int sizeinfeet\n      -caneat()\n    }\n    class zebra{\n      +bool is_wild\n      +run()\n    }\n```\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n渲染：\n\nclassdiagram\n    animal <|-- duck\n    animal <|-- fish\n    animal <|-- zebra\n    animal : +int age\n    animal : +string gender\n    animal: +ismammal()\n    animal: +mate()\n    class duck{\n      +string beakcolor\n      +swim()\n      +quack()\n    }\n    class fish{\n      -int sizeinfeet\n      -caneat()\n    }\n    class zebra{\n      +bool is_wild\n      +run()\n    }\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n\n\n\n\n\n\n\n# 18. 标签 (tag)\n\n * 标签是 obsidian 特有的一个功能，标签可以通过点击唤起快速搜索 (搜索包含该标签的所有笔记)\n\n格式：\n\n * # + 标签名\n   * #标签名\n\n\n# 关于空格\n\n * 在一段正文文本的后面添加 tag， # 的前面 需要有个空格\n   * 空格 + # + 标签名\n\n\n * # 与 标签名 之间，不能有空格，否则就变成 一级标题 了\n\n\n * 标签名的内部，不允许使用空格，若想区分标签中的词语，可使用以下三种方法：\n   1. 驼峰式大小写： #bluetopaz\n   2. 下划线： #blue_topaz\n   3. 连字符： #blue-topaz\n\n\n\n# 关于数字\n\n * 标签内允许使用数字，但不能完全由数字组成\n   * #1984 ❌\n   * #1984date ⭕\n   * #da_1984_te ⭕\n   * #date-1984 ⭕\n\n\n\n# 标签的嵌套\n\n在标签名内，使用 / 斜杠 可以实现标签的嵌套\n\n格式：\n\n * #主标签/子标签1\n * #主标签/子标签2\n * #主标签/子标签3\n\n嵌套标签可以像普通标签一样通过点击来唤起搜索，嵌套标签允许你选择搜索的层次。例如：\n\n * 搜索 #主标签 ，即可找到包含任意一个子标签的所有笔记\n   * 返回的结果会是上述的三个例子\n * 当你在一个主分类下设置了多个子分类，想找到这个主分类包含的所有内容时，该功能会很实用\n\n\n\n# 能被使用的符号\n\n综上所述，标签内能被使用的符号共有三种\n\n 1. _ 下划线\n 2. - 连字符\n 3. / 斜杠\n\n\n\n# 如何让 # 不被识别\n\n可以使用前面提到的转义符号 \\ 反斜杠，与上述的 转义标题 类似\n\n格式：\n\n\\#这里的内容不会被识别为标签\n\n效果：\n\n#这里的内容不会被识别为标签\n\n\n\n# 19. 避免标识符的滥用\n\n即使在 markdown 中，也要尽量避免标识符的滥用\n\n比如我的这篇教程，就存在一定程度的滥用\n\n * 其实是因为我这篇是教学性质的，不太一样，有些不能避免\n   * (好吧，我就是在甩锅)\n\n标识符的本质是突出显示，代表重点\n\n * 一篇笔记里的某段文本，使用各式各样的的标识符，会造成重点不清晰\n\n有三种标识，慎用！\n\n 1. 词中对单个汉字的标识\n    1. 卧==虎==藏==龙==\n 2. 短语中对单个英语单词的标识\n    1. get a ==bang== out of\n 3. 标识符的多层嵌套\n    1. 我感觉快要==原地起飞==了\n\n原因：\n\n * 词义的割裂\n * 视觉的混乱\n * 不利于搜索\n   * 卧==虎==藏==龙==\n     * 搜 卧虎 -- 搜不到\n     * 搜 藏龙 -- 搜不到',charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"how llm works",frontmatter:{title:"how llm works",date:"2024-01-02T15:32:49.000Z",permalink:"/pages/dc7035/"},regularPath:"/05.llm/01.How_LLM_Works.html",relativePath:"05.llm/01.How_LLM_Works.md",key:"v-26a9c736",path:"/pages/dc7035/",headers:[{level:3,title:"1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2012]",slug:"_1-llm-as-os-agents-as-apps-envisioning-aios-agents-and-the-aios-agent-ecosystem-2012",normalizedTitle:"1. llm as os, agents as apps: envisioning aios, agents and the aios-agent ecosystem [2012]",charIndex:174},{level:3,title:"2. NVIDIA Mastering LLM Techniques",slug:"_2-nvidia-mastering-llm-techniques",normalizedTitle:"2. nvidia mastering llm techniques",charIndex:93},{level:3,title:"3. Finetuning",slug:"_3-finetuning",normalizedTitle:"3. finetuning",charIndex:2746},{level:3,title:"4. Function Calling",slug:"_4-function-calling",normalizedTitle:"4. function calling",charIndex:3035}],headersStr:"1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2012] 2. NVIDIA Mastering LLM Techniques 3. Finetuning 4. Function Calling",content:" 1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2023]\n 2. NVIDIA Mastering LLM Techniques\n\n----------------------------------------\n\n\n# 1. LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem [2012]\n\n👍 Analogy of LLM and OS.\n\nBlog: https://huggingface.co/blog/shivance/illustrated-llm-os\n\nYoutube: Andrej Karpathy https://www.youtube.com/watch?v=kCc8FmEb1nY\n\nParallel decoding(Multi threading): This is a technique that allows multiple decoding processes to occur simultaneously, which can speed up the decoding process. For example, instead of generating one token at a time, parallel decoding can generate several tokens in parallel, using different models or different parts of the same model. This can reduce the latency and increase the throughput of the decoding process. A recent paper by Apple researchers proposed a method called Parallel Speculative Sampling (PaSS) that introduces parallel decoding for LLMs, maintaining model quality while achieving remarkable speed. Related Paper: Accelerating LLM Inference with Staged Speculative Decoding\n\nEnsemble decoding(Multi processing): This is a technique that involves using multiple models to decode a single input sequence, which can improve the accuracy of the decoding process. For example, instead of relying on one model to generate the output, ensemble decoding can combine the outputs of several models, using methods such as voting, averaging, or reranking. This can increase the diversity and robustness of the decoding process. A common approach for ensemble decoding is to use models that have been trained with different architectures, hyperparameters, or data sources.\n\nSpeculative execution: This is a technique that involves predicting the outcome of a computation before it is actually executed, which can speed up the decoding process. For example, instead of waiting for the final hidden state of the model to generate the next token, speculative execution can use the early hidden states to predict the next token and execute the model in parallel on the predicted token. This can reduce the dependency between tokens and increase the parallelism of the decoding process. A recent paper by Berkeley researchers proposed a method called SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states.\n\nRelated Paper: SPEED: Speculative Pipelined Execution for Efficient Decoding\n\n\n# 2. NVIDIA Mastering LLM Techniques\n\nLink: https://developer.nvidia.com/blog/search-posts/?q=Mastering+LLM+Techniques\n\n 1. Customization\n 2. LLMOps\n 3. Training\n 4. Inference Optimization\n 5. \n\n\n# 3. Finetuning\n\n 1. How RLHF Preference Model Tuning Works (And How Things May Go Wrong) Blog: https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/\n\nPaper: RRHF: Rank Responses to Align Language Models with Human Feedback without tears 2.\n\n\n# 4. Function Calling\n\n 1. Blog: https://crunchingthedata.com/when-to-use-function-calling-for-llms/\n 2. Paper: An LLM Compiler for Parallel Function Calling",normalizedContent:" 1. llm as os, agents as apps: envisioning aios, agents and the aios-agent ecosystem [2023]\n 2. nvidia mastering llm techniques\n\n----------------------------------------\n\n\n# 1. llm as os, agents as apps: envisioning aios, agents and the aios-agent ecosystem [2012]\n\n👍 analogy of llm and os.\n\nblog: https://huggingface.co/blog/shivance/illustrated-llm-os\n\nyoutube: andrej karpathy https://www.youtube.com/watch?v=kcc8fmeb1ny\n\nparallel decoding(multi threading): this is a technique that allows multiple decoding processes to occur simultaneously, which can speed up the decoding process. for example, instead of generating one token at a time, parallel decoding can generate several tokens in parallel, using different models or different parts of the same model. this can reduce the latency and increase the throughput of the decoding process. a recent paper by apple researchers proposed a method called parallel speculative sampling (pass) that introduces parallel decoding for llms, maintaining model quality while achieving remarkable speed. related paper: accelerating llm inference with staged speculative decoding\n\nensemble decoding(multi processing): this is a technique that involves using multiple models to decode a single input sequence, which can improve the accuracy of the decoding process. for example, instead of relying on one model to generate the output, ensemble decoding can combine the outputs of several models, using methods such as voting, averaging, or reranking. this can increase the diversity and robustness of the decoding process. a common approach for ensemble decoding is to use models that have been trained with different architectures, hyperparameters, or data sources.\n\nspeculative execution: this is a technique that involves predicting the outcome of a computation before it is actually executed, which can speed up the decoding process. for example, instead of waiting for the final hidden state of the model to generate the next token, speculative execution can use the early hidden states to predict the next token and execute the model in parallel on the predicted token. this can reduce the dependency between tokens and increase the parallelism of the decoding process. a recent paper by berkeley researchers proposed a method called speed, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states.\n\nrelated paper: speed: speculative pipelined execution for efficient decoding\n\n\n# 2. nvidia mastering llm techniques\n\nlink: https://developer.nvidia.com/blog/search-posts/?q=mastering+llm+techniques\n\n 1. customization\n 2. llmops\n 3. training\n 4. inference optimization\n 5. \n\n\n# 3. finetuning\n\n 1. how rlhf preference model tuning works (and how things may go wrong) blog: https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/\n\npaper: rrhf: rank responses to align language models with human feedback without tears 2.\n\n\n# 4. function calling\n\n 1. blog: https://crunchingthedata.com/when-to-use-function-calling-for-llms/\n 2. paper: an llm compiler for parallel function calling",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"LLM Hardware Optimization",frontmatter:{title:"LLM Hardware Optimization",date:"2024-01-02T23:32:49.000Z",permalink:"/pages/dc7036/"},regularPath:"/05.llm/02.LLM_HW_Opt.html",relativePath:"05.llm/02.LLM_HW_Opt.md",key:"v-b267e276",path:"/pages/dc7036/",headers:[{level:3,title:"1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]",slug:"_1-hat-hardware-aware-transformers-for-efficient-natural-language-processing-mit-247",normalizedTitle:"1. hat: hardware-aware transformers for efficient natural language processing [mit 247]",charIndex:1},{level:3,title:"4. Making Transformer inference faster on GPUs[Blog]",slug:"_4-making-transformer-inference-faster-on-gpus-blog",normalizedTitle:"4. making transformer inference faster on gpus[blog]",charIndex:798}],headersStr:"1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247] 4. Making Transformer inference faster on GPUs[Blog]",content:" 1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]\n 2. TurboTransformers: An Efficient GPU Serving System For Transformer Models [82]\n 3. Improving the Efficiency of Transformers for Resource-Constrained Devices [8]\n 4. Bag of Tricks for Optimizing Transformer Efficiency [5]\n 5. Making Transformer inference faster on GPUs[Blog]\n 6. Energy-efficient Inference Service of Transformer-based Deep Learning Models on GPUs [4]\n 7. Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs [TACO 2023 Ref 2]\n 8. hugging face https://huggingface.co/docs/transformers/performance\n 9. \n\n----------------------------------------\n\n\n# 1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing [MIT 247]\n\n👍 👍 👍 👍\n\n\n# 4. Making Transformer inference faster on GPUs[Blog]\n\nhttps://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190",normalizedContent:" 1. hat: hardware-aware transformers for efficient natural language processing [mit 247]\n 2. turbotransformers: an efficient gpu serving system for transformer models [82]\n 3. improving the efficiency of transformers for resource-constrained devices [8]\n 4. bag of tricks for optimizing transformer efficiency [5]\n 5. making transformer inference faster on gpus[blog]\n 6. energy-efficient inference service of transformer-based deep learning models on gpus [4]\n 7. improving computation and memory efficiency for real-world transformer inference on gpus [taco 2023 ref 2]\n 8. hugging face https://huggingface.co/docs/transformers/performance\n 9. \n\n----------------------------------------\n\n\n# 1. hat: hardware-aware transformers for efficient natural language processing [mit 247]\n\n👍 👍 👍 👍\n\n\n# 4. making transformer inference faster on gpus[blog]\n\nhttps://dev-discuss.pytorch.org/t/making-transformer-inference-faster-on-gpus/190",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Memory Usage in Training LLM",frontmatter:{title:"Memory Usage in Training LLM",date:"2024-05-29T23:32:49.000Z",permalink:"/pages/dc7038/"},regularPath:"/05.llm/04.mem_usage_llm.html",relativePath:"05.llm/04.mem_usage_llm.md",key:"v-0ee7ecc5",path:"/pages/dc7038/",headersStr:null,content:"1) Nvidia Paper on Traning LLM\nReducing Activation Recomputation in Large Transformer Models\n\n2) Blog Understanding and Estimating GPU Memory\nUnderstanding and Estimating GPU Memory Demands for Training LLMs in practice\n\n3) Blog Memory-Efficient Training\nMemory-Efficient Training of Large Language Models: Overcoming Constraints on Consumer GPUs for Large Neural Networks\n\n4）Stanford Paper Low-Memory Neural Network Training:A Technical Report\nLow-Memory Neural Network Training:A Technical Report\n\n5) Blog Gradient / Activation checkpointing\nhttps://iq.opengenus.org/gradient-checkpointing/\n\n6) Tianqi Chen Gradient Checkpointing Paper\nTraining Deep Nets with Sublinear Memory Cost\n\n7）UCSD Efficient Finetuning of LLMs\nhttps://cseweb.ucsd.edu/classes/wi24/cse234-a/slides/CSE234-GuestLecture-SumanthHegde.pdf",normalizedContent:"1) nvidia paper on traning llm\nreducing activation recomputation in large transformer models\n\n2) blog understanding and estimating gpu memory\nunderstanding and estimating gpu memory demands for training llms in practice\n\n3) blog memory-efficient training\nmemory-efficient training of large language models: overcoming constraints on consumer gpus for large neural networks\n\n4）stanford paper low-memory neural network training:a technical report\nlow-memory neural network training:a technical report\n\n5) blog gradient / activation checkpointing\nhttps://iq.opengenus.org/gradient-checkpointing/\n\n6) tianqi chen gradient checkpointing paper\ntraining deep nets with sublinear memory cost\n\n7）ucsd efficient finetuning of llms\nhttps://cseweb.ucsd.edu/classes/wi24/cse234-a/slides/cse234-guestlecture-sumanthhegde.pdf",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"How to run llama.cpp with gem5",frontmatter:{title:"How to run llama.cpp with gem5",date:"2024-01-21T23:32:49.000Z",permalink:"/pages/dc7037/"},regularPath:"/05.llm/03.gem5_LLAMA.html",relativePath:"05.llm/03.gem5_LLAMA.md",key:"v-259f44f9",path:"/pages/dc7037/",headersStr:null,content:'1) LLama.cpp\n\nllama support compilation of x86, arm and gpu.\n\na) github download llama.cpp\n\nhttps://github.com/ggerganov/llama.cpp.git\n\nb) gem5 support ARM architecture better, thus we compile llama.cpp with arm.\n\n\n\nThen we start to compile: make UNAME_M=aarch64\n\nthe compile tool chain is based on aarch64-linux-gnu-gc-10. It will generate "main" binary if compress successfully.\n\nuse file main to check the file:\n\n\n\nc) download a model to llama.cpp/models directory.\n\nHere I downloaded llama-2-7b-chat.Q2_K.gguf. It utilize 2bit quantization and only needs 3GB memory.\n\nGGML_TYPE_Q2_K - "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n\n\n\nd) then we can run the main binary and model, my prompt is "How are you".\n\n./main -m ./models/llama-2-7b-chat.Q2_K.gguf -p "How are you" -n 16\n\nThe last line in the following figure is the output.\n\n\n\n2) gem5\n\nafter we have build gem5 successfully, we can run the model with gem5.\n\nHere I plan to run with 8 core.\n\n> build/ARM/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $LLAMA_path/llama.cpp/main-arm \'--options=-m $LLAMA_path/llama-2-7b-chat.Q2_K.gguf -p Hi -n 16\' --cpu-type=ArmAtomicSimpleCPU --mem-size=8GB -n 8\n\n> The output is like the following:\n\n\n\nThe left several columns are output of LLAMA model. The followings are cpu ID with instruction executed.\n\nThe output of the model is "Hi，I\'m a 30-year-old male, and..."\n\nHowever, only 4 core has been used, since LLAMA.cpp default threads configuration is 4.\n\nThen we can configure the model with 8 thread.\n\n> build/ARM/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $LLAMA_path/llama.cpp/main-arm \'--options=-m $LLAMA_path/llama-2-7b-chat.Q2_K.gguf -p Hi -n 16 -t 8\' --cpu-type=ArmAtomicSimpleCPU --mem-size=8GB -n 8\n\n\n\nNow, you can see that by default, only CPU0 execute 2.9 billion instruction with the output of "Hi" in 8 core simulation.\n\nHowever, with default 4 core, it has to run 5.4 billion instruction to the same result. This complies to the number of cores runing parallely.',normalizedContent:'1) llama.cpp\n\nllama support compilation of x86, arm and gpu.\n\na) github download llama.cpp\n\nhttps://github.com/ggerganov/llama.cpp.git\n\nb) gem5 support arm architecture better, thus we compile llama.cpp with arm.\n\n\n\nthen we start to compile: make uname_m=aarch64\n\nthe compile tool chain is based on aarch64-linux-gnu-gc-10. it will generate "main" binary if compress successfully.\n\nuse file main to check the file:\n\n\n\nc) download a model to llama.cpp/models directory.\n\nhere i downloaded llama-2-7b-chat.q2_k.gguf. it utilize 2bit quantization and only needs 3gb memory.\n\nggml_type_q2_k - "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. block scales and mins are quantized with 4 bits. this ends up effectively using 2.5625 bits per weight (bpw)\n\n\n\nd) then we can run the main binary and model, my prompt is "how are you".\n\n./main -m ./models/llama-2-7b-chat.q2_k.gguf -p "how are you" -n 16\n\nthe last line in the following figure is the output.\n\n\n\n2) gem5\n\nafter we have build gem5 successfully, we can run the model with gem5.\n\nhere i plan to run with 8 core.\n\n> build/arm/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $llama_path/llama.cpp/main-arm \'--options=-m $llama_path/llama-2-7b-chat.q2_k.gguf -p hi -n 16\' --cpu-type=armatomicsimplecpu --mem-size=8gb -n 8\n\n> the output is like the following:\n\n\n\nthe left several columns are output of llama model. the followings are cpu id with instruction executed.\n\nthe output of the model is "hi，i\'m a 30-year-old male, and..."\n\nhowever, only 4 core has been used, since llama.cpp default threads configuration is 4.\n\nthen we can configure the model with 8 thread.\n\n> build/arm/gem5.fast --outdir=./m5out/llm_9 ./configs/example/se.py -c $llama_path/llama.cpp/main-arm \'--options=-m $llama_path/llama-2-7b-chat.q2_k.gguf -p hi -n 16 -t 8\' --cpu-type=armatomicsimplecpu --mem-size=8gb -n 8\n\n\n\nnow, you can see that by default, only cpu0 execute 2.9 billion instruction with the output of "hi" in 8 core simulation.\n\nhowever, with default 4 core, it has to run 5.4 billion instruction to the same result. this complies to the number of cores runing parallely.',charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"留言板",frontmatter:{title:"留言板",date:"2022-07-12T10:38:46.000Z",permalink:"/message-board"},regularPath:"/09.nine/01.%E7%95%99%E8%A8%80%E6%9D%BF.html",relativePath:"09.nine/01.留言板.md",key:"v-2e715932",path:"/message-board/",headersStr:null,content:"你可以在这里留下想说的内容。",normalizedContent:"你可以在这里留下想说的内容。",charsets:{cjk:!0},lastUpdated:"2024/08/27, 17:56:49"},{title:"template",frontmatter:{title:"template",date:"2020-04-05T10:38:46.000Z",permalink:"/pages/c45600/"},regularPath:"/09.nine/02.template.html",relativePath:"09.nine/02.template.md",key:"v-bb1ed876",path:"/pages/c45600/",headers:[{level:2,title:"Contents",slug:"contents",normalizedTitle:"contents",charIndex:28},{level:2,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:42},{level:2,title:"List of Papers",slug:"list-of-papers",normalizedTitle:"list of papers",charIndex:59},{level:2,title:"Paper Reviews",slug:"paper-reviews",normalizedTitle:"paper reviews",charIndex:5},{level:3,title:"Paper 1: Title of Paper 1",slug:"paper-1-title-of-paper-1",normalizedTitle:"paper 1: title of paper 1",charIndex:98},{level:3,title:"Paper 2: Title of Paper 2",slug:"paper-2-title-of-paper-2",normalizedTitle:"paper 2: title of paper 2",charIndex:130},{level:3,title:"Paper 3: Title of Paper 3",slug:"paper-3-title-of-paper-3",normalizedTitle:"paper 3: title of paper 3",charIndex:162}],headersStr:"Contents Introduction List of Papers Paper Reviews Paper 1: Title of Paper 1 Paper 2: Title of Paper 2 Paper 3: Title of Paper 3",content:"# 📚 Paper Reviews Blog\n\n\n# Contents\n\n 1. Introduction\n 2. List of Papers\n 3. Paper Reviews\n    * Paper 1: Title of Paper 1\n    * Paper 2: Title of Paper 2\n    * Paper 3: Title of Paper 3\n\n----------------------------------------\n\n\n# Introduction\n\nWelcome to my paper reviews blog! Here, I delve into various academic papers and provide insights, critiques, and summaries. Each paper offers a unique perspective on its subject matter, contributing to the ever-evolving landscape of research. Let's explore the latest findings and discussions in the academic world.\n\n\n# List of Papers\n\n * Title of Paper 1\n * Title of Paper 2\n * Title of Paper 3\n\n----------------------------------------\n\n\n# Paper Reviews\n\n\n# Paper 1: Title of Paper 1\n\n# Authors: Author Names\n\nAbstract: Brief summary of the paper's abstract.\n\nKeywords: Keywords, Keywords, Keywords\n\nIntroduction: Provide a summary of the introduction section.\n\nMethodology: Discuss the methodology used in the research.\n\nResults: Highlight the key findings of the study.\n\nDiscussion: Offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# Paper 2: Title of Paper 2\n\n# Authors: Author Names\n\nAbstract: Brief summary of the paper's abstract.\n\nKeywords: Keywords, Keywords, Keywords\n\nIntroduction: Provide a summary of the introduction section.\n\nMethodology: Discuss the methodology used in the research.\n\nResults: Highlight the key findings of the study.\n\nDiscussion: Offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# Paper 3: Title of Paper 3\n\n# Authors: Author Names\n\nAbstract: Brief summary of the paper's abstract.\n\nKeywords: Keywords, Keywords, Keywords\n\nIntroduction: Provide a summary of the introduction section.\n\nMethodology: Discuss the methodology used in the research.\n\nResults: Highlight the key findings of the study.\n\nDiscussion: Offer your thoughts and critiques on the paper.\n\n----------------------------------------",normalizedContent:"# 📚 paper reviews blog\n\n\n# contents\n\n 1. introduction\n 2. list of papers\n 3. paper reviews\n    * paper 1: title of paper 1\n    * paper 2: title of paper 2\n    * paper 3: title of paper 3\n\n----------------------------------------\n\n\n# introduction\n\nwelcome to my paper reviews blog! here, i delve into various academic papers and provide insights, critiques, and summaries. each paper offers a unique perspective on its subject matter, contributing to the ever-evolving landscape of research. let's explore the latest findings and discussions in the academic world.\n\n\n# list of papers\n\n * title of paper 1\n * title of paper 2\n * title of paper 3\n\n----------------------------------------\n\n\n# paper reviews\n\n\n# paper 1: title of paper 1\n\n# authors: author names\n\nabstract: brief summary of the paper's abstract.\n\nkeywords: keywords, keywords, keywords\n\nintroduction: provide a summary of the introduction section.\n\nmethodology: discuss the methodology used in the research.\n\nresults: highlight the key findings of the study.\n\ndiscussion: offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# paper 2: title of paper 2\n\n# authors: author names\n\nabstract: brief summary of the paper's abstract.\n\nkeywords: keywords, keywords, keywords\n\nintroduction: provide a summary of the introduction section.\n\nmethodology: discuss the methodology used in the research.\n\nresults: highlight the key findings of the study.\n\ndiscussion: offer your thoughts and critiques on the paper.\n\n----------------------------------------\n\n\n# paper 3: title of paper 3\n\n# authors: author names\n\nabstract: brief summary of the paper's abstract.\n\nkeywords: keywords, keywords, keywords\n\nintroduction: provide a summary of the introduction section.\n\nmethodology: discuss the methodology used in the research.\n\nresults: highlight the key findings of the study.\n\ndiscussion: offer your thoughts and critiques on the paper.\n\n----------------------------------------",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"how malloc works",frontmatter:{title:"how malloc works",date:"2024-01-02T15:32:49.000Z",permalink:"/pages/ec7035/"},regularPath:"/06.unix/01.malloc.html",relativePath:"06.unix/01.malloc.md",key:"v-e114a476",path:"/pages/ec7035/",headersStr:null,content:"malloc\n\n1 brk and mmap\nhttps://people.kth.se/~johanmon/courses/id2206/lectures/management-handout.pdf\n\n\nMalloc will invoke brk or mmap systemcall. The difference is based on\nFocus on Size:\n\nThe primary factor influencing malloc's choice is the requested memory size.\nbrk for Smaller Allocations:\nFor smaller allocations (often configurable through a threshold), malloc will likely use brk. brk is a system call that adjusts the program's data segment boundary. It's a relatively fast operation for requesting contiguous memory from the heap.\n\nmmap for Larger Allocations:\nWhen the requested memory size exceeds a certain threshold (often set by mallopt function), malloc might use mmap instead.\n\n2. Memory.\n\n\nEvery time you call sbrk，it will increase the brk.\n\n\n\n\n\n3. Code, Lib, Systemcall\n\n\n\n\n\n\n4. The Object in memory is organized by metadata and then data.\nThe metadata is size and bit.\n\n\nThe metadata is aligned by 16Byte. ------- This needs to be proved.\n\n\n\n\n\n5. Create Hooks for malloc.\nhttps://www.gnu.org/software/libc/manual/html_node/Hooks-for-Malloc.html\n\n/* Prototypes for __malloc_hook, __free_hook */\n#include <malloc.h>\n\n/* Prototypes for our hooks.  */\nstatic void my_init_hook (void);\nstatic void *my_malloc_hook (size_t, const void *);\nstatic void my_free_hook (void*, const void *);\n\nstatic void\nmy_init (void)\n{\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nstatic void *\nmy_malloc_hook (size_t size, const void *caller)\n{\n  void *result;\n  /* Restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* Call recursively */\n  result = malloc (size);\n  /* Save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call malloc, so protect it too. */\n  printf (\"malloc (%u) returns %p\\n\", (unsigned int) size, result);\n  /* Restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n  return result;\n}\n\nstatic void\nmy_free_hook (void *ptr, const void *caller)\n{\n  /* Restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* Call recursively */\n  free (ptr);\n  /* Save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call free, so protect it too. */\n  printf (\"freed pointer %p\\n\", ptr);\n  /* Restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nmain ()\n{\n  my_init ();\n  …\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n6. CppCon 2017 Memory Alloc”\n\nCppCon 2017: John Lakos “Local ('Arena') Memory Allocators (part 1 of 2)”",normalizedContent:"malloc\n\n1 brk and mmap\nhttps://people.kth.se/~johanmon/courses/id2206/lectures/management-handout.pdf\n\n\nmalloc will invoke brk or mmap systemcall. the difference is based on\nfocus on size:\n\nthe primary factor influencing malloc's choice is the requested memory size.\nbrk for smaller allocations:\nfor smaller allocations (often configurable through a threshold), malloc will likely use brk. brk is a system call that adjusts the program's data segment boundary. it's a relatively fast operation for requesting contiguous memory from the heap.\n\nmmap for larger allocations:\nwhen the requested memory size exceeds a certain threshold (often set by mallopt function), malloc might use mmap instead.\n\n2. memory.\n\n\nevery time you call sbrk，it will increase the brk.\n\n\n\n\n\n3. code, lib, systemcall\n\n\n\n\n\n\n4. the object in memory is organized by metadata and then data.\nthe metadata is size and bit.\n\n\nthe metadata is aligned by 16byte. ------- this needs to be proved.\n\n\n\n\n\n5. create hooks for malloc.\nhttps://www.gnu.org/software/libc/manual/html_node/hooks-for-malloc.html\n\n/* prototypes for __malloc_hook, __free_hook */\n#include <malloc.h>\n\n/* prototypes for our hooks.  */\nstatic void my_init_hook (void);\nstatic void *my_malloc_hook (size_t, const void *);\nstatic void my_free_hook (void*, const void *);\n\nstatic void\nmy_init (void)\n{\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nstatic void *\nmy_malloc_hook (size_t size, const void *caller)\n{\n  void *result;\n  /* restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* call recursively */\n  result = malloc (size);\n  /* save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call malloc, so protect it too. */\n  printf (\"malloc (%u) returns %p\\n\", (unsigned int) size, result);\n  /* restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n  return result;\n}\n\nstatic void\nmy_free_hook (void *ptr, const void *caller)\n{\n  /* restore all old hooks */\n  __malloc_hook = old_malloc_hook;\n  __free_hook = old_free_hook;\n  /* call recursively */\n  free (ptr);\n  /* save underlying hooks */\n  old_malloc_hook = __malloc_hook;\n  old_free_hook = __free_hook;\n  /* printf might call free, so protect it too. */\n  printf (\"freed pointer %p\\n\", ptr);\n  /* restore our own hooks */\n  __malloc_hook = my_malloc_hook;\n  __free_hook = my_free_hook;\n}\n\nmain ()\n{\n  my_init ();\n  …\n}\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n\n\n6. cppcon 2017 memory alloc”\n\ncppcon 2017: john lakos “local ('arena') memory allocators (part 1 of 2)”",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"leakage current moores law meets static power",frontmatter:{title:"leakage current moores law meets static power",date:"2024-08-06T15:32:49.000Z",permalink:"/pages/f00000/"},regularPath:"/10.mix/01.leakagecurrent.html",relativePath:"10.mix/01.leakagecurrent.md",key:"v-23bbae21",path:"/pages/f00000/",headers:[{level:3,title:"Leakge Current Moore's Law Meets Static Power",slug:"leakge-current-moore-s-law-meets-static-power",normalizedTitle:"leakge current moore's law meets static power",charIndex:2},{level:3,title:"Power Basics",slug:"power-basics",normalizedTitle:"power basics",charIndex:837},{level:3,title:"Overall Power Consumption",slug:"overall-power-consumption",normalizedTitle:"overall power consumption",charIndex:1129},{level:3,title:"Leakage current",slug:"leakage-current",normalizedTitle:"leakage current",charIndex:1966},{level:2,title:"REDUCING STATIC POWER CONSUMPTION",slug:"reducing-static-power-consumption",normalizedTitle:"reducing static power consumption",charIndex:3589},{level:3,title:"Retention flip-flops",slug:"retention-flip-flops",normalizedTitle:"retention flip-flops",charIndex:3627},{level:3,title:"Controlling memory leakage",slug:"controlling-memory-leakage",normalizedTitle:"controlling memory leakage",charIndex:4215},{level:2,title:"TECHNOLOGY TRENDS AND CHALLENGES",slug:"technology-trends-and-challenges",normalizedTitle:"technology trends and challenges",charIndex:5431},{level:3,title:"Multiple threshold voltages",slug:"multiple-threshold-voltages",normalizedTitle:"multiple threshold voltages",charIndex:6549},{level:3,title:"Gate length",slug:"gate-length",normalizedTitle:"gate length",charIndex:6581},{level:3,title:"Oxide tunneling",slug:"oxide-tunneling",normalizedTitle:"oxide tunneling",charIndex:6597}],headersStr:"Leakge Current Moore's Law Meets Static Power Power Basics Overall Power Consumption Leakage current REDUCING STATIC POWER CONSUMPTION Retention flip-flops Controlling memory leakage TECHNOLOGY TRENDS AND CHALLENGES Multiple threshold voltages Gate length Oxide tunneling",content:"# Leakge Current Moore's Law Meets Static Power\n\nOff-state leakage is static power, current that leaks through transistors even when they are turned off.\n\n * It is one of two principal sources of power dissipation in today’s microprocessors.\n * The other is dynamic power, which arises from the repeated capacitance charge and discharge on the output of the hundreds of millions of gates in today’s chips.\n   * subthreshold leakage, a weak inversion current across the device; and\n   * gate leakage, a tunneling current through the gate oxide insulation.\n\nDynamic power is proportional to the square of supply voltage, so reducing the voltage significantly reduces power consumption.\n\nUnfortunately, smaller geometries exacerbate leakage, so static power begins to dominate the power consumption equation in microprocessor design.\n\n\n\n\n# Power Basics\n\n# Operating Frequency and volatage\n\n\n\n\n\nwe see that f = 0 corresponds to Vnorm = Vth / Vmax, which for today’s technology is approximately 0.3.\n\nReducing the operating frequency by a particular percentage from fmax will reduce the operating voltage by a smaller percentage.\n\n\n# Overall Power Consumption\n\n\n\n * The first term is the dynamic power lost from charging and discharging the processor’s capacitive loads: A is the fraction of gates actively switching and C is the total capacitance load of all gates.\n * The second term models the static power lost due to leakage current, Ileak.\n\nIn fact, halving the voltage will reduce the power consumption by a factor of four. But Equation 2 shows that halving the voltage will reduce the processor’s maximum operating frequency by more than half.\n\nTo compensate for this performance loss, we can use either parallel or pipelined implementations.\n\nIf the implementation runs the original serial computation as two parallel subtasks or as two pipelined subtasks, the dynamic power consumption can decrease by more than a factor of two compared to the serial case.\n\n\n# Leakage current\n\nAs noted, leakage current, the source of static power consumption, is a combination of subthreshold and gate-oxide leakage: Ileak = Isub + Iox.\n\n# Subthreshold power leakage\n\n\n\nHow to reduce Isub\n\n * First, we could turn off the supply voltage—that is, set V to zero so that the factor in parentheses also becomes zero.\n * Second, we could increase the threshold voltage, which—because it appears as a negative exponent—can have a dramatic effect in even small increments. On the other hand, we know from Equation 1 that increasing Vth will reduce speed.\n\nThe problem with the first approach is loss of state;\n\nThe problem with the second approach is the loss of performance.\n\nGate width W is the other contributor to subthreshold leakage in a particular transistor. Designers often use the combined widths of all the processor’s transistors as a convenient measure of total subthreshold leakage.\n\n# Gate-oxide power leakage\n\n\n\nK2 and α are experimentally derived. The term of interest is oxide thickness, Tox.\n\nTox will reduce gate leakage. Unfortunately, it also degrades the transistor’s effectiveness because Tox must decrease proportionally with process scaling to avoid short channel effects.\n\n# Low-power architectural options\n\nBecause subthreshold and oxide leakage both depend on total gate width or, approximately, gate count.\n\nPipelined implementations can run at a lower voltage, which can reduce power consumption for both dynamic and static power compared to the serial case.\n\nParallel implementations can also run at a lower voltage, but only by roughly doubling the amount of hardware.\n\n\n# REDUCING STATIC POWER CONSUMPTION\n\n\n# Retention flip-flops\n\nFor shorter inactive periods, researchers have developed “balloon” logic, also called retention flip-flops. The idea is to use highVth latches to duplicate those latches that must preserve state.\n\nUsing doping techniques or applying a bias voltage to the substrate can increase threshold voltage by 100 mV. This in turn reduces leakage by a factor of about 10, but it increases switching time by about 15 percent.\n\nThus, lowleakage retention flops are only useful in saving state energy efficiently—their use on the processor’s critical path would slow it down.\n\n\n# Controlling memory leakage\n\nIn fact, leakage is projected to account for 70 percent of the cache power budget in 70-nm technology.\n\nBoth bitline and cell leakage result from subthreshold conduction—current flowing from the source to drain even when gate-source voltage is below the threshold voltage.\n\n# Circuit techniques.\n\n * State-destructive\n\n * State-preserving techniques vary. Drowsy caches multiplex supply voltages according to the state of each cache line or block. Waking up the drowsy cache lines is treated as a pseudo cache miss and incurs one additional cycle overhead.\n   \n   Moreover, while state-preserving techniques can only reduce leakage by about a factor of 10, compared to more than a factor of 1,000 for destructive techniques, the net difference in power consumed by the two techniques is less than 10 percent.\n\n# Control techniques.\n\n * application-sensitive controls, based on runtime performance feedback,9,13 and\n * application-insensitive controls, which periodically turn off cache lines.\n\n# Compiler techniques\n\n * Using compiler directives might make it possible to keep some loops within bank boundaries.\n * The compiler can also provide application-sensitive leakage control.\n\n\n# TECHNOLOGY TRENDS AND CHALLENGES\n\nOne approach to reducing subthreshold leakage is to actively refrigerate the chip.\n\nWhile this option seems promising for controlling the subthreshold leakage point, it does not address gateoxide leakage.\n\nDesigners assign a low threshold voltage to a few performance-critical transistors and a high threshold voltage to the majority of less timing-critical transistors.\n\nThis approach incurs a high subthreshold leakage current for the performance-critical transistors, but it can significantly reduce the overall leakage.\n\nfuture technologies are likely to offer three threshold voltages—low, high, and extra high—or even more.\n\nThis opens the way to new leakage optimizations within different portions of a cache or at different levels of its hierarchy.\n\nthe address-decoder and bus-driver circuits in a cache consume a significant portion of total access time, so a designer could construct them from high-Vth transistors, while constructing the more numerous bit cells from extra-high-Vth devices and reserving low-Vth devices for speed-critical parts of the processor core.\n\n\n# Multiple threshold voltages\n\n\n# Gate length\n\n\n# Oxide tunneling",normalizedContent:"# leakge current moore's law meets static power\n\noff-state leakage is static power, current that leaks through transistors even when they are turned off.\n\n * it is one of two principal sources of power dissipation in today’s microprocessors.\n * the other is dynamic power, which arises from the repeated capacitance charge and discharge on the output of the hundreds of millions of gates in today’s chips.\n   * subthreshold leakage, a weak inversion current across the device; and\n   * gate leakage, a tunneling current through the gate oxide insulation.\n\ndynamic power is proportional to the square of supply voltage, so reducing the voltage significantly reduces power consumption.\n\nunfortunately, smaller geometries exacerbate leakage, so static power begins to dominate the power consumption equation in microprocessor design.\n\n\n\n\n# power basics\n\n# operating frequency and volatage\n\n\n\n\n\nwe see that f = 0 corresponds to vnorm = vth / vmax, which for today’s technology is approximately 0.3.\n\nreducing the operating frequency by a particular percentage from fmax will reduce the operating voltage by a smaller percentage.\n\n\n# overall power consumption\n\n\n\n * the first term is the dynamic power lost from charging and discharging the processor’s capacitive loads: a is the fraction of gates actively switching and c is the total capacitance load of all gates.\n * the second term models the static power lost due to leakage current, ileak.\n\nin fact, halving the voltage will reduce the power consumption by a factor of four. but equation 2 shows that halving the voltage will reduce the processor’s maximum operating frequency by more than half.\n\nto compensate for this performance loss, we can use either parallel or pipelined implementations.\n\nif the implementation runs the original serial computation as two parallel subtasks or as two pipelined subtasks, the dynamic power consumption can decrease by more than a factor of two compared to the serial case.\n\n\n# leakage current\n\nas noted, leakage current, the source of static power consumption, is a combination of subthreshold and gate-oxide leakage: ileak = isub + iox.\n\n# subthreshold power leakage\n\n\n\nhow to reduce isub\n\n * first, we could turn off the supply voltage—that is, set v to zero so that the factor in parentheses also becomes zero.\n * second, we could increase the threshold voltage, which—because it appears as a negative exponent—can have a dramatic effect in even small increments. on the other hand, we know from equation 1 that increasing vth will reduce speed.\n\nthe problem with the first approach is loss of state;\n\nthe problem with the second approach is the loss of performance.\n\ngate width w is the other contributor to subthreshold leakage in a particular transistor. designers often use the combined widths of all the processor’s transistors as a convenient measure of total subthreshold leakage.\n\n# gate-oxide power leakage\n\n\n\nk2 and α are experimentally derived. the term of interest is oxide thickness, tox.\n\ntox will reduce gate leakage. unfortunately, it also degrades the transistor’s effectiveness because tox must decrease proportionally with process scaling to avoid short channel effects.\n\n# low-power architectural options\n\nbecause subthreshold and oxide leakage both depend on total gate width or, approximately, gate count.\n\npipelined implementations can run at a lower voltage, which can reduce power consumption for both dynamic and static power compared to the serial case.\n\nparallel implementations can also run at a lower voltage, but only by roughly doubling the amount of hardware.\n\n\n# reducing static power consumption\n\n\n# retention flip-flops\n\nfor shorter inactive periods, researchers have developed “balloon” logic, also called retention flip-flops. the idea is to use highvth latches to duplicate those latches that must preserve state.\n\nusing doping techniques or applying a bias voltage to the substrate can increase threshold voltage by 100 mv. this in turn reduces leakage by a factor of about 10, but it increases switching time by about 15 percent.\n\nthus, lowleakage retention flops are only useful in saving state energy efficiently—their use on the processor’s critical path would slow it down.\n\n\n# controlling memory leakage\n\nin fact, leakage is projected to account for 70 percent of the cache power budget in 70-nm technology.\n\nboth bitline and cell leakage result from subthreshold conduction—current flowing from the source to drain even when gate-source voltage is below the threshold voltage.\n\n# circuit techniques.\n\n * state-destructive\n\n * state-preserving techniques vary. drowsy caches multiplex supply voltages according to the state of each cache line or block. waking up the drowsy cache lines is treated as a pseudo cache miss and incurs one additional cycle overhead.\n   \n   moreover, while state-preserving techniques can only reduce leakage by about a factor of 10, compared to more than a factor of 1,000 for destructive techniques, the net difference in power consumed by the two techniques is less than 10 percent.\n\n# control techniques.\n\n * application-sensitive controls, based on runtime performance feedback,9,13 and\n * application-insensitive controls, which periodically turn off cache lines.\n\n# compiler techniques\n\n * using compiler directives might make it possible to keep some loops within bank boundaries.\n * the compiler can also provide application-sensitive leakage control.\n\n\n# technology trends and challenges\n\none approach to reducing subthreshold leakage is to actively refrigerate the chip.\n\nwhile this option seems promising for controlling the subthreshold leakage point, it does not address gateoxide leakage.\n\ndesigners assign a low threshold voltage to a few performance-critical transistors and a high threshold voltage to the majority of less timing-critical transistors.\n\nthis approach incurs a high subthreshold leakage current for the performance-critical transistors, but it can significantly reduce the overall leakage.\n\nfuture technologies are likely to offer three threshold voltages—low, high, and extra high—or even more.\n\nthis opens the way to new leakage optimizations within different portions of a cache or at different levels of its hierarchy.\n\nthe address-decoder and bus-driver circuits in a cache consume a significant portion of total access time, so a designer could construct them from high-vth transistors, while constructing the more numerous bit cells from extra-high-vth devices and reserving low-vth devices for speed-critical parts of the processor core.\n\n\n# multiple threshold voltages\n\n\n# gate length\n\n\n# oxide tunneling",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"assignment_qishao",frontmatter:{title:"assignment_qishao",date:"2024-08-27T17:57:02.000Z",permalink:"/pages/f6e80a/"},regularPath:"/10.mix/assignment_qishao.html",relativePath:"10.mix/assignment_qishao.md",key:"v-6ed8ffdb",path:"/pages/f6e80a/",headers:[{level:2,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:48},{level:2,title:"Robert's Lecture and My Research & Work Experience",slug:"robert-s-lecture-and-my-research-work-experience",normalizedTitle:"robert's lecture and my research &amp; work experience",charIndex:null},{level:3,title:"V-Model & My Research",slug:"v-model-my-research",normalizedTitle:"v-model &amp; my research",charIndex:null},{level:3,title:"Code Review practices & My Work Experience",slug:"code-review-practices-my-work-experience",normalizedTitle:"code review practices &amp; my work experience",charIndex:null},{level:3,title:"Verification & My Work Experience",slug:"verification-my-work-experience",normalizedTitle:"verification &amp; my work experience",charIndex:null},{level:2,title:"Guest's Lecture and My Research & Work Experience",slug:"guest-s-lecture-and-my-research-work-experience",normalizedTitle:"guest's lecture and my research &amp; work experience",charIndex:null},{level:3,title:"SAAB Survery of Software Engineering relevance changing with AI/ML",slug:"saab-survery-of-software-engineering-relevance-changing-with-ai-ml",normalizedTitle:"saab survery of software engineering relevance changing with ai/ml",charIndex:4592},{level:2,title:"CAIN Papers",slug:"cain-papers",normalizedTitle:"cain papers",charIndex:6032},{level:2,title:"References",slug:"references",normalizedTitle:"references",charIndex:6048}],headersStr:"Introduction Robert's Lecture and My Research & Work Experience V-Model & My Research Code Review practices & My Work Experience Verification & My Work Experience Guest's Lecture and My Research & Work Experience SAAB Survery of Software Engineering relevance changing with AI/ML CAIN Papers References",content:"# Software Engineering Assignment - Qi Shao\n\n\n# Introduction\n\nMy research focuses on computer microarchitecture and memory subsystem. We try to improve the performance of CPU/GPU/Memory to achieve better IPC (Instruction per Cycle) as to CPU and higher throughput for GPU. Since it is unmanageable to program from silicon level, like Verilog or VHDL language. Researchers prefer to use simulators to simulate the function of CPU/GPU at cycle accurate level to value the idea.\n\nI have been working in industry for years, using these simulators. Similar to methods mentioned in the lectures, we use regression test, combining both unit test and benchmark test to check whether added function is correct. Our team manager also make the rule that if someone modify the code that slow down the runtime of simulator, he has to buy coffee for everyone in the team. It is a soft way to prevent programmer from adding low-performance code.\n\nWe also follow the Google's C++ programming style guideline.\n\n\n# Robert's Lecture and My Research & Work Experience\n\n\n# V-Model & My Research\n\nIn the lecture, we disscussed about the flavor of SE/Processes V Model. The V Model has differente layers from upper-level Requirements to bottom-level Coding. As to my research, there are already plenty of microarchitecture simulators.\n\n * Gem5/GPGPU-sim: Cycle Accurate Simualtor\n * Ramulator: Trace Driven Simulator\n * Zsim/Pin: Instrumental-based Simulator\n\nThese simulators is build based on different requirments. Cycle-accurate simulator usually has better simulation accuracy, but lower speed. Ramulator is faster and Zsim/Pin is fastest since they run on real processor, just instrumenting code. However in the paper, I can always come accross that the results of some papers are simulated based on the simulator that does not support the function, in which it failed in the Validation Part.\n\nAs to bottom-level coding, it is sensible that cycle-accurate simulator has most workload of coding. It has to simulate at cycle & instruction level. However, meanwhile since CPU&GPU design company like Intel or NVIDIA or AMD, they just disclose their design at very high-level block design. The detailed design of each unit in cycle accurate simulator is based on estimation the structure of each unit after runing benchmark on CPU & GPU.\n\nIn my research, I also met some paper that they claimed that they found previous cited work has miscoding some unit, so that the cited performance cannot be trusted.\n\n\n# Code Review practices & My Work Experience\n\nWhen I was in industry, we also follow the guideline of Google C++ programming style. But we didn't follow Google's code review. During the literature, I found that this part very interesting.\n\nAs a freshgraduate student, I also found that I could benefit a lot from reviewing code from senior or professionals. And based on this observation, during my research, when I first stepped into new area, for example compiler, like LLVM, I tends to go back to the initial commit of LLVM project and learn how the initional function was added into LLVM Project. It is the idea of Education Maintaining norms mentioned in the paper. Another interesting finding is that as employer stays in Google longer, commemnts per change also decrease, and converge to each change 2 comments.\n\nAnother finding is similar to finding5 code review at Google still faces breakdonws. When I start to learn LLVM framework, in the beginning of user manual or guideline, one of the advantage of LLVM compared to GCC is the simplicty of LLVM. However, as years of development of LLVM, I feels the learning curve of LLVM is steeper than I imagine. And after new function are merged, the turtorial blogs are not updated. If I just follow the tutorial blogs, the function that I implement will not work. This is the mismatch of latest code and stale tutorial.\n\n\n# Verification & My Work Experience\n\nIn the verification process, I have worked in some company that the programmer is also the verifier. They design test cases for their own code. An anology could be that an athelete is also the referee of the game. The company has follow this routine for a while, but in the end we found that there are always bugs that the programmer cannot found by themselves. It is since that the cases that the programers will always work. They are blind to the test cases that would trigger the bug. If they realized it, they would have fixed it. In the end, the company decide to manage a new team of verifiers that focus on testing the program.\n\n\n# Guest's Lecture and My Research & Work Experience\n\n\n# SAAB Survery of Software Engineering relevance changing with AI/ML\n\nIt is surprising that most of the answers are \"We dont know\". I also didn't use AI to generate programming code, since I feels that I have been trained to learn how to write efficient program and the AI could not achieve, until last year. Last year during the WASP conference, I found that most of the students have been using copilot. And nowadays that cursor, a new programming assistant tool is become popular. After the WASP conference, I tried Chat-GPT to write demo code for LLVM function and it works well. Sometimes, the code does not work, but at least, it provide a demo framework or an idea or suggest for you to program. As to me, if I am very familiar with some framework, for example, simulators, I will write code and modify it by myself, since I am better than chat-GPT or at least I dont need to understand the chat-gpt's code and debug. But as to new framework that I dont know, I will ask chatgpt to present some demo. It boosts the progress of my learning new framework.\n\nAfter using chatgpt to generate code, I register WASP Natural Language Processing Code to learn why Transformers works in programming area. It is sensible that transformers at good at predict next token and it is also trained with public opensource frameworks. In this way,it could understand the programming style and generate code. And we can also view programming language\n\n\n# CAIN Papers\n\n\n# References\n\n[^1]:\n\n[^2]:",normalizedContent:"# software engineering assignment - qi shao\n\n\n# introduction\n\nmy research focuses on computer microarchitecture and memory subsystem. we try to improve the performance of cpu/gpu/memory to achieve better ipc (instruction per cycle) as to cpu and higher throughput for gpu. since it is unmanageable to program from silicon level, like verilog or vhdl language. researchers prefer to use simulators to simulate the function of cpu/gpu at cycle accurate level to value the idea.\n\ni have been working in industry for years, using these simulators. similar to methods mentioned in the lectures, we use regression test, combining both unit test and benchmark test to check whether added function is correct. our team manager also make the rule that if someone modify the code that slow down the runtime of simulator, he has to buy coffee for everyone in the team. it is a soft way to prevent programmer from adding low-performance code.\n\nwe also follow the google's c++ programming style guideline.\n\n\n# robert's lecture and my research & work experience\n\n\n# v-model & my research\n\nin the lecture, we disscussed about the flavor of se/processes v model. the v model has differente layers from upper-level requirements to bottom-level coding. as to my research, there are already plenty of microarchitecture simulators.\n\n * gem5/gpgpu-sim: cycle accurate simualtor\n * ramulator: trace driven simulator\n * zsim/pin: instrumental-based simulator\n\nthese simulators is build based on different requirments. cycle-accurate simulator usually has better simulation accuracy, but lower speed. ramulator is faster and zsim/pin is fastest since they run on real processor, just instrumenting code. however in the paper, i can always come accross that the results of some papers are simulated based on the simulator that does not support the function, in which it failed in the validation part.\n\nas to bottom-level coding, it is sensible that cycle-accurate simulator has most workload of coding. it has to simulate at cycle & instruction level. however, meanwhile since cpu&gpu design company like intel or nvidia or amd, they just disclose their design at very high-level block design. the detailed design of each unit in cycle accurate simulator is based on estimation the structure of each unit after runing benchmark on cpu & gpu.\n\nin my research, i also met some paper that they claimed that they found previous cited work has miscoding some unit, so that the cited performance cannot be trusted.\n\n\n# code review practices & my work experience\n\nwhen i was in industry, we also follow the guideline of google c++ programming style. but we didn't follow google's code review. during the literature, i found that this part very interesting.\n\nas a freshgraduate student, i also found that i could benefit a lot from reviewing code from senior or professionals. and based on this observation, during my research, when i first stepped into new area, for example compiler, like llvm, i tends to go back to the initial commit of llvm project and learn how the initional function was added into llvm project. it is the idea of education maintaining norms mentioned in the paper. another interesting finding is that as employer stays in google longer, commemnts per change also decrease, and converge to each change 2 comments.\n\nanother finding is similar to finding5 code review at google still faces breakdonws. when i start to learn llvm framework, in the beginning of user manual or guideline, one of the advantage of llvm compared to gcc is the simplicty of llvm. however, as years of development of llvm, i feels the learning curve of llvm is steeper than i imagine. and after new function are merged, the turtorial blogs are not updated. if i just follow the tutorial blogs, the function that i implement will not work. this is the mismatch of latest code and stale tutorial.\n\n\n# verification & my work experience\n\nin the verification process, i have worked in some company that the programmer is also the verifier. they design test cases for their own code. an anology could be that an athelete is also the referee of the game. the company has follow this routine for a while, but in the end we found that there are always bugs that the programmer cannot found by themselves. it is since that the cases that the programers will always work. they are blind to the test cases that would trigger the bug. if they realized it, they would have fixed it. in the end, the company decide to manage a new team of verifiers that focus on testing the program.\n\n\n# guest's lecture and my research & work experience\n\n\n# saab survery of software engineering relevance changing with ai/ml\n\nit is surprising that most of the answers are \"we dont know\". i also didn't use ai to generate programming code, since i feels that i have been trained to learn how to write efficient program and the ai could not achieve, until last year. last year during the wasp conference, i found that most of the students have been using copilot. and nowadays that cursor, a new programming assistant tool is become popular. after the wasp conference, i tried chat-gpt to write demo code for llvm function and it works well. sometimes, the code does not work, but at least, it provide a demo framework or an idea or suggest for you to program. as to me, if i am very familiar with some framework, for example, simulators, i will write code and modify it by myself, since i am better than chat-gpt or at least i dont need to understand the chat-gpt's code and debug. but as to new framework that i dont know, i will ask chatgpt to present some demo. it boosts the progress of my learning new framework.\n\nafter using chatgpt to generate code, i register wasp natural language processing code to learn why transformers works in programming area. it is sensible that transformers at good at predict next token and it is also trained with public opensource frameworks. in this way,it could understand the programming style and generate code. and we can also view programming language\n\n\n# cain papers\n\n\n# references\n\n[^1]:\n\n[^2]:",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"archives",frontmatter:{archivesPage:!0,title:"archives",permalink:"/archives/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-f8e02736",path:"/archives/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Add pictures",frontmatter:{title:"Add pictures",date:"2023-05-08T00:00:00.000Z",permalink:"/pages/904dad/"},regularPath:"/pictures/addPictures.html",relativePath:"pictures/addPictures.md",key:"v-01216c79",path:"/pages/904dad/",headersStr:null,content:"Add pictures into this foler",normalizedContent:"add pictures into this foler",charsets:{},lastUpdated:"2024/08/27, 17:56:49"},{title:"Home",frontmatter:{home:!0,heroText:"Notes in Computer System.",actionText:"Start →",actionLink:"/pages/f27694/",bannerBg:"none",postList:"simple"},regularPath:"/",relativePath:"index.md",key:"v-2f8a0a7e",path:"/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2024/08/27, 17:56:49"}],themeConfig:{nav:[{text:"Home",link:"/"},{text:"hbm",link:"/hbm/"},{text:"compiler",link:"/compiler/"},{text:"gpu",link:"/gpu/"},{text:"cpu",link:"/cpu/"},{text:"llm",link:"/llm/"},{text:"unix",link:"/unix/"},{text:"BBS",link:"/message-board/"},{text:"CSDN",link:"https://blog.csdn.net/hit_shaoqi"}],sidebarDepth:3,repo:"hitqshao/qishao-notes",searchMaxSuggestions:10,lastUpdated:"上次更新",editLinks:!0,docsDir:"docs",docsBranch:"main",editLinkText:"帮助我们改善此页面",searchPlaceholder:"按下 𝑺 搜索",category:!1,tag:!1,sidebar:{"/00.目录页/":[["00.Content.md","Content","/pages/f27694/"],["01.hbm.md","HBM","/hbm/"],["02.compiler.md","llvm & mlir","/compiler/"],["03.gpu.md","gpu","/gpu/"],["04.cpu.md","cpu","/cpu/"],["05.llm.md","llm","/llm/"],["06.unix.md","unix","/unix/"]],catalogue:{hbm:"/hbm/",compiler:"/compiler/",gpu:"/gpu/",cpu:"/cpu/",llm:"/llm/",unix:"/unix/"},"/01.hbm/":[["01.HBM_Paper_List.md","HBM Paper List","/pages/24769e/"],["02.hbm_dead_block_predictor.md","HBM Dead Block Predictor","/pages/2476af/"],["03.Dynamically_Adapting _Page_Migration_Policies_Based_on_Applications_Memory_Access_Behaviors.md","Dynamically Adapting  Page Migration Policies Based on Applications Memory Access Behaviors","/pages/24769f/"],["04.DRAM_PCM_NVM_Cache.md","DRAM PCM NVM Cache","/pages/24760e/"],["05.cache_mem_compression.md","Cache Memory Compression","/pages/2476bf/"],["06.memory ecc.md","memory-ecc","/pages/f07695/"],["07.hbm-latency.md","hbm-latency","/pages/f07696/"],["08.compression.md","compression","/pages/f07698/"],["09.compressibility_prediction.md","compressibility prediction","/pages/f07699/"],["10.software_memory_paper.md","memory management","/pages/f07692/"]],"/02.compiler/":[["01.llvm_frontend.md","llvm front end","/pages/000001/"],["02.GetStartedLLVMChap5Notes.md","Getting Started with LLVM Core Libraries-Notes Chap5 IR","/pages/000002/"],["03.GetStartedLLVMChap6Notes.md","Getting Started with LLVM Core Libraries-Notes Chap6 Backend","/pages/000003/"],["04. LearningLLVMDiary0.md","Learning LLVM Diary 00","/pages/000004/"],["05. addInstACE.md",'Add New Instruction "ACE" to LLVM',"/pages/000005/"],["06.Value&Use.md","How does LLVM perform instruction combine","/pages/000006/"],["07. UnderstaningLLVMwithSourceCode.md","Understand llvm with its source code Part 1","/pages/000007/"]],"/03.gpu/":[["01.operand_collector.md","Operand Collector","/pages/cc7034/"],["02.warp_execution.md","GPU WARP Scheduler","/pages/2476ae/"],["03.Precise Exception.md","Precision Exception","/pages/14769f/"],["04.Unified_Memory.md","Unified Memory Paper List","/pages/44771e/"],["05.TensorCore.md","TensorCore Paper List","/pages/44871e/"],["06.MemoryBehaviour.md","Memory Behaviour Paper List","/pages/45871e/"],["07.GPUVirtualization.md","GPU Virtualization Paper List","/pages/45871f/"],["08.LLM.md","Large Language Model Paper List","/pages/458720/"],["09.Simulator.md","GPU Simulator","/pages/458721/"],["10. Architectural Survey.md","Architectural Survey","/pages/458722/"],["11.IntegratedCPUGPUMemory.md","Harnessing Integrated CPU-GPU System Memory for HPC a first look into Grace Hopper","/pages/458724/"],["12.gpgpusim.md","Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (1)","/pages/458725/"],["13.gpgpusim.md","Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (2)","/pages/458726/"],["14.gpgpusim.md","Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (3)","/pages/458727/"],["15.gpgpusim.md","Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (4)","/pages/45872/"],["16.gpgpusim.md","Understanding GPGPU-SIM & GPGPU-SIM UVM_SMART (5)","/pages/45874/"],["17.warp_mem.md","Warp Related Memory Optimization","/pages/45873/"],["18.gpucoherency.md","GPU Cache Coherency","/pages/45875/"],["19.gpu_cache_mem.md","GPU Cache & Memory Hirerarchy","/pages/45876/"],["20.gpu_tlb_ptw.md","GPU TLB","/pages/45877/"],["1234.TODO.md","TO READ","/pages/47871e/"]],"/04.cpu/":[["01.checkpoint.md","checkpoint","/pages/cc7035/"],["02.topdown.md","topdown analysis","/pages/cc7036/"],["03.loadstore.md","load store unit","/pages/cc7037/"],["05.cache structure.md","cache & bank structure","/pages/cc7038/"],["06.cache timing.md","cache timing","/pages/cc7039/"],["07.register file.md","cache timing","/pages/cc7040/"],["1234.markdown.md","two-test-1","/pages/f07697/"]],"/05.llm/":[["01.How_LLM_Works.md","how llm works","/pages/dc7035/"],["02.LLM_HW_Opt.md","LLM Hardware Optimization","/pages/dc7036/"],["03.gem5_LLAMA.md","How to run llama.cpp with gem5","/pages/dc7037/"],["04.mem_usage_llm.md","Memory Usage in Training LLM","/pages/dc7038/"]],"/06.unix/":[["01.malloc.md","how malloc works","/pages/ec7035/"]],"/09.nine/":[["01.留言板.md","留言板","/message-board"],["02.template.md","template","/pages/c45600/"]],"/10.mix/":[["01.leakagecurrent.md","leakage current moores law meets static power","/pages/f00000/"]]},pageStyle:"line",updateBar:{showToArticle:!1},author:{name:"hitqishao",link:"https://github.com/hitqshao"},social:{icons:[{iconClass:"icon-github",title:"GitHub",link:"https://github.com/hitqshao"},{iconClass:"icon-youjian",title:"发邮件",link:"mailto:hitqshao@163.com"},{iconClass:"icon-gitee",title:"Gitee",link:"https://gitee.com/hitqshao"}]},footer:{createYear:2022,copyrightInfo:'Eryajf | <a href="https://github.com/hitqshao/qishao-notes/blob/main/LICENSE" target="_blank">MIT License</a>'}}};var cc=t(119),lc=t(120),dc=t(13);var hc={computed:{$filterPosts(){return this.$site.pages.filter(e=>{const{frontmatter:{pageComponent:n,article:t,home:a}}=e;return!(n||!1===t||!0===a)})},$sortPosts(){return(e=this.$filterPosts).sort((e,n)=>{const t=e.frontmatter.sticky,a=n.frontmatter.sticky;return t&&a?t==a?Object(dc.a)(e,n):t-a:t&&!a?-1:!t&&a?1:Object(dc.a)(e,n)}),e;var e},$sortPostsByDate(){return(e=this.$filterPosts).sort((e,n)=>Object(dc.a)(e,n)),e;var e},$groupPosts(){return function(e){const n={},t={};for(let a=0,i=e.length;a<i;a++){const{frontmatter:{categories:i,tags:r}}=e[a];"array"===Object(dc.n)(i)&&i.forEach(t=>{t&&(n[t]||(n[t]=[]),n[t].push(e[a]))}),"array"===Object(dc.n)(r)&&r.forEach(n=>{n&&(t[n]||(t[n]=[]),t[n].push(e[a]))})}return{categories:n,tags:t}}(this.$sortPosts)},$categoriesAndTags(){return function(e){const n=[],t=[];for(let t in e.categories)n.push({key:t,length:e.categories[t].length});for(let n in e.tags)t.push({key:n,length:e.tags[n].length});return{categories:n,tags:t}}(this.$groupPosts)}}};Wt.component(cc.default),Wt.component(lc.default);function uc(e){return e.toString().padStart(2,"0")}t(269);Wt.component("Badge",()=>Promise.all([t.e(0),t.e(3)]).then(t.bind(null,480))),Wt.component("CodeBlock",()=>Promise.resolve().then(t.bind(null,119))),Wt.component("CodeGroup",()=>Promise.resolve().then(t.bind(null,120)));t(270);
/**
  * vue-class-component v7.2.6
  * (c) 2015-present Evan You
  * @license MIT
  */
function pc(e){return(pc="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}function mc(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function fc(e){return function(e){if(Array.isArray(e)){for(var n=0,t=new Array(e.length);n<e.length;n++)t[n]=e[n];return t}}(e)||function(e){if(Symbol.iterator in Object(e)||"[object Arguments]"===Object.prototype.toString.call(e))return Array.from(e)}(e)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance")}()}function gc(){return"undefined"!=typeof Reflect&&Reflect.defineMetadata&&Reflect.getOwnMetadataKeys}function yc(e,n){bc(e,n),Object.getOwnPropertyNames(n.prototype).forEach((function(t){bc(e.prototype,n.prototype,t)})),Object.getOwnPropertyNames(n).forEach((function(t){bc(e,n,t)}))}function bc(e,n,t){(t?Reflect.getOwnMetadataKeys(n,t):Reflect.getOwnMetadataKeys(n)).forEach((function(a){var i=t?Reflect.getOwnMetadata(a,n,t):Reflect.getOwnMetadata(a,n);t?Reflect.defineMetadata(a,i,e,t):Reflect.defineMetadata(a,i,e)}))}var vc={__proto__:[]}instanceof Array;function wc(e){return function(n,t,a){var i="function"==typeof n?n:n.constructor;i.__decorators__||(i.__decorators__=[]),"number"!=typeof a&&(a=void 0),i.__decorators__.push((function(n){return e(n,t,a)}))}}function _c(e,n){var t=n.prototype._init;n.prototype._init=function(){var n=this,t=Object.getOwnPropertyNames(e);if(e.$options.props)for(var a in e.$options.props)e.hasOwnProperty(a)||t.push(a);t.forEach((function(t){Object.defineProperty(n,t,{get:function(){return e[t]},set:function(n){e[t]=n},configurable:!0})}))};var a=new n;n.prototype._init=t;var i={};return Object.keys(a).forEach((function(e){void 0!==a[e]&&(i[e]=a[e])})),i}var kc=["data","beforeCreate","created","beforeMount","mounted","beforeDestroy","destroyed","beforeUpdate","updated","activated","deactivated","render","errorCaptured","serverPrefetch"];function xc(e){var n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};n.name=n.name||e._componentTag||e.name;var t=e.prototype;Object.getOwnPropertyNames(t).forEach((function(e){if("constructor"!==e)if(kc.indexOf(e)>-1)n[e]=t[e];else{var a=Object.getOwnPropertyDescriptor(t,e);void 0!==a.value?"function"==typeof a.value?(n.methods||(n.methods={}))[e]=a.value:(n.mixins||(n.mixins=[])).push({data:function(){return mc({},e,a.value)}}):(a.get||a.set)&&((n.computed||(n.computed={}))[e]={get:a.get,set:a.set})}})),(n.mixins||(n.mixins=[])).push({data:function(){return _c(this,e)}});var a=e.__decorators__;a&&(a.forEach((function(e){return e(n)})),delete e.__decorators__);var i=Object.getPrototypeOf(e.prototype),r=i instanceof Wt?i.constructor:Wt,o=r.extend(n);return Pc(o,e,r),gc()&&yc(o,e),o}var Cc={prototype:!0,arguments:!0,callee:!0,caller:!0};function Pc(e,n,t){Object.getOwnPropertyNames(n).forEach((function(a){if(!Cc[a]){var i=Object.getOwnPropertyDescriptor(e,a);if(!i||i.configurable){var r,o,s=Object.getOwnPropertyDescriptor(n,a);if(!vc){if("cid"===a)return;var c=Object.getOwnPropertyDescriptor(t,a);if(r=s.value,o=pc(r),null!=r&&("object"===o||"function"===o)&&c&&c.value===s.value)return}0,Object.defineProperty(e,a,s)}}}))}function Tc(e){return"function"==typeof e?xc(e):function(n){return xc(n,e)}}Tc.registerHooks=function(e){kc.push.apply(kc,fc(e))};var Mc=Tc;function Ac(e){return wc((function(n,t){void 0===n.inject&&(n.inject={}),Array.isArray(n.inject)||(n.inject[t]=e||t)}))}function Sc(e){var n=function(){var t=this,a="function"==typeof e?e.call(this):e;for(var i in(a=Object.create(a||null)).__reactiveInject__=this.__reactiveInject__||{},n.managed)a[n.managed[i]]=this[i];var r=function(e){a[n.managedReactive[e]]=o[e],Object.defineProperty(a.__reactiveInject__,n.managedReactive[e],{enumerable:!0,get:function(){return t[e]}})},o=this;for(var i in n.managedReactive)r(i);return a};return n.managed={},n.managedReactive={},n}function Ic(e){return"function"!=typeof e||!e.managed&&!e.managedReactive}var Lc="undefined"!=typeof Reflect&&void 0!==Reflect.getMetadata;function Dc(e,n,t){if(Lc&&!Array.isArray(e)&&"function"!=typeof e&&void 0===e.type){var a=Reflect.getMetadata("design:type",n,t);a!==Object&&(e.type=a)}}function zc(e){return void 0===e&&(e={}),function(n,t){Dc(e,n,t),wc((function(n,t){(n.props||(n.props={}))[t]=e}))(n,t)}}function Uc(e,n){void 0===n&&(n={});var t=n.deep,a=void 0!==t&&t,i=n.immediate,r=void 0!==i&&i;return wc((function(n,t){"object"!=typeof n.watch&&(n.watch=Object.create(null));var i=n.watch;"object"!=typeof i[e]||Array.isArray(i[e])?void 0===i[e]&&(i[e]=[]):i[e]=[i[e]],i[e].push({handler:t,deep:a,immediate:r})}))}var Oc=t(16);const Rc=(e,n)=>`${e}${Object(Oc.stringify)(n,{addQueryPrefix:!0})}`,Ec=(e,n)=>`${e.replace(/\/$/,"")}/${n.replace(/^\//,"")}`;var Gc=t(116),qc=t.n(Gc);const Bc=e=>qc()(e,"YYYY-MM-DD HH:mm:ss"),jc=e=>(e.split("#")[0]||"").split("?")[0]||"";
/*!
 * vue-i18n v8.28.2 
 * (c) 2022 kazuya kawaguchi
 * Released under the MIT License.
 */
var Nc=["compactDisplay","currency","currencyDisplay","currencySign","localeMatcher","notation","numberingSystem","signDisplay","style","unit","unitDisplay","useGrouping","minimumIntegerDigits","minimumFractionDigits","maximumFractionDigits","minimumSignificantDigits","maximumSignificantDigits"],$c=["dateStyle","timeStyle","calendar","localeMatcher","hour12","hourCycle","timeZone","formatMatcher","weekday","era","year","month","day","hour","minute","second","timeZoneName"];function Fc(e,n){"undefined"!=typeof console&&(console.warn("[vue-i18n] "+e),n&&console.warn(n.stack))}var Vc=Array.isArray;function Hc(e){return null!==e&&"object"==typeof e}function Wc(e){return"string"==typeof e}var Kc=Object.prototype.toString;function Zc(e){return"[object Object]"===Kc.call(e)}function Yc(e){return null==e}function Xc(e){return"function"==typeof e}function Qc(){for(var e=[],n=arguments.length;n--;)e[n]=arguments[n];var t=null,a=null;return 1===e.length?Hc(e[0])||Vc(e[0])?a=e[0]:"string"==typeof e[0]&&(t=e[0]):2===e.length&&("string"==typeof e[0]&&(t=e[0]),(Hc(e[1])||Vc(e[1]))&&(a=e[1])),{locale:t,params:a}}function Jc(e){return JSON.parse(JSON.stringify(e))}function el(e,n){return!!~e.indexOf(n)}var nl=Object.prototype.hasOwnProperty;function tl(e,n){return nl.call(e,n)}function al(e){for(var n=arguments,t=Object(e),a=1;a<arguments.length;a++){var i=n[a];if(null!=i){var r=void 0;for(r in i)tl(i,r)&&(Hc(i[r])?t[r]=al(t[r],i[r]):t[r]=i[r])}}return t}function il(e,n){if(e===n)return!0;var t=Hc(e),a=Hc(n);if(!t||!a)return!t&&!a&&String(e)===String(n);try{var i=Vc(e),r=Vc(n);if(i&&r)return e.length===n.length&&e.every((function(e,t){return il(e,n[t])}));if(i||r)return!1;var o=Object.keys(e),s=Object.keys(n);return o.length===s.length&&o.every((function(t){return il(e[t],n[t])}))}catch(e){return!1}}function rl(e){return null!=e&&Object.keys(e).forEach((function(n){"string"==typeof e[n]&&(e[n]=e[n].replace(/</g,"&lt;").replace(/>/g,"&gt;").replace(/"/g,"&quot;").replace(/'/g,"&apos;"))})),e}var ol={name:"i18n",functional:!0,props:{tag:{type:[String,Boolean,Object],default:"span"},path:{type:String,required:!0},locale:{type:String},places:{type:[Array,Object]}},render:function(e,n){var t=n.data,a=n.parent,i=n.props,r=n.slots,o=a.$i18n;if(o){var s=i.path,c=i.locale,l=i.places,d=r(),h=o.i(s,c,function(e){var n;for(n in e)if("default"!==n)return!1;return Boolean(n)}(d)||l?function(e,n){var t=n?function(e){0;return Array.isArray(e)?e.reduce(cl,{}):Object.assign({},e)}(n):{};if(!e)return t;var a=(e=e.filter((function(e){return e.tag||""!==e.text.trim()}))).every(ll);0;return e.reduce(a?sl:cl,t)}(d.default,l):d),u=i.tag&&!0!==i.tag||!1===i.tag?i.tag:"span";return u?e(u,t,h):h}}};function sl(e,n){return n.data&&n.data.attrs&&n.data.attrs.place&&(e[n.data.attrs.place]=n),e}function cl(e,n,t){return e[t]=n,e}function ll(e){return Boolean(e.data&&e.data.attrs&&e.data.attrs.place)}var dl,hl={name:"i18n-n",functional:!0,props:{tag:{type:[String,Boolean,Object],default:"span"},value:{type:Number,required:!0},format:{type:[String,Object]},locale:{type:String}},render:function(e,n){var t=n.props,a=n.parent,i=n.data,r=a.$i18n;if(!r)return null;var o=null,s=null;Wc(t.format)?o=t.format:Hc(t.format)&&(t.format.key&&(o=t.format.key),s=Object.keys(t.format).reduce((function(e,n){var a;return el(Nc,n)?Object.assign({},e,((a={})[n]=t.format[n],a)):e}),null));var c=t.locale||r.locale,l=r._ntp(t.value,c,o,s),d=l.map((function(e,n){var t,a=i.scopedSlots&&i.scopedSlots[e.type];return a?a(((t={})[e.type]=e.value,t.index=n,t.parts=l,t)):e.value})),h=t.tag&&!0!==t.tag||!1===t.tag?t.tag:"span";return h?e(h,{attrs:i.attrs,class:i.class,staticClass:i.staticClass},d):d}};function ul(e,n,t){fl(e,t)&&gl(e,n,t)}function pl(e,n,t,a){if(fl(e,t)){var i=t.context.$i18n;(function(e,n){var t=n.context;return e._locale===t.$i18n.locale})(e,t)&&il(n.value,n.oldValue)&&il(e._localeMessage,i.getLocaleMessage(i.locale))||gl(e,n,t)}}function ml(e,n,t,a){if(t.context){var i=t.context.$i18n||{};n.modifiers.preserve||i.preserveDirectiveContent||(e.textContent=""),e._vt=void 0,delete e._vt,e._locale=void 0,delete e._locale,e._localeMessage=void 0,delete e._localeMessage}else Fc("Vue instance does not exists in VNode context")}function fl(e,n){var t=n.context;return t?!!t.$i18n||(Fc("VueI18n instance does not exists in Vue instance"),!1):(Fc("Vue instance does not exists in VNode context"),!1)}function gl(e,n,t){var a,i,r=function(e){var n,t,a,i;Wc(e)?n=e:Zc(e)&&(n=e.path,t=e.locale,a=e.args,i=e.choice);return{path:n,locale:t,args:a,choice:i}}(n.value),o=r.path,s=r.locale,c=r.args,l=r.choice;if(o||s||c)if(o){var d=t.context;e._vt=e.textContent=null!=l?(a=d.$i18n).tc.apply(a,[o,l].concat(yl(s,c))):(i=d.$i18n).t.apply(i,[o].concat(yl(s,c))),e._locale=d.$i18n.locale,e._localeMessage=d.$i18n.getLocaleMessage(d.$i18n.locale)}else Fc("`path` is required in v-t directive");else Fc("value type not supported")}function yl(e,n){var t=[];return e&&t.push(e),n&&(Array.isArray(n)||Zc(n))&&t.push(n),t}function bl(e,n){void 0===n&&(n={bridge:!1}),bl.installed=!0;(dl=e).version&&Number(dl.version.split(".")[0]);(function(e){e.prototype.hasOwnProperty("$i18n")||Object.defineProperty(e.prototype,"$i18n",{get:function(){return this._i18n}}),e.prototype.$t=function(e){for(var n=[],t=arguments.length-1;t-- >0;)n[t]=arguments[t+1];var a=this.$i18n;return a._t.apply(a,[e,a.locale,a._getMessages(),this].concat(n))},e.prototype.$tc=function(e,n){for(var t=[],a=arguments.length-2;a-- >0;)t[a]=arguments[a+2];var i=this.$i18n;return i._tc.apply(i,[e,i.locale,i._getMessages(),this,n].concat(t))},e.prototype.$te=function(e,n){var t=this.$i18n;return t._te(e,t.locale,t._getMessages(),n)},e.prototype.$d=function(e){for(var n,t=[],a=arguments.length-1;a-- >0;)t[a]=arguments[a+1];return(n=this.$i18n).d.apply(n,[e].concat(t))},e.prototype.$n=function(e){for(var n,t=[],a=arguments.length-1;a-- >0;)t[a]=arguments[a+1];return(n=this.$i18n).n.apply(n,[e].concat(t))}})(dl),dl.mixin(function(e){function n(){this!==this.$root&&this.$options.__INTLIFY_META__&&this.$el&&this.$el.setAttribute("data-intlify",this.$options.__INTLIFY_META__)}return void 0===e&&(e=!1),e?{mounted:n}:{beforeCreate:function(){var e=this.$options;if(e.i18n=e.i18n||(e.__i18nBridge||e.__i18n?{}:null),e.i18n)if(e.i18n instanceof Ul){if(e.__i18nBridge||e.__i18n)try{var n=e.i18n&&e.i18n.messages?e.i18n.messages:{};(e.__i18nBridge||e.__i18n).forEach((function(e){n=al(n,JSON.parse(e))})),Object.keys(n).forEach((function(t){e.i18n.mergeLocaleMessage(t,n[t])}))}catch(e){0}this._i18n=e.i18n,this._i18nWatcher=this._i18n.watchI18nData()}else if(Zc(e.i18n)){var t=this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof Ul?this.$root.$i18n:null;if(t&&(e.i18n.root=this.$root,e.i18n.formatter=t.formatter,e.i18n.fallbackLocale=t.fallbackLocale,e.i18n.formatFallbackMessages=t.formatFallbackMessages,e.i18n.silentTranslationWarn=t.silentTranslationWarn,e.i18n.silentFallbackWarn=t.silentFallbackWarn,e.i18n.pluralizationRules=t.pluralizationRules,e.i18n.preserveDirectiveContent=t.preserveDirectiveContent),e.__i18nBridge||e.__i18n)try{var a=e.i18n&&e.i18n.messages?e.i18n.messages:{};(e.__i18nBridge||e.__i18n).forEach((function(e){a=al(a,JSON.parse(e))})),e.i18n.messages=a}catch(e){0}var i=e.i18n.sharedMessages;i&&Zc(i)&&(e.i18n.messages=al(e.i18n.messages,i)),this._i18n=new Ul(e.i18n),this._i18nWatcher=this._i18n.watchI18nData(),(void 0===e.i18n.sync||e.i18n.sync)&&(this._localeWatcher=this.$i18n.watchLocale()),t&&t.onComponentInstanceCreated(this._i18n)}else 0;else this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof Ul?this._i18n=this.$root.$i18n:e.parent&&e.parent.$i18n&&e.parent.$i18n instanceof Ul&&(this._i18n=e.parent.$i18n)},beforeMount:function(){var e=this.$options;e.i18n=e.i18n||(e.__i18nBridge||e.__i18n?{}:null),e.i18n?(e.i18n instanceof Ul||Zc(e.i18n))&&(this._i18n.subscribeDataChanging(this),this._subscribing=!0):(this.$root&&this.$root.$i18n&&this.$root.$i18n instanceof Ul||e.parent&&e.parent.$i18n&&e.parent.$i18n instanceof Ul)&&(this._i18n.subscribeDataChanging(this),this._subscribing=!0)},mounted:n,beforeDestroy:function(){if(this._i18n){var e=this;this.$nextTick((function(){e._subscribing&&(e._i18n.unsubscribeDataChanging(e),delete e._subscribing),e._i18nWatcher&&(e._i18nWatcher(),e._i18n.destroyVM(),delete e._i18nWatcher),e._localeWatcher&&(e._localeWatcher(),delete e._localeWatcher)}))}}}}(n.bridge)),dl.directive("t",{bind:ul,update:pl,unbind:ml}),dl.component(ol.name,ol),dl.component(hl.name,hl),dl.config.optionMergeStrategies.i18n=function(e,n){return void 0===n?e:n}}var vl=function(){this._caches=Object.create(null)};vl.prototype.interpolate=function(e,n){if(!n)return[e];var t=this._caches[e];return t||(t=function(e){var n=[],t=0,a="";for(;t<e.length;){var i=e[t++];if("{"===i){a&&n.push({type:"text",value:a}),a="";var r="";for(i=e[t++];void 0!==i&&"}"!==i;)r+=i,i=e[t++];var o="}"===i,s=wl.test(r)?"list":o&&_l.test(r)?"named":"unknown";n.push({value:r,type:s})}else"%"===i?"{"!==e[t]&&(a+=i):a+=i}return a&&n.push({type:"text",value:a}),n}(e),this._caches[e]=t),function(e,n){var t=[],a=0,i=Array.isArray(n)?"list":Hc(n)?"named":"unknown";if("unknown"===i)return t;for(;a<e.length;){var r=e[a];switch(r.type){case"text":t.push(r.value);break;case"list":t.push(n[parseInt(r.value,10)]);break;case"named":"named"===i&&t.push(n[r.value]);break;case"unknown":0}a++}return t}(t,n)};var wl=/^(?:\d)+/,_l=/^(?:\w)+/;var kl=[];kl[0]={ws:[0],ident:[3,0],"[":[4],eof:[7]},kl[1]={ws:[1],".":[2],"[":[4],eof:[7]},kl[2]={ws:[2],ident:[3,0],0:[3,0],number:[3,0]},kl[3]={ident:[3,0],0:[3,0],number:[3,0],ws:[1,1],".":[2,1],"[":[4,1],eof:[7,1]},kl[4]={"'":[5,0],'"':[6,0],"[":[4,2],"]":[1,3],eof:8,else:[4,0]},kl[5]={"'":[4,0],eof:8,else:[5,0]},kl[6]={'"':[4,0],eof:8,else:[6,0]};var xl=/^\s?(?:true|false|-?[\d.]+|'[^']*'|"[^"]*")\s?$/;function Cl(e){if(null==e)return"eof";switch(e.charCodeAt(0)){case 91:case 93:case 46:case 34:case 39:return e;case 95:case 36:case 45:return"ident";case 9:case 10:case 13:case 160:case 65279:case 8232:case 8233:return"ws"}return"ident"}function Pl(e){var n,t,a,i=e.trim();return("0"!==e.charAt(0)||!isNaN(e))&&(a=i,xl.test(a)?(t=(n=i).charCodeAt(0))!==n.charCodeAt(n.length-1)||34!==t&&39!==t?n:n.slice(1,-1):"*"+i)}var Tl=function(){this._cache=Object.create(null)};Tl.prototype.parsePath=function(e){var n=this._cache[e];return n||(n=function(e){var n,t,a,i,r,o,s,c=[],l=-1,d=0,h=0,u=[];function p(){var n=e[l+1];if(5===d&&"'"===n||6===d&&'"'===n)return l++,a="\\"+n,u[0](),!0}for(u[1]=function(){void 0!==t&&(c.push(t),t=void 0)},u[0]=function(){void 0===t?t=a:t+=a},u[2]=function(){u[0](),h++},u[3]=function(){if(h>0)h--,d=4,u[0]();else{if(h=0,void 0===t)return!1;if(!1===(t=Pl(t)))return!1;u[1]()}};null!==d;)if(l++,"\\"!==(n=e[l])||!p()){if(i=Cl(n),8===(r=(s=kl[d])[i]||s.else||8))return;if(d=r[0],(o=u[r[1]])&&(a=void 0===(a=r[2])?n:a,!1===o()))return;if(7===d)return c}}(e))&&(this._cache[e]=n),n||[]},Tl.prototype.getPathValue=function(e,n){if(!Hc(e))return null;var t=this.parsePath(n);if(0===t.length)return null;for(var a=t.length,i=e,r=0;r<a;){var o=i[t[r]];if(null==o)return null;i=o,r++}return i};var Ml,Al=/<\/?[\w\s="/.':;#-\/]+>/,Sl=/(?:@(?:\.[a-zA-Z]+)?:(?:[\w\-_|./]+|\([\w\-_:|./]+\)))/g,Il=/^@(?:\.([a-zA-Z]+))?:/,Ll=/[()]/g,Dl={upper:function(e){return e.toLocaleUpperCase()},lower:function(e){return e.toLocaleLowerCase()},capitalize:function(e){return""+e.charAt(0).toLocaleUpperCase()+e.substr(1)}},zl=new vl,Ul=function(e){var n=this;void 0===e&&(e={}),!dl&&"undefined"!=typeof window&&window.Vue&&bl(window.Vue);var t=e.locale||"en-US",a=!1!==e.fallbackLocale&&(e.fallbackLocale||"en-US"),i=e.messages||{},r=e.dateTimeFormats||e.datetimeFormats||{},o=e.numberFormats||{};this._vm=null,this._formatter=e.formatter||zl,this._modifiers=e.modifiers||{},this._missing=e.missing||null,this._root=e.root||null,this._sync=void 0===e.sync||!!e.sync,this._fallbackRoot=void 0===e.fallbackRoot||!!e.fallbackRoot,this._fallbackRootWithEmptyString=void 0===e.fallbackRootWithEmptyString||!!e.fallbackRootWithEmptyString,this._formatFallbackMessages=void 0!==e.formatFallbackMessages&&!!e.formatFallbackMessages,this._silentTranslationWarn=void 0!==e.silentTranslationWarn&&e.silentTranslationWarn,this._silentFallbackWarn=void 0!==e.silentFallbackWarn&&!!e.silentFallbackWarn,this._dateTimeFormatters={},this._numberFormatters={},this._path=new Tl,this._dataListeners=new Set,this._componentInstanceCreatedListener=e.componentInstanceCreatedListener||null,this._preserveDirectiveContent=void 0!==e.preserveDirectiveContent&&!!e.preserveDirectiveContent,this.pluralizationRules=e.pluralizationRules||{},this._warnHtmlInMessage=e.warnHtmlInMessage||"off",this._postTranslation=e.postTranslation||null,this._escapeParameterHtml=e.escapeParameterHtml||!1,"__VUE_I18N_BRIDGE__"in e&&(this.__VUE_I18N_BRIDGE__=e.__VUE_I18N_BRIDGE__),this.getChoiceIndex=function(e,t){var a=Object.getPrototypeOf(n);if(a&&a.getChoiceIndex)return a.getChoiceIndex.call(n,e,t);var i,r;return n.locale in n.pluralizationRules?n.pluralizationRules[n.locale].apply(n,[e,t]):(i=e,r=t,i=Math.abs(i),2===r?i?i>1?1:0:1:i?Math.min(i,2):0)},this._exist=function(e,t){return!(!e||!t)&&(!Yc(n._path.getPathValue(e,t))||!!e[t])},"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||Object.keys(i).forEach((function(e){n._checkLocaleMessage(e,n._warnHtmlInMessage,i[e])})),this._initVM({locale:t,fallbackLocale:a,messages:i,dateTimeFormats:r,numberFormats:o})},Ol={vm:{configurable:!0},messages:{configurable:!0},dateTimeFormats:{configurable:!0},numberFormats:{configurable:!0},availableLocales:{configurable:!0},locale:{configurable:!0},fallbackLocale:{configurable:!0},formatFallbackMessages:{configurable:!0},missing:{configurable:!0},formatter:{configurable:!0},silentTranslationWarn:{configurable:!0},silentFallbackWarn:{configurable:!0},preserveDirectiveContent:{configurable:!0},warnHtmlInMessage:{configurable:!0},postTranslation:{configurable:!0},sync:{configurable:!0}};Ul.prototype._checkLocaleMessage=function(e,n,t){var a=function(e,n,t,i){if(Zc(t))Object.keys(t).forEach((function(r){var o=t[r];Zc(o)?(i.push(r),i.push("."),a(e,n,o,i),i.pop(),i.pop()):(i.push(r),a(e,n,o,i),i.pop())}));else if(Vc(t))t.forEach((function(t,r){Zc(t)?(i.push("["+r+"]"),i.push("."),a(e,n,t,i),i.pop(),i.pop()):(i.push("["+r+"]"),a(e,n,t,i),i.pop())}));else if(Wc(t)){if(Al.test(t)){var r="Detected HTML in message '"+t+"' of keypath '"+i.join("")+"' at '"+n+"'. Consider component interpolation with '<i18n>' to avoid XSS. See https://bit.ly/2ZqJzkp";"warn"===e?Fc(r):"error"===e&&function(e,n){"undefined"!=typeof console&&(console.error("[vue-i18n] "+e),n&&console.error(n.stack))}(r)}}};a(n,e,t,[])},Ul.prototype._initVM=function(e){var n=dl.config.silent;dl.config.silent=!0,this._vm=new dl({data:e,__VUE18N__INSTANCE__:!0}),dl.config.silent=n},Ul.prototype.destroyVM=function(){this._vm.$destroy()},Ul.prototype.subscribeDataChanging=function(e){this._dataListeners.add(e)},Ul.prototype.unsubscribeDataChanging=function(e){!function(e,n){if(e.delete(n));}(this._dataListeners,e)},Ul.prototype.watchI18nData=function(){var e=this;return this._vm.$watch("$data",(function(){for(var n,t,a=(n=e._dataListeners,t=[],n.forEach((function(e){return t.push(e)})),t),i=a.length;i--;)dl.nextTick((function(){a[i]&&a[i].$forceUpdate()}))}),{deep:!0})},Ul.prototype.watchLocale=function(e){if(e){if(!this.__VUE_I18N_BRIDGE__)return null;var n=this,t=this._vm;return this.vm.$watch("locale",(function(a){t.$set(t,"locale",a),n.__VUE_I18N_BRIDGE__&&e&&(e.locale.value=a),t.$forceUpdate()}),{immediate:!0})}if(!this._sync||!this._root)return null;var a=this._vm;return this._root.$i18n.vm.$watch("locale",(function(e){a.$set(a,"locale",e),a.$forceUpdate()}),{immediate:!0})},Ul.prototype.onComponentInstanceCreated=function(e){this._componentInstanceCreatedListener&&this._componentInstanceCreatedListener(e,this)},Ol.vm.get=function(){return this._vm},Ol.messages.get=function(){return Jc(this._getMessages())},Ol.dateTimeFormats.get=function(){return Jc(this._getDateTimeFormats())},Ol.numberFormats.get=function(){return Jc(this._getNumberFormats())},Ol.availableLocales.get=function(){return Object.keys(this.messages).sort()},Ol.locale.get=function(){return this._vm.locale},Ol.locale.set=function(e){this._vm.$set(this._vm,"locale",e)},Ol.fallbackLocale.get=function(){return this._vm.fallbackLocale},Ol.fallbackLocale.set=function(e){this._localeChainCache={},this._vm.$set(this._vm,"fallbackLocale",e)},Ol.formatFallbackMessages.get=function(){return this._formatFallbackMessages},Ol.formatFallbackMessages.set=function(e){this._formatFallbackMessages=e},Ol.missing.get=function(){return this._missing},Ol.missing.set=function(e){this._missing=e},Ol.formatter.get=function(){return this._formatter},Ol.formatter.set=function(e){this._formatter=e},Ol.silentTranslationWarn.get=function(){return this._silentTranslationWarn},Ol.silentTranslationWarn.set=function(e){this._silentTranslationWarn=e},Ol.silentFallbackWarn.get=function(){return this._silentFallbackWarn},Ol.silentFallbackWarn.set=function(e){this._silentFallbackWarn=e},Ol.preserveDirectiveContent.get=function(){return this._preserveDirectiveContent},Ol.preserveDirectiveContent.set=function(e){this._preserveDirectiveContent=e},Ol.warnHtmlInMessage.get=function(){return this._warnHtmlInMessage},Ol.warnHtmlInMessage.set=function(e){var n=this,t=this._warnHtmlInMessage;if(this._warnHtmlInMessage=e,t!==e&&("warn"===e||"error"===e)){var a=this._getMessages();Object.keys(a).forEach((function(e){n._checkLocaleMessage(e,n._warnHtmlInMessage,a[e])}))}},Ol.postTranslation.get=function(){return this._postTranslation},Ol.postTranslation.set=function(e){this._postTranslation=e},Ol.sync.get=function(){return this._sync},Ol.sync.set=function(e){this._sync=e},Ul.prototype._getMessages=function(){return this._vm.messages},Ul.prototype._getDateTimeFormats=function(){return this._vm.dateTimeFormats},Ul.prototype._getNumberFormats=function(){return this._vm.numberFormats},Ul.prototype._warnDefault=function(e,n,t,a,i,r){if(!Yc(t))return t;if(this._missing){var o=this._missing.apply(null,[e,n,a,i]);if(Wc(o))return o}else 0;if(this._formatFallbackMessages){var s=Qc.apply(void 0,i);return this._render(n,r,s.params,n)}return n},Ul.prototype._isFallbackRoot=function(e){return(this._fallbackRootWithEmptyString?!e:Yc(e))&&!Yc(this._root)&&this._fallbackRoot},Ul.prototype._isSilentFallbackWarn=function(e){return this._silentFallbackWarn instanceof RegExp?this._silentFallbackWarn.test(e):this._silentFallbackWarn},Ul.prototype._isSilentFallback=function(e,n){return this._isSilentFallbackWarn(n)&&(this._isFallbackRoot()||e!==this.fallbackLocale)},Ul.prototype._isSilentTranslationWarn=function(e){return this._silentTranslationWarn instanceof RegExp?this._silentTranslationWarn.test(e):this._silentTranslationWarn},Ul.prototype._interpolate=function(e,n,t,a,i,r,o){if(!n)return null;var s,c=this._path.getPathValue(n,t);if(Vc(c)||Zc(c))return c;if(Yc(c)){if(!Zc(n))return null;if(!Wc(s=n[t])&&!Xc(s))return null}else{if(!Wc(c)&&!Xc(c))return null;s=c}return Wc(s)&&(s.indexOf("@:")>=0||s.indexOf("@.")>=0)&&(s=this._link(e,n,s,a,"raw",r,o)),this._render(s,i,r,t)},Ul.prototype._link=function(e,n,t,a,i,r,o){var s=t,c=s.match(Sl);for(var l in c)if(c.hasOwnProperty(l)){var d=c[l],h=d.match(Il),u=h[0],p=h[1],m=d.replace(u,"").replace(Ll,"");if(el(o,m))return s;o.push(m);var f=this._interpolate(e,n,m,a,"raw"===i?"string":i,"raw"===i?void 0:r,o);if(this._isFallbackRoot(f)){if(!this._root)throw Error("unexpected error");var g=this._root.$i18n;f=g._translate(g._getMessages(),g.locale,g.fallbackLocale,m,a,i,r)}f=this._warnDefault(e,m,f,a,Vc(r)?r:[r],i),this._modifiers.hasOwnProperty(p)?f=this._modifiers[p](f):Dl.hasOwnProperty(p)&&(f=Dl[p](f)),o.pop(),s=f?s.replace(d,f):s}return s},Ul.prototype._createMessageContext=function(e,n,t,a){var i=this,r=Vc(e)?e:[],o=Hc(e)?e:{},s=this._getMessages(),c=this.locale;return{list:function(e){return r[e]},named:function(e){return o[e]},values:e,formatter:n,path:t,messages:s,locale:c,linked:function(e){return i._interpolate(c,s[c]||{},e,null,a,void 0,[e])}}},Ul.prototype._render=function(e,n,t,a){if(Xc(e))return e(this._createMessageContext(t,this._formatter||zl,a,n));var i=this._formatter.interpolate(e,t,a);return i||(i=zl.interpolate(e,t,a)),"string"!==n||Wc(i)?i:i.join("")},Ul.prototype._appendItemToChain=function(e,n,t){var a=!1;return el(e,n)||(a=!0,n&&(a="!"!==n[n.length-1],n=n.replace(/!/g,""),e.push(n),t&&t[n]&&(a=t[n]))),a},Ul.prototype._appendLocaleToChain=function(e,n,t){var a,i=n.split("-");do{var r=i.join("-");a=this._appendItemToChain(e,r,t),i.splice(-1,1)}while(i.length&&!0===a);return a},Ul.prototype._appendBlockToChain=function(e,n,t){for(var a=!0,i=0;i<n.length&&"boolean"==typeof a;i++){var r=n[i];Wc(r)&&(a=this._appendLocaleToChain(e,r,t))}return a},Ul.prototype._getLocaleChain=function(e,n){if(""===e)return[];this._localeChainCache||(this._localeChainCache={});var t=this._localeChainCache[e];if(!t){n||(n=this.fallbackLocale),t=[];for(var a,i=[e];Vc(i);)i=this._appendBlockToChain(t,i,n);(i=Wc(a=Vc(n)?n:Hc(n)?n.default?n.default:null:n)?[a]:a)&&this._appendBlockToChain(t,i,null),this._localeChainCache[e]=t}return t},Ul.prototype._translate=function(e,n,t,a,i,r,o){for(var s,c=this._getLocaleChain(n,t),l=0;l<c.length;l++){var d=c[l];if(!Yc(s=this._interpolate(d,e[d],a,i,r,o,[a])))return s}return null},Ul.prototype._t=function(e,n,t,a){for(var i,r=[],o=arguments.length-4;o-- >0;)r[o]=arguments[o+4];if(!e)return"";var s=Qc.apply(void 0,r);this._escapeParameterHtml&&(s.params=rl(s.params));var c=s.locale||n,l=this._translate(t,c,this.fallbackLocale,e,a,"string",s.params);if(this._isFallbackRoot(l)){if(!this._root)throw Error("unexpected error");return(i=this._root).$t.apply(i,[e].concat(r))}return l=this._warnDefault(c,e,l,a,r,"string"),this._postTranslation&&null!=l&&(l=this._postTranslation(l,e)),l},Ul.prototype.t=function(e){for(var n,t=[],a=arguments.length-1;a-- >0;)t[a]=arguments[a+1];return(n=this)._t.apply(n,[e,this.locale,this._getMessages(),null].concat(t))},Ul.prototype._i=function(e,n,t,a,i){var r=this._translate(t,n,this.fallbackLocale,e,a,"raw",i);if(this._isFallbackRoot(r)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.i(e,n,i)}return this._warnDefault(n,e,r,a,[i],"raw")},Ul.prototype.i=function(e,n,t){return e?(Wc(n)||(n=this.locale),this._i(e,n,this._getMessages(),null,t)):""},Ul.prototype._tc=function(e,n,t,a,i){for(var r,o=[],s=arguments.length-5;s-- >0;)o[s]=arguments[s+5];if(!e)return"";void 0===i&&(i=1);var c={count:i,n:i},l=Qc.apply(void 0,o);return l.params=Object.assign(c,l.params),o=null===l.locale?[l.params]:[l.locale,l.params],this.fetchChoice((r=this)._t.apply(r,[e,n,t,a].concat(o)),i)},Ul.prototype.fetchChoice=function(e,n){if(!e||!Wc(e))return null;var t=e.split("|");return t[n=this.getChoiceIndex(n,t.length)]?t[n].trim():e},Ul.prototype.tc=function(e,n){for(var t,a=[],i=arguments.length-2;i-- >0;)a[i]=arguments[i+2];return(t=this)._tc.apply(t,[e,this.locale,this._getMessages(),null,n].concat(a))},Ul.prototype._te=function(e,n,t){for(var a=[],i=arguments.length-3;i-- >0;)a[i]=arguments[i+3];var r=Qc.apply(void 0,a).locale||n;return this._exist(t[r],e)},Ul.prototype.te=function(e,n){return this._te(e,this.locale,this._getMessages(),n)},Ul.prototype.getLocaleMessage=function(e){return Jc(this._vm.messages[e]||{})},Ul.prototype.setLocaleMessage=function(e,n){"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||this._checkLocaleMessage(e,this._warnHtmlInMessage,n),this._vm.$set(this._vm.messages,e,n)},Ul.prototype.mergeLocaleMessage=function(e,n){"warn"!==this._warnHtmlInMessage&&"error"!==this._warnHtmlInMessage||this._checkLocaleMessage(e,this._warnHtmlInMessage,n),this._vm.$set(this._vm.messages,e,al(void 0!==this._vm.messages[e]&&Object.keys(this._vm.messages[e]).length?Object.assign({},this._vm.messages[e]):{},n))},Ul.prototype.getDateTimeFormat=function(e){return Jc(this._vm.dateTimeFormats[e]||{})},Ul.prototype.setDateTimeFormat=function(e,n){this._vm.$set(this._vm.dateTimeFormats,e,n),this._clearDateTimeFormat(e,n)},Ul.prototype.mergeDateTimeFormat=function(e,n){this._vm.$set(this._vm.dateTimeFormats,e,al(this._vm.dateTimeFormats[e]||{},n)),this._clearDateTimeFormat(e,n)},Ul.prototype._clearDateTimeFormat=function(e,n){for(var t in n){var a=e+"__"+t;this._dateTimeFormatters.hasOwnProperty(a)&&delete this._dateTimeFormatters[a]}},Ul.prototype._localizeDateTime=function(e,n,t,a,i,r){for(var o=n,s=a[o],c=this._getLocaleChain(n,t),l=0;l<c.length;l++){var d=c[l];if(o=d,!Yc(s=a[d])&&!Yc(s[i]))break}if(Yc(s)||Yc(s[i]))return null;var h,u=s[i];if(r)h=new Intl.DateTimeFormat(o,Object.assign({},u,r));else{var p=o+"__"+i;(h=this._dateTimeFormatters[p])||(h=this._dateTimeFormatters[p]=new Intl.DateTimeFormat(o,u))}return h.format(e)},Ul.prototype._d=function(e,n,t,a){if(!t)return(a?new Intl.DateTimeFormat(n,a):new Intl.DateTimeFormat(n)).format(e);var i=this._localizeDateTime(e,n,this.fallbackLocale,this._getDateTimeFormats(),t,a);if(this._isFallbackRoot(i)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.d(e,t,n)}return i||""},Ul.prototype.d=function(e){for(var n=[],t=arguments.length-1;t-- >0;)n[t]=arguments[t+1];var a=this.locale,i=null,r=null;return 1===n.length?(Wc(n[0])?i=n[0]:Hc(n[0])&&(n[0].locale&&(a=n[0].locale),n[0].key&&(i=n[0].key)),r=Object.keys(n[0]).reduce((function(e,t){var a;return el($c,t)?Object.assign({},e,((a={})[t]=n[0][t],a)):e}),null)):2===n.length&&(Wc(n[0])&&(i=n[0]),Wc(n[1])&&(a=n[1])),this._d(e,a,i,r)},Ul.prototype.getNumberFormat=function(e){return Jc(this._vm.numberFormats[e]||{})},Ul.prototype.setNumberFormat=function(e,n){this._vm.$set(this._vm.numberFormats,e,n),this._clearNumberFormat(e,n)},Ul.prototype.mergeNumberFormat=function(e,n){this._vm.$set(this._vm.numberFormats,e,al(this._vm.numberFormats[e]||{},n)),this._clearNumberFormat(e,n)},Ul.prototype._clearNumberFormat=function(e,n){for(var t in n){var a=e+"__"+t;this._numberFormatters.hasOwnProperty(a)&&delete this._numberFormatters[a]}},Ul.prototype._getNumberFormatter=function(e,n,t,a,i,r){for(var o=n,s=a[o],c=this._getLocaleChain(n,t),l=0;l<c.length;l++){var d=c[l];if(o=d,!Yc(s=a[d])&&!Yc(s[i]))break}if(Yc(s)||Yc(s[i]))return null;var h,u=s[i];if(r)h=new Intl.NumberFormat(o,Object.assign({},u,r));else{var p=o+"__"+i;(h=this._numberFormatters[p])||(h=this._numberFormatters[p]=new Intl.NumberFormat(o,u))}return h},Ul.prototype._n=function(e,n,t,a){if(!Ul.availabilities.numberFormat)return"";if(!t)return(a?new Intl.NumberFormat(n,a):new Intl.NumberFormat(n)).format(e);var i=this._getNumberFormatter(e,n,this.fallbackLocale,this._getNumberFormats(),t,a),r=i&&i.format(e);if(this._isFallbackRoot(r)){if(!this._root)throw Error("unexpected error");return this._root.$i18n.n(e,Object.assign({},{key:t,locale:n},a))}return r||""},Ul.prototype.n=function(e){for(var n=[],t=arguments.length-1;t-- >0;)n[t]=arguments[t+1];var a=this.locale,i=null,r=null;return 1===n.length?Wc(n[0])?i=n[0]:Hc(n[0])&&(n[0].locale&&(a=n[0].locale),n[0].key&&(i=n[0].key),r=Object.keys(n[0]).reduce((function(e,t){var a;return el(Nc,t)?Object.assign({},e,((a={})[t]=n[0][t],a)):e}),null)):2===n.length&&(Wc(n[0])&&(i=n[0]),Wc(n[1])&&(a=n[1])),this._n(e,a,i,r)},Ul.prototype._ntp=function(e,n,t,a){if(!Ul.availabilities.numberFormat)return[];if(!t)return(a?new Intl.NumberFormat(n,a):new Intl.NumberFormat(n)).formatToParts(e);var i=this._getNumberFormatter(e,n,this.fallbackLocale,this._getNumberFormats(),t,a),r=i&&i.formatToParts(e);if(this._isFallbackRoot(r)){if(!this._root)throw Error("unexpected error");return this._root.$i18n._ntp(e,n,t,a)}return r||[]},Object.defineProperties(Ul.prototype,Ol),Object.defineProperty(Ul,"availabilities",{get:function(){if(!Ml){var e="undefined"!=typeof Intl;Ml={dateTimeFormat:e&&void 0!==Intl.DateTimeFormat,numberFormat:e&&void 0!==Intl.NumberFormat}}return Ml}}),Ul.install=bl,Ul.version="8.28.2";var Rl=Ul;
/*!
 * vssue - A vue-powered issue-based comment plugin
 *
 * @version v1.4.8
 * @link https://vssue.js.org
 * @license MIT
 * @copyright 2018-2021 meteorlxy
 */
/*! *****************************************************************************
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use
this file except in compliance with the License. You may obtain a copy of the
License at http://www.apache.org/licenses/LICENSE-2.0

THIS CODE IS PROVIDED ON AN *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
MERCHANTABLITY OR NON-INFRINGEMENT.

See the Apache Version 2.0 License for specific language governing permissions
and limitations under the License.
***************************************************************************** */function El(e,n,t,a){var i,r=arguments.length,o=r<3?n:null===a?a=Object.getOwnPropertyDescriptor(n,t):a;if("object"==typeof Reflect&&"function"==typeof Reflect.decorate)o=Reflect.decorate(e,n,t,a);else for(var s=e.length-1;s>=0;s--)(i=e[s])&&(o=(r<3?i(o):r>3?i(n,t,o):i(n,t))||o);return r>3&&o&&Object.defineProperty(n,t,o),o}var Gl=Wt.extend({name:"Iconfont"});function ql(e,n,t,a,i,r,o,s,c,l){"boolean"!=typeof o&&(c=s,s=o,o=!1);const d="function"==typeof t?t.options:t;let h;if(e&&e.render&&(d.render=e.render,d.staticRenderFns=e.staticRenderFns,d._compiled=!0,i&&(d.functional=!0)),a&&(d._scopeId=a),r?(h=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),n&&n.call(this,c(e)),e&&e._registeredComponents&&e._registeredComponents.add(r)},d._ssrRegister=h):n&&(h=o?function(e){n.call(this,l(e,this.$root.$options.shadowRoot))}:function(e){n.call(this,s(e))}),h)if(d.functional){const e=d.render;d.render=function(n,t){return h.call(t),e(n,t)}}else{const e=d.beforeCreate;d.beforeCreate=e?[].concat(e,h):[h]}return t}"undefined"!=typeof navigator&&/msie [6-9]\\b/.test(navigator.userAgent.toLowerCase());const Bl=ql({render:function(e,n){var t=n._c;return t("svg",{directives:[{name:"show",rawName:"v-show",value:!1,expression:"false"}]},[t("symbol",{attrs:{id:"vssue-icon-bitbucket",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M579.5522464 489.45249493q4.8371808 38.38537173-30.81752427 61.55702827t-67.95459093 3.66689493q-23.79580907-10.37653333-32.6119616-35.34262826t-0.31207573-50.01020907 31.67573333-35.34262827q21.92335253-11.00068587 44.1587808-7.33379093t39.00952427 21.61127573 16.77409493 41.1160384zM647.19476053 476.65737173q-8.50407573-65.22392427-68.8908192-99.9424t-120.07131413-7.9579424q-38.38537173 17.08617173-61.24495253 53.9111616t-21.0651424 78.95527574q2.41859093 55.4715424 47.20152426 94.48106666t100.87862827 34.1723424q55.4715424-4.8371808 92.60860907-51.18049493t30.50544746-102.43900907zM792.93434133 146.32472427q-12.17097173-16.4620192-34.1723424-27.15062827t-35.34262826-13.41927573-43.30057174-7.64586667q-177.33729493-28.63299093-345.00022826 1.24830507-26.2144 4.29104747-40.25782827 7.33379093t-33.54819093 13.41927573-30.50544747 26.2144q18.2564576 17.08617173 46.34331413 27.6967616t44.78293334 13.41927574 53.36502826 7.02171413q138.95192427 17.71032427 273.06666667 0.62415253 38.38537173-4.8371808 54.53531413-7.33379093t44.1587808-13.1072 45.7191616-28.32091413zM827.65281813 777.10872427q-4.8371808 15.83786667-9.44030506 46.65539093t-8.50407574 51.18049493-17.39824746 42.6764192-35.34262827 34.4064q-52.4288 29.2571424-115.46819093 43.61264747t-123.1140576 13.41927573-122.8019808-11.3127616q-28.0088384-4.8371808-49.69813334-11.00068586t-46.65539093-16.4620192-44.4708576-26.52647574-31.67573333-37.4491424q-15.21371413-58.51428587-34.71847574-177.96144746l3.66689494-9.7523808 11.00068586-5.46133334q135.9091808 90.1900192 308.72137174 90.1900192t309.34552426-90.1900192q12.79512427 3.66689493 14.5895616 14.04342827t-3.0427424 27.46270507-4.8371808 22.54750506zM937.97175147 191.41973333q-15.83786667 101.8148576-67.64251414 399.22346667-3.0427424 18.2564576-16.4620192 34.1723424t-26.52647573 24.3419424-33.23611413 18.88060907q-153.61950507 76.7707424-371.8387808 53.67710506-151.12289493-16.4620192-240.14262827-84.72868586-9.12822827-7.33379093-15.52579093-16.1499424t-10.37653334-21.2992-5.46133333-20.75306667-3.66689493-24.10788587-3.3548192-21.2992q-5.46133333-30.50544747-16.1499424-91.43832426t-17.08617174-98.4600384-14.35550506-89.8779424-13.41927574-96.27550507q1.7944384-15.83786667 10.68860907-29.5692192t19.19268587-22.8595808 27.46270506-18.2564576 28.0088384-13.73135253 29.2571424-11.3127616q76.22460907-28.0088384 190.75657174-39.00952427 231.0144-22.54750507 412.01859093 30.50544747 94.48106667 28.0088384 131.072 74.35215253 9.7523808 12.17097173 10.0644576 31.0515808t-3.3548192 32.9240384z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitea",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M184.31868985 236.10860742C106.94832667 235.94086648 3.32655508 285.13080468 9.02973665 408.46209936c8.93218827 192.65010787 206.32096845 210.5144844 285.20099725 212.06608453 8.63864186 36.14810496 101.48307766 160.77938883 170.21479898 167.32127321h301.09442177c180.57278288-11.99345499 315.77172611-546.07960359 215.54670217-548.09249109-165.7696721 7.79993906-264.02374305 11.74184405-348.27147151 12.41280591v166.69224585l-26.25140843-11.61603761-0.16773997-154.99233728c-96.70246985-0.04193548-181.83083757-4.52899687-343.4069947-12.49667687-20.21274496-0.12580547-48.39316992-3.5644886-78.67035236-3.64835859z m10.94507577 68.14462849h9.22573371c10.98701124 98.75729283 28.85138778 156.50200291 64.99949274 244.73357185-92.25734394-10.90314029-170.75995634-37.69970509-185.18564974-137.75698809-7.46445813-51.78991757 17.69663558-105.84433456 110.96042329-107.01851827z m358.83913087 97.07988723c6.29027343 0.08386999 12.70635233 1.25805468 18.74501482 4.02577499l31.40943263 13.54505513-22.51917887 41.05451824a28.18042496 25.03528825 0 0 0-10.10637297 1.59353561 28.18042496 25.03528825 0 0 0-16.98373825 32.038459 28.18042496 25.03528825 0 0 0 4.69673781 7.29671718l-38.83195528 70.70267333a28.18042496 25.03528825 0 0 0-9.30960467 1.59353659 28.18042496 25.03528825 0 0 0-16.98373825 32.038459 28.18042496 25.03528825 0 0 0 36.06423497 15.09665623 28.18042496 25.03528825 0 0 0 16.94180276-32.08039449 28.18042496 25.03528825 0 0 0-6.62575434-9.22573468l37.82551056-68.85752581a28.18042496 25.03528825 0 0 0 12.28700044-1.25805469 28.18042496 25.03528825 0 0 0 8.93218826-4.69673783c14.59343435 6.12253248 26.54495386 11.11281671 35.14166122 15.34826717 12.91602778 6.37414341 17.48696012 10.60959485 18.87082027 15.30633169 1.38386015 4.61286685-0.12580547 13.50312062-7.42252263 29.10299872-5.45157063 11.61603859-14.46762889 28.09655497-25.11915823 47.51253164a28.18042496 25.03528825 0 0 0-10.52572486 1.59353659 28.18042496 25.03528825 0 0 0-16.98373826 32.038459 28.18042496 25.03528825 0 0 0 36.06423498 15.09665623 28.18042496 25.03528825 0 0 0 16.94180278-32.03845901 28.18042496 25.03528825 0 0 0-5.74511608-8.47090188c10.52572388-19.20630122 19.58371762-35.72875308 25.41270465-48.14155897 7.88380904-16.85793279 11.99345499-29.39654416 8.38703091-41.51580463-3.60642311-12.11926046-14.67730434-20.0030695-29.35460966-27.25785217-9.6450856-4.73867233-21.68047607-9.77089106-36.06423399-15.80955357a28.18042496 25.03528825 0 0 0-1.59353562-10.022502 28.18042496 25.03528825 0 0 0-6.08059796-8.7644483l22.14176246-40.38355541 122.61839638 52.96410227c22.14176247 9.6031511 31.2836262 33.12877372 20.54822685 52.8382968l-84.28966393 154.32137544c-10.77733482 19.66758857-37.23841869 27.80300855-59.38018118 18.24179293l-173.48574115-74.98005927c-22.14176247-9.5612156-31.32556167-33.12877372-20.54822687-52.83829679l84.28966395-154.27943995c7.38058716-13.54505513 22.22563246-21.59660511 37.951317-22.22563246h2.68384935z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitee",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M978.404275 409.561604H455.061338c-25.117645 0-45.499734 20.382089-45.499734 45.499734l-0.031997 113.781333c0 25.117645 20.350092 45.499734 45.499734 45.531731h318.594132c25.117645 0 45.499734 20.382089 45.499734 45.499735v22.749867a136.5312 136.5312 0 0 1-136.5312 136.5312H250.248539a45.499734 45.499734 0 0 1-45.499734-45.499734V341.343999a136.5312 136.5312 0 0 1 136.5312-136.5312L978.308284 204.780802c25.117645 0 45.499734-20.350092 45.499734-45.467738L1023.904009 45.531731h0.031997A45.499734 45.499734 0 0 0 978.468269 0h-0.031997L341.343999 0.031997C152.84967 0.031997 0.031997 152.84967 0.031997 341.343999v637.092273c0 25.117645 20.382089 45.499734 45.499734 45.499734h671.233072a307.171203 307.171203 0 0 0 307.171203-307.171203v-261.671468c0-25.117645-20.382089-45.499734-45.499734-45.499734z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-github",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M512 20.4425c-278.334 0-504 225.6345-504 504 0 222.6735 144.4275 411.6105 344.673 478.233 25.2 4.662 34.461-10.9305 34.461-24.255 0-12.0015-0.4725-51.723-0.693-93.8385-140.238 30.492-169.8165-59.472-169.8165-59.472-22.932-58.2435-55.944-73.7415-55.944-73.7415-45.738-31.2795 3.465-30.6495 3.465-30.6495 50.589 3.5595 77.238 51.9435 77.238 51.9435 44.9505 77.049 117.9045 54.7785 146.664 41.895 4.5045-32.571 17.577-54.81 32.004-67.41-111.951-12.726-229.635-55.9755-229.635-249.0705 0-55.0305 19.6875-99.981 51.9435-135.2925-5.229-12.6945-22.491-63.945 4.8825-133.371 0 0 42.336-13.545 138.6315 51.66 40.194-11.1825 83.3175-16.758 126.1575-16.9785 42.8085 0.189 85.9635 5.796 126.252 16.9785 96.201-65.205 138.4425-51.66 138.4425-51.66 27.4365 69.426 10.1745 120.6765 4.9455 133.371 32.319 35.28 51.8805 80.262 51.8805 135.2925 0 193.5675-117.9045 236.187-230.139 248.6925 18.081 15.6555 34.1775 46.305 34.1775 93.3345 0 67.4415-0.5985 121.716-0.5985 138.3165 0 13.419 9.072 29.1375 34.6185 24.192 200.151-66.717 344.3895-255.5595 344.3895-478.17 0-278.3655-225.666-504-504-504z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-gitlab",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M66.61375986 405.11600042L512.11376028 976.03999972 23.84576 621.65599958a39.312 39.312 0 0 1-14.07600042-43.30799944l56.8080007-173.26800028z m259.88400014 0h371.26800014L512.14975986 976.03999972zM215.11376 60.88400042l111.384 344.232H66.61375986l111.384-344.232a19.72800014 19.72800014 0 0 1 37.11600014 0z m742.49999972 344.232l56.8080007 173.2679993a39.23999986 39.23999986 0 0 1-14.07600042 43.30800042l-488.26800028 354.38400014 445.50000042-570.92400028z m0 0h-259.88400014l111.384-344.232a19.72800014 19.72800014 0 0 1 37.11600014 0z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-loading",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M843.307 742.24c0 3.217 2.607 5.824 5.824 5.824s5.824-2.607 5.824-5.824a5.823 5.823 0 0 0-5.824-5.824 5.823 5.823 0 0 0-5.824 5.824zM714.731 874.912c0 6.398 5.186 11.584 11.584 11.584s11.584-5.186 11.584-11.584-5.186-11.584-11.584-11.584-11.584 5.186-11.584 11.584zM541.419 943.2c0 9.614 7.794 17.408 17.408 17.408s17.408-7.794 17.408-17.408-7.794-17.408-17.408-17.408-17.408 7.794-17.408 17.408z m-186.56-9.152c0 12.795 10.373 23.168 23.168 23.168s23.168-10.373 23.168-23.168-10.373-23.168-23.168-23.168-23.168 10.373-23.168 23.168zM189.355 849.12c0 16.012 12.98 28.992 28.992 28.992s28.992-12.98 28.992-28.992-12.98-28.992-28.992-28.992-28.992 12.98-28.992 28.992zM74.731 704.736c0 19.228 15.588 34.816 34.816 34.816s34.816-15.588 34.816-34.816-15.588-34.816-34.816-34.816-34.816 15.588-34.816 34.816z m-43.008-177.28c0 22.41 18.166 40.576 40.576 40.576s40.576-18.166 40.576-40.576-18.166-40.576-40.576-40.576-40.576 18.166-40.576 40.576z m35.392-176.128c0 25.626 20.774 46.4 46.4 46.4s46.4-20.774 46.4-46.4c0-25.626-20.774-46.4-46.4-46.4-25.626 0-46.4 20.774-46.4 46.4z m106.176-142.016c0 28.843 23.381 52.224 52.224 52.224s52.224-23.381 52.224-52.224c0-28.843-23.381-52.224-52.224-52.224-28.843 0-52.224 23.381-52.224 52.224z m155.904-81.344c0 32.024 25.96 57.984 57.984 57.984s57.984-25.96 57.984-57.984-25.96-57.984-57.984-57.984-57.984 25.96-57.984 57.984z m175.104-5.056c0 35.24 28.568 63.808 63.808 63.808s63.808-28.568 63.808-63.808c0-35.24-28.568-63.808-63.808-63.808-35.24 0-63.808 28.568-63.808 63.808z m160.32 72.128c0 38.421 31.147 69.568 69.568 69.568s69.568-31.147 69.568-69.568-31.147-69.568-69.568-69.568-69.568 31.147-69.568 69.568z m113.92 135.488c0 41.638 33.754 75.392 75.392 75.392s75.392-33.754 75.392-75.392-33.754-75.392-75.392-75.392-75.392 33.754-75.392 75.392z m45.312 175.488c0 44.854 36.362 81.216 81.216 81.216s81.216-36.362 81.216-81.216c0-44.854-36.362-81.216-81.216-81.216-44.854 0-81.216 36.362-81.216 81.216z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-like",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M885.9 533.7c16.8-22.2 26.1-49.4 26.1-77.7 0-44.9-25.1-87.4-65.5-111.1a67.67 67.67 0 0 0-34.3-9.3H572.4l6-122.9c1.4-29.7-9.1-57.9-29.5-79.4-20.5-21.5-48.1-33.4-77.9-33.4-52 0-98 35-111.8 85.1l-85.9 311H144c-17.7 0-32 14.3-32 32v364c0 17.7 14.3 32 32 32h601.3c9.2 0 18.2-1.8 26.5-5.4 47.6-20.3 78.3-66.8 78.3-118.4 0-12.6-1.8-25-5.4-37 16.8-22.2 26.1-49.4 26.1-77.7 0-12.6-1.8-25-5.4-37 16.8-22.2 26.1-49.4 26.1-77.7-0.2-12.6-2-25.1-5.6-37.1zM184 852V568h81v284h-81z m636.4-353l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 16.5-7.2 32.2-19.6 43l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 16.5-7.2 32.2-19.6 43l-21.9 19 13.9 25.4c4.6 8.4 6.9 17.6 6.9 27.3 0 22.4-13.2 42.6-33.6 51.8H329V564.8l99.5-360.5c5.2-18.9 22.5-32.2 42.2-32.3 7.6 0 15.1 2.2 21.1 6.7 9.9 7.4 15.2 18.6 14.6 30.5l-9.6 198.4h314.4C829 418.5 840 436.9 840 456c0 16.5-7.2 32.1-19.6 43z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-unlike",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M885.9 490.3c3.6-12 5.4-24.4 5.4-37 0-28.3-9.3-55.5-26.1-77.7 3.6-12 5.4-24.4 5.4-37 0-28.3-9.3-55.5-26.1-77.7 3.6-12 5.4-24.4 5.4-37 0-51.6-30.7-98.1-78.3-118.4-8.3-3.6-17.2-5.4-26.5-5.4H144c-17.7 0-32 14.3-32 32v364c0 17.7 14.3 32 32 32h129.3l85.8 310.8C372.9 889 418.9 924 470.9 924c29.7 0 57.4-11.8 77.9-33.4 20.5-21.5 31-49.7 29.5-79.4l-6-122.9h239.9c12.1 0 23.9-3.2 34.3-9.3 40.4-23.5 65.5-66.1 65.5-111 0-28.3-9.3-55.5-26.1-77.7zM184 456V172h81v284h-81z m627.2 160.4H496.8l9.6 198.4c0.6 11.9-4.7 23.1-14.6 30.5-6.1 4.5-13.6 6.8-21.1 6.7-19.6-0.1-36.9-13.4-42.2-32.3L329 459.2V172h415.4c20.4 9.2 33.6 29.4 33.6 51.8 0 9.7-2.3 18.9-6.9 27.3l-13.9 25.4 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 9.7-2.3 18.9-6.9 27.3l-13.9 25.4 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 9.7-2.3 18.9-6.9 27.3l-14 25.5 21.9 19c12.5 10.8 19.6 26.5 19.6 43 0 19.1-11 37.5-28.8 48.4z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-heart",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M923 283.6c-13.4-31.1-32.6-58.9-56.9-82.8-24.3-23.8-52.5-42.4-84-55.5-32.5-13.5-66.9-20.3-102.4-20.3-49.3 0-97.4 13.5-139.2 39-10 6.1-19.5 12.8-28.5 20.1-9-7.3-18.5-14-28.5-20.1-41.8-25.5-89.9-39-139.2-39-35.5 0-69.9 6.8-102.4 20.3-31.4 13-59.7 31.7-84 55.5-24.4 23.9-43.5 51.7-56.9 82.8-13.9 32.3-21 66.6-21 101.9 0 33.3 6.8 68 20.3 103.3 11.3 29.5 27.5 60.1 48.2 91 32.8 48.9 77.9 99.9 133.9 151.6 92.8 85.7 184.7 144.9 188.6 147.3l23.7 15.2c10.5 6.7 24 6.7 34.5 0l23.7-15.2c3.9-2.5 95.7-61.6 188.6-147.3 56-51.7 101.1-102.7 133.9-151.6 20.7-30.9 37-61.5 48.2-91 13.5-35.3 20.3-70 20.3-103.3 0.1-35.3-7-69.6-20.9-101.9zM512 814.8S156 586.7 156 385.5C156 283.6 240.3 201 344.3 201c73.1 0 136.5 40.8 167.7 100.4C543.2 241.8 606.6 201 679.7 201c104 0 188.3 82.6 188.3 184.5 0 201.2-356 429.3-356 429.3z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-edit",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M723.2 917.76H286.72c-65.28 0-118.4-51.2-118.4-113.92V261.76C168.32 198.4 221.44 147.2 286.72 147.2h375.04c17.92 0 32 14.08 32 32s-14.08 32-32 32H286.72c-30.08 0-54.4 22.4-54.4 49.92v542.08c0 27.52 24.32 49.92 54.4 49.92H723.2c30.08 0 54.4-22.4 54.4-49.92V440.32c0-17.92 14.08-32 32-32s32 14.08 32 32v363.52c0 62.72-53.12 113.92-118.4 113.92z"}}),n._v(" "),t("path",{attrs:{d:"M499.84 602.24c-7.68 0-14.72-2.56-21.12-7.68-13.44-11.52-14.72-32-3.2-45.44L780.16 198.4c11.52-13.44 32-14.72 45.44-3.2s14.72 32 3.2 45.44L524.16 591.36c-6.4 7.04-15.36 10.88-24.32 10.88z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-delete",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M677.647059 256l0-90.352941c0-37.436235-23.461647-60.235294-61.771294-60.235294L408.094118 105.411765c-38.249412 0-61.741176 22.799059-61.741176 60.235294l0 90.352941-180.705882 0 0 60.235294 60.235294 0 0 512c0 54.272 33.972706 90.352941 90.352941 90.352941l391.529412 0c55.085176 0 90.352941-33.490824 90.352941-90.352941l0-512 60.235294 0 0-60.235294L677.647059 256zM406.588235 165.647059l210.823529 0-1.264941 90.352941L406.588235 256 406.588235 165.647059zM737.882353 858.352941l-451.764706 0 0-542.117647 451.764706 0L737.882353 858.352941zM466.823529 376.470588l-58.729412 0-1.505882 391.529412 60.235294 0L466.823529 376.470588zM617.411765 376.470588l-60.235294 0 0 391.529412 60.235294 0L617.411765 376.470588z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-reply",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M426.666667 384 426.666667 213.333333 128 512 426.666667 810.666667 426.666667 635.733333C640 635.733333 789.333333 704 896 853.333333 853.333333 640 725.333333 426.666667 426.666667 384Z"}})]),n._v(" "),t("symbol",{attrs:{id:"vssue-icon-error",viewBox:"0 0 1024 1024"}},[t("path",{attrs:{d:"M512 720m-48 0a48 48 0 1 0 96 0 48 48 0 1 0-96 0Z"}}),n._v(" "),t("path",{attrs:{d:"M480 416v184c0 4.4 3.6 8 8 8h48c4.4 0 8-3.6 8-8V416c0-4.4-3.6-8-8-8h-48c-4.4 0-8 3.6-8 8z"}}),n._v(" "),t("path",{attrs:{d:"M955.7 856l-416-720c-6.2-10.7-16.9-16-27.7-16s-21.6 5.3-27.7 16l-416 720C56 877.4 71.4 904 96 904h832c24.6 0 40-26.6 27.7-48z m-783.5-27.9L512 239.9l339.8 588.2H172.2z"}})])])},staticRenderFns:[]},void 0,Gl,void 0,!0,void 0,!1,void 0,void 0,void 0);const jl=ql({},void 0,Wt.extend({name:"TransitionFade",functional:!0,props:{group:{type:Boolean,required:!1,default:!1},tag:{type:String,required:!1,default:"div"}},render:(e,{props:n,children:t})=>e(n.group?"TransitionGroup":"Transition",{props:{name:"fade",mode:"out-in",appear:!0,tag:n.tag}},t)}),void 0,void 0,void 0,!1,void 0,void 0,void 0);const Nl=ql({},void 0,Wt.extend({name:"VssueIcon",functional:!0,props:{name:{type:String,required:!0},title:{type:String,required:!1,default:null}},render:(e,{props:n,data:t})=>e("svg",Object.assign(Object.assign({},t),{class:["vssue-icon","vssue-icon-"+n.name],attrs:{"aria-hidden":"true"}}),[e("title",n.title),e("use",{attrs:{"xlink:href":"#vssue-icon-"+n.name}})])}),void 0,void 0,void 0,!1,void 0,void 0,void 0);let $l=class extends Wt{constructor(){super(...arguments),this.editMode=!1,this.editContent=this.comment.contentRaw,this.creatingReactions=[],this.isPutingComment=!1,this.isDeletingComment=!1}get currentUser(){return this.vssue.user?this.vssue.user.username:null}get content(){return this.comment.content}get author(){return this.comment.author}get createdAt(){return Bc(this.comment.createdAt)}get updatedAt(){return Bc(this.comment.updatedAt)}get showReactions(){return Boolean(this.vssue.API&&this.vssue.API.platform.meta.reactable&&this.comment.reactions&&!this.editMode)}get reactionKeys(){return["heart","like","unlike"]}get editContentRows(){return this.editContent.split("\n").length-1}get editInputRows(){return this.editContentRows<3?5:this.editContentRows+2}async postReaction({reaction:e}){try{if(this.creatingReactions.includes(e))return;this.creatingReactions.push(e);await this.vssue.postCommentReaction({commentId:this.comment.id,reaction:e})||this.vssue.$emit("error",new Error(this.vssue.$t("reactionGiven",{reaction:this.vssue.$t(e)})));const n=await this.vssue.getCommentReactions({commentId:this.comment.id});n&&(this.comment.reactions=n)}finally{this.creatingReactions.splice(this.creatingReactions.findIndex(n=>n===e),1)}}enterEdit(){this.editMode=!0,this.$nextTick(()=>{this.$refs.input.focus()})}resetEdit(){this.editMode=!1,this.editContent=this.comment.contentRaw}async putComment(){try{if(this.vssue.isPending)return;if(this.editContent!==this.comment.contentRaw){this.isPutingComment=!0,this.vssue.isUpdatingComment=!0;const e=await this.vssue.putComment({commentId:this.comment.id,content:this.editContent});e&&this.vssue.comments.data.splice(this.vssue.comments.data.findIndex(e=>e.id===this.comment.id),1,e)}this.editMode=!1}finally{this.isPutingComment=!1,this.vssue.isUpdatingComment=!1}}async deleteComment(){try{if(this.vssue.isPending)return;if(!window.confirm(this.vssue.$t("deleteConfirm")))return;this.isDeletingComment=!0,this.vssue.isUpdatingComment=!0;await this.vssue.deleteComment({commentId:this.comment.id})?(this.vssue.comments.count-=1,this.vssue.comments.data.length>1&&this.vssue.comments.data.splice(this.vssue.comments.data.findIndex(e=>e.id===this.comment.id),1),this.vssue.query.page>1&&this.vssue.query.page>Math.ceil(this.vssue.comments.count/this.vssue.query.perPage)?this.vssue.query.page-=1:await this.vssue.getComments()):this.vssue.$emit("error",new Error(this.vssue.$t("deleteFailed")))}finally{this.isDeletingComment=!1,this.vssue.isUpdatingComment=!1}}};El([zc({type:Object,required:!0})],$l.prototype,"comment",void 0),El([Ac()],$l.prototype,"vssue",void 0),$l=El([Mc({components:{VssueIcon:Nl}})],$l);const Fl=ql({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-comment",class:{"vssue-comment-edit-mode":e.editMode,"vssue-comment-disabled":e.isDeletingComment||e.isPutingComment}},[t("div",{staticClass:"vssue-comment-avatar"},[t("a",{attrs:{href:e.author.homepage,title:e.author.username,target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:e.author.avatar,alt:e.author.username}})])]),e._v(" "),t("div",{staticClass:"vssue-comment-body"},[e._t("body",[t("div",{staticClass:"vssue-comment-header"},[t("span",{staticClass:"vssue-comment-author"},[t("a",{attrs:{href:e.author.homepage,title:e.author.username,target:"_blank",rel:"noopener noreferrer"}},[e._v("\n            "+e._s(e.author.username)+"\n          ")])]),e._v(" "),t("span",{staticClass:"vssue-comment-created-at"},[e._v("\n          "+e._s(e.createdAt)+"\n        ")])]),e._v(" "),t("div",{staticClass:"vssue-comment-main"},[e.editMode?t("textarea",{directives:[{name:"model",rawName:"v-model",value:e.editContent,expression:"editContent"}],ref:"input",staticClass:"vssue-edit-comment-input",attrs:{rows:e.editInputRows},domProps:{value:e.editContent},on:{keyup:function(n){return!n.type.indexOf("key")&&e._k(n.keyCode,"enter",13,n.key,"Enter")?null:n.ctrlKey?e.putComment():null},input:function(n){n.target.composing||(e.editContent=n.target.value)}}}):t("article",{staticClass:"markdown-body",domProps:{innerHTML:e._s(e.content)}})]),e._v(" "),t("div",{staticClass:"vssue-comment-footer"},[e.editMode?t("span",{staticClass:"vssue-comment-hint"},[e._v("\n          "+e._s(e.vssue.$t("editMode"))+"\n        ")]):e._e(),e._v(" "),e.showReactions?t("span",{staticClass:"vssue-comment-reactions"},e._l(e.reactionKeys,(function(n){return t("span",{key:n,staticClass:"vssue-comment-reaction",attrs:{title:e.vssue.$t(e.creatingReactions.includes(n)?"loading":n)},on:{click:function(t){return e.postReaction({reaction:n})}}},[t("VssueIcon",{attrs:{name:e.creatingReactions.includes(n)?"loading":n,title:e.vssue.$t(e.creatingReactions.includes(n)?"loading":n)}}),e._v(" "),t("span",{staticClass:"vssue-comment-reaction-number"},[e._v("\n              "+e._s(e.comment.reactions[n])+"\n            ")])],1)})),0):e._e(),e._v(" "),t("span",{staticClass:"vssue-comment-operations"},[e.comment.author.username===e.currentUser&&e.editMode?t("span",{staticClass:"vssue-comment-operation",class:{"vssue-comment-operation-muted":e.isPutingComment},attrs:{title:e.vssue.$t(e.isPutingComment?"loading":"submit")},on:{click:function(n){return e.putComment()}}},[t("VssueIcon",{directives:[{name:"show",rawName:"v-show",value:e.isPutingComment,expression:"isPutingComment"}],attrs:{name:"loading",title:e.vssue.$t("loading")}}),e._v("\n\n            "+e._s(e.vssue.$t("submit"))+"\n          ")],1):e._e(),e._v(" "),e.comment.author.username===e.currentUser&&e.editMode?t("span",{staticClass:"vssue-comment-operation vssue-comment-operation-muted",attrs:{title:e.vssue.$t("cancel")},on:{click:function(n){return e.resetEdit()}}},[e._v("\n            "+e._s(e.vssue.$t("cancel"))+"\n          ")]):e._e(),e._v(" "),e.comment.author.username===e.currentUser?t("span",{directives:[{name:"show",rawName:"v-show",value:!e.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(n){return e.enterEdit()}}},[t("VssueIcon",{attrs:{name:"edit",title:e.vssue.$t("edit")}})],1):e._e(),e._v(" "),e.comment.author.username===e.currentUser||e.vssue.isAdmin?t("span",{directives:[{name:"show",rawName:"v-show",value:!e.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(n){return e.deleteComment()}}},[t("VssueIcon",{attrs:{name:e.isDeletingComment?"loading":"delete",title:e.vssue.$t(e.isDeletingComment?"loading":"delete")}})],1):e._e(),e._v(" "),t("span",{directives:[{name:"show",rawName:"v-show",value:!e.editMode,expression:"!editMode"}],staticClass:"vssue-comment-operation",on:{click:function(n){return e.vssue.$emit("reply-comment",e.comment)}}},[t("VssueIcon",{attrs:{name:"reply",title:e.vssue.$t("reply")}})],1)])])])],2)])},staticRenderFns:[]},void 0,$l,void 0,!1,void 0,!1,void 0,void 0,void 0);let Vl=class extends Wt{get disabled(){return this.vssue.isPending}get pageCount(){const e=Math.ceil(this.vssue.comments.count/this.vssue.comments.perPage);return e>1?e:1}get perPageOptions(){const e=[5,10,20,50];return!e.includes(this.vssue.options.perPage)&&this.vssue.options.perPage<100&&e.push(this.vssue.options.perPage),e.sort((e,n)=>e-n)}get page(){return this.vssue.query.page>this.pageCount?this.pageCount:this.vssue.query.page}set page(e){e>0&&e<=this.pageCount&&(this.vssue.query.page=e)}get perPage(){return this.vssue.query.perPage}set perPage(e){this.perPageOptions.includes(e)&&(this.vssue.query.perPage=e)}};El([Ac()],Vl.prototype,"vssue",void 0),Vl=El([Mc({components:{VssueIcon:Nl}})],Vl);const Hl=ql({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-pagination"},[t("div",{staticClass:"vssue-pagination-per-page"},[t("label",[t("select",{directives:[{name:"model",rawName:"v-model",value:e.perPage,expression:"perPage"}],staticClass:"vssue-pagination-select",attrs:{disabled:e.disabled},on:{change:function(n){var t=Array.prototype.filter.call(n.target.options,(function(e){return e.selected})).map((function(e){return"_value"in e?e._value:e.value}));e.perPage=n.target.multiple?t:t[0]}}},e._l(e.perPageOptions,(function(n){return t("option",{key:n,domProps:{value:n}},[e._v("\n          "+e._s(n)+"\n        ")])})),0),e._v(" "),t("span",[e._v("\n        "+e._s(e.vssue.$t("perPage"))+"\n      ")])]),e._v(" "),e.vssue.API.platform.meta.sortable?t("span",{class:{"vssue-pagination-link":!0,disabled:e.disabled},attrs:{title:e.vssue.$t("sort")},on:{click:function(n){e.vssue.query.sort="asc"===e.vssue.query.sort?"desc":"asc"}}},[e._v("\n      "+e._s("asc"===e.vssue.query.sort?"↑":"↓")+"\n    ")]):e._e()]),e._v(" "),t("div",{staticClass:"vssue-pagination-page"},[t("span",{class:{"vssue-pagination-link":!0,disabled:1===e.page||e.disabled},attrs:{title:e.vssue.$t("prev")},domProps:{textContent:e._s("<")},on:{click:function(n){e.page-=1}}}),e._v(" "),t("label",[t("span",[e._v("\n        "+e._s(e.vssue.$t("page"))+"\n      ")]),e._v(" "),t("select",{directives:[{name:"show",rawName:"v-show",value:e.pageCount>1,expression:"pageCount > 1"},{name:"model",rawName:"v-model",value:e.page,expression:"page"}],staticClass:"vssue-pagination-select",attrs:{disabled:e.disabled},on:{change:function(n){var t=Array.prototype.filter.call(n.target.options,(function(e){return e.selected})).map((function(e){return"_value"in e?e._value:e.value}));e.page=n.target.multiple?t:t[0]}}},e._l(e.pageCount,(function(n){return t("option",{key:n,domProps:{value:n}},[e._v("\n          "+e._s(n)+"\n        ")])})),0),e._v(" "),t("span",{directives:[{name:"show",rawName:"v-show",value:e.pageCount<2,expression:"pageCount < 2"}],domProps:{textContent:e._s(e.page)}}),e._v(" "),t("span",{domProps:{textContent:e._s(" / "+e.pageCount+" ")}})]),e._v(" "),t("span",{class:{"vssue-pagination-link":!0,disabled:e.page===e.pageCount||e.disabled},attrs:{title:e.vssue.$t("next")},domProps:{textContent:e._s(">")},on:{click:function(n){e.page+=1}}})])])},staticRenderFns:[]},void 0,Vl,void 0,!1,void 0,!1,void 0,void 0,void 0);let Wl=class extends Wt{};El([Ac()],Wl.prototype,"vssue",void 0),Wl=El([Mc({components:{TransitionFade:jl,VssueComment:Fl,VssuePagination:Hl}})],Wl);const Kl=ql({render:function(){var e=this.$createElement,n=this._self._c||e;return n("div",{staticClass:"vssue-comments"},[n("VssuePagination"),this._v(" "),n("TransitionFade",{attrs:{group:""}},this._l(this.vssue.comments.data,(function(e){return n("VssueComment",{key:e.id,attrs:{comment:e}})})),1),this._v(" "),n("VssuePagination",{directives:[{name:"show",rawName:"v-show",value:this.vssue.comments.data.length>5,expression:"vssue.comments.data.length > 5"}]})],1)},staticRenderFns:[]},void 0,Wl,void 0,!1,void 0,!1,void 0,void 0,void 0);const Zl=ql({},void 0,Wt.extend({name:"VssueIcon",functional:!0,props:{type:{type:String,required:!1,default:"default"}},render:(e,{props:n,data:t,children:a})=>e("button",Object.assign(Object.assign({},t),{class:["vssue-button","vssue-button-"+n.type]}),a)}),void 0,void 0,void 0,!1,void 0,void 0,void 0);let Yl=class extends Wt{constructor(){super(...arguments),this.content=""}get user(){return this.vssue.user}get platform(){return this.vssue.API&&this.vssue.API.platform.name}get isInputDisabled(){return this.loading||null===this.user||null===this.vssue.issue}get isSubmitDisabled(){return""===this.content||this.vssue.isPending||null===this.vssue.issue}get loading(){return this.vssue.isCreatingComment}get contentRows(){return this.content.split("\n").length-1}get inputRows(){return this.contentRows<3?5:this.contentRows+2}created(){this.vssue.$on("reply-comment",e=>{const n=e.contentRaw.replace(/\n/g,"\n> "),t=`@${e.author.username}\n\n> ${n}\n\n`;this.content=this.content.concat(t),this.focus()})}beforeDestroy(){this.vssue.$off("reply-comment")}focus(){this.$refs.input.focus()}async submit(){this.isSubmitDisabled||(await this.vssue.postComment({content:this.content}),this.content="",await this.vssue.getComments())}};El([Ac()],Yl.prototype,"vssue",void 0),Yl=El([Mc({components:{VssueButton:Zl,VssueIcon:Nl}})],Yl);const Xl=ql({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-new-comment"},[t("div",{staticClass:"vssue-comment-avatar"},[e.user?t("a",{attrs:{href:e.user.homepage,title:e.user.username,target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:e.user.avatar,alt:e.user.username}})]):t("VssueIcon",{attrs:{name:e.platform.toLowerCase(),title:e.vssue.$t("loginToComment",{platform:e.platform})},on:{click:function(n){return e.vssue.login()}}})],1),e._v(" "),t("div",{staticClass:"vssue-new-comment-body"},[t("textarea",{directives:[{name:"model",rawName:"v-model",value:e.content,expression:"content"}],ref:"input",staticClass:"vssue-new-comment-input",attrs:{rows:e.inputRows,disabled:e.isInputDisabled,placeholder:e.vssue.$t(e.user?"placeholder":"noLoginPlaceHolder"),spellcheck:!1,"aria-label":"leave a comment"},domProps:{value:e.content},on:{keyup:function(n){return!n.type.indexOf("key")&&e._k(n.keyCode,"enter",13,n.key,"Enter")?null:n.ctrlKey?e.submit():null},input:function(n){n.target.composing||(e.content=n.target.value)}}})]),e._v(" "),t("div",{staticClass:"vssue-new-comment-footer"},[e.user?t("span",{staticClass:"vssue-current-user"},[t("span",[e._v(e._s(e.vssue.$t("currentUser"))+" - "+e._s(e.user.username)+" - ")]),e._v(" "),t("a",{staticClass:"vssue-logout",on:{click:function(n){return e.vssue.logout()}}},[e._v("\n        "+e._s(e.vssue.$t("logout"))+"\n      ")])]):t("span",{staticClass:"vssue-current-user"},[e._v("\n      "+e._s(e.vssue.$t("loginToComment",{platform:e.platform}))+"\n    ")]),e._v(" "),t("div",{staticClass:"vssue-new-comment-operations"},[e.user?t("VssueButton",{staticClass:"vssue-button-submit-comment",attrs:{type:"primary",disabled:e.isSubmitDisabled},on:{click:function(n){return e.submit()}}},[t("VssueIcon",{directives:[{name:"show",rawName:"v-show",value:e.loading,expression:"loading"}],attrs:{name:"loading"}}),e._v("\n\n        "+e._s(e.vssue.$t(e.loading?"submitting":"submitComment"))+"\n      ")],1):t("VssueButton",{staticClass:"vssue-button-login",attrs:{type:"primary",title:e.vssue.$t("loginToComment",{platform:e.platform})},on:{click:function(n){return e.vssue.login()}}},[e._v("\n        "+e._s(e.vssue.$t("login",{platform:e.platform}))+"\n      ")])],1)])])},staticRenderFns:[]},void 0,Yl,void 0,!1,void 0,!1,void 0,void 0,void 0);let Ql=class extends Wt{constructor(){super(...arguments),this.progress={show:!1,percent:0,timer:null,speed:200},this.alert={show:!1,message:null,timer:null}}onLoadingCommentsChange(e){this.vssue.comments&&(e?this.progressStart():this.progressDone())}created(){this.vssue.$on("error",e=>this.alertShow(e.message))}beforeDestroy(){this.vssue.$off("error"),null!==this.progress.timer&&window.clearTimeout(this.progress.timer),null!==this.alert.timer&&window.clearTimeout(this.alert.timer)}progressStart(){this.progress.show=!0,this.progress.percent=0,this.progress.timer=window.setInterval(()=>{this.progress.percent+=5,this.progress.percent>94&&null!==this.progress.timer&&window.clearInterval(this.progress.timer)},this.progress.speed)}progressDone(){this.progress.percent=100,null!==this.progress.timer&&window.clearTimeout(this.progress.timer),this.progress.timer=null,window.setTimeout(()=>{this.progress.show=!1},this.progress.speed)}alertShow(e){this.alert.show=!0,this.alert.message=e,null!==this.alert.timer&&window.clearTimeout(this.alert.timer),this.alert.timer=window.setTimeout(()=>{this.alertHide()},3e3)}alertHide(){this.alert.show=!1,null!==this.alert.timer&&window.clearTimeout(this.alert.timer),this.alert.timer=null}};El([Ac()],Ql.prototype,"vssue",void 0),El([Uc("vssue.isLoadingComments")],Ql.prototype,"onLoadingCommentsChange",null),Ql=El([Mc({components:{TransitionFade:jl}})],Ql);const Jl=ql({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-notice"},[t("div",{directives:[{name:"show",rawName:"v-show",value:e.progress.show,expression:"progress.show"}],staticClass:"vssue-progress",style:{width:e.progress.percent+"%",transition:"all "+e.progress.speed+"ms linear"}}),e._v(" "),t("TransitionFade",[t("div",{directives:[{name:"show",rawName:"v-show",value:e.alert.show,expression:"alert.show"}],staticClass:"vssue-alert",domProps:{textContent:e._s(e.alert.message)},on:{click:function(n){return e.alertHide()}}})])],1)},staticRenderFns:[]},void 0,Ql,void 0,!1,void 0,!1,void 0,void 0,void 0);let ed=class extends Wt{get status(){return this.vssue.isFailed?"failed":this.vssue.isInitializing?"initializing":this.vssue.isIssueNotCreated&&!this.vssue.isCreatingIssue?this.vssue.isAdmin||!this.vssue.isLogined?"issueNotCreated":"failed":this.vssue.isLoginRequired?"loginRequired":!this.vssue.comments||this.vssue.isCreatingIssue?"loadingComments":0===this.vssue.comments.data.length?"noComments":null}handleClick(){"issueNotCreated"===this.status?this.vssue.postIssue():"loginRequired"===this.status&&this.vssue.login()}};El([Ac()],ed.prototype,"vssue",void 0),ed=El([Mc({components:{TransitionFade:jl,VssueIcon:Nl}})],ed);const nd=ql({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("TransitionFade",[e.status?t("div",{key:e.status,staticClass:"vssue-status"},[["failed","loadingComments","initializing"].includes(e.status)?t("VssueIcon",{attrs:{name:"failed"===e.status?"error":"loading"}}):e._e(),e._v(" "),t("p",{staticClass:"vssue-status-info"},[t(["issueNotCreated","loginRequired"].includes(e.status)?"a":"span",{tag:"Component",on:{click:e.handleClick}},[e._v("\n        "+e._s(e.vssue.$t(e.status))+"\n      ")])],1)],1):e._e()])},staticRenderFns:[]},void 0,ed,void 0,!1,void 0,!1,void 0,void 0,void 0);let td=class extends Wt{};El([Ac()],td.prototype,"vssue",void 0),td=El([Mc({components:{TransitionFade:jl,VssueIcon:Nl,VssueComments:Kl,VssueNewComment:Xl,VssueNotice:Jl,VssueStatus:nd}})],td);const ad=ql({render:function(){var e=this.$createElement,n=this._self._c||e;return n("TransitionFade",[this.vssue.isInitializing?n("VssueStatus"):n("div",{staticClass:"vssue-body"},[this.vssue.API?n("VssueNewComment"):this._e(),this._v(" "),n("VssueNotice"),this._v(" "),n("TransitionFade",[this.vssue.comments&&this.vssue.comments.data.length>0?n("VssueComments"):n("VssueStatus")],1)],1)],1)},staticRenderFns:[]},void 0,td,void 0,!1,void 0,!1,void 0,void 0,void 0);let id=class extends Wt{};El([Ac()],id.prototype,"vssue",void 0),id=El([Mc],id);const rd=ql({render:function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"vssue-header"},[t("a",{staticClass:"vssue-header-comments-count",attrs:{href:e.vssue.issue?e.vssue.issue.link:null,target:"_blank",rel:"noopener noreferrer"}},[t("span",[e._v("\n      "+e._s(e.vssue.comments?e.vssue.$tc("comments",e.vssue.comments.count,{count:e.vssue.comments.count}):e.vssue.$tc("comments",0))+"\n    ")])]),e._v(" "),t("span",{staticClass:"vssue-header-powered-by"},[t("span",[e._v("Powered by")]),e._v(" "),e.vssue.API?t("span",[t("a",{attrs:{href:e.vssue.API.platform.link,title:e.vssue.API.platform.name+" API "+e.vssue.API.platform.version,target:"_blank",rel:"noopener noreferrer"}},[e._v("\n        "+e._s(e.vssue.API.platform.name)+"\n      ")]),e._v(" "),t("span",[e._v("&")])]):e._e(),e._v(" "),t("a",{attrs:{href:"https://github.com/meteorlxy/vssue",title:"Vssue v"+e.vssue.version,target:"_blank",rel:"noopener noreferrer"}},[e._v("\n      Vssue\n    ")])])])},staticRenderFns:[]},void 0,id,void 0,!1,void 0,!1,void 0,void 0,void 0),od={login:"Login with {platform}",logout:"Logout",currentUser:"Current User",loading:"Loading",submit:"Submit",submitting:"Submitting",submitComment:"Submit Comment",cancel:"Cancel",edit:"Edit",editMode:"Edit Mode",delete:"Delete",reply:"Reply",heart:"Heart",like:"Like",unlike:"Unlike",perPage:"Comments per page",sort:"Click to change the sort direction",page:"Page",prev:"Previous Page",next:"Next Page",comments:"Comments | {count} Comment | {count} Comments",loginToComment:"Login with {platform} account to leave a comment",placeholder:"Leave a comment. Styling with Markdown is supported. Ctrl + Enter to submit.",noLoginPlaceHolder:"Login to leave a comment. Styling with Markdown is supported. ",failed:"Failed to load comments",initializing:"Initializing...",issueNotCreated:"Click to create issue",loadingComments:"Loading comments...",loginRequired:"Login to view comments",noComments:"No comments yet. Leave the first comment !",reactionGiven:"Already given '{reaction}' reaction",deleteConfirm:"Confirm to delete this comment ?",deleteFailed:"Failed to delete comment"},sd={login:"使用 {platform} 登录",logout:"退出登录",currentUser:"当前用户",loading:"加载中",submit:"提交",submitting:"发表中",submitComment:"发表评论",cancel:"取消",edit:"编辑",editMode:"编辑模式",delete:"删除",reply:"回复",heart:"喜欢",like:"赞",unlike:"踩",perPage:"每页评论数",sort:"点击改变排序方式",page:"页数",prev:"上一页",next:"下一页",comments:"评论 | {count} 条评论 | {count} 条评论",loginToComment:"使用 {platform} 帐号登录后发表评论",placeholder:"留下你的评论丨支持 Markdown 语法丨Ctrl + Enter 发表评论",noLoginPlaceHolder:"登录后才能发表评论丨支持 Markdown 语法",failed:"评论加载失败",initializing:"正在初始化...",issueNotCreated:"点击创建 Issue",loadingComments:"正在加载评论...",loginRequired:"登录后查看评论",noComments:"还没有评论，来发表第一条评论吧！",reactionGiven:"已经添加过 '{reaction}' 了",deleteConfirm:"确认要删除该评论吗？",deleteFailed:"评论删除失败"},cd={login:"Entrar com {platform}",logout:"Sair",currentUser:"Usuário Atual",loading:"Carregando",submit:"Enviar",submitting:"Enviando",submitComment:"Enviar Comentário",cancel:"Cancelar",edit:"Editar",editMode:"Modo de Edição",delete:"Apagar",reply:"Responder",heart:"Heart",like:"Like",unlike:"Unlike",perPage:"Comentários por página",sort:"Clique para alterar a ordenação",page:"Página",prev:"Página Anterior",next:"Próxima Página",comments:"Comentários | {count} Comentário | {count} Comentários",loginToComment:"Entre com uma conta {platform} para deixar um comentário",placeholder:"Deixe um comentário. Estilos com Markdown suportados. Ctrl + Enter para enviar.",noLoginPlaceHolder:"Entre para deixar um comentário. Estilos com Markdown suportados. ",failed:"Falha ao carregar comentários",initializing:"Inicializando...",issueNotCreated:"Click to create issue",loadingComments:"Carregando comentários...",loginRequired:"Entrar para visualizar comentários",noComments:"Nenhum comentário. Deixe o primeiro comentário!",reactionGiven:"Já reagiu com '{reaction}'",deleteConfirm:"Apagar este comentário?",deleteFailed:"Falha ao apagar comentário"},ld={login:"{platform} でログイン",logout:"ログアウト",currentUser:"現在のユーザー",loading:"読み込み中",submit:"送信",submitting:"送信中",submitComment:"コメントを送信",cancel:"キャンセル",edit:"編集",editMode:"編集モード",delete:"削除",reply:"返信",heart:"ハート",like:"高評価",unlike:"低評価",perPage:"コメント/ページ",sort:"並び順を変更するにはクリックしてください",page:"ページ",prev:"前のページ",next:"次のページ",comments:"コメント | {count} コメント | {count} コメント",loginToComment:"コメントを残すには {platform} アカウントでログインしてください。",placeholder:"コメントを残してください。Markdown 記法をサポートしています。 Ctrl + Enter で送信できます。",noLoginPlaceHolder:"コメントを残すにはログインしてください。マークダウン記法をサポートしています。",failed:"コメントの読み込みに失敗しました",initializing:"初期化中...",issueNotCreated:"Click to create issue",loadingComments:"コメントの読み込み中...",loginRequired:"コメントを見るにはログインしてください",noComments:"まだコメントがありません。最初のコメントを残しましょう！",reactionGiven:"既に '{reaction}' のリアクションをしています",deleteConfirm:"本当にコメントを削除してもいいですか？",deleteFailed:"コメントの削除に失敗しました"},dd={login:"התחברו עם {platform}",logout:"התנתקו",currentUser:"משתמש/ת נוכחי/ת",loading:"טוען",submit:"שליחה",submitting:"שולח",submitComment:"שליחת תגובה",cancel:"ביטל",edit:"עריכה",editMode:"מצב עריכה",delete:"מחיקה",reply:"תשובה",heart:"לב",like:"לייק",unlike:"אנלייק",perPage:"תגובות לדף",sort:"לחצו כדי לשנות את כיוון המיון",page:"דף",prev:"הדף הקודם",next:"הדף הבא",comments:"תגובות | {count} תגובה | {count} תגובות",loginToComment:"התחברו עם חשבון {platform} כדי להשאיר תגובה",placeholder:"השאירו תגובה. יש תמיכה בעיצוב בעזרת Markdown. Ctrl + Enter כדי לשלוח.",noLoginPlaceHolder:"התחברו כדי להשאיר תגובה. יש תמיכה בעיצוב בעזרת Markdown. ",failed:"כשלון בטעינת התגובות",initializing:"מאתחל...",issueNotCreated:"לחצו ליצירת issue",loadingComments:"טוען תגובות...",loginRequired:"התחברו כדי לצפות בתגובות",noComments:"עדיין אין תגובות. השאירו תגובה ראשונה !",reactionGiven:"כבר ניתן חיווי '{reaction}'",deleteConfirm:"בטוחים במחיקת התגובה ?",deleteFailed:"כשלון במחיקת התגובה"};Object.prototype.hasOwnProperty.call(Wt,"$i18n")||Wt.use(Rl);const hd=new Rl({locale:"en",fallbackLocale:"en",messages:{en:od,"en-US":od,zh:sd,"zh-CN":sd,pt:cd,"pt-BR":cd,ja:ld,"ja-JP":ld,he:dd,"he-IL":dd}});let ud=class extends Wt{constructor(){super(...arguments),this.title=e=>`${e.prefix}${document.title}`,this.issueId=null,this.options=null,this.API=null,this.accessToken=null,this.user=null,this.issue=null,this.comments=null,this.query={page:1,perPage:10,sort:"desc"},this.isInitializing=!0,this.isIssueNotCreated=!1,this.isLoginRequired=!1,this.isFailed=!1,this.isCreatingIssue=!1,this.isLoadingComments=!1,this.isCreatingComment=!1,this.isUpdatingComment=!1}get version(){return"1.4.8"}get issueTitle(){return null===this.options?"":"function"==typeof this.title?this.title(this.options):`${this.options.prefix}${this.title}`}get isPending(){return this.isLoadingComments||this.isCreatingComment||this.isUpdatingComment}get isLogined(){return null!==this.accessToken&&null!==this.user}get isAdmin(){return null!==this.options&&null!==this.accessToken&&null!==this.user&&(this.user.username===this.options.owner||this.options.admins.includes(this.user.username))}get accessTokenKey(){return this.API?`Vssue.${this.API.platform.name.toLowerCase()}.access_token`:""}onQueryPerPageChange(){this.query.page=1,this.getComments()}onQueryChange(){this.getComments()}setOptions(e){this.options=Object.assign({labels:["Vssue"],state:"Vssue",prefix:"[Vssue]",admins:[],perPage:10,proxy:e=>"https://cors-anywhere.azm.workers.dev/"+e,issueContent:({url:e})=>e,autoCreateIssue:!1},e);const n=["api","owner","repo","clientId"];for(const e of n)this.options[e]||console.warn(`[Vssue] the option '${e}' is required`);if(this.options.locale)this.$i18n.locale=this.options.locale;else{const e=Object.keys(this.$i18n.messages),n=window.navigator.languages;this.$i18n.locale=n.filter(n=>e.includes(n)).shift()||"en"}}async init(){try{await this.initStore(),await this.initComments()}catch(e){e.response&&[401,403].includes(e.response.status)?this.isLoginRequired=!0:this.isFailed=!0,console.error(e)}}async initStore(){try{if(!this.options)throw new Error("Options are required to initialize Vssue");this.API=null,this.accessToken=null,this.user=null,this.issue=null,this.comments=null,this.query={page:1,perPage:this.options.perPage,sort:"desc"},this.isInitializing=!0,this.isIssueNotCreated=!1,this.isLoginRequired=!1,this.isFailed=!1,this.isCreatingIssue=!1,this.isLoadingComments=!1,this.isCreatingComment=!1,this.isUpdatingComment=!1;const e=this.options.api;this.API=new e({baseURL:this.options.baseURL,labels:this.options.labels,state:this.options.state,owner:this.options.owner,repo:this.options.repo,clientId:this.options.clientId,clientSecret:this.options.clientSecret,proxy:this.options.proxy}),await this.handleAuth()}finally{this.isInitializing=!1}}async initComments(){if(this.API&&this.options)if(this.issueId){const[e,n]=await Promise.all([this.API.getIssue({accessToken:this.accessToken,issueId:this.issueId}),this.API.getComments({accessToken:this.accessToken,issueId:this.issueId,query:this.query})]);this.issue=e,this.comments=n}else this.issue=await this.API.getIssue({accessToken:this.accessToken,issueTitle:this.issueTitle}),null===this.issue?(this.isIssueNotCreated=!0,this.options.autoCreateIssue&&await this.postIssue()):await this.getComments()}async postIssue(){if(this.API&&this.options&&!this.issue&&!this.issueId&&(this.isLogined||this.login(),this.isAdmin))try{this.isCreatingIssue=!0;const e=await this.API.postIssue({title:this.issueTitle,content:await this.options.issueContent({options:this.options,url:jc(window.location.href)}),accessToken:this.accessToken});this.issue=e,this.isIssueNotCreated=!1,await this.getComments()}catch(e){this.isFailed=!0}finally{this.isCreatingIssue=!1}}async getComments(){try{if(!this.API||!this.issue||this.isLoadingComments)return;this.isLoadingComments=!0;const e=await this.API.getComments({accessToken:this.accessToken,issueId:this.issue.id,query:this.query});return this.comments=e,this.query.page!==e.page&&(this.query.page=e.page),this.query.perPage!==e.perPage&&(this.query.perPage=e.perPage),e}catch(e){if(!e.response||![401,403].includes(e.response.status)||this.isLogined)throw this.$emit("error",e),e;this.isLoginRequired=!0}finally{this.isLoadingComments=!1}}async postComment({content:e}){try{if(!this.API||!this.issue||this.isCreatingComment)return;this.isCreatingComment=!0;return await this.API.postComment({accessToken:this.accessToken,content:e,issueId:this.issue.id})}catch(e){throw this.$emit("error",e),e}finally{this.isCreatingComment=!1}}async putComment({commentId:e,content:n}){try{if(!this.API||!this.issue)return;return await this.API.putComment({accessToken:this.accessToken,issueId:this.issue.id,commentId:e,content:n})}catch(e){throw this.$emit("error",e),e}}async deleteComment({commentId:e}){try{if(!this.API||!this.issue)return;return await this.API.deleteComment({accessToken:this.accessToken,issueId:this.issue.id,commentId:e})}catch(e){throw this.$emit("error",e),e}}async getCommentReactions({commentId:e}){try{if(!this.API||!this.issue)return;return await this.API.getCommentReactions({accessToken:this.accessToken,issueId:this.issue.id,commentId:e})}catch(e){throw this.$emit("error",e),e}}async postCommentReaction({commentId:e,reaction:n}){try{if(!this.API||!this.issue)return!1;return await this.API.postCommentReaction({accessToken:this.accessToken,issueId:this.issue.id,commentId:e,reaction:n})}catch(e){throw this.$emit("error",e),e}}login(){this.API&&this.API.redirectAuth()}logout(){this.setAccessToken(null),this.user=null}async handleAuth(){if(!this.API)return;const e=await this.API.handleAuth();e?(this.setAccessToken(e),this.user=await this.API.getUser({accessToken:e})):this.getAccessToken()?this.user=await this.API.getUser({accessToken:this.accessToken}):(this.setAccessToken(null),this.user=null)}getAccessToken(){return this.accessToken=window.localStorage.getItem(this.accessTokenKey),this.accessToken}setAccessToken(e){null===e?window.localStorage.removeItem(this.accessTokenKey):window.localStorage.setItem(this.accessTokenKey,e),this.accessToken=e}};El([Uc("query.perPage")],ud.prototype,"onQueryPerPageChange",null),El([Uc("query.page"),Uc("query.sort")],ud.prototype,"onQueryChange",null),ud=El([Mc({i18n:hd})],ud);var pd=ud;let md=class extends Wt{constructor(){super(...arguments),this.vssue=new pd}onOptionsChange(e){this.vssue.setOptions(e)}mounted(){null!==this.title&&(this.vssue.title=this.title),null!==this.issueId&&(this.vssue.issueId=this.issueId),this.vssue.setOptions(this.options),this.vssue.init()}};var fd;El([zc({type:[String,Function],required:!1,default:null})],md.prototype,"title",void 0),El([zc({type:[String,Number],required:!1,default:null})],md.prototype,"issueId",void 0),El([zc({type:Object,required:!1,default:()=>({})})],md.prototype,"options",void 0),El([(fd="vssue",wc((function(e,n){var t=e.provide;Ic(t)&&(t=e.provide=Sc(t)),t.managed[n]=fd||n})))],md.prototype,"vssue",void 0),El([Uc("options",{deep:!0})],md.prototype,"onOptionsChange",null),md=El([Mc({components:{Iconfont:Bl,VssueBody:ad,VssueHeader:rd}})],md);const gd=ql({render:function(){var e=this.$createElement,n=this._self._c||e;return n("div",{staticClass:"vssue"},[n("Iconfont"),this._v(" "),n("VssueHeader"),this._v(" "),n("VssueBody")],1)},staticRenderFns:[]},void 0,md,void 0,!1,void 0,!1,void 0,void 0,void 0);var yd=t(117),bd=t.n(yd);function vd(e){return{username:e.login,avatar:e.avatar_url,homepage:e.html_url}}function wd(e){return{id:e.number,title:e.title,content:e.body,link:e.html_url}}function _d(e){return{like:e["+1"],unlike:e[-1],heart:e.heart}}function kd(e){return{id:e.id,content:e.body_html,contentRaw:e.body,author:vd(e.user),createdAt:e.created_at,updatedAt:e.updated_at,reactions:_d(e.reactions)}}function xd(e){return"like"===e?"+1":"unlike"===e?"-1":e}class Cd{constructor({baseURL:e="https://github.com",owner:n,repo:t,labels:a,clientId:i,clientSecret:r,state:o,proxy:s}){if(void 0===r||void 0===s)throw new Error("clientSecret and proxy is required for GitHub V3");this.baseURL=e,this.owner=n,this.repo=t,this.labels=a,this.clientId=i,this.clientSecret=r,this.state=o,this.proxy=s,this.$http=bd.a.create({baseURL:"https://github.com"===e?"https://api.github.com":Ec(e,"api/v3"),headers:{Accept:"application/vnd.github.v3+json"}}),this.$http.interceptors.response.use(e=>e.data&&e.data.error?Promise.reject(new Error(e.data.error_description)):e,e=>(void 0===e.response&&"Network Error"===e.message&&(e.response={status:403}),Promise.reject(e)))}get platform(){return{name:"GitHub",link:this.baseURL,version:"v3",meta:{reactable:!0,sortable:!1}}}redirectAuth(){window.location.href=Rc(Ec(this.baseURL,"login/oauth/authorize"),{client_id:this.clientId,redirect_uri:window.location.href,scope:"public_repo",state:this.state})}async handleAuth(){const e=(n=window.location.search,Object(Oc.parse)(n,{ignoreQueryPrefix:!0}));var n;if(e.code){if(e.state!==this.state)return null;const n=e.code;delete e.code,delete e.state;const t=Rc(jc(window.location.href),e)+window.location.hash;window.history.replaceState(null,"",t);return await this.getAccessToken({code:n})}return null}async getAccessToken({code:e}){const n=Ec(this.baseURL,"login/oauth/access_token"),t="function"==typeof this.proxy?this.proxy(n):this.proxy,{data:a}=await this.$http.post(t,{client_id:this.clientId,client_secret:this.clientSecret,code:e},{headers:{Accept:"application/json"}});return a.access_token}async getUser({accessToken:e}){const{data:n}=await this.$http.get("user",{headers:{Authorization:"token "+e}});return vd(n)}async getIssue({accessToken:e,issueId:n,issueTitle:t}){const a={};if(e&&(a.headers={Authorization:"token "+e}),!n){a.params={q:[`"${t}"`,"is:issue","in:title",`repo:${this.owner}/${this.repo}`,"is:public",...this.labels.map(e=>"label:"+e)].join(" "),timestamp:Date.now()};const{data:e}=await this.$http.get("search/issues",a);return e.items.map(wd).find(e=>e.title===t)||null}try{a.params={timestamp:Date.now()};const{data:e}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/${n}`,a);return wd(e)}catch(e){if(e.response&&404===e.response.status)return null;throw e}}async postIssue({accessToken:e,title:n,content:t}){const{data:a}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues`,{title:n,body:t,labels:this.labels},{headers:{Authorization:"token "+e}});return wd(a)}async getComments({accessToken:e,issueId:n,query:{page:t=1,perPage:a=10}={}}){const i={params:{timestamp:Date.now()}},r={params:{page:t,per_page:a,timestamp:Date.now()},headers:{Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}};e&&(i.headers={Authorization:"token "+e},r.headers.Authorization="token "+e);const[o,s]=await Promise.all([this.$http.get(`repos/${this.owner}/${this.repo}/issues/${n}`,i),this.$http.get(`repos/${this.owner}/${this.repo}/issues/${n}/comments`,r)]),c=s.headers.link||null,l=/rel="next"/.test(c)?Number(c.replace(/^.*[^_]page=(\d*).*rel="next".*$/,"$1"))-1:/rel="prev"/.test(c)?Number(c.replace(/^.*[^_]page=(\d*).*rel="prev".*$/,"$1"))+1:1,d=c?Number(c.replace(/^.*per_page=(\d*).*$/,"$1")):a;return{count:Number(o.data.comments),page:l,perPage:d,data:s.data.map(kd)}}async postComment({accessToken:e,issueId:n,content:t}){const{data:a}=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/${n}/comments`,{body:t},{headers:{Authorization:"token "+e,Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}});return kd(a)}async putComment({accessToken:e,commentId:n,content:t}){const{data:a}=await this.$http.patch(`repos/${this.owner}/${this.repo}/issues/comments/${n}`,{body:t},{headers:{Authorization:"token "+e,Accept:["application/vnd.github.v3.raw+json","application/vnd.github.v3.html+json","application/vnd.github.squirrel-girl-preview"]}});return kd(a)}async deleteComment({accessToken:e,commentId:n}){const{status:t}=await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${n}`,{headers:{Authorization:"token "+e}});return 204===t}async getCommentReactions({accessToken:e,commentId:n}){const{data:t}=await this.$http.get(`repos/${this.owner}/${this.repo}/issues/comments/${n}`,{params:{timestamp:Date.now()},headers:{Authorization:"token "+e,Accept:"application/vnd.github.squirrel-girl-preview"}});return _d(t.reactions)}async postCommentReaction({accessToken:e,commentId:n,reaction:t}){const a=await this.$http.post(`repos/${this.owner}/${this.repo}/issues/comments/${n}/reactions`,{content:xd(t)},{headers:{Authorization:"token "+e,Accept:"application/vnd.github.squirrel-girl-preview"}});return 200===a.status?this.deleteCommentReaction({accessToken:e,commentId:n,reactionId:a.data.id}):201===a.status}async deleteCommentReaction({accessToken:e,commentId:n,reactionId:t}){return 204===(await this.$http.delete(`repos/${this.owner}/${this.repo}/issues/comments/${n}/reactions/${t}`,{headers:{Authorization:"token "+e,Accept:"application/vnd.github.squirrel-girl-preview"}})).status}}var Pd=t(118),Td=t.n(Pd),Md=(t(326),{components:{VssueComponent:gd},data:()=>({isLoad:!1}),watch:{"$page.key"(e,n){e&&n!==e&&this.initialize()}},mounted(){this.$nextTick(()=>{this.initialize()})},methods:{initialize(){this.$el.remove(),this.isLoad=!1,setTimeout(()=>{if(this.needComment(this.$frontmatter)){document.querySelector("main").appendChild(this.$el),this.title=Td.a.render("[Comment]<%- frontmatter.title %>",{frontmatter:this.$frontmatter}),this.options={api:Cd,autoCreateIssue:!0,clientId:"adb9fb0ac1159e00ce7f",clientSecret:"27da8dc85f808c2bd1b6e44da5ae69c4ddf17d8d",owner:"eryajf",repo:"qishao-notes"},this.isLoad=!0}},1e3)},needComment:e=>!1!==e.comment&&!1!==e.comments}}),Ad=Object(ic.a)(Md,(function(){var e=this._self._c;return this.isLoad?e("div",[e("VssueComponent",{attrs:{title:this.title,options:this.options}})],1):this._e()}),[],!1,null,null,null).exports,Sd=[({Vue:e,options:n,router:t,siteData:a})=>{},({Vue:e,options:n,router:t,siteData:a})=>{a.pages.map(e=>{const{frontmatter:{date:n,author:t}}=e;"string"==typeof n&&"Z"===n.charAt(n.length-1)&&(e.frontmatter.date=function(e){e instanceof Date||(e=new Date(e));return`${e.getUTCFullYear()}-${uc(e.getUTCMonth()+1)}-${uc(e.getUTCDate())} ${uc(e.getUTCHours())}:${uc(e.getUTCMinutes())}:${uc(e.getUTCSeconds())}`}(n)),t?e.author=t:a.themeConfig.author&&(e.author=a.themeConfig.author)}),e.mixin(hc)},{},({Vue:e})=>{e.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},{},({Vue:e})=>{e.component("Vssue",Ad)}],Id=["Vssue"];class Ld extends class{constructor(){this.store=new Wt({data:{state:{}}})}$get(e){return this.store.state[e]}$set(e,n){Wt.set(this.store.state,e,n)}$emit(...e){this.store.$emit(...e)}$on(...e){this.store.$on(...e)}}{}Object.assign(Ld.prototype,{getPageAsyncComponent:ss,getLayoutAsyncComponent:cs,getAsyncComponent:ls,getVueComponent:ds});var Dd={install(e){const n=new Ld;e.$vuepress=n,e.prototype.$vuepress=n}};function zd(e,n){return e.options.routes.filter(e=>e.path.toLowerCase()===n.toLowerCase()).length>0}var Ud={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(e){const n=this.pageKey||this.$parent.$page.key;return us("pageKey",n),Wt.component(n)||Wt.component(n,ss(n)),Wt.component(n)?e(n):e("")}},Od={functional:!0,props:{slotKey:String,required:!0},render:(e,{props:n,slots:t})=>e("div",{class:["content__"+n.slotKey]},t()[n.slotKey])},Rd={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Ed=(t(327),t(328),Object(ic.a)(Rd,(function(){var e=this._self._c;return e("span",[e("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[e("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),e("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),e("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),Gd={functional:!0,render(e,{parent:n,children:t}){if(n._isMounted)return t;n.$once("hook:mounted",()=>{n.$forceUpdate()})}};Wt.config.productionTip=!1,Wt.use(Vo),Wt.use(Dd),Wt.mixin(function(e,n,t=Wt){!function(e){e.locales&&Object.keys(e.locales).forEach(n=>{e.locales[n].path=n});Object.freeze(e)}(n),t.$vuepress.$set("siteData",n);const a=new(e(t.$vuepress.$get("siteData"))),i=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(a)),r={};return Object.keys(i).reduce((e,n)=>(n.startsWith("$")&&(e[n]=i[n].get),e),r),{computed:r}}(e=>class{setPage(e){this.__page=e}get $site(){return e}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:e={}}=this.$site;let n,t;for(const a in e)"/"===a?t=e[a]:0===this.$page.path.indexOf(a)&&(n=e[a]);return n||t||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:e}=this.$page.frontmatter;return"string"==typeof e&&e}get $title(){const e=this.$page,{metaTitle:n}=this.$page.frontmatter;if("string"==typeof n)return n;const t=this.$siteTitle,a=e.frontmatter.home?null:e.frontmatter.title||e.title;return t?a?a+" | "+t:t:a||"VuePress"}get $description(){const e=function(e){if(e){const n=e.filter(e=>"description"===e.name)[0];if(n)return n.content}}(this.$page.frontmatter.meta);return e||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(e,n){for(let t=0;t<e.length;t++){const a=e[t];if(a.path.toLowerCase()===n.toLowerCase())return a}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},sc)),Wt.component("Content",Ud),Wt.component("ContentSlotsDistributor",Od),Wt.component("OutboundLink",Ed),Wt.component("ClientOnly",Gd),Wt.component("Layout",cs("Layout")),Wt.component("NotFound",cs("NotFound")),Wt.prototype.$withBase=function(e){const n=this.$site.base;return"/"===e.charAt(0)?n+e.slice(1):e},window.__VUEPRESS__={version:"1.8.0",hash:"b4e5ea8"},async function(e){const n="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:sc.routerBase||sc.base,t=new Vo({base:n,mode:"history",fallback:!1,routes:oc,scrollBehavior:(e,n,t)=>t||(e.hash?!Wt.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(e.hash)}:{x:0,y:0})});!function(e){e.beforeEach((n,t,a)=>{if(zd(e,n.path))a();else if(/(\/|\.html)$/.test(n.path))if(/\/$/.test(n.path)){const t=n.path.replace(/\/$/,"")+".html";zd(e,t)?a(t):a()}else a();else{const t=n.path+"/",i=n.path+".html";zd(e,i)?a(i):zd(e,t)?a(t):a()}})}(t);const a={};try{await Promise.all(Sd.filter(e=>"function"==typeof e).map(n=>n({Vue:Wt,options:a,router:t,siteData:sc,isServer:e})))}catch(e){console.error(e)}return{app:new Wt(Object.assign(a,{router:t,render:e=>e("div",{attrs:{id:"app"}},[e("RouterView",{ref:"layout"}),e("div",{class:"global-ui"},Id.map(n=>e(n)))])})),router:t}}(!1).then(({app:e,router:n})=>{n.onReady(()=>{e.$mount("#app")})})}]);