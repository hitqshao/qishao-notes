(window.webpackJsonp=window.webpackJsonp||[]).push([[51],{461:function(e,a,t){"use strict";t.r(a);var s=t(5),i=Object(s.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("[22] Adaptive Memory-Side Last-Level GPU Chacing")]),e._v(" "),a("li",[e._v("[42] Understaning the Tradeoffs between Software-Managed vs. Hardware-Managed Caches in GPUs")])]),e._v(" "),a("hr"),e._v(" "),a("h1",{attrs:{id:"_1-adaptive-memory-side-last-level-gpu-chacing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-adaptive-memory-side-last-level-gpu-chacing"}},[e._v("#")]),e._v(" 1. Adaptive Memory-Side Last-Level GPU Chacing")]),e._v(" "),a("h2",{attrs:{id:"introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),a("p",[e._v("GPUs typically feature a two-level on-chip cache hierarchy in which the first-level caches are private to each SM while the last-level cache (LLC) is a shared memory-side cache that is partitioned into\nequally-sized slices and accessed via the NoC.")]),e._v(" "),a("p",[e._v("In fact, we find that GPU workloads have large read-only shared data footprints. For such sharing-intensive workloads, multiple SMs experience a bandwidth bottleneck when\nthey serialize on accesses to the same shared cache line.")]),e._v(" "),a("p",[e._v("While this is a practical solution for a limited number of cores in a CPU, it does not scale to a large number of SMs due to limitations in scaling GPU die size.")]),e._v(" "),a("p",[e._v("Shared LLCs incur a performance bottleneck for workloads that frequently access data shared by multiple SMs.")]),e._v(" "),a("p",[e._v("A shared memory-side LLC consists of multiple slices each caching a specific memory partition, i.e., a specific address range of the entire memory space is served by a particular\nmemory controller.")]),e._v(" "),a("p",[e._v("As a result, "),a("strong",[e._v("a shared cache line appears in a single LLC slice, which leads to a severe performance bottleneck if multiple SMs concurrently access the same shared data.")])]),e._v(" "),a("p",[e._v("We find that GPU applications with "),a("strong",[e._v("high degrees of read-only data sharing significantly benefit from a private LLC organization.")])]),e._v(" "),a("p",[e._v("To that end, this paper proposes adaptive memory-side caching to dynamically choose between a shared or private memory-side LLC.")]),e._v(" "),a("p",[e._v("These observations suggest an opportunity to improve performance by dynamically adapting a memory-side LLC to the needs of an application’s sharing behavior.")]),e._v(" "),a("p",[a("strong",[e._v("adaptive memory-side caching to dynamically choose between a shared or private memory-side LLC.")]),e._v(" "),a("br"),e._v("\nSelecting a shared versus private LLC is done using a lightweight performance model.")]),e._v(" "),a("p",[e._v("By default, the GPU assumes a shared LLC.")]),e._v(" "),a("p",[e._v("Profiling information is periodically collected to predict LLC miss rate and bandwidth under a private LLC organization while executing under a shared LLC. If deemed beneficial, the LLC is adapted to\na private cache.")]),e._v(" "),a("p",[e._v("The LLC reverts back to a shared organization periodically and when a new kernel gets launched.")]),e._v(" "),a("h3",{attrs:{id:"main-idea"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#main-idea"}},[e._v("#")]),e._v(" Main Idea")]),e._v(" "),a("p",[e._v("Private LLC could have replicate data, but waste memory space.")]),e._v(" "),a("p",[e._v("Shared LLC could access shared data a lot, introducing bottleneck.")]),e._v(" "),a("p",[e._v("Switch between those two modes.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/8138dd9c-12e8-430d-8c65-43cae6d84a53",alt:"image"}})]),e._v(" "),a("hr"),e._v(" "),a("h1",{attrs:{id:"_2-understanding-the-tradeoffs-between-software-managed-vs-hardware-managed-caches-in-gpus"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-understanding-the-tradeoffs-between-software-managed-vs-hardware-managed-caches-in-gpus"}},[e._v("#")]),e._v(" 2. Understanding the Tradeoffs between Software-Managed vs. Hardware-Managed Caches in GPUs")]),e._v(" "),a("p",[e._v("On one hand, the kernels utilizing the L1 caches may support higher degrees of thread-level parallelism, offer more opportunities for data to be\nallocated in registers, and sometimes result in lower dynamic instruction counts.")]),e._v(" "),a("p",[e._v("On the other hand, the applications utilizing shared memory enable more coalesced accesses and tend to achieve higher degrees of memory-level parallelism.")]),e._v(" "),a("h2",{attrs:{id:"main-idea-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#main-idea-2"}},[e._v("#")]),e._v(" Main Idea")]),e._v(" "),a("p",[e._v("Even if a matrix totally fits into cache, the "),a("em",[e._v("L1 D-cache version is surprisingly much slower (43.8%) than the shared-memory version.")])]),e._v(" "),a("p",[e._v("Our results show that for most applications, the GPU kernels utilizing shared memory deliver significantly higher performance than those leveraging L1 D-caches."),a("br"),e._v("\nThe fundamental reasons are MLP and coalescing.")]),e._v(" "),a("p",[e._v("For a few benchmarks for which the L1 D-cache versions have higher performance, "),a("em",[e._v("the performance impact is mainly due to improved thread-level parallelism (TLP) and allocating more data to registers")]),e._v(".")]),e._v(" "),a("p",[e._v("Overall, rather than cache hit rates, the subtle factors including MLP, coalescing, and TLP often have more profound performance impacts.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/0e261ecc-f3b0-49b6-b395-279278728977",alt:"image"}})]),e._v(" "),a("h2",{attrs:{id:"study"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#study"}},[e._v("#")]),e._v(" Study")]),e._v(" "),a("p",[a("strong",[e._v("the register usage is the limiting factor on how many TBs can run concurrently on an SM.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/0f68ab5a-0e84-49da-862f-9754f545b857",alt:"image"}})]),e._v(" "),a("p",[e._v("(a) Why is the D-cache version slightly slower than the shared-memory version even with a perfect cache?\nSince the array access ‘A[a+WA*ty+k]’ of the D-cache version cannot be coalesced into a single cache access, it suffers from additional pipeline stalls even though all accesses hit in cache.")]),e._v(" "),a("p",[e._v("(b) With a realistic cache, why is the D-cache version much slower?\nWe found that performance is not determined by the total number of cache misses."),a("br"),e._v("\nInstead it depends more on how these cache-misses overlap with each other, i.e., the degrees of memory-level parallelism (MLP).")]),e._v(" "),a("p",[e._v("In cache-version, the cache-misses overlap is bad, as seen in the figure.")]),e._v(" "),a("blockquote",[a("p",[e._v("In other words, in cache version, each iteration it has a cache miss. In shared-memory version, in the first two load-to-sharememory, there is large MLP.\nCache-version is like a man eat food every day. Sharedmemory-version is like a man only eat food in first ten years of his life.")])])])}),[],!1,null,null,null);a.default=i.exports}}]);