(window.webpackJsonp=window.webpackJsonp||[]).push([[73],{516:function(e,a,t){"use strict";t.r(a);var i=t(8),n=Object(i.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_1-large-language-model-inference-acceleration-a-comprehensive-hardware-perspective"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-large-language-model-inference-acceleration-a-comprehensive-hardware-perspective"}},[e._v("#")]),e._v(" 1. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective")]),e._v(" "),a("h4",{attrs:{id:"optimizations-on-hardware-platforms"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#optimizations-on-hardware-platforms"}},[e._v("#")]),e._v(" Optimizations on Hardware Platforms")]),e._v(" "),a("h5",{attrs:{id:"quantization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#quantization"}},[e._v("#")]),e._v(" Quantization")]),e._v(" "),a("p",[e._v("Data Format")]),e._v(" "),a("ul",[a("li",[e._v("Uniform Quantization")]),e._v(" "),a("li",[e._v("Non-uniform Quantization")])]),e._v(" "),a("p",[e._v("Granularity")]),e._v(" "),a("ul",[a("li",[e._v("group-wise: Group-wise granularity is a coarser approach where multiple channels or layers are quantized with the same parameters.")]),e._v(" "),a("li",[e._v("channel-wise: Channel-wise granularity involves quantizing each channel individually within the model.")]),e._v(" "),a("li",[e._v("tensor-wise: Tensor-wise granularity is the most detailed approach, where each tensor (such as weight tensors or activation tensors) is quantized separately.")])]),e._v(" "),a("p",[a("strong",[e._v("Weight-Only Quantization")])]),e._v(" "),a("p",[e._v("Uniform & Norn Uniform")]),e._v(" "),a("p",[e._v("Matrix decomposition quantization is a specialized method where a large matrix is approximated by the product of several smaller matrices.")]),e._v(" "),a("p",[a("strong",[e._v("Weight-Activation Quantization")])]),e._v(" "),a("p",[e._v("Quantization include the activations generated during model inference.")]),e._v(" "),a("p",[e._v("In this method, both the weights and the activations at each layer are quantized to lower precision formats.")]),e._v(" "),a("p",[e._v("This reduces memory bandwidth requirements and enhances inference speed.")]),e._v(" "),a("p",[e._v("The challenge with weight-activation quantization is to manage the trade-off between quantization errors and model accuracy.")]),e._v(" "),a("p",[e._v("Techniques such as dynamic range quantization or specific quantization schemes are used to balance precision and computational efficiency.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/c7bae637-2223-4d24-88de-c89d3252e34a",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"weight-only-quantization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#weight-only-quantization"}},[e._v("#")]),e._v(" Weight-Only Quantization")]),e._v(" "),a("p",[e._v("Shen et al. [97] leverage Intel Neural Compressor to automate the INT4 quantization process with negligible accuracy loss, supporting various quantization recipes such as "),a("strong",[e._v("GPTQ")]),e._v(", "),a("strong",[e._v("AWQ")]),e._v(" and "),a("strong",[e._v("TEQ")]),e._v(".")]),e._v(" "),a("p",[e._v("Due to the overheads of weight dequantization from integer to floating, T-MAC leverages lookup tables (LUTs) for efficient low-bit LLM inference on edge CPUs, circumventing the need for dequantization and mixed precision matrix multiplication.")])])}),[],!1,null,null,null);a.default=n.exports}}]);