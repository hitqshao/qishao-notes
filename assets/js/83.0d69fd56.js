(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{542:function(e,t,i){"use strict";i.r(t);var n=i(8),a=Object(n.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("[1] Resource-efficient Algorithms and Systems of Foundation Models: A Survey")]),e._v(" "),t("hr"),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/539f3a36-1e67-486a-a1af-d7b637cfaf8e",alt:"image"}}),e._v(" "),t("em",[e._v("Source: Resource-efficient")])]),e._v(" "),t("p",[e._v("Computation complexity of attention is O(T 2D), whereas that of the FFN is O(TD2), where T represents the sequence length and D the hidden state dimension of the model."),t("br"),e._v("\nThe FFN layer is the most computationally intensive component.")]),e._v(" "),t("h2",{attrs:{id:"_1-resource-efficient-architectures"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-resource-efficient-architectures"}},[e._v("#")]),e._v(" 1. Resource-Efficient Architectures")]),e._v(" "),t("h3",{attrs:{id:"_1-1-efficient-attention"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-efficient-attention"}},[e._v("#")]),e._v(" 1.1 Efficient Attention")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/d87f3dae-6a64-46ed-af28-51b4f2c16f3d",alt:"image"}})]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Sparse Attention")]),e._v(": Reduces complexity (e.g., Longformer, BigBird)."),t("br"),e._v("\nMotivated by graph sparsification, sparse attention aims to build a sparse attention matrix."),t("br"),e._v("\nThis approach aims to retain the empirical advantages of a fully quadratic self-attention scheme while employing a reduced number of inner products.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Approximate Attention")]),e._v(": Low-rank approximations (e.g., Linformer, Reformer).\nApproximate attention mainly includes low-rank approximations of the self-attention matrix and innovative reformulations of the self-attention.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Attention-Free Approaches")]),e._v(": Alternatives like Hyena, Mamba."),t("br"),e._v("\nDespite the dominance of attention-based transformer architectures in large FMs, several works have put forth innovative architectures that hold the potential to replace the traditional transformer model.")])])]),e._v(" "),t("h3",{attrs:{id:"_1-2-dynamic-neural-network"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-dynamic-neural-network"}},[e._v("#")]),e._v(" 1.2 Dynamic Neural Network")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/baefb07e-0c3b-46c2-a728-5cb2e94a21b5",alt:"image"}})]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Mixture of Experts (MoE)")]),e._v(": (e.g., Switch Transformer, GLaM, MoEfication, FFF).")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Early Exiting")]),e._v(": Premature termination based on confidence (e.g., FREE, SkipDecode,DeeBERT, PABEE).\nearly-exiting optimization is a strategy that allows a model to terminate its computational process prematurely when it attains high confidence in the prediction or encounters resource constraints.")])])]),e._v(" "),t("h3",{attrs:{id:"_1-3-diffusion-specific-optimization"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-diffusion-specific-optimization"}},[e._v("#")]),e._v(" 1.3 Diffusion-Specific Optimization")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Efficient Sampling")])]),e._v(" "),t("li",[t("strong",[e._v("Diffusion in Latent Space")])]),e._v(" "),t("li",[t("strong",[e._v("Diffusion Architecture Variants")])])]),e._v(" "),t("h3",{attrs:{id:"_1-4-vit-specific-optimizations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-vit-specific-optimizations"}},[e._v("#")]),e._v(" 1.4 ViT-Specific Optimizations")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Efficient ViT Variants")]),e._v(": MobileViT, EfficientFormer, EdgeViT.")])]),e._v(" "),t("h2",{attrs:{id:"_2-resource-efficient-algorithms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-resource-efficient-algorithms"}},[e._v("#")]),e._v(" 2. Resource-Efficient Algorithms")]),e._v(" "),t("h3",{attrs:{id:"_2-1-pre-training-algorithms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-pre-training-algorithms"}},[e._v("#")]),e._v(" 2.1 Pre-Training Algorithms")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Training Data Quality Control")]),e._v(": DataComp, DFN."),t("br"),e._v("\nA portion of work focus on controlling the quality of training data.")]),e._v(" "),t("li",[t("strong",[e._v("Training Data Reduction")]),e._v(": Deduplication, image patch removal."),t("br"),e._v("\nPre-training for large FMs needs a dataset at the trillion scale, exemplified by 0.3 trillion tokens for GPT-3-175B [25] and 2 trillion tokens for LLaMa-2-70B [238]."),t("br"),e._v("\nprior literature resorts to reduce vast training data through two aspects: deduplicating text datasets and image patch removal.")]),e._v(" "),t("li",[t("strong",[e._v("Progressive Learning")]),e._v(": StackingBERT, CompoundGrow.\nProgressive learning is a training strategy that begins by training a small model and then gradually increases the model size, throughout the training process.")]),e._v(" "),t("li",[t("strong",[e._v("Mixed Precision Training")]),e._v(": Mesa, GACT.\nMixed precision training often utilizes half-precision floating-point data representation instead of single precision. This approach significantly reduces memory\nrequirements, approximately halving the storage space needed for weights, activations, and gradients.")])]),e._v(" "),t("h3",{attrs:{id:"_2-2-fine-tuning-algorithms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-fine-tuning-algorithms"}},[e._v("#")]),e._v(" 2.2 Fine-Tuning Algorithms")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/1ed489c1-3451-4874-8f36-fa99fc65e0ca",alt:"image"}})]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Additive Tuning")]),e._v(":")]),e._v(" "),t("ul",[t("li",[t("em",[e._v("Adapter tuning")]),e._v(" aims to reduce training costs by introducing adapter modules to specific layers (or all layers) of pre-trained large FMs.\nDuring tuning, the backbone of the pre-trained model remains frozen, and adapter modules are utilized to acquire task-specific knowledge.")]),e._v(" "),t("li",[t("em",[e._v("prompt tuning")]),e._v(" involves designing a task-specific prompt for each task, with the aim of replacing the traditional fine-tuning of pre-trained large FMs parameters."),t("br"),e._v("\nBy tuning the input prompts instead, this method significantly reduces the resources and time required for the fine-tuning.")]),e._v(" "),t("li",[t("em",[e._v("prefix tuning")]),e._v(" introduces a trainable, task-specific prefix part to each layer of large FMs.\nThis technique aims to reduce the tuning cost by limiting the updates to the parameters in this prefix.")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Selective Tuning")]),e._v(": Freezing most parameters, selective updates.\nSelective tuning aims to maintain high performance on new tasks with low training costs by freezing the majority of parameters in large FMs and selectively updating only a small portion of the parameters.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Re-parameter Tuning")]),e._v(": Low-rank adaptation (e.g., "),t("strong",[e._v("LoRA")]),e._v(", Delta-LoRA).\n"),t("img",{attrs:{src:"https://github.com/user-attachments/assets/920fd758-54f1-492c-bf1e-0a5b4209f2b4",alt:"image"}})]),e._v(" "),t("p",[e._v("Re-parameter tuning adapts large FMs by targeting a significantly smaller subspace than the original, expansive training space."),t("br"),e._v("\nThis approach involves fine-tuning low-rank matrix parameters, a technique that effectively reduces the overall training cost.")])])]),e._v(" "),t("h3",{attrs:{id:"_2-3-inference-algorithms"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-inference-algorithms"}},[e._v("#")]),e._v(" 2.3 Inference Algorithms")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Opportunistic Decoding")]),e._v(":")]),e._v(" "),t("ul",[t("li",[e._v("Speculative decoding ("),t("em",[e._v("SpecInfer, LLMCad")]),e._v(") generating sequences autoregressively with a cost-efficient small model, followed by parallel token verification using a larger model.")]),e._v(" "),t("li",[e._v("Look-ahead decoding accelerates inference in large FMs without relying on a draft model or data store, reducing decoding steps in proportion to log(FLOPs).")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Input Filtering and Compression")]),e._v(":")]),e._v(" "),t("ul",[t("li",[e._v("Prompt compression(LLMLingua,LLMZip,ICAE,COT-Max)\nLLMZip [241] employs LLaMA-7B for compressing natural language. Experimental results demonstrate that LLMZip outperforms cutting-edge text compression methods, including BSC, ZPAQ, and paq8h.")]),e._v(" "),t("li",[e._v("Token pruning Pruning of input sequences for transformers, often involving the incremental removal of less important tokens during inference.")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("KV Cache Optimization")]),e._v(": Memory-efficient sparse attention.\nmost sparse attention designs, which primarily target the reduction of computational complexity [24, 294], do not necessarily lead to a reduction in KV cache memory consumption."),t("br"),e._v("\nThis is because achieving a reduced memory footprint for the KV cache necessitates a more stringent sparsity pattern.")]),e._v(" "),t("ul",[t("li",[t("em",[e._v("H2O")]),e._v(" KV cache eviction stragegy: employs attention scores to identify and select the least important KV cache tokens in the current state for eviction")]),e._v(" "),t("li",[t("em",[e._v("Dynamic Context Pruning")]),e._v(" learns a memory-efficient KV cache eviction strategy during the pre-training phase.")]),e._v(" "),t("li",[t("em",[e._v("Scissorhands")]),e._v(": innovative compact KV cache")]),e._v(" "),t("li",[t("em",[e._v("Landmark Attention")]),e._v(" enables the storage of most KV caches in a slower but larger capacity memory")])])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Long Context Handling")]),e._v(": LM-Infinite, StreamingLLM. Due to the quadratic computational cost associated with attention mechanisms, various resource-efficient optimizations have been proposed to handle long inputs.")]),e._v(" "),t("ul",[t("li",[e._v("LM-Infinite introduces a Λ-shaped attention mechanism to handle long contexts efficiently.")]),e._v(" "),t("li",[e._v("StreamingLLM facilitates large FMs trained with a finite-length attention window to generalize to infinite stream decoding without the need for any fine-tuning.")])])])]),e._v(" "),t("h3",{attrs:{id:"_2-4-model-compression"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-model-compression"}},[e._v("#")]),e._v(" 2.4 Model Compression")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/d3d1214d-8f83-4553-97bd-467a2b914dd4",alt:"image"}})]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Pruning")]),e._v(" "),t("ul",[t("li",[e._v("Structured pruning eliminates entire structural components, such as groups of consecutive parameters or hierarchical structures"),t("br"),e._v("\nLLM Pruner selectively removes non-essential model structures based on gradient information and incorporates LoRA to recover the model’s accuracy after pruning."),t("br"),e._v("\nStructured pruning is also employed in training."),t("br"),e._v(" "),t("em",[e._v("Sheared LLaMA")]),e._v(" adopts an end-to-end to remove channels, encompassing layers, attention heads, intermediate layers, and hidden layers."),t("br"),e._v(" "),t("em",[e._v("AdaPrune")]),e._v(" accelerates neural network training using transposable masks."),t("br"),e._v(" "),t("em",[e._v("GUM")]),e._v(" considers neuron specificity and introduces pruning through network component-based global mobility and local uniqueness scores."),t("br"),e._v(" "),t("em",[e._v("PLATON")]),e._v(" tackles the uncertainty in importance scores during pruning by employing the upper confidence bound of importance estimation.")]),e._v(" "),t("li",[e._v("Unstructred pruning It removes neurons with weights below a threshold, thereby compressing the model.\n"),t("em",[e._v("SparseGPT")]),e._v(" sparse regression solver"),t("br"),e._v(" "),t("em",[e._v("Wanda")]),e._v(" prunes weights with smallest magnitude multiplied by corresponding input activations."),t("br"),e._v(" "),t("em",[e._v("SIGE")]),e._v(" converts computation reduction into latency reduction.")]),e._v(" "),t("li",[t("strong",[e._v("Contextual pruning")]),e._v(" selects the sparse state of each layer.\n"),t("em",[e._v("Deja Vu")]),e._v(" dynamically predicts the sparsity of the next layer using the activations of the previous layer. It determines which neurons of MLP blocks and the heads of attention blocks need to be retained. To mitigate the overhead of this predictor, Deja Vu asynchronously predicts the next layer."),t("br"),e._v(" "),t("em",[e._v("PowerInfer")]),e._v(" utilizes the sparsity of activation to dynamically predict the hotactivated neurons of the next layer and computes them on the GPU, whereas other cold-activated neurons are computed on the CPU.")])])]),e._v(" "),t("li",[t("strong",[e._v("Knowledge Distillation")]),e._v(": Black-box and white-box KD.")]),e._v(" "),t("li",[t("strong",[e._v("Quantization")]),e._v(": Quantization-aware training (QAT), post-training quantization (PTQ).\n"),t("ul",[t("li",[e._v("Quantization-aware training\n"),t("em",[e._v("LLM-QAT")]),e._v(" obtains training data for LLMs by leveraging pre-trained models to generate samples through data-free distillation. it quantizes weights, activations, and KV cache, thereby improving training throughput."),t("br"),e._v(" "),t("em",[e._v("QuantGPT")]),e._v(" incorporats contrastive distillation from a full-precision teacher model and distilling logit information to a quantized student model during autoregressive pre-training.\n"),t("em",[e._v("BitNet")]),e._v(" pioneers QAT for 1-bit language models, training the language model with 1-bit weights and activations.")]),e._v(" "),t("li",[e._v("Post-training quantization\n"),t("ul",[t("li",[e._v("Weights-only Quantization")]),e._v(" "),t("li",[e._v("Weights-Activation Coquantization"),t("br"),e._v(" "),t("em",[e._v("SmoothQuant takes advantage of the similarity in the channel-wise activations of different tokens and performs quantization on both weight and activation using per-channel scaling transforms."),t("br"),e._v("\nRPTQ recognizes the substantial range differences across different channels, reordering the channels for quantization and integrating them into layer normalization and linear layer weights."),t("br"),e._v("\nOliVe adopts outlier-victim pair quantization and locally processes outliers."),t("br"),e._v("\nOutlier Suppression+ builds upon Outlier Suppression, discovering that harmful outliers exhibit an asymmetric distribution mainly concentrated in specific channels. Considering the asymmetry of outliers and quantization errors from the weights of the next layer, this approach performs channel-level translation and scaling operations."),t("br"),e._v("\nQLLM addresses the issue of activation outliers through an adaptive channel reassembly method and mitigates the information loss caused by quantization using calibration data."),t("br"),e._v("\nLLM-FP4 quantizes weights into 4-bit float points, proposes per-channel activation quantization, and reparameters additional scaling factors as exponential biases of weights."),t("br"),e._v("\nZeroQuant combines layer-wise KD and optimized quantization support to achieve 8-bit quantization."),t("br"),e._v("\nFlexRound updates the quantization scale of weights and activations by minimizing the error between the quantized values and the full-precision values."),t("br"),e._v("\nATOM significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.")])])])])])]),e._v(" "),t("li",[t("strong",[e._v("Low-Rank Decomposition (LoRD)")]),e._v(": TensorGPT, LoSparse.")])]),e._v(" "),t("h2",{attrs:{id:"_3-resource-efficient-systems"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-resource-efficient-systems"}},[e._v("#")]),e._v(" 3. Resource-Efficient Systems")]),e._v(" "),t("h3",{attrs:{id:"_3-1-distributed-training"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-distributed-training"}},[e._v("#")]),e._v(" 3.1 Distributed Training")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Resilience")]),e._v(": Checkpointing, redundant computation.")]),e._v(" "),t("li",[t("strong",[e._v("Parallelism")]),e._v(": Data, model, and sequence parallelism.")]),e._v(" "),t("li",[t("strong",[e._v("Communication")]),e._v(": Compression, overlapping with computation.")]),e._v(" "),t("li",[t("strong",[e._v("Storage")]),e._v(": Offloading, heterogeneous GPUs.")]),e._v(" "),t("li",[t("strong",[e._v("MoE Optimization")]),e._v(": MegaBlocks, Tutel.")])]),e._v(" "),t("h3",{attrs:{id:"_3-2-hardware-aware-optimizations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-hardware-aware-optimizations"}},[e._v("#")]),e._v(" 3.2 Hardware-Aware Optimizations")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("EdgeBERT")]),e._v(": Latency-aware energy optimization.")]),e._v(" "),t("li",[t("strong",[e._v("FlightLLM")]),e._v(": FPGA-based LLM inference.")]),e._v(" "),t("li",[t("strong",[e._v("SpAtten")]),e._v(": Sparse attention with cascade token pruning.")])]),e._v(" "),t("h3",{attrs:{id:"_3-3-serving-on-cloud"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-serving-on-cloud"}},[e._v("#")]),e._v(" 3.3 Serving on Cloud")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Inference Accelerating")]),e._v(": Kernel optimization, request batching.")]),e._v(" "),t("li",[t("strong",[e._v("Memory Saving")]),e._v(": KV cache management, offloading.")]),e._v(" "),t("li",[t("strong",[e._v("Emerging Platforms")]),e._v(": Spot instances, heterogeneous GPUs.")])]),e._v(" "),t("h3",{attrs:{id:"_3-4-serving-on-edge"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-serving-on-edge"}},[e._v("#")]),e._v(" 3.4 Serving on Edge")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Edge-Cloud Collaboration")]),e._v(": EdgeFM.")]),e._v(" "),t("li",[t("strong",[e._v("On-Device MoE")]),e._v(": EdgeMoE, PC-MoE.")]),e._v(" "),t("li",[t("strong",[e._v("Memory Optimization")]),e._v(": LLMCad, PowerInfer.")]),e._v(" "),t("li",[t("strong",[e._v("I/O Optimization")]),e._v(": STI, LLM in a flash.")]),e._v(" "),t("li",[t("strong",[e._v("Kernel Optimization")]),e._v(": Integer-based edge kernels.")])])])}),[],!1,null,null,null);t.default=a.exports}}]);