(window.webpackJsonp=window.webpackJsonp||[]).push([[48],{461:function(e,a,t){"use strict";t.r(a);var s=t(5),r=Object(s.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("[248] Dissecting GPU Memory Hierarchy through Microbenchmarking")]),e._v(" "),a("li",[e._v("[75] Benchmarking the Memory Hierarchy of Modern GPUs")]),e._v(" "),a("li",[e._v("[18] Benchmarking the GPU memory at the warp level")]),e._v(" "),a("li",[e._v("[90] Dissecting the NVidia Turing T4 GPU via Microbenchmarking")]),e._v(" "),a("li",[e._v("[38] Exploring Modern GPU Memory System Design Challenges through Accurate Modeling üëç üëç üëç")]),e._v(" "),a("li",[e._v("[9] OSM: Off-Chip Shared Memory for GPUs")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_1-dissecting-gpu-memory-hierarchy-through-microbenchmarking"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-dissecting-gpu-memory-hierarchy-through-microbenchmarking"}},[e._v("#")]),e._v(" 1. Dissecting GPU Memory Hierarchy through Microbenchmarking")]),e._v(" "),a("p",[e._v("A paper in 2015, profile memory in Fermi, Kepler and Maxwell")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/683d67af-3feb-4d35-9ecf-dfeafb814c37",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"parameter"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parameter"}},[e._v("#")]),e._v(" Parameter")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/08215b14-4856-4d3a-8c6a-b5050f905f02",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/5daed100-0155-4fed-9358-e26681294b2a",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/60213279-226b-4a30-aa05-36271e9ac0ff",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"l1-data-cache"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#l1-data-cache"}},[e._v("#")]),e._v(" L1 Data Cache")]),e._v(" "),a("p",[e._v("On the Fermi and Kepler devices, the L1 data cache and shared memory are physically implemented together."),a("br"),e._v("\nOn the Maxwell devices, the L1 data cache is unified with the texture cache.")]),e._v(" "),a("p",[e._v("The 16 KB L1 cache has 128 cache lines mapped onto four cache ways."),a("br"),e._v("\nFor each cache way, 32 cache sets are divided into 8 major sets. Each major set contains 16 cache lines.")]),e._v(" "),a("p",[e._v("The data mapping is also unconventional."),a("br"),e._v("\nThe 12-13th bits in the memory address define the cache way, the 9-11th bits define the major set, and the 0-6th bits define the memory offset inside the cache line.\n"),a("img",{attrs:{src:"https://github.com/user-attachments/assets/f997bf94-4b5b-4948-882c-7f72dd7bd506",alt:"image"}})]),e._v(" "),a("p",[e._v("One distinctive feature of the Fermi L1 cache is that its replacement policy is not LRU, as pointed out by Meltzer et.al.\nAmong the four cache ways, cache way 2 is three times more likely to be replaced than the other three cache ways.")]),e._v(" "),a("p",[a("strong",[e._v("Another paper[4]")]),e._v(" We found that when the L1 data cache saturates, Turing randomly evicts 4 consecutive cache lines (128 B)."),a("br"),e._v("\nWe observed that once a block of cache lines are evicted, the second scan will cause more cache lines from the same set to be evicted.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/ae6a8abd-7d57-4e0c-98ea-12264a37ae75",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"l2-data-cache"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#l2-data-cache"}},[e._v("#")]),e._v(" L2 Data Cache")]),e._v(" "),a("ul",[a("li",[e._v("The replacement policy of the L2 cache is not LRU")]),e._v(" "),a("li",[a("strong",[e._v("The L2 cache line size is 32 bytes")]),e._v(" by observing the memory access pattern of overflowing the cache and visiting array element one by one.")]),e._v(" "),a("li",[e._v("The data mapping is sophisticated and not conventional bits-defined")]),e._v(" "),a("li",[e._v("a hardware-level pre-fetching mechanism from the DRAM to the L2 data cache on all three platforms."),a("br"),e._v(" "),a("strong",[e._v("The pre-fetching size is about 2/3 of the L2 cache size and the prefetching is sequential. This is deduced from that if we load an array smaller than 2/3 of the L2 data cache size, there is no cold cache miss patterns.")]),a("br"),e._v("\nüôã(Maybe they can cover the gap just by prefetching sequential line.)")])]),e._v(" "),a("h4",{attrs:{id:"global-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#global-memory"}},[e._v("#")]),e._v(" Global Memory")]),e._v(" "),a("p",[e._v("global memory access involves accessing the DRAM, L1 and L2 data caches, TLBs and page tables.")]),e._v(" "),a("h5",{attrs:{id:"global-memory-throughput"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#global-memory-throughput"}},[e._v("#")]),e._v(" Global Memory Throughput")]),e._v(" "),a("p",[e._v("The theoretical bandwidth is calculated as fmem * bus width * DDR factor.\n"),a("img",{attrs:{src:"https://github.com/user-attachments/assets/dbb8cdc6-e0cd-4bc6-aec8-f9450ea6d0bf",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/707f8b05-88e6-40b2-b3e4-426f984d4405",alt:"image"}})]),e._v(" "),a("p",[e._v("the throughput of a larger ILP saturates faster.")]),e._v(" "),a("p",[e._v("The GTX780 has the highest throughput as it benefits from the highest bus width,"),a("br"),e._v("\nbut its convergence speed is the slowest, i.e., it requires the most memory requests to hide the pipeline latency.")]),e._v(" "),a("p",[a("strong",[e._v("This could be part of the reason that NVIDIA reduced the bus width back to 256 bits in Maxwell devices.")])]),e._v(" "),a("h5",{attrs:{id:"global-memory-latency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#global-memory-latency"}},[e._v("#")]),e._v(" Global Memory Latency")]),e._v(" "),a("p",[a("strong",[e._v("The global memory access latency is the whole time accessing a data located in DRAM/L2 or L1 cache, including the latency of page table look-ups.")])]),e._v(" "),a("ul",[a("li",[e._v("very large s1 = 32 MB to construct the TLB/page table miss and cache miss (P5&P6)")]),e._v(" "),a("li",[e._v("set s2 = 1 MB to construct the L1 TLB hit but cache miss (P4)")]),e._v(" "),a("li",[e._v("After a total of 65 data accesses, 65 data lines are loaded into the cache."),a("br"),e._v("\nWe then visit the cached data lines with s1 again for several times, to construct cache hit but TLB miss (P2&P3).")]),e._v(" "),a("li",[e._v("set s3 = 1 element and repeatedly load the data in a cache line so that every memory access is a cache hit (P1).")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/e5c397e8-f4b4-46ba-b7ee-41c34fa08b33",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/1f616c8e-a758-4151-b1eb-61f15c810246",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/822fc563-d0dc-4708-afd2-89549adb7ec4",alt:"image"}})]),e._v(" "),a("ul",[a("li",[e._v("The Maxwell and Kepler devices have a unique memory access pattern (P6) for page table context switching. "),a("br"),e._v("\nWhen a kernel is launched, only memory page entries of 512 MB are activated. "),a("br"),e._v("\nIf the thread visits an inactivate page entry, the hardware needs a rather long time to switch between page tables."),a("br"),e._v("\nThis phenomena is also reported in [22] as page table ‚Äúmiss‚Äù.")]),e._v(" "),a("li",[e._v("The Maxwell L1 data cache addressing does not go through the TLBs or page tables."),a("br"),e._v("\nOn the GTX980, there is no TLB miss pattern (i.e., P2 and P3) when the L1 data cache is hit."),a("br"),e._v("\nOnce the L1 cache is missed, the access latency increases from tens of cycles to hundreds or even thousands of cycles.\n"),a("strong",[e._v("My comments: But if we look at GTX560Ti in P2, the latency is different with P1. So does this means that in Fermi, the memory request has to go through TLB first, and then access L1 DataCache? This might be the reason that the latency is longer. But this will degrade the performance....")])]),e._v(" "),a("li",[e._v("The TLBs are off-chip. we infer that the physical memory locations of the L1 TLB and L2 data cache are close. "),a("br"),e._v("\nThe physical memory locations of the L1 TLB and L2 TLB are also close, which means that the L1/L2 TLB and L2 data cache are shared off-chip by all SMs.")]),e._v(" "),a("li",[e._v("The GTX780 generally has the shortest global memory latencies, almost half that of the Fermi, with an access pattern of P2-P5."),a("br"),e._v("\nThe page table context switching of the GTX980 is also much more expensive than that of the GTX780.")])]),e._v(" "),a("p",[e._v("To summarize, the Maxwell device has "),a("em",[e._v("long global memory access latencies")]),e._v(" for cold cache misses and page table context switching."),a("br"),e._v("\nExcept for these rare access patterns, its access latency cycles are close to those of the Kepler device. "),a("br"),e._v("\nbecause the GTX980 has higher fmem than the GTX780, it actually offers the shortest global memory access time (P2-P4).")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/8d12e01f-1a6e-49e7-894c-28de28c9f864",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"shared-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory"}},[e._v("#")]),e._v(" Shared Memory")]),e._v(" "),a("p",[e._v("In CUDA programming, different CTAs assigned to the same SM have to share the same physical memory space."),a("br"),e._v("\nOn the Fermi and Kepler platforms, the shared memory is physically integrated with the L1 cache."),a("br"),e._v("\nOn the Maxwell platform, it occupies a separate memory space.\n"),a("strong",[e._v("Note that the shared memory and L1 cache are separated since Maxwell architecture.")])]),e._v(" "),a("p",[a("em",[e._v("Programmers")]),e._v(" move the data into and out of shared memory from global memory before and after arithmetic execution,"),a("br"),e._v("\nto avoid the frequent occurrence of long global memory access latencies.")]),e._v(" "),a("p",[a("strong",[e._v("We report a dramatic improvement in performance for the Maxwell device.")])]),e._v(" "),a("h5",{attrs:{id:"shared-memory-throughput"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-throughput"}},[e._v("#")]),e._v(" Shared Memory Throughput")]),e._v(" "),a("p",[e._v("the shared memory is organized as 32 memory banks [15]."),a("br"),e._v("\nThe bank width of the "),a("strong",[e._v("Fermi and Maxwell devices is 4 bytes")]),e._v(", while that of the Kepler device is 8 bytes.\nThe theoretical peak throughput of each SM (WSM) is calculated as fcore ‚àó Wbank ‚àó 32.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/3bf6da77-b196-4e13-b2fe-af410ee750a4",alt:"image"}})]),e._v(" "),a("p",[a("strong",[e._v("The achieved throughput per SM is calculated as 2 * fcore * sizeof(int) * (number of active threads per SM) * ILP / (total latency of each SM).")]),e._v("\nUsually a large value of ILP results in less active warps per SM."),a("br"),e._v("\nThe peak throughput W0SM denotes the respective maximum throughput of the abovecombinations."),a("br"),e._v("\nTwo key factors that affect the throughput are the number of active warps per SM and the ILP level.")]),e._v(" "),a("p",[e._v("The GTX980 reaches its peak throughput when the CTA size = 256, CTAs per SM = 2 and ILP = 8, i.e., 16 active warps per SM. The peak throughput is 137.41 GB/s, about "),a("em",[e._v("83.9%")]),e._v(" of the theoretical bandwidth.\nThe Maxwell device shows the best use of its shared memory bandwidth, and the Kepler device shows the worst.")]),e._v(" "),a("p",[e._v("GTX980 exhibits similar behavior as GTX780: high ILP is required to achieve high throughput for high SM occupancy.")]),e._v(" "),a("p",[e._v("According to Little‚Äôs Law, we roughly have: number of active warps * ILP = latency cycles * throughput.")]),e._v(" "),a("p",[a("strong",[e._v("GTX780 sucks in ILP = 1, since its limited 64 warps at most to be scheduled concurrently.")]),a("br"),e._v("\nWe consider this to be the main reason the achieved throughput of the GTX780 is poor compared with its designed value.")]),e._v(" "),a("h4",{attrs:{id:"shared-memory-latency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-latency"}},[e._v("#")]),e._v(" Shared Memory Latency")]),e._v(" "),a("p",[a("strong",[e._v("The shared memory latencies on Fermi, Kepler and Maxwell devices are 50, 47 and 28 cycles, respectively.")])]),e._v(" "),a("p",[a("strong",[e._v("Fermi and Maxwell devices have the same number of potential bank conflicts because they have the same architecture.")])]),e._v(" "),a("p",[e._v("The shared memory space is divided into 32 banks."),a("br"),e._v("\nSuccessive words are allocated to successive banks."),a("br"),e._v("\nIf two threads in the same warp access memory spaces in the same bank, a 2-way bank conflict occurs.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/c5ef66d3-c05e-46b7-84d6-ace224aafeab",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/c8e4560b-68aa-4188-9621-05a9f90fca32",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/f8e02655-9d5d-4d80-9bfb-fb6e5aefde8f",alt:"image"}})]),e._v(" "),a("p",[e._v("For the Fermi and Kepler devices, where there is a 32-way bank conflict, it takes much longer to access shared memory than regular global memory (TLB hit, cache miss). "),a("br"),e._v("\nSurprisingly, the effect of a bank conflict on shared memory access latency on the Maxwell device is mild."),a("br"),e._v("\nEven the longest shared memory access latency is still at the same level as L1 data cache latency.")]),e._v(" "),a("p",[e._v("In summary, although the shared memory has very short access latency, it can be rather long if there are many ways of bank conflicts."),a("br"),e._v("\nThis is most obvious on the Fermi hardware."),a("br"),e._v("\nThe Kepler device tries to solve it by doubling the bank width of shared memory."),a("br"),e._v("\nCompared with the Fermi, the Kepler‚Äôs 4-byte mode shared memory halves the chance of bank conflict, and the 8-byte mode reduces it further.")]),e._v(" "),a("p",[e._v("However, we also find that the Kepler‚Äôs shared memory is inefficient in terms of throughput."),a("br"),e._v("\nThe Maxwell device has the best shared memory performance."),a("br"),e._v("\nWith the same architecture as the Fermi device, the Maxwell hardware shows a 2x size, 2x memory access speedup and achieves the highest throughput."),a("br"),e._v("\nMost importantly, the Maxwell device‚Äôs shared memory has been optimized to avoid the long latency caused by bank conflicts.")]),e._v(" "),a("h4",{attrs:{id:"conclusion"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[e._v("#")]),e._v(" Conclusion")]),e._v(" "),a("p",[e._v("The memory capacity is significantly enhanced in both Kepler and Maxwell as compared with Fermi."),a("br"),e._v("\nThe Kepler device is performance-oriented and incorporates several aggressive elements in its design, such as increasing the bus width of DRAM and doubling the bank width of shared memory."),a("br"),e._v("\nThese designs have some side-effects."),a("br"),e._v("\nThe theoretical bandwidths of both global memory and shared memory are difficult to saturate, and hardware resources are imbalanced with a low utilization rate."),a("br"),e._v("\nThe Maxwell device has a more efficient and conservative design."),a("br"),e._v("\nIt has a reduced bus width and bank width, and the on-chip cache architectures are adjusted, including doubling the shared memory size and the read-only data cache size."),a("br"),e._v("\nFurthermore, it sharply decreases the shared memory latency caused under bank conflicts.")]),e._v(" "),a("h3",{attrs:{id:"_4-dissecting-the-nvidia-turing-t4-gpu-via-microbenchmarking"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-dissecting-the-nvidia-turing-t4-gpu-via-microbenchmarking"}},[e._v("#")]),e._v(" 4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking")]),e._v(" "),a("h4",{attrs:{id:"result"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#result"}},[e._v("#")]),e._v(" Result")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/eb21b04f-6ce8-44ef-8307-d26c35fa8a86",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/bae4d3b8-b2df-463b-a8f8-c095fbb53c9d",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"shared-memory-latency-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-latency-2"}},[e._v("#")]),e._v(" Shared Memory Latency")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/ae7a4300-20fb-4be0-9404-e1c39a223d7d",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"bandwidth"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bandwidth"}},[e._v("#")]),e._v(" Bandwidth")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/74dc0901-e5a8-4084-b6a1-d8d71175926f",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"_4-benchmarking-the-gpu-memory-at-the-warp-level"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-benchmarking-the-gpu-memory-at-the-warp-level"}},[e._v("#")]),e._v(" 4. Benchmarking the GPU memory at the warp level")]),e._v(" "),a("p",[e._v("In this work, we investigate the data accessing capability of a warp of threads: broadcasting and parallel accessing.\\")]),e._v(" "),a("ul",[a("li",[e._v("Broadcasting occurs when multiple threads access the same data element, i.e., multiple threads request a single data element (MTSD).")]),e._v(" "),a("li",[e._v("We refer the case of multiple threads accessing multiple distinct data elements (MTMD) as parallel accessing.")])]),e._v(" "),a("h5",{attrs:{id:"local-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#local-memory"}},[e._v("#")]),e._v(" Local Memory")]),e._v(" "),a("ul",[a("li",[e._v("For the simple memory access patterns, we should allocate a sufficient small array to guarantee that it is located in registers.")]),e._v(" "),a("li",[e._v("For the complex memory access patterns, we should simplify codes to exploit registers. For example, we merge a three-level loop into an one-level loop so that a larger temporal vector can be allocated in registers.")]),e._v(" "),a("li")]),e._v(" "),a("h5",{attrs:{id:"shared-memory-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-2"}},[e._v("#")]),e._v(" Shared Memory")]),e._v(" "),a("ul",[a("li",[e._v("Bank conflicts must be avoided by the ways of e.g., data padding.")]),e._v(" "),a("li",[e._v("Shared memory supports both broadcasting and parallel accessing.")]),e._v(" "),a("li",[e._v("Neither consecutively accessing nor aligned accessing is a must.")]),e._v(" "),a("li",[e._v("The latency decreases when the number of threads increase, and thus we should use a sufficiently large thread block.")]),e._v(" "),a("li",[e._v("Replacing global memory with shared memory, because the latency of shared memory is smaller than that of global memory.")]),e._v(" "),a("li",[e._v("Using shared memory bares an overhead (i.e., buffer allocation and data movement) and reusing data in it is a must for improved performance.")])]),e._v(" "),a("h5",{attrs:{id:"constant-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#constant-memory"}},[e._v("#")]),e._v(" Constant Memory")]),e._v(" "),a("p",[e._v("But constant memory does not support parallel accessing."),a("br"),e._v("\nThat is, constant memory can only be accessed serially when requesting different data elements."),a("br"),e._v("\nOn the one hand, constant memory is used to store a small amount of read-only data, which is not sensitive to bandwidth."),a("br"),e._v("\nSo parallel accessing is not a must for constant memory.")]),e._v(" "),a("ul",[a("li",[e._v("Constant memory supports the accessing capability of broadcasting.")]),e._v(" "),a("li",[e._v("Constant memory does not support parallel accessing, and satisfies parallel memory requests in a serial manner.")])]),e._v(" "),a("h5",{attrs:{id:"shared-memory-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-3"}},[e._v("#")]),e._v(" Shared Memory")]),e._v(" "),a("ul",[a("li",[e._v("Global memory supports both broadcasting and parallel accessing.")]),e._v(" "),a("li",[e._v("The data types of 4 or 8 bytes can obtain the near upper-bounded bandwidth of global memory, while the data types cannot."),a("br"),e._v("\nSo the char data should be coalesced into the char4 type for improved bandwidth.")]),e._v(" "),a("li",[e._v("Global memory accesses should be consecutive, but aligned accessing is not necessary for global memory.")]),e._v(" "),a("li",[e._v("When memory accessing is non-consecutive, the latency changes with the number of threads, but not with the number of blocks.\nSo we should configure the thread dimensionality.")])]),e._v(" "),a("h3",{attrs:{id:"_5-exploring-modern-gpu-memory-system-design-challenges-through-accurate-modeling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-exploring-modern-gpu-memory-system-design-challenges-through-accurate-modeling"}},[e._v("#")]),e._v(" 5. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling")]),e._v(" "),a("p",[e._v("üëç üëç üëç")]),e._v(" "),a("h5",{attrs:{id:"memory-coalescer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#memory-coalescer"}},[e._v("#")]),e._v(" Memory Coalescer")]),e._v(" "),a("p",[e._v("the eviction granularity of the cache is 128B, indicating that the L1 cache has 128B lines with 32B sectors."),a("br"),e._v("\nFurthermore, the coalescer operates across eight threads, i.e. the coalescer tries to coalesce each group of eight threads separately to generate sectored accesses.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/c3d84400-b121-4152-931c-c40074848909",alt:"image"}})]),e._v(" "),a("p",[e._v("When the stride=32, the memory access is converged, and all the threads within the same warp will access the same cache line,"),a("br"),e._v("\nhowever we receive four read accesses at L1 cache.")]),e._v(" "),a("p",[a("strong",[e._v("8 Thread register 32bit == 32Byte.")])]),e._v(" "),a("h5",{attrs:{id:"l2-cache"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#l2-cache"}},[e._v("#")]),e._v(" L2 Cache")]),e._v(" "),a("p",[e._v("L2 cache applies something similar to "),a("strong",[e._v("write-validate")]),e._v(" not "),a("strong",[e._v("fetch on write")]),e._v(".\\ üò±\nHowever, all the reads received by L2 caches from the coalescer are 32-byte sectored accesses."),a("br"),e._v("\nThus, the read access granularity (32 bytes) is different from the write access granularity (one byte)."),a("br"),e._v("\nTo handle this, the L2 cache applies a different write allocation policy, which we named lazy fetch-on-read, that is a compromise between write-validate and fetch-on-write.")]),e._v(" "),a("p",[e._v("When a sector read request is received to a modified sector, it first checks if the sector write-mask is complete, i.e. all the bytes have been written to and the line is\nfully readable."),a("br"),e._v("\nIf so, it reads the sector, otherwise, similar to fetch-on-write, it generates a read request for this sector and merges it with the modified bytes.")]),e._v(" "),a("h5",{attrs:{id:"streaming-throughput-oriented-l1-cache"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#streaming-throughput-oriented-l1-cache"}},[e._v("#")]),e._v(" Streaming Throughput-oriented L1 Cache")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/8edd3919-c6ff-40df-a207-a0853fcfa161",alt:"image"}})]),e._v(" "),a("p",[e._v("The L1 cache in Volta is what NVIDIA is calling a streaming cache [33]."),a("br"),e._v("\nIt is streaming because the documentation states that it allows "),a("strong",[e._v("unlimited cache misses")]),e._v(" to be in flight regardless the number of cache lines per cache set [10].")]),e._v(" "),a("p",[e._v("independent of the number of L1 configured size, the number of MSHRs available are the same, even if more of the on-chip SRAM storage is devoted to shared memory.")]),e._v(" "),a("p",[e._v("We believe that unified cache is a plain SRAM where sectored data blocks are shared between the L1D and the CUDA shared memory."),a("br"),e._v("\nIt can be configured adaptively by the driver as we discussed earlier."),a("br"),e._v("\nWe assume that the L1D‚Äôs TAG and MSHR merging functionality are combined together in a separate table structure (TAG-MSHR table)."),a("br"),e._v("\nSince, the filling policy is now ON FILL, we can have more TAG entries and outstanding requests than the assigned L1D cache lines.")]),e._v(" "),a("p",[e._v("If it is a hit to a reserved sector (i.e. the status is pending), it sets its corresponding warp bit in the merging mask (64 bits for 64 warps)."),a("br"),e._v("\nWhen the pending request comes back, it allocates a cache line/sector in the data block and sets the allocated block index in the table."),a("br"),e._v("\nThen, the merged warps access the sector, on a cycle-by-cyle basis.")]),e._v(" "),a("h3",{attrs:{id:"_6-osm-off-chip-shared-memory-for-gpus"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-osm-off-chip-shared-memory-for-gpus"}},[e._v("#")]),e._v(" 6. OSM: Off-Chip Shared Memory for GPUs")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/0fadce93-433a-4889-b130-e2c330d3b334",alt:"image"}})]),e._v(" "),a("p",[e._v("L1-D cache and shared memory use the same 32-bank memory structure (4 KB capacity per bank) as shown in Fig. 4;"),a("br"),e._v("\nhowever, they have some differences."),a("br"),e._v("\nWe can "),a("strong",[e._v("access 32-bit shared memory arrays via a thread-index directly")]),e._v(", while for accessing L1-D cache, we should read 128B (four 32B sectors) of the cache block."),a("br"),e._v("\nIn addition, L1 cache requires an extra hardware for managing tags and implementing LRU replacement policy.")])])}),[],!1,null,null,null);a.default=r.exports}}]);