(window.webpackJsonp=window.webpackJsonp||[]).push([[82],{543:function(e,n,i){"use strict";i.r(n);var o=i(8),a=Object(o.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("p",[e._v("Too many paper on llms...")]),e._v(" "),n("p",[n("strong",[e._v("Survey")])]),e._v(" "),n("ol",[n("li",[e._v("[C60 2024] Understanding LLMs: A Comprehensive Overview from Training to Inference")]),e._v(" "),n("li",[e._v("[C20 2024] Mobile Edge Intelligence for Large Language Models: A Contemporary Survey")]),e._v(" "),n("li",[e._v("[P 58] A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms")]),e._v(" "),n("li",[e._v("[P 30] A Comprehensive Evaluation of Quantization Strategies for Large Language Models")]),e._v(" "),n("li",[e._v("[P 8]  A Comprehensive Study on Quantization Techniques for Large Language Models")]),e._v(" "),n("li",[e._v("[C24] Model Compression and Efficient Inference for Large Language Models: A Survey")]),e._v(" "),n("li",[e._v("[C26 2024] Towards Better Chain-of-Thought Prompting Strategies: A Survey")]),e._v(" "),n("li",[e._v("[C24 2024] A Survey of Reasoning with Foundation Models")])]),e._v(" "),n("p",[n("strong",[e._v("KV Cache")])]),e._v(" "),n("ol",[n("li",[e._v("[C60 2024 USENIX] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management  üëç")]),e._v(" "),n("li",[e._v("[C1 2024]LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management")]),e._v(" "),n("li",[e._v("[C1 2024 wei] Unifying KV Cache Compression for Large Language Models with LeanKV")])]),e._v(" "),n("p",[n("strong",[e._v("Quantization")])]),e._v(" "),n("ol",[n("li",[e._v("[C8 Y2024] An Empirical Study of LLaMA3 Quantization: From LLMs to MLLMs")])]),e._v(" "),n("p",[n("strong",[e._v("Cross-layer Attention")])]),e._v(" "),n("ol",[n("li",[e._v("[C25 Y2024] "),n("strong",[e._v("Reducing Transformer Key-Value Cache Size")]),e._v(" with Cross-Layer Attention")]),e._v(" "),n("li",[e._v("[C4 2024] Cross-layer Attention Sharing for Large Language Models")])]),e._v(" "),n("p",[n("strong",[e._v("Attention")])]),e._v(" "),n("ol",[n("li",[e._v("[C573 2018] Efficient Attention: Attention With Linear Complexities")]),e._v(" "),n("li",[e._v("[C24 2024] Tensor Attention Training: Provably Efficient Learning of Higher-order Transformers")]),e._v(" "),n("li",[e._v("[2024] When Attention Sink Emerges in Language Models: An Empirical View üëç")]),e._v(" "),n("li",[e._v("[C40 2024] Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon")])]),e._v(" "),n("p",[n("strong",[e._v("Is All You Need")])]),e._v(" "),n("ol",[n("li",[e._v("[2025] Element-wise Attention Is All You Need")]),e._v(" "),n("li",[e._v("[2025] Tensor Product Attention Is All You Need")])]),e._v(" "),n("p",[n("strong",[e._v("Feedforward Layers")])]),e._v(" "),n("ol",[n("li",[e._v("[2024] Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers")]),e._v(" "),n("li",[e._v("[C3 2024] FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference")])]),e._v(" "),n("p",[n("strong",[e._v("Attatch Memory")])]),e._v(" "),n("ol",[n("li",[e._v("[C2 2024] Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU")]),e._v(" "),n("li",[e._v("[C63 2023] LLM in a flash: Efficient Large Language Model Inference with Limited Memory")])]),e._v(" "),n("p",[n("strong",[e._v("Novel LLM")])]),e._v(" "),n("ol",[n("li",[e._v("[C2 2024] Larimar: Large Language Models with Episodic Memory Control")])]),e._v(" "),n("p",[n("strong",[e._v("Batch")])]),e._v(" "),n("ol",[n("li",[e._v("[C59 2023] Batch Prompting: Efficient Inference with Large Language Model APIs")])]),e._v(" "),n("p",[n("strong",[e._v("Pruning")])]),e._v(" "),n("ol",[n("li",[e._v("[C25 2024] ZipLM: Inference-Aware Structured Pruning of Language Models")])]),e._v(" "),n("p",[n("strong",[e._v("Speculative decoding")])]),e._v(" "),n("ol",[n("li",[e._v("[C25 2024] Efficient Inference for Large Language Model-based Generative Recommendation")]),e._v(" "),n("li",[e._v("[C39 2024] Enhancing Inference Efficiency and Accuracy in Large Language Models through Next-Phrase Prediction  üëç")]),e._v(" "),n("li",[e._v("[C5 2023] Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy")]),e._v(" "),n("li",[e._v("[C64 2024] Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding")])]),e._v(" "),n("p",[n("strong",[e._v("Interesting")])]),e._v(" "),n("ol",[n("li",[e._v("[C60 2024] SnapKV: LLM Knows What You Are Looking for before Generation  üëç")]),e._v(" "),n("li",[e._v("[C6 2024] Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training")]),e._v(" "),n("li",[e._v("[C9 ISCA 2024] LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference")]),e._v(" "),n("li",[e._v("[C6 2024] Efficient Large Foundation Models Design: A Perspective From Model and System Co-Design")]),e._v(" "),n("li",[e._v("[C83 2023] Compressing Context to Enhance Inference Efficiency of Large Language Models  üëç")]),e._v(" "),n("li",[e._v("[C13 2024 OSDI] ServerlessLLM: Low-Latency Serverless Inference for Large Language Models")]),e._v(" "),n("li",[e._v("[C12 2024] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models")]),e._v(" "),n("li",[e._v("[C418 2022] Transformers Learn In-Context by Gradient Descent üëç")]),e._v(" "),n("li",[e._v("[C51 2024] Massive Activations in Large Language Models üëç")]),e._v(" "),n("li",[e._v("[C2007 2019] Generating Long Sequences with Sparse Transformers üëç")]),e._v(" "),n("li",[e._v("[C1890 2019] What does BERT look at? An Analysis of BERT‚Äôs Attention")]),e._v(" "),n("li",[e._v("[C402 2019] Analyzing the Structure of Attention in a Transformer Language Model")]),e._v(" "),n("li",[e._v("[C38517 2020] Language Models are Few-Shot Learners üëç")]),e._v(" "),n("li",[e._v("[C24 2024] What can a Single Attention Layer Learn? A Study Through the Random Features Lens")]),e._v(" "),n("li",[e._v("[C9 2023] Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps")]),e._v(" "),n("li",[e._v("[C606 2021] Transformer Feed-Forward Layers Are Key-Value Memories üëç")]),e._v(" "),n("li",[e._v("[Blog] The Feedforward Demystified: A Core Operation of Transformers")]),e._v(" "),n("li",[e._v("[C211 2024] ConvBERT: Improving BERT with Span-based Dynamic Convolution  üëç")]),e._v(" "),n("li",[e._v("[C15 2020] Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models")]),e._v(" "),n("li",[e._v("[C147 2023] Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective")]),e._v(" "),n("li",[e._v("[C24 2024] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")])]),e._v(" "),n("p",[n("strong",[e._v("Why Infer?")])]),e._v(" "),n("ol",[n("li",[e._v("[C78 2024] PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU  üëç")]),e._v(" "),n("li",[e._v("[C25 2024] Powerinfer-2: fast large language model inference on a smartphone")]),e._v(" "),n("li",[e._v("[C6 2024] InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference")])]),e._v(" "),n("p",[n("strong",[e._v("Chain of Thoughts")])]),e._v(" "),n("ol",[n("li",[e._v("[C1954 2024] Tree of Thoughts: Deliberate Problem Solving with Large Language Models üëç")]),e._v(" "),n("li",[e._v("[C795 2022] Automatic Chain of Thought Prompting in Large Language Models")]),e._v(" "),n("li",[e._v("[C109 2022] Iteratively Prompt Pre-trained Language Models for Chain of Thought")]),e._v(" "),n("li",[e._v("[C2 2024] Reducing Costs - The Path of Optimization for ChainofThought Reasoning via Sparse Attention Mechanism")]),e._v(" "),n("li",[e._v("[C17 2024] Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future")]),e._v(" "),n("li",[e._v("[C63 2024] Chain of Thought Empowers Transformers to Solve Inherently Serial Problems")])]),e._v(" "),n("p",[n("strong",[e._v("General Efficient")])]),e._v(" "),n("ol",[n("li",[e._v("[C44 2023] Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity")]),e._v(" "),n("li",[e._v("[C4 2024] Efficient Training and Inference: Techniques for Large Language Models Using Llama")]),e._v(" "),n("li",[e._v("[C226 2023] H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models")])]),e._v(" "),n("p",[n("strong",[e._v("Big tech")])]),e._v(" "),n("ol",[n("li",[e._v("[NVIDIA 1868] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism")]),e._v(" "),n("li",[e._v("[2579] Scaling Laws for Neural Language Models")]),e._v(" "),n("li",[e._v("[C510 2021] Baidu ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation")]),e._v(" "),n("li",[e._v("[C6 2024] Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent")]),e._v(" "),n("li",[e._v("[C1671] Qwen Technical Report")]),e._v(" "),n("li",[e._v("[C1212] Constitutional AI: Harmlessness from AI Feedback")]),e._v(" "),n("li",[e._v("[C1082] Scaling Language Models: Methods, Analysis & Insights from Training Gopher")]),e._v(" "),n("li",[e._v("[C11289] Llama 2: Open Foundation and Fine-Tuned Chat Models")])]),e._v(" "),n("p",[n("strong",[e._v("Hyperparameter")])]),e._v(" "),n("ol",[n("li",[e._v("[C2070 2020] Designing Network Design Spaces")]),e._v(" "),n("li",[e._v("[C25389 2019] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks")]),e._v(" "),n("li",[e._v("[C1422 2018] A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay")])])])}),[],!1,null,null,null);n.default=a.exports}}]);