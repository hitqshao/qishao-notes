(window.webpackJsonp=window.webpackJsonp||[]).push([[90],{549:function(e,t,s){"use strict";s.r(t);var a=s(8),r=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[C93 2021] Sparse is Enough in Scaling Transformers")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"c93-2021-sparse-is-enough-in-scaling-transformers"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#c93-2021-sparse-is-enough-in-scaling-transformers"}},[e._v("#")]),e._v(" [C93 2021] Sparse is Enough in Scaling Transformers")]),e._v(" "),t("p",[e._v("pretrained on C4.")]),e._v(" "),t("p",[t("strong",[e._v("Key Contributions:")])]),e._v(" "),t("ol",[t("li",[t("strong",[e._v("Sparse Feedforward Layers:")]),e._v(" "),t("ul",[t("li",[e._v("Uses dynamic sparsity by activating only a subset of weights during inference.")]),e._v(" "),t("li",[e._v("Reduces computational cost while maintaining similar perplexity to dense models.")])])])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/18175da6-5e32-4c01-88d7-36fcb8342a04",alt:"image"}})]),e._v(" "),t("ol",{attrs:{start:"2"}},[t("li",[t("strong",[e._v("Sparse Attention (QKV) Layers:")]),e._v(" "),t("ul",[t("li",[e._v("Uses a "),t("strong",[e._v("multiplicative layer")]),e._v("  to efficiently represent any permutation of input tokens.")]),e._v(" "),t("li",[e._v("Introduces convolution-based sparsity to further reduce the computational burden.")])])])]),e._v(" "),t("p",[e._v("Following figure (a) shows the multiplicative dense layer:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/69fde5c5-a906-429e-b6f0-073691afdf3b",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/669ddf38-eaad-4f86-b4c6-27e47f80206e",alt:"image"}})]),e._v(" "),t("p",[e._v('Please notice the "S" in formula and the output of the blue block is S*M dimension.')]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/90a4879f-1c9d-4a89-89a5-cf7e5f96f02c",alt:"image"}})]),e._v(" "),t("blockquote",[t("p",[e._v("We process this tensor with a two-dimensional convolutional layer, treating the length dimension and number of modules S like height and width of an image.\nThis layer uses M filters and a kernel size of F × F so that each filter looks at F modules (‘S’ axis) of the last F tokens\n(‘length’ axis).")])]),e._v(" "),t("ol",{attrs:{start:"3"}},[t("li",[t("strong",[e._v("Speed and Efficiency Gains:")]),e._v(" "),t("ul",[t("li",[e._v("Sparse feedforward and QKV layers achieve a "),t("strong",[e._v("3.05× speedup")]),e._v("  in decoding for an 800M parameter model and "),t("strong",[e._v("20× speedup")]),e._v("  for a 17B parameter model.")]),e._v(" "),t("li",[e._v("The full "),t("strong",[e._v("Terraformer")]),e._v("  model (with sparse layers and reversible architecture) achieves "),t("strong",[e._v("37× speedup")]),e._v("  in inference for large models.")])])])])])}),[],!1,null,null,null);t.default=r.exports}}]);