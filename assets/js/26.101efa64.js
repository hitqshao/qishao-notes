(window.webpackJsonp=window.webpackJsonp||[]).push([[26],{428:function(e,t,n){"use strict";n.r(t);var a=n(5),i=Object(a.a)({},(function(){var e=this._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":this.$parent.slotKey}},[e("ol",[e("li",[this._v("Efficient Memory Management for Large Language Model Serving with PagedAttention [2023]")])]),this._v(" "),e("hr"),this._v(" "),e("h3",{attrs:{id:"_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention"}},[this._v("#")]),this._v(" 1. Efficient Memory Management for Large Language Model Serving with PagedAttention")]),this._v(" "),e("p",[this._v("Disscussed the GEMM in prompt and GEMV in auto regression.\nIn GEMV, LLM is memory bound. There is lot of fragment in KVCache.\nIt also quantize the memory necessity for parameter in KV Cache.\nThey came up the method similar to paging in OS to manage KV in KV cache, reducing the fragment.")])])}),[],!1,null,null,null);t.default=i.exports}}]);