(window.webpackJsonp=window.webpackJsonp||[]).push([[59],{470:function(e,n,r){"use strict";r.r(n);var a=r(5),i=Object(a.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("ol",[n("li",[e._v("[354] Benchmarking TPU, GPU, and CPU Platforms for Deep Learning")]),e._v(" "),n("li",[e._v("[59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI")]),e._v(" "),n("li",[e._v("[12] Effective Elastic Scaling of Deep Learning Workloads")]),e._v(" "),n("li",[e._v("[30 Year:2024] LLM Inference Unveiled: Survey and Roofline Model Insights")]),e._v(" "),n("li",[e._v("[1] Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference")]),e._v(" "),n("li",[e._v("[Blog] LLM Inference Series: 5. Dissecting model performance")]),e._v(" "),n("li",[e._v("[47 Year:2024] Understanding LLMs: A Comprehensive Overview from Training to Inference")]),e._v(" "),n("li",[e._v("[37 Year:2022] Reveal training performance mystery between TensorFlow and PyTorch in the single GPU environment üëç  üëç  üëç  üëç  üëç")])]),e._v(" "),n("hr"),e._v(" "),n("h3",{attrs:{id:"_1-benchmarking-tpu-gpu-and-cpu-platforms-for-deep-learning"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-benchmarking-tpu-gpu-and-cpu-platforms-for-deep-learning"}},[e._v("#")]),e._v(" 1. Benchmarking TPU, GPU, and CPU Platforms for Deep Learning")]),e._v(" "),n("p",[e._v("Paper from Harvard")]),e._v(" "),n("p",[n("img",{attrs:{src:"https://github.com/user-attachments/assets/110ddbc0-1ddf-40fa-b360-9e3f589494c6",alt:"image"}})])])}),[],!1,null,null,null);n.default=i.exports}}]);