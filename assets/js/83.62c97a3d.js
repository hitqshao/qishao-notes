(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{544:function(t,e,i){"use strict";i.r(e);var a=i(8),n=Object(a.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[t._v("[1] Resource-efficient Algorithms and Systems of Foundation Models: A Survey")]),t._v(" "),e("hr"),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/539f3a36-1e67-486a-a1af-d7b637cfaf8e",alt:"image"}}),t._v(" "),e("em",[t._v("Source: Resource-efficient")])]),t._v(" "),e("p",[t._v("Computation complexity of attention is O(T 2D), whereas that of the FFN is O(TD2), where T represents the sequence length and D the hidden state dimension of the model."),e("br"),t._v("\nThe FFN layer is the most computationally intensive component.")]),t._v(" "),e("h2",{attrs:{id:"_1-resource-efficient-architectures"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-resource-efficient-architectures"}},[t._v("#")]),t._v(" 1. Resource-Efficient Architectures")]),t._v(" "),e("h3",{attrs:{id:"_1-1-efficient-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-efficient-attention"}},[t._v("#")]),t._v(" 1.1 Efficient Attention")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/d87f3dae-6a64-46ed-af28-51b4f2c16f3d",alt:"image"}})]),t._v(" "),e("ul",[e("li",[e("p",[e("strong",[t._v("Sparse Attention")]),t._v(": Reduces complexity (e.g., Longformer, BigBird)."),e("br"),t._v("\nMotivated by graph sparsification, sparse attention aims to build a sparse attention matrix."),e("br"),t._v("\nThis approach aims to retain the empirical advantages of a fully quadratic self-attention scheme while employing a reduced number of inner products.")])]),t._v(" "),e("li",[e("p",[e("strong",[t._v("Approximate Attention")]),t._v(": Low-rank approximations (e.g., Linformer, Reformer).\nApproximate attention mainly includes low-rank approximations of the self-attention matrix and innovative reformulations of the self-attention.")])]),t._v(" "),e("li",[e("p",[e("strong",[t._v("Attention-Free Approaches")]),t._v(": Alternatives like Hyena, Mamba."),e("br"),t._v("\nDespite the dominance of attention-based transformer architectures in large FMs, several works have put forth innovative architectures that hold the potential to replace the traditional transformer model.")])])]),t._v(" "),e("h3",{attrs:{id:"_1-2-dynamic-neural-network"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-dynamic-neural-network"}},[t._v("#")]),t._v(" 1.2 Dynamic Neural Network")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/baefb07e-0c3b-46c2-a728-5cb2e94a21b5",alt:"image"}})]),t._v(" "),e("ul",[e("li",[e("p",[e("strong",[t._v("Mixture of Experts (MoE)")]),t._v(": (e.g., Switch Transformer, GLaM, MoEfication, FFF).")])]),t._v(" "),e("li",[e("p",[e("strong",[t._v("Early Exiting")]),t._v(": Premature termination based on confidence (e.g., FREE, SkipDecode,DeeBERT, PABEE).\nearly-exiting optimization is a strategy that allows a model to terminate its computational process prematurely when it attains high confidence in the prediction or encounters resource constraints.")])])]),t._v(" "),e("h3",{attrs:{id:"_1-3-diffusion-specific-optimization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-diffusion-specific-optimization"}},[t._v("#")]),t._v(" 1.3 Diffusion-Specific Optimization")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Efficient Sampling")])]),t._v(" "),e("li",[e("strong",[t._v("Diffusion in Latent Space")])]),t._v(" "),e("li",[e("strong",[t._v("Diffusion Architecture Variants")])])]),t._v(" "),e("h3",{attrs:{id:"_1-4-vit-specific-optimizations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-vit-specific-optimizations"}},[t._v("#")]),t._v(" 1.4 ViT-Specific Optimizations")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Efficient ViT Variants")]),t._v(": MobileViT, EfficientFormer, EdgeViT.")])]),t._v(" "),e("h2",{attrs:{id:"_2-resource-efficient-algorithms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-resource-efficient-algorithms"}},[t._v("#")]),t._v(" 2. Resource-Efficient Algorithms")]),t._v(" "),e("h3",{attrs:{id:"_2-1-pre-training-algorithms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-pre-training-algorithms"}},[t._v("#")]),t._v(" 2.1 Pre-Training Algorithms")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Training Data Quality Control")]),t._v(": DataComp, DFN."),e("br"),t._v("\nA portion of work focus on controlling the quality of training data.")]),t._v(" "),e("li",[e("strong",[t._v("Training Data Reduction")]),t._v(": Deduplication, image patch removal."),e("br"),t._v("\nPre-training for large FMs needs a dataset at the trillion scale, exemplified by 0.3 trillion tokens for GPT-3-175B [25] and 2 trillion tokens for LLaMa-2-70B [238]."),e("br"),t._v("\nprior literature resorts to reduce vast training data through two aspects: deduplicating text datasets and image patch removal.")]),t._v(" "),e("li",[e("strong",[t._v("Progressive Learning")]),t._v(": StackingBERT, CompoundGrow.\nProgressive learning is a training strategy that begins by training a small model and then gradually increases the model size, throughout the training process.")]),t._v(" "),e("li",[e("strong",[t._v("Mixed Precision Training")]),t._v(": Mesa, GACT.\nMixed precision training often utilizes half-precision floating-point data representation instead of single precision. This approach significantly reduces memory\nrequirements, approximately halving the storage space needed for weights, activations, and gradients.")])]),t._v(" "),e("h3",{attrs:{id:"_2-2-fine-tuning-algorithms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-fine-tuning-algorithms"}},[t._v("#")]),t._v(" 2.2 Fine-Tuning Algorithms")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://github.com/user-attachments/assets/1ed489c1-3451-4874-8f36-fa99fc65e0ca",alt:"image"}})]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Additive Tuning")]),t._v(":\n"),e("ul",[e("li",[e("em",[t._v("Adapter tuning")]),t._v(" aims to reduce training costs by introducing adapter modules to specific layers (or all layers) of pre-trained large FMs.\nDuring tuning, the backbone of the pre-trained model remains frozen, and adapter modules are utilized to acquire task-specific knowledge.")]),t._v(" "),e("li",[e("em",[t._v("prompt tuning")]),t._v(" involves designing a task-specific prompt for each task, with the aim of replacing the traditional fine-tuning of pre-trained large FMs parameters."),e("br"),t._v("\nBy tuning the input prompts instead, this method significantly reduces the resources and time required for the fine-tuning.")]),t._v(" "),e("li",[e("em",[t._v("prefix tuning")]),t._v(" introduces a trainable, task-specific prefix part to each layer of large FMs.\nThis technique aims to reduce the tuning cost by limiting the updates to the parameters in this prefix.")])])]),t._v(" "),e("li",[e("strong",[t._v("Selective Tuning")]),t._v(": Freezing most parameters, selective updates.")]),t._v(" "),e("li",[e("strong",[t._v("Re-parameter Tuning")]),t._v(": Low-rank adaptation (e.g., LoRA, Delta-LoRA).")])]),t._v(" "),e("h3",{attrs:{id:"_2-3-inference-algorithms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-inference-algorithms"}},[t._v("#")]),t._v(" 2.3 Inference Algorithms")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Opportunistic Decoding")]),t._v(": Speculative decoding, look-ahead decoding.")]),t._v(" "),e("li",[e("strong",[t._v("Input Filtering and Compression")]),t._v(": Prompt compression, token pruning.")]),t._v(" "),e("li",[e("strong",[t._v("KV Cache Optimization")]),t._v(": Memory-efficient sparse attention.")]),t._v(" "),e("li",[e("strong",[t._v("Long Context Handling")]),t._v(": LM-Infinite, StreamingLLM.")])]),t._v(" "),e("h3",{attrs:{id:"_2-4-model-compression"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-model-compression"}},[t._v("#")]),t._v(" 2.4 Model Compression")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Pruning")]),t._v(": Structured, unstructured, and contextual pruning.")]),t._v(" "),e("li",[e("strong",[t._v("Knowledge Distillation")]),t._v(": Black-box and white-box KD.")]),t._v(" "),e("li",[e("strong",[t._v("Quantization")]),t._v(": Quantization-aware training (QAT), post-training quantization (PTQ).")]),t._v(" "),e("li",[e("strong",[t._v("Low-Rank Decomposition (LoRD)")]),t._v(": TensorGPT, LoSparse.")])]),t._v(" "),e("h2",{attrs:{id:"_3-resource-efficient-systems"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-resource-efficient-systems"}},[t._v("#")]),t._v(" 3. Resource-Efficient Systems")]),t._v(" "),e("h3",{attrs:{id:"_3-1-distributed-training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-distributed-training"}},[t._v("#")]),t._v(" 3.1 Distributed Training")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Resilience")]),t._v(": Checkpointing, redundant computation.")]),t._v(" "),e("li",[e("strong",[t._v("Parallelism")]),t._v(": Data, model, and sequence parallelism.")]),t._v(" "),e("li",[e("strong",[t._v("Communication")]),t._v(": Compression, overlapping with computation.")]),t._v(" "),e("li",[e("strong",[t._v("Storage")]),t._v(": Offloading, heterogeneous GPUs.")]),t._v(" "),e("li",[e("strong",[t._v("MoE Optimization")]),t._v(": MegaBlocks, Tutel.")])]),t._v(" "),e("h3",{attrs:{id:"_3-2-hardware-aware-optimizations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-hardware-aware-optimizations"}},[t._v("#")]),t._v(" 3.2 Hardware-Aware Optimizations")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("EdgeBERT")]),t._v(": Latency-aware energy optimization.")]),t._v(" "),e("li",[e("strong",[t._v("FlightLLM")]),t._v(": FPGA-based LLM inference.")]),t._v(" "),e("li",[e("strong",[t._v("SpAtten")]),t._v(": Sparse attention with cascade token pruning.")])]),t._v(" "),e("h3",{attrs:{id:"_3-3-serving-on-cloud"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-serving-on-cloud"}},[t._v("#")]),t._v(" 3.3 Serving on Cloud")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Inference Accelerating")]),t._v(": Kernel optimization, request batching.")]),t._v(" "),e("li",[e("strong",[t._v("Memory Saving")]),t._v(": KV cache management, offloading.")]),t._v(" "),e("li",[e("strong",[t._v("Emerging Platforms")]),t._v(": Spot instances, heterogeneous GPUs.")])]),t._v(" "),e("h3",{attrs:{id:"_3-4-serving-on-edge"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-serving-on-edge"}},[t._v("#")]),t._v(" 3.4 Serving on Edge")]),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Edge-Cloud Collaboration")]),t._v(": EdgeFM.")]),t._v(" "),e("li",[e("strong",[t._v("On-Device MoE")]),t._v(": EdgeMoE, PC-MoE.")]),t._v(" "),e("li",[e("strong",[t._v("Memory Optimization")]),t._v(": LLMCad, PowerInfer.")]),t._v(" "),e("li",[e("strong",[t._v("I/O Optimization")]),t._v(": STI, LLM in a flash.")]),t._v(" "),e("li",[e("strong",[t._v("Kernel Optimization")]),t._v(": Integer-based edge kernels.")])])])}),[],!1,null,null,null);e.default=n.exports}}]);