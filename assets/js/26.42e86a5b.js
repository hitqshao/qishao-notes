(window.webpackJsonp=window.webpackJsonp||[]).push([[26],{428:function(e,n,a){"use strict";a.r(n);var t=a(5),i=Object(t.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("ol",[n("li",[e._v("Efficient Memory Management for Large Language Model Serving with PagedAttention [2023]")]),e._v(" "),n("li",[e._v("LLM in a flash: Efficient Large Language Model Inference with Limited Memory [Apple 2023]")])]),e._v(" "),n("hr"),e._v(" "),n("h3",{attrs:{id:"_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_1-efficient-memory-management-for-large-language-model-serving-with-pagedattention"}},[e._v("#")]),e._v(" 1. Efficient Memory Management for Large Language Model Serving with PagedAttention")]),e._v(" "),n("p",[e._v("Disscussed the GEMM in prompt and GEMV in auto regression.\nIn GEMV, LLM is memory bound. There is lot of fragment in KVCache.\nIt also quantize the memory necessity for parameter in KV Cache.\nThey came up the method similar to paging in OS to manage KV in KV cache, reducing the fragment.")]),e._v(" "),n("h3",{attrs:{id:"_2-llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_2-llm-in-a-flash-efficient-large-language-model-inference-with-limited-memory"}},[e._v("#")]),e._v(" 2. LLM in a flash: Efficient Large Language Model Inference with Limited Memory")]),e._v(" "),n("p",[e._v("Upproject matrix and downprojection matrix:\nhttps://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/\nRelated paper:\nParameter-Efficient Transfer Learning for NLP")])])}),[],!1,null,null,null);n.default=i.exports}}]);