(window.webpackJsonp=window.webpackJsonp||[]).push([[30],{480:function(t,e,r){"use strict";r.r(e);var a=r(8),n=Object(a.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("hr"),t._v(" "),e("h2",{attrs:{id:"_1-model-parsing-and-relay-ir-construction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-model-parsing-and-relay-ir-construction"}},[t._v("#")]),t._v(" 1. Model Parsing and Relay IR Construction")]),t._v(" "),e("p",[t._v("In TVM, high-level optimization in the Relay IR phase includes several graph-level optimizations, data layout transformations, and other functional passes.\nThese optimizations are implemented in various source files under src/relay/transform and src/relay/op directories.\nBelow is a categorized list of these optimizations along with their corresponding source code files and functions:")]),t._v(" "),e("h3",{attrs:{id:"_1-1-graph-level-optimizations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-graph-level-optimizations"}},[t._v("#")]),t._v(" 1.1 Graph-Level Optimizations")]),t._v(" "),e("p",[t._v("Graph-level optimizations restructure or simplify the computation graph for better performance.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Optimization\tSource")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Constant Folding")]),t._v(" "),e("td",[t._v("src/relay/transform/fold_constant.cc")]),t._v(" "),e("td",[t._v("FoldConstant, ConstantFolder")])]),t._v(" "),e("tr",[e("td",[t._v("Operator Fusion")]),t._v(" "),e("td",[t._v("src/relay/transform/fuse_ops.cc")]),t._v(" "),e("td",[t._v("FuseOps, FuseMutator, PatternMatcher")])]),t._v(" "),e("tr",[e("td",[t._v("Dead Code Elimination (DCE)")]),t._v(" "),e("td",[t._v("src/relay/transform/eliminate_common_subexpr.cc")]),t._v(" "),e("td",[t._v("EliminateCommonSubexpr")])]),t._v(" "),e("tr",[e("td",[t._v("Common Subexpression Elimination")]),t._v(" "),e("td",[t._v("src/relay/transform/eliminate_common_subexpr.cc")]),t._v(" "),e("td",[t._v("EliminateCommonSubexpr")])]),t._v(" "),e("tr",[e("td",[t._v("Simplify Inference")]),t._v(" "),e("td",[t._v("src/relay/transform/simplify_inference.cc")]),t._v(" "),e("td",[t._v("SimplifyInference, SimplifyInferenceMutator")])]),t._v(" "),e("tr",[e("td",[t._v("Call Folding")]),t._v(" "),e("td",[t._v("src/relay/transform/fold_call.cc")]),t._v(" "),e("td",[t._v("FoldCall")])]),t._v(" "),e("tr",[e("td",[t._v("Inline Functions")]),t._v(" "),e("td",[t._v("src/relay/transform/inline.cc")]),t._v(" "),e("td",[t._v("Inline, InlineMutator")])]),t._v(" "),e("tr",[e("td",[t._v("Prune Unused Functions")]),t._v(" "),e("td",[t._v("src/relay/transform/prune_unused_functions.cc")]),t._v(" "),e("td",[t._v("PruneUnusedFunctions")])])])]),t._v(" "),e("h3",{attrs:{id:"_1-2-data-layout-transformations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-data-layout-transformations"}},[t._v("#")]),t._v(" 1.2 Data Layout Transformations")]),t._v(" "),e("p",[t._v("These optimizations adjust the layout of tensors for better memory access patterns and compatibility with target hardware.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Transformation")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Alter Layout")]),t._v(" "),e("td",[t._v("src/relay/transform/alter_op_layout.cc")]),t._v(" "),e("td",[t._v("AlterOpLayout, AlterOpLayoutRewriter")])]),t._v(" "),e("tr",[e("td",[t._v("Convert Layout")]),t._v(" "),e("td",[t._v("s\tsrc/relay/transform/convert_layout.cc")]),t._v(" "),e("td",[t._v("ConvertLayout")])]),t._v(" "),e("tr",[e("td",[t._v("Fold Scale Axis")]),t._v(" "),e("td",[t._v("src/relay/transform/fold_scale_axis.cc")]),t._v(" "),e("td",[t._v("FoldScaleAxis, ScaleAxisSimplifier")])]),t._v(" "),e("tr",[e("td",[t._v("Layout Optimization")]),t._v(" "),e("td",[t._v("src/relay/transform/layout_rewrite.cc")]),t._v(" "),e("td",[t._v("LayoutRewrite")])])])]),t._v(" "),e("h3",{attrs:{id:"_1-3-quantization-and-precision-management"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-quantization-and-precision-management"}},[t._v("#")]),t._v(" 1.3 Quantization and Precision Management")]),t._v(" "),e("p",[t._v("TVM supports quantization optimizations for reduced precision operations.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Optimization")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Quantize")]),t._v(" "),e("td",[t._v("src/relay/quantize/quantize.cc")]),t._v(" "),e("td",[t._v("Quantize, CreateQuantizePass")])]),t._v(" "),e("tr",[e("td",[t._v("Dequantize")]),t._v(" "),e("td",[t._v("src/relay/quantize/dequantize.cc")]),t._v(" "),e("td",[t._v("Dequantize")])]),t._v(" "),e("tr",[e("td",[t._v("SimplifyQuantize")]),t._v(" "),e("td",[t._v("src/relay/transform/simplify_quantize.cc")]),t._v(" "),e("td",[t._v("SimplifyQuantize, SimplifyQuantizeRewriter")])])])]),t._v(" "),e("h3",{attrs:{id:"_1-4-automatic-differentiation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-automatic-differentiation"}},[t._v("#")]),t._v(" 1.4 Automatic Differentiation")]),t._v(" "),e("p",[t._v("TVM includes an autodiff system for neural networks.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Transformation")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Reverse Mode Autodiff")]),t._v(" "),e("td",[t._v("src/relay/transforms/gradient.cc")]),t._v(" "),e("td",[t._v("AutomaticDifferentiation, ReverseAD")])])])]),t._v(" "),e("h3",{attrs:{id:"_1-5-high-level-hardware-aware-optimizations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-5-high-level-hardware-aware-optimizations"}},[t._v("#")]),t._v(" 1.5 High-Level Hardware-Aware Optimizations")]),t._v(" "),e("p",[t._v("These optimizations modify operations based on the target hardware.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Optimization")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Annotate Target")]),t._v(" "),e("td",[t._v("src/relay/transform/annotate_target.cc")]),t._v(" "),e("td",[t._v("AnnotateTarget")])]),t._v(" "),e("tr",[e("td",[t._v("Partition Graph")]),t._v(" "),e("td",[t._v("src/relay/transform/partition_graph.cc")]),t._v(" "),e("td",[t._v("PartitionGraph")])]),t._v(" "),e("tr",[e("td",[t._v("Merge Compiler Regions")]),t._v(" "),e("td",[t._v("src/relay/transform/merge_compiler_regions.cc")]),t._v(" "),e("td",[t._v("MergeCompilerRegions")])])])]),t._v(" "),e("h3",{attrs:{id:"_1-6-device-placement"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-6-device-placement"}},[t._v("#")]),t._v(" 1.6 Device Placement")]),t._v(" "),e("p",[t._v("These passes assign operations to devices for heterogeneous execution.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Transformation")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Rewrite Annotated Ops")]),t._v(" "),e("td",[t._v("src/relay/transform/rewrite_annotated_ops.cc")]),t._v(" "),e("td",[t._v("RewriteAnnotatedOps")])]),t._v(" "),e("tr",[e("td",[t._v("Device Annotation")]),t._v(" "),e("td",[t._v("src/relay/transform/device_annotation.cc")]),t._v(" "),e("td",[t._v("DeviceAnnotation")])])])]),t._v(" "),e("h3",{attrs:{id:"_1-7-meta-pass-management"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-7-meta-pass-management"}},[t._v("#")]),t._v(" 1.7 Meta-Pass Management")]),t._v(" "),e("p",[t._v("Relay provides a meta-pass system to manage and sequence passes.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Meta-Pass")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Sequential Pass Manager")]),t._v(" "),e("td",[t._v("src/relay/transform/sequential.cc")]),t._v(" "),e("td",[t._v("Sequential, PassManager")])]),t._v(" "),e("tr",[e("td",[t._v("Pass Context")]),t._v(" "),e("td",[t._v("src/relay/transform/pass.cc")]),t._v(" "),e("td",[t._v("PassContext, WithPassContext")])])])]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-lowering-to-tensor-expression-te-scheduing-in-te-and-lowering-into-tir"}},[t._v("#")]),t._v(" 2 Lowering to Tensor Expression (TE), Scheduing in TE and Lowering into TIR")]),t._v(" "),e("p",[t._v("The lowering process from Relay IR to Tensor Expression (TE) and Tensor IR (TIR) in TVM involves multiple phases.\nThese include converting Relay IR to TE, applying tensor computation abstractions, performing scheduling, and constructing device-specific low-level TIR.\nHereâ€™s a detailed breakdown of the corresponding TVM source code files and functions for these stages:")]),t._v(" "),e("h3",{attrs:{id:"_2-1-converting-relay-ir-to-tensor-expression-te"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-converting-relay-ir-to-tensor-expression-te"}},[t._v("#")]),t._v(" 2.1 Converting Relay IR to Tensor Expression (TE)")]),t._v(" "),e("p",[t._v("This phase converts high-level Relay IR into the computation abstractions provided by TE.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Process")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Relay to TE Lowering")]),t._v(" "),e("td",[t._v("src/relay/backend/te_compiler.cc")]),t._v(" "),e("td",[t._v("LowerToTE, CreateSchedule, ScheduleGetter")])]),t._v(" "),e("tr",[e("td",[t._v("Operator Strategy")]),t._v(" "),e("td",[t._v("src/relay/op/strategy/generic.cc")]),t._v(" "),e("td",[t._v("GenericFunc, OpStrategy")])]),t._v(" "),e("tr",[e("td",[t._v("Relay to TE Bridge")]),t._v(" "),e("td",[t._v("src/relay/backend/te_compiler_cache.cc")]),t._v(" "),e("td",[t._v("TECompiler, LowerTE")])]),t._v(" "),e("tr",[e("td",[t._v("Shape Function Lowering")]),t._v(" "),e("td",[t._v("src/relay/backend/te_compiler.cc")]),t._v(" "),e("td",[t._v("LowerShapeFunc")])])])]),t._v(" "),e("p",[t._v("Explanation:")]),t._v(" "),e("ul",[e("li",[t._v("The Relay IR graph is analyzed, and for each operator, TVM retrieves a corresponding TE function using OpStrategy.")]),t._v(" "),e("li",[t._v("TE functions define high-level operations like matrix multiplication, element-wise addition, etc.")])]),t._v(" "),e("h3",{attrs:{id:"_2-2-abstraction-of-computation-in-tensor-expression-te"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-abstraction-of-computation-in-tensor-expression-te"}},[t._v("#")]),t._v(" 2.2 Abstraction of Computation in Tensor Expression (TE)")]),t._v(" "),e("p",[t._v("TE provides a declarative way to express computation. This includes operations like tiling, unrolling, and vectorizing.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Process")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Tensor Expression Build")]),t._v(" "),e("td",[t._v("src/te/operation/create_primfunc.cc")]),t._v(" "),e("td",[t._v("CreatePrimFunc, ComputeBody, ScheduleOps")])]),t._v(" "),e("tr",[e("td",[t._v("Compute Definition")]),t._v(" "),e("td",[t._v("src/te/operation/compute_op.cc")]),t._v(" "),e("td",[t._v("ComputeOpNode, ComputeOp")])]),t._v(" "),e("tr",[e("td",[t._v("Tensor Compute Intrinsics")]),t._v(" "),e("td",[t._v("src/te/operation/tensorize.cc")]),t._v(" "),e("td",[t._v("Tensorize, CreateIntrinBody")])])])]),t._v(" "),e("p",[t._v("Explanation:")]),t._v(" "),e("ul",[e("li",[t._v("High-level computations are abstracted into a declarative format using ComputeOp.")]),t._v(" "),e("li",[t._v("Intrinsic support for tensorization is added for specialized hardware operations.")])]),t._v(" "),e("h3",{attrs:{id:"_2-3-scheduling-in-tensor-expression"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-scheduling-in-tensor-expression"}},[t._v("#")]),t._v(" 2.3 Scheduling in Tensor Expression")]),t._v(" "),e("p",[t._v("Scheduling is where TVM optimizes how computations are performed on the target device.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Process")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Tile, Unroll, Vectorize")]),t._v(" "),e("td",[t._v("src/te/schedule/schedule_dataflow_rewrite.cc")]),t._v(" "),e("td",[t._v("ScheduleDataFlowRewrite, Tile, Unroll, Vectorize")])]),t._v(" "),e("tr",[e("td",[t._v("Thread and Block Mapping")]),t._v(" "),e("td",[t._v("src/te/schedule/schedule_lang.cc")]),t._v(" "),e("td",[t._v("bind, split, reorder, fuse")])]),t._v(" "),e("tr",[e("td",[t._v("AutoScheduler Interface")]),t._v(" "),e("td",[t._v("src/auto_scheduler/compute_dag.cc")]),t._v(" "),e("td",[t._v("ComputeDAG, ApplySteps")])]),t._v(" "),e("tr",[e("td",[t._v("Lowering Schedule to TIR")]),t._v(" "),e("td",[t._v("src/te/schedule/graph.cc")]),t._v(" "),e("td",[t._v("ScheduleGraph, LowerSchedule")])])])]),t._v(" "),e("p",[t._v("Explanation:")]),t._v(" "),e("ul",[e("li",[t._v("This phase defines how computations should be split into smaller tiles, mapped to hardware threads and blocks, and optimized using unrolling and vectorization.")]),t._v(" "),e("li",[t._v("Tensor schedules are converted into lower-level forms through ScheduleGraph.")])]),t._v(" "),e("h3",{attrs:{id:"_2-4-constructing-low-level-tensor-ir-tir"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-constructing-low-level-tensor-ir-tir"}},[t._v("#")]),t._v(" 2.4 Constructing Low-Level Tensor IR (TIR)")]),t._v(" "),e("p",[t._v("TIR represents a low-level, device-specific IR used to generate target-specific code.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Process")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("TIR Construction")]),t._v(" "),e("td",[t._v("src/tir/stmt_functor.cc")]),t._v(" "),e("td",[t._v("StmtFunctor, VisitStmt, MakeStmt")])]),t._v(" "),e("tr",[e("td",[t._v("Lowering to TIR")]),t._v(" "),e("td",[t._v("src/tir/transforms/lower_tir.cc")]),t._v(" "),e("td",[t._v("LowerTIR, TransformTIR")])]),t._v(" "),e("tr",[e("td",[t._v("Memory Planning")]),t._v(" "),e("td",[t._v("src/tir/transforms/storage_rewrite.cc")]),t._v(" "),e("td",[t._v("StorageRewrite, PlanMemory")])]),t._v(" "),e("tr",[e("td",[t._v("Device-Specific TIR")]),t._v(" "),e("td",[t._v("src/target/codegen.cc")]),t._v(" "),e("td",[t._v("Build, BuildIRModule")])])])]),t._v(" "),e("p",[t._v("Explanation:")]),t._v(" "),e("ul",[e("li",[t._v("TE schedules are converted into TIR, which provides explicit control over memory accesses and device-specific optimizations.")]),t._v(" "),e("li",[t._v("StorageRewrite optimizes memory allocation and reuse.")])]),t._v(" "),e("h3",{attrs:{id:"_2-5-device-specific-optimizations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-device-specific-optimizations"}},[t._v("#")]),t._v(" 2.5 Device-Specific Optimizations")]),t._v(" "),e("p",[t._v("Device-specific optimizations tailor the generated code for the target platform (e.g., CUDA).")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Transformation")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Thread/Block Mapping")]),t._v(" "),e("td",[t._v("src/tir/transforms/thread_storage_sync.cc")]),t._v(" "),e("td",[t._v("ThreadStorageSync")])]),t._v(" "),e("tr",[e("td",[t._v("Loop Partitioning")]),t._v(" "),e("td",[t._v("src/tir/transforms/loop_partition.cc")]),t._v(" "),e("td",[t._v("LoopPartition")])]),t._v(" "),e("tr",[e("td",[t._v("Device Codegen")]),t._v(" "),e("td",[t._v("src/target/source/codegen_cuda.cc")]),t._v(" "),e("td",[t._v("CodeGenCUDA, PrintKernel")])])])]),t._v(" "),e("p",[t._v("High-Level Summary of the Workflow")]),t._v(" "),e("ul",[e("li",[t._v("Relay to TE:"),e("br"),t._v("\nConverts high-level operations into Tensor Expression (TE) definitions using strategies (src/relay/backend/te_compiler.cc).")]),t._v(" "),e("li",[t._v("Computation Abstraction:\nDefines computations in TE with ComputeOp (src/te/operation/compute_op.cc).")]),t._v(" "),e("li",[t._v("Scheduling:"),e("br"),t._v("\nApplies optimizations like tiling, unrolling, and mapping computations to threads/blocks (src/te/schedule/schedule_lang.cc).")]),t._v(" "),e("li",[t._v("Lowering to TIR:"),e("br"),t._v("\nTranslates the schedule into TIR, which explicitly handles device memory and control flow (src/tir/transforms/lower_tir.cc).")]),t._v(" "),e("li",[t._v("Device-Specific Codegen:"),e("br"),t._v("\nEmits target-specific code (e.g., CUDA) via CodeGenCUDA (src/target/source/codegen_cuda.cc).")])]),t._v(" "),e("hr"),t._v(" "),e("h3",{attrs:{id:"_3-code-generation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-code-generation"}},[t._v("#")]),t._v(" 3. Code Generation")]),t._v(" "),e("h3",{attrs:{id:"_3-1-gpu-code-generation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-gpu-code-generation"}},[t._v("#")]),t._v(" 3.1 GPU Code Generation")]),t._v(" "),e("p",[t._v("This phase translates Tensor IR (TIR) into GPU-compatible low-level code, generating CUDA kernels and API calls.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Process")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("TIR to CUDA Kernel")]),t._v(" "),e("td",[t._v("src/target/source/codegen_cuda.cc")]),t._v(" "),e("td",[t._v("CodeGenCUDA, GenerateKernel, PrintStmt")])]),t._v(" "),e("tr",[e("td",[t._v("CodeGen Base Class")]),t._v(" "),e("td",[t._v("src/target/source/codegen_c.cc")]),t._v(" "),e("td",[t._v("CodeGenC, PrintExpr")])]),t._v(" "),e("tr",[e("td",[t._v("Shared Memory Handling")]),t._v(" "),e("td",[t._v("src/target/source/codegen_cuda.cc")]),t._v(" "),e("td",[t._v("PrintStorageScope, PrintStorageSync")])]),t._v(" "),e("tr",[e("td",[t._v("Thread/Block Synchronization")]),t._v(" "),e("td",[t._v("src/tir/transforms/thread_storage_sync.cc")]),t._v(" "),e("td",[t._v("ThreadStorageSync")])])])]),t._v(" "),e("p",[e("strong",[t._v("Explanation:")]),t._v("\nCodeGenCUDA translates TIR to CUDA kernels, emitting device-side code and managing constructs like thread/block mappings, shared memory, and synchronization.\nSynchronization points are inserted using PrintStorageSync.")]),t._v(" "),e("h3",{attrs:{id:"_3-2-kernel-construction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-kernel-construction"}},[t._v("#")]),t._v(" 3.2. Kernel Construction")]),t._v(" "),e("p",[t._v("Kernel construction involves creating CUDA device kernels and host-side launcher code to invoke them.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Process")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Kernel Emission")]),t._v(" "),e("td",[t._v("src/target/source/codegen_cuda.cc")]),t._v(" "),e("td",[t._v("PrintFuncBody, EmitFunction")])]),t._v(" "),e("tr",[e("td",[t._v("Kernel Launch Code")]),t._v(" "),e("td",[t._v("src/runtime/cuda/cuda_module.cc")]),t._v(" "),e("td",[t._v("CUDAWrappedFunc, LaunchKernel")])]),t._v(" "),e("tr",[e("td",[t._v("Kernel Metadata Management")]),t._v(" "),e("td",[t._v("src/runtime/module.cc")]),t._v(" "),e("td",[t._v("PackImports, ExportModule")])])])]),t._v(" "),e("p",[t._v("Explanation:\nThe EmitFunction generates kernel function declarations and definitions for execution on the GPU.\nHost-side kernel launchers are defined in cuda_module.cc.")]),t._v(" "),e("h3",{attrs:{id:"_3-3-cublas-cutlass-integration"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-cublas-cutlass-integration"}},[t._v("#")]),t._v(" 3.3. cuBLAS/CUTLASS Integration")]),t._v(" "),e("p",[t._v("When using cuBLAS or CUTLASS for tensor computations (e.g., GEMM), TVM generates calls to these libraries instead of writing explicit CUDA kernels.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Process")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("cuBLAS Integration")]),t._v(" "),e("td",[t._v("src/runtime/contrib/cublas/cublas.cc")]),t._v(" "),e("td",[t._v("CUBLASCall, InitCUBLASHandle, GemmOp")])]),t._v(" "),e("tr",[e("td",[t._v("CUTLASS Integration")]),t._v(" "),e("td",[t._v("src/contrib/cutlass/gen_cutlass_gemm.cc")]),t._v(" "),e("td",[t._v("GenerateCutlassGemm, EmitCutlassCode")])]),t._v(" "),e("tr",[e("td",[t._v("External Code Generation")]),t._v(" "),e("td",[t._v("src/relay/backend/contrib/cublas_codegen.cc")]),t._v(" "),e("td",[t._v("CUBLASFunction, CodegenCUBLAS")])])])]),t._v(" "),e("p",[t._v("Explanation:\ncublas.cc provides wrappers for cuBLAS API calls like cublasSgemm, with TVM handling data layout transformations as needed.\nCUTLASS integration uses template-based code generation in gen_cutlass_gemm.cc, emitting optimized kernels for matrix operations.")]),t._v(" "),e("h3",{attrs:{id:"_3-4-target-specific-optimizations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-target-specific-optimizations"}},[t._v("#")]),t._v(" 3.4. Target-Specific Optimizations")]),t._v(" "),e("p",[t._v("Target-specific optimizations fine-tune the generated CUDA code based on the GPU architecture and memory hierarchy.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Process")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Thread/Block Mapping")]),t._v(" "),e("td",[t._v("src/tir/transforms/thread_storage_sync.cc")]),t._v(" "),e("td",[t._v("ThreadStorageSync, OptimizeThreads")])]),t._v(" "),e("tr",[e("td",[t._v("Loop Partitioning")]),t._v(" "),e("td",[t._v("src/tir/transforms/loop_partition.cc")]),t._v(" "),e("td",[t._v("LoopPartition")])]),t._v(" "),e("tr",[e("td",[t._v("Memory Planning")]),t._v(" "),e("td",[t._v("src/tir/transforms/storage_rewrite.cc")]),t._v(" "),e("td",[t._v("StorageRewrite, PlanMemory")])]),t._v(" "),e("tr",[e("td",[t._v("Warp-Level Optimization")]),t._v(" "),e("td",[t._v("src/tir/transforms/vectorize_loop.cc")]),t._v(" "),e("td",[t._v("VectorizeLoop, Vectorizer")])])])]),t._v(" "),e("p",[t._v("Explanation:\nThread and block mapping ensures optimal utilization of GPU threads and memory.\nLoop partitioning and vectorization optimize data access patterns for warp-level efficiency.\nStorageRewrite minimizes memory usage by analyzing reuse patterns and adjusting allocation.")]),t._v(" "),e("h3",{attrs:{id:"_3-5-memory-management"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-memory-management"}},[t._v("#")]),t._v(" 3.5. Memory Management")]),t._v(" "),e("p",[t._v("Efficient memory management involves optimizing shared/global memory usage and enabling memory reuse.")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Process")]),t._v(" "),e("th",[t._v("File")]),t._v(" "),e("th",[t._v("Key Functions/Classes")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Shared Memory Usage")]),t._v(" "),e("td",[t._v("src/target/source/codegen_cuda.cc")]),t._v(" "),e("td",[t._v("PrintStorageScope, EmitSharedMemory")])]),t._v(" "),e("tr",[e("td",[t._v("Memory Allocation")]),t._v(" "),e("td",[t._v("src/tir/transforms/storage_rewrite.cc")]),t._v(" "),e("td",[t._v("PlanMemory, ReuseMemory")])]),t._v(" "),e("tr",[e("td",[t._v("Memory Alignment")]),t._v(" "),e("td",[t._v("src/target/source/codegen_cuda.cc")]),t._v(" "),e("td",[t._v("PrintStorageAlloc")])])])]),t._v(" "),e("p",[t._v("Explanation:\nShared memory scopes are explicitly emitted during CUDA codegen (EmitSharedMemory).\nPlanMemory optimizes allocation to minimize fragmentation and overhead.")]),t._v(" "),e("h3",{attrs:{id:"_3-6-overall-codegen-workflow"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-overall-codegen-workflow"}},[t._v("#")]),t._v(" 3.6. Overall Codegen Workflow")]),t._v(" "),e("p",[t._v("Key Stages and Their Files")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("TIR Lowering:"),e("br"),t._v("\nFile: src/tir/transforms/lower_tir.cc"),e("br"),t._v("\nFunction: LowerTIR, TransformTIR")])]),t._v(" "),e("li",[e("p",[t._v("CUDA Kernel Emission:"),e("br"),t._v("\nFile: src/target/source/codegen_cuda.cc"),e("br"),t._v("\nFunction: EmitFunction, GenerateKernel")])]),t._v(" "),e("li",[e("p",[t._v("cuBLAS Integration:"),e("br"),t._v("\nFile: src/runtime/contrib/cublas/cublas.cc"),e("br"),t._v("\nFunction: CUBLASCall, InitCUBLASHandle")])]),t._v(" "),e("li",[e("p",[t._v("CUTLASS Integration:"),e("br"),t._v("\nFile: src/contrib/cutlass/gen_cutlass_gemm.cc"),e("br"),t._v("\nFunction: GenerateCutlassGemm, EmitCutlassCode")])]),t._v(" "),e("li",[e("p",[t._v("Target-Specific Optimizations:"),e("br"),t._v("\nFile: src/tir/transforms/thread_storage_sync.cc"),e("br"),t._v("\nFunction: ThreadStorageSync, OptimizeThreads")])])])])}),[],!1,null,null,null);e.default=n.exports}}]);