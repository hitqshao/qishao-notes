(window.webpackJsonp=window.webpackJsonp||[]).push([[35],{446:function(e,a,t){"use strict";t.r(a);var r=t(5),i=Object(r.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity")])]),e._v(" "),a("hr"),e._v(" "),a("h1",{attrs:{id:"_1-a-survey-of-architectural-approaches-for-improving-gpgpu-performance-programmability-and-heterogeneity"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-a-survey-of-architectural-approaches-for-improving-gpgpu-performance-programmability-and-heterogeneity"}},[e._v("#")]),e._v(" 1. A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity")]),e._v(" "),a("p",[e._v("Four major improvement")]),e._v(" "),a("ul",[a("li",[e._v("mitigating the impact of control flow divergence")]),e._v(" "),a("li",[e._v("alleviating resource contention and efficient utilization of memory bandwidth across the entire memory hierarchy, including caches, interconnection and main\nmemory")]),e._v(" "),a("li",[e._v("increasing the available parallelism and concurrency")]),e._v(" "),a("li",[e._v("improving pipeline execution and exploiting scalarization opportunities.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/d9c6b47b-d469-4154-9dc5-b0b0f4168d70",alt:"image"}})]),e._v(" "),a("h2",{attrs:{id:"control-flow-divergence"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#control-flow-divergence"}},[e._v("#")]),e._v(" "),a("strong",[e._v("Control flow divergence")])]),e._v(" "),a("ol",[a("li",[e._v("First, GPUs employ PDOM stack-based mechanism that serializes the execution of divergent paths. This serialization of divergent paths reduces the available thread level parallelism\n(i.e., the number of active warps at a time) which limits the ability of GPUs to hide long memory instruction latency.")]),e._v(" "),a("li",[e._v("Control divergence limits the number of active threads in the running warps. As a result, SIMD execution units are not efficiently utilized when a diverged warp is executed.")]),e._v(" "),a("li",[e._v("Control divergence may also lead memory divergence wherein threads in the same warp access different regions of memory and thus the memory coalescing unit fails to reduce memory requests. Memory divergence causes huge pressure on memory resources and leads long memory latency and performance degradation.")]),e._v(" "),a("li",[e._v("Irregular applications tend to cause workload imbalance in such a way that assigned work (i.e., active threads per CTAs) to some GPU cores are larger than others.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/f9f7621e-ba28-4261-84bf-73b7ba00d9c4",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"_1-regrouping-divergent-warps"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-regrouping-divergent-warps"}},[e._v("#")]),e._v(" 1. "),a("strong",[e._v("Regrouping Divergent warps")]),a("br")]),e._v(" "),a("p",[e._v("Instead, DWF dynamically re-forms divergent warps into new non-divergent warps on the fly."),a("br"),e._v("\nMoreover, DWF does not reconverge diverged warp at IPDOM in order to amortize coalesced memory address of converged warps.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/c19066f6-bbe5-4589-9ae5-58be982e465b",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/bae42a54-ea88-477b-8103-61fd313f03c1",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"_2-large-warp-cta-compaction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-large-warp-cta-compaction"}},[e._v("#")]),e._v(" 2.  "),a("strong",[e._v("Large Warp/CTA compaction")])]),e._v(" "),a("ul",[a("li",[a("p",[e._v("Thread Block Compaction (TBC)"),a("br"),e._v("\nAllows a group of warps, that belong to the same thread block, to share the same PDOM stack."),a("br"),e._v(" "),a("strong",[e._v("However, TBC stalls all warps within a CTA on any potentially divergent branch until all warps reach the branch point.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/0f41fa75-67e0-40e3-9f3b-fe1e55ca9d3e",alt:"image"}})])])]),e._v(" "),a("p",[e._v("The major difference between 1) and TBC is that 1) can only merge threads in a warp when they are ready in a queue. Thus it miss some potentials.")]),e._v(" "),a("p",[e._v("TBC replace per-warp convergence stack with in-threadblock stack.")]),e._v(" "),a("ul",[a("li",[a("p",[e._v("CAPRI"),a("br"),e._v("\nCAPRI dynamically identifies the compaction effectiveness of a branch and only stalls threads that are predicted to benefit from compaction."),a("br")])]),e._v(" "),a("li",[a("p",[e._v("SLP\nproposed SIMD lane permutation (SLP) as an optimization to expand the applicability of compaction in case of conventional compaction technique is ineffective.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/5c3a5f0d-40c5-4b17-9d73-e73d064fa170",alt:"image"}})])])]),e._v(" "),a("h3",{attrs:{id:"_3-multi-path-execution"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-multi-path-execution"}},[e._v("#")]),e._v(" 3. "),a("strong",[e._v("Multi-path execution")]),a("br")]),e._v(" "),a("ul",[a("li",[e._v("DPS"),a("br"),e._v("\nDual-path Stack"),a("br")]),e._v(" "),a("li",[e._v("Multi-path Execution"),a("br")])]),e._v(" "),a("h3",{attrs:{id:"_4-mimd-like-architecture"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-mimd-like-architecture"}},[e._v("#")]),e._v(" 4. "),a("strong",[e._v("MIMD-like architecture")]),a("br")]),e._v(" "),a("p",[e._v("Rogers et al. [194] observed that regular applications perform better with a wider warp size, whereas divergent applications achieve better performance with a  smaller warp size. "),a("br"),e._v("\nVWS groups sets of these smaller warps together by ganging their execution in the warp scheduler and thus amortizing the energy consumed by fetch, decode, and warp scheduling across more threads.")]),e._v(" "),a("h3",{attrs:{id:"_5-dynamic-kernels-threads"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-dynamic-kernels-threads"}},[e._v("#")]),e._v(" 5. "),a("strong",[e._v("Dynamic kernels/threads")]),e._v(" "),a("br")]),e._v(" "),a("p",[e._v("Related Paper:\n"),a("strong",[e._v("Characterization and Analysis of Dynamic Parallelism in Unstructured GPU Applications.")]),e._v(" [108] "),a("br"),e._v(" "),a("strong",[e._v("Dynamic Thread Block Launch: A Lightweight Execution Mechanism to Support Irregular Applications on GPUs")]),e._v(". [85] "),a("br"),e._v("\nBy wang jing NVIDIA")]),e._v(" "),a("p",[e._v("üëç üëç üëç\n"),a("strong",[e._v("These two paper has very thorough explanation of how kernels are launched, kernel parameters are gained and how thread create subkernel.")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/6e30803e-5aaf-4bad-b626-f301c3487c1f",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/eba31c47-1954-4251-bd72-ab05c1f9ce64",alt:"image"}})]),e._v(" "),a("h2",{attrs:{id:"cuda-enables-dynamic-parallsim-creating-subkernels-from-each-thread"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cuda-enables-dynamic-parallsim-creating-subkernels-from-each-thread"}},[e._v("#")]),e._v(" CUDA enables dynamic parallsim, creating subkernels from each thread.")]),e._v(" "),a("p",[a("strong",[e._v('Copied from "Characterization"')])]),e._v(" "),a("p",[e._v("When a child kernel is launched, the parameter buffer pointer of the kernel is retrieved through the device runtime API cudaGetParameterBuffer.\nThen the argument values are stored in the parameter buffer and the kernel is launched by calling cudaLaunchDevice.\nAfter that, the device runtime manager appends the child kernels to an execution queue and dispatches the kernel to SMXs according to a certain scheduling policy.\nThe CDP kernel launching overhead comprises of kernel parameter parsing, calling cudaGetParameterBuffer and cudaLaunchDevice, as well as the process that device runtime manager setups,enqueues, manages and dispatches the child kernels.")]),e._v(" "),a("hr"),e._v(" "),a("p",[e._v("however, the huge kernel launching overhead could negate the performance benefit of DFP. The overhead is due to the large number of launched kernels, the associated memory footprint and the low number of running warps per core.The CPU launches GPU kernels by dispatching kernel launching commands. Kernel parameters are passed from CPU to\nGPU at the kernel launching time and stored in the GPU global")]),e._v(" "),a("p",[e._v("Wang et al. [236] proposed new mechanism, called Dynamic Thread Block Launch (DTBL), that employs light-weight thread block rather than heavy-weight device kernel for DFP.")]),e._v(" "),a("hr"),e._v(" "),a("h2",{attrs:{id:"efficient-utilization-of-memory-bandwidth"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#efficient-utilization-of-memory-bandwidth"}},[e._v("#")]),e._v(" "),a("strong",[e._v("Efficient utilization of memory bandwidth")])]),e._v(" "),a("h3",{attrs:{id:"alleviating-cache-thrashing-and-resource-contention"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#alleviating-cache-thrashing-and-resource-contention"}},[e._v("#")]),e._v(" "),a("strong",[e._v("Alleviating cache thrashing, and resource contention")])]),e._v(" "),a("h4",{attrs:{id:"_1-two-level-warp-scheduling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-two-level-warp-scheduling"}},[e._v("#")]),e._v(" 1. "),a("strong",[e._v("Two-level warp scheduling")])]),e._v(" "),a("ul",[a("li",[e._v("TLRR"),a("br"),e._v(" "),a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/586e0677-70fd-4623-b4ee-beaf001ad3dc",alt:"image"}})])]),e._v(" "),a("p",[e._v("They proposed two-level round-robin warp scheduling (TL-RR), in which the warps are split into fetch groups."),a("br"),e._v("\nTL-RR executes only one fetch group at a time and it schedules warps from the same fetch group in a round-robin fashion."),a("br"),e._v("\nWhen the running warps reach a long latency operation, then the next fetch group is prioritized."),a("br"),e._v("\nThey try to alleviate the issue of threads in all warps "),a("strong",[e._v("arrive the same memory latency instruction at the same time")]),e._v("."),a("br")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("![image](https://github.com/hitqshao/qishao-notes/assets/23403286/6851655b-1a31-45ec-bc79-90cf4b97435d)\n\n![image](https://github.com/hitqshao/qishao-notes/assets/23403286/2ff9dbbb-d756-42e1-b821-bc698417a9a0)\n")])])]),a("ul",[a("li",[e._v("OWL\nOWL augments the TL-RR with CTA-awareness, such that warps are split into groups of CTAs basis rather than warps basis, resulting in increased intra-CTA locality. "),a("br"),e._v("\nOWL gives a group of CTAs higher priority when their data exist at the L1 cache such that they get the opportunity to reuse it, therefore improving L1 hit rates and alleviating cache contention."),a("br")])]),e._v(" "),a("h4",{attrs:{id:"_2-coarse-grained-cta-throttling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-coarse-grained-cta-throttling"}},[e._v("#")]),e._v(" 2. "),a("strong",[e._v("Coarse-grained CTA throttling")])]),e._v(" "),a("p",[a("strong",[e._v("DYNCTA")]),e._v(" "),a("br"),e._v("\n- Always executing the maximum possible number of CTAs on a GPU core (i.e., increasing TLP to the maximum) does not always lead to better performance.\n- To alleviate resource contention, they proposed dynamic CTA scheduling mechanism (DYNCTA), which aims to allocate the optimal number of CTAs per GPU core that alleviate memory contention according to an application characteristics.\n- DYNCTA dynamically adjusts over sampling periods the number of active CTAs per GPU core that reduces the memory latency without sacrificing the available TLP.\n"),a("strong",[e._v("LCS")]),e._v(" "),a("br"),e._v("\nIn contrast to DYNCTA that monitors the workload behavior for the entire kernel execution, LCS leverages GTO scheduler to find the optimal number of thread blocks at the early beginning of kernel execution.")]),e._v(" "),a("h4",{attrs:{id:"_3-fine-grained-warp-throttling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-fine-grained-warp-throttling"}},[e._v("#")]),e._v(" 3. "),a("strong",[e._v("Fine-grained warp throttling")]),e._v("  ")]),e._v(" "),a("p",[e._v("due to the massive multithreading and the limited capacity of L1 cache, divergent GPGPU applications cause severe cache contention."),a("br")]),e._v(" "),a("ul",[a("li",[a("strong",[e._v("CCWS")]),e._v(" "),a("br"),e._v("\nuses a victim tag array, called lost locality detector, to detect warps that have lost locality due to thrashing. These warps are prioritized till they exploit their locality while other warps are descheduled."),a("br")]),e._v(" "),a("li",[a("strong",[e._v("DAWS")]),e._v(" "),a("br"),e._v("\nDAWS is a divergence-based cache footprint predictor to calculate the amount of locality in loops required by each warp. "),a("br"),e._v("\nDAWS uses these predictions to prioritize a group of warps such that the cache footprint of these warps do not exceed the capacity of the L1 cache. "),a("br")])]),e._v(" "),a("h4",{attrs:{id:"_4-throttling-and-cache-bypassing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-throttling-and-cache-bypassing"}},[e._v("#")]),e._v(" 4. "),a("strong",[e._v("Throttling and cache bypassing")]),e._v("  ")]),e._v(" "),a("p",[e._v("previous CTA or warp throttling techniques leave memory bandwidth and other chip resources (L2 cache, interconnection and execution units) significantly underutilized.")]),e._v(" "),a("ul",[a("li",[a("strong",[e._v("PCAL")]),e._v(" "),a("br"),e._v("\nAt the beginning of kernel execution, PCAL executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources.\nThus, PCAL reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.")]),e._v(" "),a("li",[a("strong",[e._v("CCA")]),e._v(" "),a("br"),e._v("\nAt the beginning of kernel execution, PCAL executes an optimal number of active warps, that alleviates thrashing and conflicts, then extra inactive warps are allowed to bypass cache and utilize the other on-chip resources. Thus, PCAL reduces cache thrashing and effectively utilizes the chip resources that would otherwise go unused by a pure thread throttling approach.")])]),e._v(" "),a("h4",{attrs:{id:"_5-critical-warp-awareness"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-critical-warp-awareness"}},[e._v("#")]),e._v(" 5. "),a("strong",[e._v("Critical warp awareness")])]),e._v(" "),a("p",[e._v("some warps may be assigned more workload and exhibit longer latency compared to other warps within the same Thread Block.\nHence, fast warps are idle at a synchronization barrier or at the end of kernel execution until the critical (i.e., the slowest) warp finishes execution.\nThus, the overall execution time is dominated by the performance of these critical warps.")]),e._v(" "),a("p",[a("strong",[e._v("CAWA")]),e._v(" dynamically identifies critical warps and coordinates warp scheduling and cache prioritization to accelerate the critical warp execution.")]),e._v(" "),a("h4",{attrs:{id:"_6-cache-management-and-bypassing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-cache-management-and-bypassing"}},[e._v("#")]),e._v(" 6. "),a("strong",[e._v("Cache management and bypassing")])]),e._v(" "),a("p",[a("strong",[e._v("GCache")]),e._v("\nTo detect thrashing, they equip L2 cache tag array with extra bits (victim bits) to provide L1 cache with some information about the hot lines that have been evicted before. An adaptive cache replacement policy is used by L1 cache to protect these hot lines.")]),e._v(" "),a("h4",{attrs:{id:"_7-ordering-buffers"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-ordering-buffers"}},[e._v("#")]),e._v(" 7. "),a("strong",[e._v("Ordering buffers")])]),e._v(" "),a("p",[e._v("The idea of MRPB is two-fold."),a("br"),e._v("\nFirst, a FIFO requests buffer is used to reorder memory references so that requests from the same warp are grouped and sent to the cache together in a more cache-friendly order. This results in drastically reducing cache contention and improving use of the limited per-thread cache capacity."),a("br"),e._v("\nSecond, MRPB allows memory request that encounters associativity stall to bypass L1 cache."),a("br")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/0db0f277-f82f-4dcf-b319-8646080ad6c5",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/8321f4fa-56cd-4046-b2b3-d61b96bdba7c",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"_8-resource-tuning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8-resource-tuning"}},[e._v("#")]),e._v(" 8. "),a("strong",[e._v("Resource tuning")])]),e._v(" "),a("p",[e._v("Equalizer, a dynamic runtime system that tunes number of thread blocks, core and memory frequency to match the requirements of the running kernel, leading to efficient execution and energy saving.")]),e._v(" "),a("h3",{attrs:{id:"_2-high-bandwidth-many-thread-aware-memory-hierarchy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-high-bandwidth-many-thread-aware-memory-hierarchy"}},[e._v("#")]),e._v(" 2. "),a("strong",[e._v("High-bandwidth many-thread-aware memory hierarchy")])]),e._v(" "),a("h4",{attrs:{id:"_1-mitigating-off-chip-bandwidth-bottleneck"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-mitigating-off-chip-bandwidth-bottleneck"}},[e._v("#")]),e._v(" 1. "),a("strong",[e._v("Mitigating off-chip bandwidth bottleneck")])]),e._v(" "),a("p",[a("strong",[e._v("LAMAR")]),e._v("\nEmerging irregular workloads benefit from fine-grain (FG) memory access by avoiding unnecessary data transfers, that may be happened under CG policy,")]),e._v(" "),a("p",[e._v("they proposed a locality-aware memory hierarchy (LAMAR) that adaptively tunes the memory access granularity for the running kernel.\nLAMAR employs CG accesses for kernels with high temporal and spatial locality, while applying FG accesses for irregular divergent workloads in attempt to reduce\nmemory over-fetching.")]),e._v(" "),a("p",[a("strong",[e._v("CABA")]),e._v("\nVijaykumar et al. [231] proposed, Core-Assisted Bottleneck Acceleration (CABA) framework, that exploits the underutilized computational resources to perform useful work and alleviate different bottlenecks in GPU execution.")]),e._v(" "),a("p",[e._v("For instance, to alleviate memory bandwidth bottleneck, CABA dynamically creates assist warps that execute with the original warps side by side on the same GPU\ncore.")]),e._v(" "),a("p",[e._v("Assist warps opportunistically use idle computational units to perform data decompression for the incoming compressed cache blocks and compression for the outgoing cache blocks, leading to less transferring data from memory and mitigating memory bandwidth problem.")]),e._v(" "),a("p",[a("strong",[e._v("Approximation")]),e._v("\nAn approximation technique in which the GPU drops some portion of load requests which miss in the cache after approximating their values.")]),e._v(" "),a("h4",{attrs:{id:"_2-memory-divergence-normalization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-memory-divergence-normalization"}},[e._v("#")]),e._v(" 2. "),a("strong",[e._v("Memory divergence normalization")])]),e._v(" "),a("p",[a("strong",[e._v("Orchestrated Scheduling and Prefetching for GPGPUs")])]),e._v(" "),a("p",[e._v("they proposed prefetch-aware warp scheduling, that coordinates simple data prefetcher and warp scheduling in an intelligent manner such that the scheduling of\ntwo consecutive warps are separated in time, and thus prefetching becomes more effective.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/933a4834-0b89-4496-bf94-6f76f1a003ba",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"_3-fine-grained-warp-throttling-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-fine-grained-warp-throttling-2"}},[e._v("#")]),e._v(" 3. "),a("strong",[e._v("Fine-grained warp throttling")])]),e._v(" "),a("h4",{attrs:{id:"_4-throttling-and-cache-bypassing-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-throttling-and-cache-bypassing-2"}},[e._v("#")]),e._v(" 4. "),a("strong",[e._v("Throttling and cache bypassing")])]),e._v(" "),a("h4",{attrs:{id:"_5-critical-warp-awareness-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-critical-warp-awareness-2"}},[e._v("#")]),e._v(" 5. "),a("strong",[e._v("Critical warp awareness")])]),e._v(" "),a("h4",{attrs:{id:"_6-cache-management-and-bypassing-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-cache-management-and-bypassing-2"}},[e._v("#")]),e._v(" 6. "),a("strong",[e._v("Cache management and bypassing")])]),e._v(" "),a("h4",{attrs:{id:"_7-ordering-buffers-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-ordering-buffers-2"}},[e._v("#")]),e._v(" 7. "),a("strong",[e._v("Ordering buffers")])]),e._v(" "),a("h4",{attrs:{id:"_8-resource-tuning-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8-resource-tuning-2"}},[e._v("#")]),e._v(" 8. "),a("strong",[e._v("Resource tuning")])]),e._v(" "),a("h4",{attrs:{id:"_9-cache-management-and-bypassing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_9-cache-management-and-bypassing"}},[e._v("#")]),e._v(" 9. "),a("strong",[e._v("Cache management and bypassing")])]),e._v(" "),a("h3",{attrs:{id:"high-bandwidth-many-thread-aware-memory-hierarchy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#high-bandwidth-many-thread-aware-memory-hierarchy"}},[e._v("#")]),e._v(" "),a("strong",[e._v("High-bandwidth many-thread-aware memory hierarchy")])]),e._v(" "),a("h4",{attrs:{id:"_1-mitigating-off-chip-bandwidth-bottleneck-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-mitigating-off-chip-bandwidth-bottleneck-2"}},[e._v("#")]),e._v(" 1. "),a("strong",[e._v("Mitigating off-chip bandwidth bottleneck")])]),e._v(" "),a("h4",{attrs:{id:"_2-memory-divergence-normalization-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-memory-divergence-normalization-2"}},[e._v("#")]),e._v(" 2. "),a("strong",[e._v("Memory divergence normalization")])]),e._v(" "),a("h4",{attrs:{id:"_3-interconnection-network"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-interconnection-network"}},[e._v("#")]),e._v(" 3. "),a("strong",[e._v("Interconnection network")])]),e._v(" "),a("h4",{attrs:{id:"_4-main-memory-scheduling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-main-memory-scheduling"}},[e._v("#")]),e._v(" 4. "),a("strong",[e._v("Main memory scheduling")])]),e._v(" "),a("h4",{attrs:{id:"_5-heterogeneous-memory-management"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-heterogeneous-memory-management"}},[e._v("#")]),e._v(" 5. "),a("strong",[e._v("Heterogeneous memory management")])]),e._v(" "),a("h4",{attrs:{id:"_6-cpu-gpu-memory-transfer-overhead"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-cpu-gpu-memory-transfer-overhead"}},[e._v("#")]),e._v(" 6. "),a("strong",[e._v("CPU‚ÄìGPU memory transfer overhead")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"increasing-parallelism-and-improving-execution-pipelining"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#increasing-parallelism-and-improving-execution-pipelining"}},[e._v("#")]),e._v(" "),a("strong",[e._v("Increasing parallelism and improving execution pipelining")])]),e._v(" "),a("h4",{attrs:{id:"_1-reducing-resource-fragmentation-and-increasing-parallelism"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-reducing-resource-fragmentation-and-increasing-parallelism"}},[e._v("#")]),e._v(" 1. "),a("strong",[e._v("Reducing resource fragmentation and increasing parallelism")])]),e._v(" "),a("p",[e._v("üëç üëç üëç")]),e._v(" "),a("p",[a("strong",[e._v("Unifying")]),e._v("they proposed a unified local memory which integrates the register file, L1 cache, and scratchpad memory into one large on-chip storage. Then, the hardware can dynamically partition the on-chip storage according to each application‚Äôs needs.")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/4d79262c-410b-4601-8d2a-55f983a2e924",alt:"image"}})]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/f372f9b7-ed97-4d20-a439-b7e067e2c08a",alt:"image"}})]),e._v(" "),a("h4",{attrs:{id:"_2-gpu-multitasking"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-gpu-multitasking"}},[e._v("#")]),e._v(" 2. "),a("strong",[e._v("GPU multitasking")])]),e._v(" "),a("h4",{attrs:{id:"_3-exploiting-scalar-and-value-similarity-opportunities"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-exploiting-scalar-and-value-similarity-opportunities"}},[e._v("#")]),e._v(" 3. "),a("strong",[e._v("Exploiting scalar and value similarity opportunities")])]),e._v(" "),a("h4",{attrs:{id:"_4-improving-execution-pipelining"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-improving-execution-pipelining"}},[e._v("#")]),e._v(" 4. "),a("strong",[e._v("Improving execution pipelining")])]),e._v(" "),a("h3",{attrs:{id:"enhancing-gpgpu-programmability"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#enhancing-gpgpu-programmability"}},[e._v("#")]),e._v(" "),a("strong",[e._v("Enhancing GPGPU programmability")])]),e._v(" "),a("h4",{attrs:{id:"_1-coherence-and-consistency-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-coherence-and-consistency-model"}},[e._v("#")]),e._v(" 1. "),a("strong",[e._v("Coherence and consistency model")])]),e._v(" "),a("h4",{attrs:{id:"_2-transactional-memory"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-transactional-memory"}},[e._v("#")]),e._v(" 2. "),a("strong",[e._v("Transactional memory")])]),e._v(" "),a("h4",{attrs:{id:"_3-deterministic-gpu"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-deterministic-gpu"}},[e._v("#")]),e._v(" 3. "),a("strong",[e._v("Deterministic GPU")])]),e._v(" "),a("h4",{attrs:{id:"_4-memory-management"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-memory-management"}},[e._v("#")]),e._v(" 4. "),a("strong",[e._v("Memory management")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/aced8bea-9237-43f8-8fbb-2238f1d401b7",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"cpu-gpu-heterogeneous-architecture"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cpu-gpu-heterogeneous-architecture"}},[e._v("#")]),e._v(" "),a("strong",[e._v("CPU‚ÄìGPU heterogeneous architecture")])]),e._v(" "),a("h4",{attrs:{id:"_1-impacts-of-cpu-gpu-integration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-impacts-of-cpu-gpu-integration"}},[e._v("#")]),e._v(" 1. "),a("strong",[e._v("Impacts of CPU‚ÄìGPU integration")])]),e._v(" "),a("h4",{attrs:{id:"_2-cpu-gpu-programmability"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-cpu-gpu-programmability"}},[e._v("#")]),e._v(" 2. "),a("strong",[e._v("CPU‚ÄìGPU programmability")])]),e._v(" "),a("p",[e._v("Heterogeneous System Coherence for Integrated CPU-GPU Systems")]),e._v(" "),a("h4",{attrs:{id:"_3-exploiting-heterogeneity"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-exploiting-heterogeneity"}},[e._v("#")]),e._v(" 3. "),a("strong",[e._v("Exploiting heterogeneity")])]),e._v(" "),a("h4",{attrs:{id:"_4-shared-resources-management"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-shared-resources-management"}},[e._v("#")]),e._v(" 4. "),a("strong",[e._v("Shared resources management")])]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/hitqshao/qishao-notes/assets/23403286/908d6507-51ce-4f35-8b88-095ae5eb78e5",alt:"image"}})])])}),[],!1,null,null,null);a.default=i.exports}}]);