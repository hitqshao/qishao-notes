(window.webpackJsonp=window.webpackJsonp||[]).push([[82],{545:function(e,n,o){"use strict";o.r(n);var a=o(8),i=Object(a.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("p",[e._v("Too many paper on llms...")]),e._v(" "),n("p",[e._v("Survey")]),e._v(" "),n("ol",[n("li",[e._v("[C60 2024] Understanding LLMs: A Comprehensive Overview from Training to Inference")]),e._v(" "),n("li",[e._v("[P 58] A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms")]),e._v(" "),n("li",[e._v("[P 30] A Comprehensive Evaluation of Quantization Strategies for Large Language Models")]),e._v(" "),n("li",[e._v("[P 8]  A Comprehensive Study on Quantization Techniques for Large Language Models")]),e._v(" "),n("li")]),e._v(" "),n("p",[e._v("KV Cache")]),e._v(" "),n("ol",[n("li",[e._v("[C60 2024 USENIX] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management")])]),e._v(" "),n("p",[e._v("Quantization")]),e._v(" "),n("ol",[n("li",[e._v("[C8 Y2024] An Empirical Study of LLaMA3 Quantization: From LLMs to MLLMs")])]),e._v(" "),n("p",[e._v("Cross-layer Attention")]),e._v(" "),n("ol",[n("li",[e._v("[C25 Y2024] "),n("strong",[e._v("Reducing Transformer Key-Value Cache Size")]),e._v(" with Cross-Layer Attention")]),e._v(" "),n("li",[e._v("[C4 2024] Cross-layer Attention Sharing for Large Language Models")])]),e._v(" "),n("p",[e._v("Attatch Memory")]),e._v(" "),n("ol",[n("li",[e._v("[C2 2024] Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU")])]),e._v(" "),n("p",[e._v("Novel LLM")]),e._v(" "),n("ol",[n("li",[e._v("[C2 2024] Larimar: Large Language Models with Episodic Memory Control")])]),e._v(" "),n("p",[e._v("Batch")]),e._v(" "),n("ol",[n("li",[e._v("[C59 2023] Batch Prompting: Efficient Inference with Large Language Model APIs")])]),e._v(" "),n("p",[e._v("Pruning")]),e._v(" "),n("ol",[n("li",[e._v("[C25 2024] ZipLM: Inference-Aware Structured Pruning of Language Models")])]),e._v(" "),n("p",[e._v("Speculative decoding")]),e._v(" "),n("ol",[n("li",[e._v("[C25 2024] Efficient Inference for Large Language Model-based Generative Recommendation")]),e._v(" "),n("li",[e._v("[C39 2024] Enhancing Inference Efficiency and Accuracy in Large Language Models through Next-Phrase Prediction")]),e._v(" "),n("li",[e._v("[C5 2023] Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy")])]),e._v(" "),n("p",[e._v("Interesting")]),e._v(" "),n("ol",[n("li",[e._v("[C60 2024] SnapKV: LLM Knows What You Are Looking for before Generation")]),e._v(" "),n("li",[e._v("[C6 2024] Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training")]),e._v(" "),n("li",[e._v("[C9 ISCA 2024] LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference")]),e._v(" "),n("li",[e._v("[C6 2024] Efficient Large Foundation Models Design: A Perspective From Model and System Co-Design")]),e._v(" "),n("li",[e._v("[C83 2023] Compressing Context to Enhance Inference Efficiency of Large Language Models")]),e._v(" "),n("li",[e._v("[C13 2024 OSDI] ServerlessLLM: Low-Latency Serverless Inference for Large Language Models")]),e._v(" "),n("li",[e._v("[C12 2024] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models")])])])}),[],!1,null,null,null);n.default=i.exports}}]);