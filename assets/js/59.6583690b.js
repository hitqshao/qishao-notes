(window.webpackJsonp=window.webpackJsonp||[]).push([[59],{470:function(e,a,n){"use strict";n.r(a);var r=n(5),t=Object(r.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("ol",[a("li",[e._v("[354] Benchmarking TPU, GPU, and CPU Platforms for Deep Learning")]),e._v(" "),a("li",[e._v("[59 Year:2019] Characterizing Deep Learning Training Workloads on Alibaba-PAI")]),e._v(" "),a("li",[e._v("[12] Effective Elastic Scaling of Deep Learning Workloads")]),e._v(" "),a("li",[e._v("[30 Year:2024] LLM Inference Unveiled: Survey and Roofline Model Insights")]),e._v(" "),a("li",[e._v("[1] Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference")]),e._v(" "),a("li",[e._v("[Blog] LLM Inference Series: 5. Dissecting model performance")]),e._v(" "),a("li",[e._v("[47 Year:2024] Understanding LLMs: A Comprehensive Overview from Training to Inference")]),e._v(" "),a("li",[e._v("[37 Year:2022] Reveal training performance mystery between TensorFlow and PyTorch in the single GPU environment üëç  üëç  üëç  üëç  üëç")]),e._v(" "),a("li",[e._v("[46 Year:2019] Performance Characterization of DNN Training using TensorFlow and PyTorch on Modern Clusters")]),e._v(" "),a("li",[e._v("[163 Year:2020] Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference")])]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"_1-benchmarking-tpu-gpu-and-cpu-platforms-for-deep-learning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-benchmarking-tpu-gpu-and-cpu-platforms-for-deep-learning"}},[e._v("#")]),e._v(" 1. Benchmarking TPU, GPU, and CPU Platforms for Deep Learning")]),e._v(" "),a("p",[e._v("Paper from Harvard")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/110ddbc0-1ddf-40fa-b360-9e3f589494c6",alt:"image"}})]),e._v(" "),a("h3",{attrs:{id:"_10-inducing-and-exploiting-activation-sparsity-for-fast-neural-network-inference"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_10-inducing-and-exploiting-activation-sparsity-for-fast-neural-network-inference"}},[e._v("#")]),e._v(" 10. Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference")]),e._v(" "),a("p",[e._v("Sparsity across channels:")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/d0ab98ae-adcd-438a-97fe-db8c31becb3f",alt:"image"}})]),e._v(" "),a("p",[e._v("Sparsity across layers:\n"),a("img",{attrs:{src:"https://github.com/user-attachments/assets/68c5159a-f536-4714-aadf-0772b6c73dde",alt:"image"}})]),e._v(" "),a("p",[a("em",[a("strong",[e._v("CSCC: Convolution Split Compression Calculation Algorithm for Deep Neural Network")])]),e._v("\nSparsity across layers:\n"),a("img",{attrs:{src:"https://github.com/user-attachments/assets/0496984d-6cad-4727-8ee7-f63023a1656c",alt:"image"}})])])}),[],!1,null,null,null);a.default=t.exports}}]);