(window.webpackJsonp=window.webpackJsonp||[]).push([[84],{544:function(e,a,t){"use strict";t.r(a);var i=t(8),n=Object(i.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"llm-training-time"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#llm-training-time"}},[e._v("#")]),e._v(" LLM Training time")]),e._v(" "),a("p",[e._v("Llama3 405B")]),e._v(" "),a("p",[e._v("C = 6 * N * D")]),e._v(" "),a("ul",[a("li",[e._v("C compute")]),e._v(" "),a("li",[e._v("N parameter number")]),e._v(" "),a("li",[e._v("D token number")])]),e._v(" "),a("p",[e._v("C = 6 * 405 * 10^9 * 15*10^12 = 3.6 * 10^25")]),e._v(" "),a("p",[e._v("C/(16KGPU*TFlops)")]),e._v(" "),a("p",[e._v("C/(16 * 1000 * 400 * 10^12) = 97 days")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://www.factorialfunds.com/blog/thoughts-on-llama-3",target:"_blank",rel:"noopener noreferrer"}},[e._v("Source"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("same as:")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/723e951d-9eab-40b9-b42c-916c5c3084cc",alt:"image"}})]),e._v(" "),a("p",[a("a",{attrs:{href:"%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"}},[e._v("Page125")])]),e._v(" "),a("p",[e._v("Also in")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/4635d893-655c-403d-82f5-fc16e972a670",alt:"image"}})]),e._v(" "),a("p",[a("a",{attrs:{href:"https://arxiv.org/pdf/2305.10403#page=36&zoom=100,63,210",target:"_blank",rel:"noopener noreferrer"}},[e._v("Source"),a("OutboundLink")],1),e._v("\nFrom paper "),a("em",[e._v("PaLM 2 Technical Report")])]),e._v(" "),a("p",[e._v("This is far different from:")]),e._v(" "),a("p",[a("img",{attrs:{src:"https://github.com/user-attachments/assets/e18267cb-ad5f-465a-b19f-1a13eef6d49f",alt:"image"}})]),e._v(" "),a("p",[a("em",[e._v("Source:Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM")])]),e._v(" "),a("p",[e._v("Still needs time to check.")]),e._v(" "),a("p",[e._v("Paper waiting to be read:")]),e._v(" "),a("ul",[a("li",[e._v("Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM")]),e._v(" "),a("li",[e._v("ZeRO: Memory Optimizations Toward Training Trillion Parameter Models")]),e._v(" "),a("li",[e._v("How Does Critical Batch Size Scale in Pre-training?")]),e._v(" "),a("li",[e._v("Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management üëç\n"),a("img",{attrs:{src:"https://github.com/user-attachments/assets/77b9b55d-54c7-46e4-b66a-70d11305704e",alt:"image"}})]),e._v(" "),a("li",[e._v("Comparative Study of Large Language Model Architectures on Frontier")]),e._v(" "),a("li",[e._v("Optimizing Distributed Training on Frontier for Large Language Models")]),e._v(" "),a("li",[e._v("Deep Optimizer States: Towards Scalable Training of Transformer Models Using Interleaved Offloading")]),e._v(" "),a("li",[e._v("The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities")]),e._v(" "),a("li",[e._v("Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies")]),e._v(" "),a("li",[e._v("Towards Scalable Automated Alignment of LLMs: A Survey")]),e._v(" "),a("li",[e._v("MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length")]),e._v(" "),a("li",[e._v("Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models")]),e._v(" "),a("li",[e._v("[C1731] Training Compute-Optimal Large Language Models üëç")]),e._v(" "),a("li",[e._v("[C138] Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers")]),e._v(" "),a("li",[e._v("[C1082] Scaling Language Models: Methods, Analysis & Insights from Training Gopher")])])])}),[],!1,null,null,null);a.default=n.exports}}]);