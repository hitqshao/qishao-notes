(window.webpackJsonp=window.webpackJsonp||[]).push([[48],{461:function(e,t,a){"use strict";a.r(t);var s=a(5),r=Object(s.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[248] Dissecting GPU Memory Hierarchy through Microbenchmarking")]),e._v(" "),t("li",[e._v("[75] Benchmarking the Memory Hierarchy of Modern GPUs")]),e._v(" "),t("li",[e._v("[18] Benchmarking the GPU memory at the warp level")]),e._v(" "),t("li",[e._v("[90] Dissecting the NVidia Turing T4 GPU via Microbenchmarking")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_1-dissecting-gpu-memory-hierarchy-through-microbenchmarking"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-dissecting-gpu-memory-hierarchy-through-microbenchmarking"}},[e._v("#")]),e._v(" 1. Dissecting GPU Memory Hierarchy through Microbenchmarking")]),e._v(" "),t("p",[e._v("A paper in 2015, profile memory in Fermi, Kepler and Maxwell")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/683d67af-3feb-4d35-9ecf-dfeafb814c37",alt:"image"}})]),e._v(" "),t("h4",{attrs:{id:"parameter"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#parameter"}},[e._v("#")]),e._v(" Parameter")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/08215b14-4856-4d3a-8c6a-b5050f905f02",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/5daed100-0155-4fed-9358-e26681294b2a",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/60213279-226b-4a30-aa05-36271e9ac0ff",alt:"image"}})]),e._v(" "),t("h4",{attrs:{id:"l1-data-cache"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#l1-data-cache"}},[e._v("#")]),e._v(" L1 Data Cache")]),e._v(" "),t("p",[e._v("On the Fermi and Kepler devices, the L1 data cache and shared memory are physically implemented together."),t("br"),e._v("\nOn the Maxwell devices, the L1 data cache is unified with the texture cache.")]),e._v(" "),t("p",[e._v("The 16 KB L1 cache has 128 cache lines mapped onto four cache ways."),t("br"),e._v("\nFor each cache way, 32 cache sets are divided into 8 major sets. Each major set contains 16 cache lines.")]),e._v(" "),t("p",[e._v("The data mapping is also unconventional."),t("br"),e._v("\nThe 12-13th bits in the memory address define the cache way, the 9-11th bits define the major set, and the 0-6th bits define the memory offset inside the cache line.\n"),t("img",{attrs:{src:"https://github.com/user-attachments/assets/f997bf94-4b5b-4948-882c-7f72dd7bd506",alt:"image"}})]),e._v(" "),t("p",[e._v("One distinctive feature of the Fermi L1 cache is that its replacement policy is not LRU, as pointed out by Meltzer et.al.\nAmong the four cache ways, cache way 2 is three times more likely to be replaced than the other three cache ways.")]),e._v(" "),t("p",[t("strong",[e._v("Another paper[4]")]),e._v(" We found that when the L1 data cache saturates, Turing randomly evicts 4 consecutive cache lines (128 B)."),t("br"),e._v("\nWe observed that once a block of cache lines are evicted, the second scan will cause more cache lines from the same set to be evicted.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/ae6a8abd-7d57-4e0c-98ea-12264a37ae75",alt:"image"}})]),e._v(" "),t("h4",{attrs:{id:"l2-data-cache"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#l2-data-cache"}},[e._v("#")]),e._v(" L2 Data Cache")]),e._v(" "),t("ul",[t("li",[e._v("The replacement policy of the L2 cache is not LRU")]),e._v(" "),t("li",[t("strong",[e._v("The L2 cache line size is 32 bytes")]),e._v(" by observing the memory access pattern of overflowing the cache and visiting array element one by one.")]),e._v(" "),t("li",[e._v("The data mapping is sophisticated and not conventional bits-defined")]),e._v(" "),t("li",[e._v("a hardware-level pre-fetching mechanism from the DRAM to the L2 data cache on all three platforms."),t("br"),e._v(" "),t("strong",[e._v("The pre-fetching size is about 2/3 of the L2 cache size and the prefetching is sequential. This is deduced from that if we load an array smaller than 2/3 of the L2 data cache size, there is no cold cache miss patterns.")]),t("br"),e._v("\nüôã(Maybe they can cover the gap just by prefetching sequential line.)")])]),e._v(" "),t("h4",{attrs:{id:"global-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#global-memory"}},[e._v("#")]),e._v(" Global Memory")]),e._v(" "),t("p",[e._v("global memory access involves accessing the DRAM, L1 and L2 data caches, TLBs and page tables.")]),e._v(" "),t("h5",{attrs:{id:"global-memory-throughput"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#global-memory-throughput"}},[e._v("#")]),e._v(" Global Memory Throughput")]),e._v(" "),t("p",[e._v("The theoretical bandwidth is calculated as fmem * bus width * DDR factor.\n"),t("img",{attrs:{src:"https://github.com/user-attachments/assets/dbb8cdc6-e0cd-4bc6-aec8-f9450ea6d0bf",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/707f8b05-88e6-40b2-b3e4-426f984d4405",alt:"image"}})]),e._v(" "),t("p",[e._v("the throughput of a larger ILP saturates faster.")]),e._v(" "),t("p",[e._v("The GTX780 has the highest throughput as it benefits from the highest bus width,"),t("br"),e._v("\nbut its convergence speed is the slowest, i.e., it requires the most memory requests to hide the pipeline latency.")]),e._v(" "),t("p",[t("strong",[e._v("This could be part of the reason that NVIDIA reduced the bus width back to 256 bits in Maxwell devices.")])]),e._v(" "),t("h5",{attrs:{id:"global-memory-latency"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#global-memory-latency"}},[e._v("#")]),e._v(" Global Memory Latency")]),e._v(" "),t("p",[t("strong",[e._v("The global memory access latency is the whole time accessing a data located in DRAM/L2 or L1 cache, including the latency of page table look-ups.")])]),e._v(" "),t("ul",[t("li",[e._v("very large s1 = 32 MB to construct the TLB/page table miss and cache miss (P5&P6)")]),e._v(" "),t("li",[e._v("set s2 = 1 MB to construct the L1 TLB hit but cache miss (P4)")]),e._v(" "),t("li",[e._v("After a total of 65 data accesses, 65 data lines are loaded into the cache."),t("br"),e._v("\nWe then visit the cached data lines with s1 again for several times, to construct cache hit but TLB miss (P2&P3).")]),e._v(" "),t("li",[e._v("set s3 = 1 element and repeatedly load the data in a cache line so that every memory access is a cache hit (P1).")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/e5c397e8-f4b4-46ba-b7ee-41c34fa08b33",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/1f616c8e-a758-4151-b1eb-61f15c810246",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/822fc563-d0dc-4708-afd2-89549adb7ec4",alt:"image"}})]),e._v(" "),t("ul",[t("li",[e._v("The Maxwell and Kepler devices have a unique memory access pattern (P6) for page table context switching. "),t("br"),e._v("\nWhen a kernel is launched, only memory page entries of 512 MB are activated. "),t("br"),e._v("\nIf the thread visits an inactivate page entry, the hardware needs a rather long time to switch between page tables."),t("br"),e._v("\nThis phenomena is also reported in [22] as page table ‚Äúmiss‚Äù.")]),e._v(" "),t("li",[e._v("The Maxwell L1 data cache addressing does not go through the TLBs or page tables."),t("br"),e._v("\nOn the GTX980, there is no TLB miss pattern (i.e., P2 and P3) when the L1 data cache is hit."),t("br"),e._v("\nOnce the L1 cache is missed, the access latency increases from tens of cycles to hundreds or even thousands of cycles.\n"),t("strong",[e._v("My comments: But if we look at GTX560Ti in P2, the latency is different with P1. So does this means that in Fermi, the memory request has to go through TLB first, and then access L1 DataCache? This might be the reason that the latency is longer. But this will degrade the performance....")])]),e._v(" "),t("li",[e._v("The TLBs are off-chip. we infer that the physical memory locations of the L1 TLB and L2 data cache are close. "),t("br"),e._v("\nThe physical memory locations of the L1 TLB and L2 TLB are also close, which means that the L1/L2 TLB and L2 data cache are shared off-chip by all SMs.")]),e._v(" "),t("li",[e._v("The GTX780 generally has the shortest global memory latencies, almost half that of the Fermi, with an access pattern of P2-P5."),t("br"),e._v("\nThe page table context switching of the GTX980 is also much more expensive than that of the GTX780.")])]),e._v(" "),t("p",[e._v("To summarize, the Maxwell device has "),t("em",[e._v("long global memory access latencies")]),e._v(" for cold cache misses and page table context switching."),t("br"),e._v("\nExcept for these rare access patterns, its access latency cycles are close to those of the Kepler device. "),t("br"),e._v("\nbecause the GTX980 has higher fmem than the GTX780, it actually offers the shortest global memory access time (P2-P4).")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/8d12e01f-1a6e-49e7-894c-28de28c9f864",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"shared-memory"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory"}},[e._v("#")]),e._v(" Shared Memory")]),e._v(" "),t("p",[e._v("In CUDA programming, different CTAs assigned to the same SM have to share the same physical memory space."),t("br"),e._v("\nOn the Fermi and Kepler platforms, the shared memory is physically integrated with the L1 cache."),t("br"),e._v("\nOn the Maxwell platform, it occupies a separate memory space.\n"),t("strong",[e._v("Note that the shared memory and L1 cache are separated since Maxwell architecture.")])]),e._v(" "),t("p",[t("em",[e._v("Programmers")]),e._v(" move the data into and out of shared memory from global memory before and after arithmetic execution,"),t("br"),e._v("\nto avoid the frequent occurrence of long global memory access latencies.")]),e._v(" "),t("p",[t("strong",[e._v("We report a dramatic improvement in performance for the Maxwell device.")])]),e._v(" "),t("h5",{attrs:{id:"shared-memory-throughput"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-throughput"}},[e._v("#")]),e._v(" Shared Memory Throughput")]),e._v(" "),t("p",[e._v("the shared memory is organized as 32 memory banks [15]."),t("br"),e._v("\nThe bank width of the "),t("strong",[e._v("Fermi and Maxwell devices is 4 bytes")]),e._v(", while that of the Kepler device is 8 bytes.\nThe theoretical peak throughput of each SM (WSM) is calculated as fcore ‚àó Wbank ‚àó 32.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/3bf6da77-b196-4e13-b2fe-af410ee750a4",alt:"image"}})]),e._v(" "),t("p",[t("strong",[e._v("The achieved throughput per SM is calculated as 2 * fcore * sizeof(int) * (number of active threads per SM) * ILP / (total latency of each SM).")]),e._v("\nUsually a large value of ILP results in less active warps per SM."),t("br"),e._v("\nThe peak throughput W0SM denotes the respective maximum throughput of the abovecombinations."),t("br"),e._v("\nTwo key factors that affect the throughput are the number of active warps per SM and the ILP level.")]),e._v(" "),t("p",[e._v("The GTX980 reaches its peak throughput when the CTA size = 256, CTAs per SM = 2 and ILP = 8, i.e., 16 active warps per SM. The peak throughput is 137.41 GB/s, about "),t("em",[e._v("83.9%")]),e._v(" of the theoretical bandwidth.\nThe Maxwell device shows the best use of its shared memory bandwidth, and the Kepler device shows the worst.")]),e._v(" "),t("p",[e._v("GTX980 exhibits similar behavior as GTX780: high ILP is required to achieve high throughput for high SM occupancy.")]),e._v(" "),t("p",[e._v("According to Little‚Äôs Law, we roughly have: number of active warps * ILP = latency cycles * throughput.")]),e._v(" "),t("p",[t("strong",[e._v("GTX780 sucks in ILP = 1, since its limited 64 warps at most to be scheduled concurrently.")]),t("br"),e._v("\nWe consider this to be the main reason the achieved throughput of the GTX780 is poor compared with its designed value.")]),e._v(" "),t("h4",{attrs:{id:"shared-memory-latency"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-latency"}},[e._v("#")]),e._v(" Shared Memory Latency")]),e._v(" "),t("p",[t("strong",[e._v("The shared memory latencies on Fermi, Kepler and Maxwell devices are 50, 47 and 28 cycles, respectively.")])]),e._v(" "),t("p",[t("strong",[e._v("Fermi and Maxwell devices have the same number of potential bank conflicts because they have the same architecture.")])]),e._v(" "),t("p",[e._v("The shared memory space is divided into 32 banks."),t("br"),e._v("\nSuccessive words are allocated to successive banks."),t("br"),e._v("\nIf two threads in the same warp access memory spaces in the same bank, a 2-way bank conflict occurs.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c5ef66d3-c05e-46b7-84d6-ace224aafeab",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c8e4560b-68aa-4188-9621-05a9f90fca32",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/f8e02655-9d5d-4d80-9bfb-fb6e5aefde8f",alt:"image"}})]),e._v(" "),t("p",[e._v("For the Fermi and Kepler devices, where there is a 32-way bank conflict, it takes much longer to access shared memory than regular global memory (TLB hit, cache miss). "),t("br"),e._v("\nSurprisingly, the effect of a bank conflict on shared memory access latency on the Maxwell device is mild."),t("br"),e._v("\nEven the longest shared memory access latency is still at the same level as L1 data cache latency.")]),e._v(" "),t("p",[e._v("In summary, although the shared memory has very short access latency, it can be rather long if there are many ways of bank conflicts."),t("br"),e._v("\nThis is most obvious on the Fermi hardware."),t("br"),e._v("\nThe Kepler device tries to solve it by doubling the bank width of shared memory."),t("br"),e._v("\nCompared with the Fermi, the Kepler‚Äôs 4-byte mode shared memory halves the chance of bank conflict, and the 8-byte mode reduces it further.")]),e._v(" "),t("p",[e._v("However, we also find that the Kepler‚Äôs shared memory is inefficient in terms of throughput."),t("br"),e._v("\nThe Maxwell device has the best shared memory performance."),t("br"),e._v("\nWith the same architecture as the Fermi device, the Maxwell hardware shows a 2x size, 2x memory access speedup and achieves the highest throughput."),t("br"),e._v("\nMost importantly, the Maxwell device‚Äôs shared memory has been optimized to avoid the long latency caused by bank conflicts.")]),e._v(" "),t("h4",{attrs:{id:"conclusion"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[e._v("#")]),e._v(" Conclusion")]),e._v(" "),t("p",[e._v("The memory capacity is significantly enhanced in both Kepler and Maxwell as compared with Fermi."),t("br"),e._v("\nThe Kepler device is performance-oriented and incorporates several aggressive elements in its design, such as increasing the bus width of DRAM and doubling the bank width of shared memory."),t("br"),e._v("\nThese designs have some side-effects."),t("br"),e._v("\nThe theoretical bandwidths of both global memory and shared memory are difficult to saturate, and hardware resources are imbalanced with a low utilization rate."),t("br"),e._v("\nThe Maxwell device has a more efficient and conservative design."),t("br"),e._v("\nIt has a reduced bus width and bank width, and the on-chip cache architectures are adjusted, including doubling the shared memory size and the read-only data cache size. Furthermore, it sharply decreases the shared memory latency caused under bank conflicts.")]),e._v(" "),t("h3",{attrs:{id:"_4-dissecting-the-nvidia-turing-t4-gpu-via-microbenchmarking"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-dissecting-the-nvidia-turing-t4-gpu-via-microbenchmarking"}},[e._v("#")]),e._v(" 4. Dissecting the NVidia Turing T4 GPU via Microbenchmarking")]),e._v(" "),t("h4",{attrs:{id:"result"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#result"}},[e._v("#")]),e._v(" Result")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/eb21b04f-6ce8-44ef-8307-d26c35fa8a86",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/bae4d3b8-b2df-463b-a8f8-c095fbb53c9d",alt:"image"}})]),e._v(" "),t("h4",{attrs:{id:"shared-memory-latency-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#shared-memory-latency-2"}},[e._v("#")]),e._v(" Shared Memory Latency")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/ae7a4300-20fb-4be0-9404-e1c39a223d7d",alt:"image"}})]),e._v(" "),t("h4",{attrs:{id:"bandwidth"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#bandwidth"}},[e._v("#")]),e._v(" Bandwidth")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/74dc0901-e5a8-4084-b6a1-d8d71175926f",alt:"image"}})])])}),[],!1,null,null,null);t.default=r.exports}}]);