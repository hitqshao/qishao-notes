(window.webpackJsonp=window.webpackJsonp||[]).push([[55],{468:function(e,t,a){"use strict";a.r(t);var i=a(5),r=Object(i.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("1.[2023] Evaluating Unified Memory Performance in HIP\n2.[97] Unlocking Bandwidth for GPUs in CC-NUMA Systems")]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-2023-evaluating-unified-memory-performance-in-hip"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-2023-evaluating-unified-memory-performance-in-hip"}},[e._v("#")]),e._v(" 1.[2023] Evaluating Unified Memory Performance in HIP")]),e._v(" "),t("p",[e._v("UM only works on recent AMD GPUs, including Vega10 and MI100."),t("br"),e._v("\nThere are two flavors of the support: XNACK-enabled and XNACK-disabled.\\")]),e._v(" "),t("ul",[t("li",[e._v("In the XNACK-enabled mode,a GPU can handle retry of a memory access after page-faults, which enables mapping and migrating data on demand, as well\nas memory overcommitment.")]),e._v(" "),t("li",[e._v("In the XNACK-disabled mode, all memory must be resident and mapped in GPU page tables when the GPU is executing application code."),t("br"),e._v("\nThe XNACK-enabled mode only has experimental support.")])]),e._v(" "),t("p",[e._v("The experimental results show that the performance of the applications using UM is closely related to data transfer size and memory accesses of a kernel. Compared to “UM”, prefetching\nmemory as a memory usage hint leads to significant data transfers between the host and device.")]),e._v(" "),t("p",[e._v("Compared to “UM”, prefetching memory as a memory usage hint leads to significant data transfers between the host and device.")]),e._v(" "),t("ul",[t("li",[e._v("“UM-hint” and “UM” indicate unified memory with and without memory usage hints, respectively.")]),e._v(" "),t("li",[e._v("“ZeroCopy” uses zero-copy buffers for data migration.")]),e._v(" "),t("li",[e._v("“PageableCopy” copies data from pageable host memory to device memory")]),e._v(" "),t("li",[e._v("“PageLockedCopy” transfers data from page-locked host memory to device memory")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/76a68408-3cee-458e-b1d0-ad3a2fc7ae0a",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"result-analysis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#result-analysis"}},[e._v("#")]),e._v(" Result Analysis")]),e._v(" "),t("p",[e._v("The result shows that the stall rate is highly sensitive to the increase of memory size in UM.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/84ec16fc-6579-407a-b624-17365e348d9b",alt:"image"}})]),e._v(" "),t("p",[e._v("The decrease of the kernel execution time ranges from approximately 1.1X to 2.8X with respect to the vector length for the three optimization techniques.、\nHowever, the execution time is still approximately 1.4X to 74.8X "),t("strong",[e._v("longer than that of the kernel that takes the copy-then-execute")]),e._v(" approach.")]),e._v(" "),t("p",[e._v("In [28], the authors present 32 open-source UM benchmarks in CUDA and evaluate their performance on an NVIDIA Pascal GPU."),t("br"),e._v("\nThey find that across the benchmarks the performance of the UM benchmarks is on average "),t("strong",[e._v("34.2%")]),e._v(" slower compared with the benchmarks without UM due to the cost of page fault\nhandling")]),e._v(" "),t("blockquote",[t("p",[e._v("[28] "),t("em",[e._v("UVMBench: A Comprehensive Benchmark Suite for Researching Unified Virtual Memory in GPU")])])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-97-unlocking-bandwidth-for-gpus-in-cc-numa-systems"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-97-unlocking-bandwidth-for-gpus-in-cc-numa-systems"}},[e._v("#")]),e._v(" 2.[97] Unlocking Bandwidth for GPUs in CC-NUMA Systems")]),e._v(" "),t("p",[t("em",[e._v("Nvidia with umich")])]),e._v(" "),t("h3",{attrs:{id:"main-idea-in-short"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#main-idea-in-short"}},[e._v("#")]),e._v(" Main Idea in Short")]),e._v(" "),t("ul",[t("li",[e._v("Mainly focus on how many pages that covers the page-fault pages should be migrated.")]),e._v(" "),t("li",[e._v("Prefetching with upgraded range, which balance the prefetching and also reduce the number of TLB shootdowns")]),e._v(" "),t("li",[e._v("TLB shootdown is estimated at 100 cycles")]),e._v(" "),t("li",[t("strong",[e._v("Memory Oversubscription and Eviction is not considered.")])]),e._v(" "),t("li",[e._v("Page Migration Threshold accustomed to each workload is complex. And not worth it. It is better to just migrate on first touch.")])]),e._v(" "),t("h3",{attrs:{id:"introduction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/369a641b-18b1-4eb2-8c4e-3d83c1861ade",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"contribution"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#contribution"}},[e._v("#")]),e._v(" Contribution")]),e._v(" "),t("ul",[t("li",[e._v("Counter-based metrics to determine when to migrate pages from the CPU to GPU are insufficient for finding an optimal migration policy to exploit GPU memory bandwidth."),t("br"),e._v("\nIn streaming workloads, where each page may be accessed only a few times, waiting for N accesses to occur before migrating a page will actually limit the number of accesses that occur after migration, reducing the efficacy of the page migration operation.")])]),e._v(" "),t("ol",{attrs:{start:"2"}},[t("li",[e._v("TLB shootdown and refill overhead can significantly degrade the performance of any page migration policy for GPUs."),t("br"),e._v("\nWe show that combining reactive migration with virtual address locality information to aggressively prefetch pages can mitigate much of this overhead, resulting in increased GPU throughput.")])]),e._v(" "),t("h3",{attrs:{id:"interesting-experiment"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#interesting-experiment"}},[e._v("#")]),e._v(" Interesting Experiment")]),e._v(" "),t("p",[e._v("Performance comparson of DDR and GDDR Experiments")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/e5c30717-100c-4792-b0b9-6d15d3f72144",alt:"image"}})]),e._v(" "),t("p",[e._v("This choice is motivated by our observation that the performance of some GPU compute workloads would degrade by as much as 66% if the traditional GDDR memory on a GPU were replaced with standard DDR memory, as seen in Figure 2.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/7eaec0c6-bc35-4f6d-b9aa-ae31d8b23f06",alt:"image"}})]),e._v(" "),t("p",[t("em",[e._v("Still confused about the following Figure.")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/76b5a722-f83b-458d-a8ab-bd03022702ff",alt:"image"}})]),e._v(" "),t("h3",{attrs:{id:"interesting-finding"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#interesting-finding"}},[e._v("#")]),e._v(" Interesting Finding")]),e._v(" "),t("h4",{attrs:{id:"clustered-page"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#clustered-page"}},[e._v("#")]),e._v(" Clustered Page")]),e._v(" "),t("p",[e._v("Page Accessing is clusted by memory arranges."),t("br"),e._v("\nPart of continuous virtual address is hot."),t("br"),e._v("\nThis clustering is key to range expansion because it suggests that if a page is identified for migration, then other neighboring pages in the virtual address space are likely to have a similar number of total touches.")]),e._v(" "),t("h4",{attrs:{id:"threshold-to-trigger-page-migration"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#threshold-to-trigger-page-migration"}},[e._v("#")]),e._v(" Threshold to trigger page migration")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/dced8b99-52e5-4390-9b4c-f825afe21cf2",alt:"image"}})]),e._v(" "),t("p",[e._v("a first touch policy (threshold-1) requires no tracking information and can be trivially implemented by migrating a page the first time the GPU\ntranslates an address for the page.")]),e._v(" "),t("p",[e._v("Considering the performance differential seen across thresholds, we believe the overhead of implementing the necessary hardware counters to track all pages within a system to differentiate their access counts is not worth the improvement over a vastly simpler first-touch migration policy.")]),e._v(" "),t("h3",{attrs:{id:"tlb"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#tlb"}},[e._v("#")]),e._v(" TLB")]),e._v(" "),t("p",[e._v("The runtime system also must be cognizant that performing TLB invalidations (an integral part of page migration) on a GPU does not just halt a single processor, but thousands of compute pipelines that may be accessing these pages through a large shared TLB structure."),t("br"),e._v("\nThis shared TLB structure makes page migrations between a CPU and GPU potentially much more costly (in terms of the opportunity cost of lost execution throughput) than in CPU-only systems.")]),e._v(" "),t("p",[e._v("Recent papers have provided proposals about how to efficiently implement general purpose TLBs that are, or could be, optimized for a GPU’s needs [28]–[30]."),t("br"),e._v("\nOthers have recently looked at improving TLB reach by exploiting locality within the virtual to physical memory remapping, or avoiding this layer completely [31]–[33]."),t("br"),e._v("\nFinally, Gerofi et al. [34] recently examined TLB performance of the Xeon Phi for applications with large footprints, while McCurdy et al. [35]\ninvestigated the effect of superpages and TLB coverage for HPC applications in the context of CPUs.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/7365332a-8dc1-4314-9eca-11ae29d117c2",alt:"image"}})])])}),[],!1,null,null,null);t.default=r.exports}}]);