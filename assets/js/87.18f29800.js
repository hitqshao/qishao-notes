(window.webpackJsonp=window.webpackJsonp||[]).push([[87],{549:function(e,t,a){"use strict";a.r(t);var r=a(8),i=Object(r.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"memory-optimizations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#memory-optimizations"}},[e._v("#")]),e._v(" "),t("strong",[e._v("Memory Optimizations")])]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Activation Checkpointing")]),t("br"),e._v("\nRecomputation during backward pass.")]),e._v(" "),t("li",[t("strong",[e._v("Quantization-Aware Training (QAT)")]),t("br"),e._v("\nTrain with INT8/FP8 precision.")]),e._v(" "),t("li",[t("strong",[e._v("Dynamic Memory Allocation")]),t("br"),e._v("\nBuffer reuse to avoid fragmentation.")]),e._v(" "),t("li",[t("strong",[e._v("Low-Rank Gradient Projection (GaLore)")]),t("br"),e._v(" "),t("strong",[e._v("NEW")]),e._v(" Compress gradients via low-rank approximations during training.")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#c99-y2024-full-parameter-fine-tuning-for-large-language-models-with-limited-resources"}},[e._v("#")]),e._v(" [C99 Y2024] Full Parameter Fine-tuning for Large Language Models with Limited Resources")]),e._v(" "),t("ul",[t("li",[e._v("Use SGD instead of Adam for fine-tuning weights.")]),e._v(" "),t("li",[e._v("Update layer by layer in backward pass. Traditional Adam will backward probgation all layers and then update weigths.")]),e._v(" "),t("li",[e._v("SGD also avoid state memory of ADAM.")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/c1e7d018-84e2-49f7-bd74-cdf4edaa0343",alt:"image"}})]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/a98a808f-1cb1-4432-b0d0-7c51e9f4ee48",alt:"image"}})]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#c25-y2024-flora-low-rank-adapters-are-secretly-gradient-compressors"}},[e._v("#")]),e._v(" [C25 Y2024] FLORA: Low-Rank Adapters Are Secretly Gradient Compressors")]),e._v(" "),t("p",[e._v("This paper discovers that LORA can be approximated by a random projection.")]),e._v(" "),t("p",[e._v("LORA restricts overall weights update matrices to be low-rank.")]),e._v(" "),t("p",[e._v("FLORA use "),t("em",[e._v("random projection matrix")]),e._v(", which allows high-rank update gradients.")]),e._v(" "),t("blockquote",[t("p",[e._v("Our intuition arises from investigating LoRA and observing that a LoRA update is dominated by a random projection, which compresses the gradient into a\nlower-dimensional space.\nOur FLORA resamples the random projection and is able to mitigate the low-rank limitation of LoRA. Further, our approach only stores the compressed gradient\naccumulation and momentum, thus saving the memory usage of optimization states to the sublinear level.")])]),e._v(" "),t("p",[e._v("Gradident Accumulation:")]),e._v(" "),t("ul",[t("li",[e._v("Gradient accumulation stores the sum of gradients over multiple batches to simulate a larger effective batch size (helpful when memory limits prevent using large batches).")]),e._v(" "),t("li",[e._v("Normally, this requires a memory buffer equal to the model size to store the full gradient matrix.")])]),e._v(" "),t("p",[e._v("Momentum")]),e._v(" "),t("ul",[t("li",[e._v("Momentum smooths gradient updates by keeping an exponentially weighted moving average (EMA) of past gradients.")]),e._v(" "),t("li",[e._v("Maintaining momentum for large models requires significant memory since an additional buffer equal to the model size is needed.")])]),e._v(" "),t("p",[e._v("FLORA Compression:")]),e._v(" "),t("ul",[t("li",[e._v("compress gradients accumulation: Applying a random projection matrix A to reduce the dimensionality of the gradients.")]),e._v(" "),t("li",[e._v("compress momentum: Using random projection to compress the momentum term M.")])]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#c42-y2024-galore-memory-efficient-llm-training-by-gradient-low-rank-projection"}},[e._v("#")]),e._v(" [C42 Y2024] GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://github.com/user-attachments/assets/3ddb7188-8d90-4232-8be1-cb570a74bc56",alt:"image"}})]),e._v(" "),t("blockquote",[t("p",[e._v("Key idea is to leverage the slowchanging low-rank structure of the gradient G(m√ón) of the weight matrix W, rather than trying to approximate the weight matrix itself as low rank.\nwhile the weight matrices are not necessarily low-rank, the gradient indeed becomes low-rank during the training for certain gradient forms and associated network\narchitectures.")])])])}),[],!1,null,null,null);t.default=i.exports}}]);