(window.webpackJsonp=window.webpackJsonp||[]).push([[87],{543:function(a,t,e){"use strict";e.r(t);var s=e(8),n=Object(s.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"aten"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aten"}},[a._v("#")]),a._v(" aten")]),a._v(" "),t("h2",{attrs:{id:"src"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#src"}},[a._v("#")]),a._v(" src")]),a._v(" "),t("h3",{attrs:{id:"aten-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aten-2"}},[a._v("#")]),a._v(" Aten")]),a._v(" "),t("h4",{attrs:{id:"core"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#core"}},[a._v("#")]),a._v(" core")]),a._v(" "),t("h5",{attrs:{id:"dispatch"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dispatch"}},[a._v("#")]),a._v(" dispatch")]),a._v(" "),t("p",[a._v("Dispatcher")]),a._v(" "),t("details",[t("summary",[a._v("Code")]),a._v(" "),t("div",{staticClass:"language-cpp line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-cpp"}},[t("code",[a._v("  "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("/**\n   * Register a new operator schema.\n   *\n   * If a schema with the same operator name and overload name already exists,\n   * this function will check that both schemas are exactly identical.\n   */")]),a._v("\n  RegistrationHandleRAII "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("registerDef")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("FunctionSchema schema"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" std"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("string debug"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" std"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("vector"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("at"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("Tag"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v(" tags "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("/**\n   * Register a kernel to the dispatch table for an operator.\n   * If dispatch_key is nullopt, then this registers a fallback kernel.\n   *\n   * @return A RAII object that manages the lifetime of the registration.\n   *         Once that object is destructed, the kernel will be deregistered.\n   */")]),a._v("\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("// NB: steals the inferred function schema, as we may need to hold on to")]),a._v("\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("// it for a bit until the real schema turns up")]),a._v("\n  RegistrationHandleRAII "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("registerImpl")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("OperatorName op_name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" c10"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("optional"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("DispatchKey"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v(" dispatch_key"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" KernelFunction kernel"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" c10"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("optional"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("impl"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("CppSignature"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v(" cpp_signature"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" std"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("unique_ptr"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("FunctionSchema"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v(" inferred_function_schema"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" std"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("string debug"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("/**\n   * Register a new operator by name.\n   */")]),a._v("\n  RegistrationHandleRAII "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("registerName")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("OperatorName op_name"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("/**\n   * Register a fallback kernel for a backend.\n   * If an operator is called but there is no concrete kernel for the dispatch\n   * key of the given operator arguments, it will check if there is such a\n   * fallback kernel for the given dispatch key and, if yes, call that one.\n   */")]),a._v("\n  RegistrationHandleRAII "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("registerFallback")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("DispatchKey dispatch_key"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" KernelFunction kernel"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" std"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("string debug"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("/**\n   * Use to register whenever we had a TORCH_LIBRARY declaration in the frontend\n   * API.  These invocations are only permitted once per program, so we raise\n   * an error if this is called again for the same namespace.\n   */")]),a._v("\n  RegistrationHandleRAII "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("registerLibrary")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("std"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("string ns"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" std"),t("span",{pre:!0,attrs:{class:"token double-colon punctuation"}},[a._v("::")]),a._v("string debug"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br"),t("span",{staticClass:"line-number"},[a._v("10")]),t("br"),t("span",{staticClass:"line-number"},[a._v("11")]),t("br"),t("span",{staticClass:"line-number"},[a._v("12")]),t("br"),t("span",{staticClass:"line-number"},[a._v("13")]),t("br"),t("span",{staticClass:"line-number"},[a._v("14")]),t("br"),t("span",{staticClass:"line-number"},[a._v("15")]),t("br"),t("span",{staticClass:"line-number"},[a._v("16")]),t("br"),t("span",{staticClass:"line-number"},[a._v("17")]),t("br"),t("span",{staticClass:"line-number"},[a._v("18")]),t("br"),t("span",{staticClass:"line-number"},[a._v("19")]),t("br"),t("span",{staticClass:"line-number"},[a._v("20")]),t("br"),t("span",{staticClass:"line-number"},[a._v("21")]),t("br"),t("span",{staticClass:"line-number"},[a._v("22")]),t("br"),t("span",{staticClass:"line-number"},[a._v("23")]),t("br"),t("span",{staticClass:"line-number"},[a._v("24")]),t("br"),t("span",{staticClass:"line-number"},[a._v("25")]),t("br"),t("span",{staticClass:"line-number"},[a._v("26")]),t("br"),t("span",{staticClass:"line-number"},[a._v("27")]),t("br"),t("span",{staticClass:"line-number"},[a._v("28")]),t("br"),t("span",{staticClass:"line-number"},[a._v("29")]),t("br"),t("span",{staticClass:"line-number"},[a._v("30")]),t("br"),t("span",{staticClass:"line-number"},[a._v("31")]),t("br"),t("span",{staticClass:"line-number"},[a._v("32")]),t("br"),t("span",{staticClass:"line-number"},[a._v("33")]),t("br"),t("span",{staticClass:"line-number"},[a._v("34")]),t("br"),t("span",{staticClass:"line-number"},[a._v("35")]),t("br"),t("span",{staticClass:"line-number"},[a._v("36")]),t("br"),t("span",{staticClass:"line-number"},[a._v("37")]),t("br"),t("span",{staticClass:"line-number"},[a._v("38")]),t("br")])])]),a._v(" "),t("h1",{attrs:{id:"c10"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#c10"}},[a._v("#")]),a._v(" C10")]),a._v(" "),t("h2",{attrs:{id:"core-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#core-2"}},[a._v("#")]),a._v(" core")]),a._v(" "),t("p",[t("strong",[a._v("GeneratorImpl.h")])]),a._v(" "),t("p",[a._v("Random Number Generator")]),a._v(" "),t("blockquote",[t("p",[a._v("A Pseudo Random Number Generator (PRNG) is an engine that uses an algorithm to generate a seemingly random sequence of numbers, that may be later be used in creating a random distribution. "),t("br"),a._v("\nSuch an engine almost always maintains a state and requires a seed to start off the creation of random numbers."),t("br"),a._v("\nOften times, users have found it beneficial to be able to explicitly create, retain, and destroy PRNG states and also be able to have control over the seed value.\\")])]),a._v(" "),t("h1",{attrs:{id:"torch"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#torch"}},[a._v("#")]),a._v(" torch")]),a._v(" "),t("h2",{attrs:{id:"csrc"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#csrc"}},[a._v("#")]),a._v(" csrc")]),a._v(" "),t("h3",{attrs:{id:"autograd"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#autograd"}},[a._v("#")]),a._v(" autograd")]),a._v(" "),t("p",[t("strong",[a._v("variable.h")])]),a._v(" "),t("p",[a._v("Variable")]),a._v(" "),t("p",[a._v("A "),t("code",[a._v("Variable")]),a._v(" augments a "),t("code",[a._v("Tensor")]),a._v(" with the ability to interact in our autograd machinery."),t("br"),a._v("\nConceptually, "),t("code",[a._v("Variable")]),a._v("s travel along "),t("code",[a._v("Edge")]),a._v("s between "),t("code",[a._v("Node")]),a._v("s in the autograd graph."),t("br"),a._v("\nA "),t("code",[a._v("Variable")]),a._v(" can either be a leaf, like a weight in a neural network, or an interior variable, when it is the result of an operation between variables. Every "),t("code",[a._v("Variable")]),a._v(" also stores another "),t("code",[a._v("Variable")]),a._v(" called its "),t("code",[a._v("grad")]),a._v(" (gradient)."),t("br"),a._v("\nIf the variable is a leaf, its\ngradient will be accumulated into this variable.")]),a._v(" "),t("p",[a._v("Every Tensor is a Variable, but sometimes we colloquially refer to Variables that don't require gradients as Tensors (since none of the autograd machinery for Variables applies)."),t("br"),a._v("\nHistorically, Variables and Tensors were separate concepts, but now they are exactly the same (i.e. we have using Variable = at::Tensor`).")]),a._v(" "),t("p",[a._v("Gradient Edges")]),a._v(" "),t("p",[a._v("Furthermore, "),t("code",[a._v("Variable")]),a._v("s have the notion of a "),t("code",[a._v("gradient_edge")]),a._v(", which is the edge in the autograd graph that connects the variable to a particular input\nof the gradient function that will be invoked with the variable during the backward pass.")]),a._v(" "),t("p",[a._v("More precisely, this gradient function can be one of two things:")]),a._v(" "),t("ul",[t("li",[t("ol",[t("li",[a._v("A "),t("code",[a._v("grad_fn")]),a._v(", if the variable is in the interior of the graph. This is the gradient of the function that produced the variable.")])])]),a._v(" "),t("li",[t("ol",{attrs:{start:"2"}},[t("li",[a._v("A "),t("code",[a._v("grad_accumulator")]),a._v(", if the variable is a leaf, which accumulates a scalar gradient value into its "),t("code",[a._v("grad")]),a._v(" variable.")])])])]),a._v(" "),t("p",[a._v("Versioning")]),a._v(" "),t("p",[a._v("Another major feature of "),t("code",[a._v("Variable")]),a._v("s are "),t("em",[a._v("versions")]),a._v(". "),t("br"),a._v("\nVersions are incremented when an in-place mutation of a variable occurs."),t("br"),a._v("\nVersions are useful when constructing "),t("code",[a._v("SavedVariable")]),a._v("s, which take a snapshot of a "),t("code",[a._v("Variable")]),a._v(" at a certain version."),t("br"),a._v("\nYou can retrieve a "),t("code",[a._v("Variable")]),a._v("'s version through its "),t("code",[a._v("current_version()")]),a._v(" method.")]),a._v(" "),t("p",[a._v("Views")]),a._v(" "),t("p",[a._v("It is possible for a  "),t("code",[a._v("Variable")]),a._v(" to be a "),t("em",[a._v("view")]),a._v(" of another "),t("code",[a._v("Variable")]),a._v(", in which case it tracks that "),t("code",[a._v("Variable")]),a._v("'s data and autograd history. Beyond\nconstruction, the interface of a view is identical to that of a regular "),t("code",[a._v("Variable")]),a._v(". "),t("br"),a._v("\nYou can determine whether "),t("code",[a._v("Variable")]),a._v(" is in fact a view by probing its "),t("code",[a._v("is_view()")]),a._v(" method. "),t("br"),a._v("\nNote that the "),t("em",[a._v("view")]),a._v(" semantics are only meaningful for "),t("code",[a._v("Variable")]),a._v(" relations that are relevant to autograd.")])])}),[],!1,null,null,null);t.default=n.exports}}]);