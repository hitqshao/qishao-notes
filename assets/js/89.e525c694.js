(window.webpackJsonp=window.webpackJsonp||[]).push([[89],{550:function(e,t,i){"use strict";i.r(t);var n=i(8),a=Object(n.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ol",[t("li",[e._v("[139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization")])]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-139-understanding-and-overcoming-the-challenges-of-efficient-transformer-quantization"}},[e._v("#")]),e._v(" 1. [139] Understanding and Overcoming the Challenges of Efficient Transformer Quantization")]),e._v(" "),t("h3",{attrs:{id:"problems"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#problems"}},[e._v("#")]),e._v(" Problems")]),e._v(" "),t("p",[e._v("The problems encountered when attempting to quantize transformer models like BERT, as highlighted in the paper:")]),e._v(" "),t("ul",[t("li",[e._v("High Dynamic Range of Activations\n"),t("ul",[t("li",[e._v("The authors observed that the activation tensors in transformer models, particularly in the residual connections, exhibit a very high dynamic range.")]),e._v(" "),t("li",[e._v("This means that the values within these tensors vary significantly in magnitude.")]),e._v(" "),t("li",[e._v("Quantization, especially when using low-bit fixed-point formats (like 8-bit integer), struggles to accurately represent such a wide range of values.")]),e._v(" "),t("li",[e._v("Trying to capture both very small and very large values within the limited range of a low-bit format inevitably leads to significant quantization errors.")])])]),e._v(" "),t("li",[e._v("Presence of Structured Outliers\n"),t("ul",[t("li",[e._v("The authors identified the presence of structured outliers within the activation tensors, particularly in the residual connections after the feed-forward network (FFN).")]),e._v(" "),t("li",[e._v("These outliers are not random; they appear to be correlated with specific input tokens and embedding dimensions.")]),e._v(" "),t("li",[e._v("Further analysis revealed that these outliers influence the attention mechanism in the subsequent layers, encouraging the model to attend to specific tokens like the separator token ([SEP]).")]),e._v(" "),t("li",[e._v("While this attention behavior might be beneficial for the model's performance, the outliers that cause it also create challenges for quantization.")])])]),e._v(" "),t("li",[e._v("Sensitivity to Quantization Noise\n"),t("ul",[t("li",[e._v("Different parts of the transformer model exhibit varying sensitivities to the noise introduced by quantization.")]),e._v(" "),t("li",[e._v("Some components, like the residual connections and certain attention heads, are particularly sensitive, and quantizing them aggressively can lead to a significant drop in accuracy.")]),e._v(" "),t("li",[e._v("This sensitivity necessitates careful consideration of which parts of the model to quantize and at what bit-width.")])])])]),e._v(" "),t("h3",{attrs:{id:"solutions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#solutions"}},[e._v("#")]),e._v(" Solutions")]),e._v(" "),t("p",[e._v("solutions proposed in the paper:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("Mixed-precision PTQ")]),e._v(" "),t("ul",[t("li",[e._v("The authors observed that different parts of the BERT model have varying sensitivities to quantization noise.")]),e._v(" "),t("li",[e._v("To address this, they proposed using a higher bit-width (16-bit) for the more sensitive activation tensors, particularly the residual sum after the feed-forward network (FFN).")]),e._v(" "),t("li",[e._v("This higher bit-width allows for more accurate representation of both the FFN's input and output, minimizing potential errors.")]),e._v(" "),t("li",[e._v("Additionally, they explored using low-bit (2-4) quantization for weights and token embeddings, which can significantly reduce model size without much accuracy loss.")])])]),e._v(" "),t("li",[t("p",[e._v("Per-embedding-group PTQ")]),e._v(" "),t("ul",[t("li",[e._v("The authors identified that outliers in the activation tensors primarily reside in a few specific embedding dimensions.")]),e._v(" "),t("li",[e._v("To address this, they proposed a novel per-embedding-group (PEG) quantization scheme, where distinct quantization parameters are used for different embedding dimensions or groups of dimensions.")]),e._v(" "),t("li",[e._v("This method involves splitting the activation tensor into groups along the embedding dimension and applying separate quantization parameters to each group.")]),e._v(" "),t("li",[e._v("To optimize this process, they introduced a range-based permutation step to ensure that all outliers are grouped together.")]),e._v(" "),t("li",[e._v("This approach effectively handles outliers without significantly increasing computational overhead.")])])]),e._v(" "),t("li",[t("p",[e._v("Quantization-aware training (QAT)")]),e._v(" "),t("ul",[t("li",[e._v("The authors also explored QAT, where the model is trained with simulated quantization operations.")]),e._v(" "),t("li",[e._v("This allows the model to adapt to the quantization noise, leading to improved performance compared to post-training quantization.")]),e._v(" "),t("li",[e._v("During QAT, they used learnable ranges for both weights and activations, further enhancing the model's adaptability to quantization.")])])])])])}),[],!1,null,null,null);t.default=a.exports}}]);