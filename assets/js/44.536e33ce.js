(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{454:function(n,e,t){"use strict";t.r(e);var i=t(5),r=Object(i.a)({},(function(){var n=this,e=n._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[e("p",[e("strong",[n._v("1) Nvidia Paper on Traning LLM")]),n._v("\nReducing Activation Recomputation in Large Transformer Models")]),n._v(" "),e("p",[e("strong",[n._v("2) Blog Understanding and Estimating GPU Memory")]),n._v("\nUnderstanding and Estimating GPU Memory Demands for Training LLMs in practice")]),n._v(" "),e("p",[e("strong",[n._v("3) Blog Memory-Efficient Training")]),n._v("\nMemory-Efficient Training of Large Language Models: Overcoming Constraints on Consumer GPUs for Large Neural Networks")]),n._v(" "),e("p",[e("strong",[n._v("4）Stanford Paper Low-Memory Neural Network Training:A Technical Report")]),n._v("\nLow-Memory Neural Network Training:A Technical Report")]),n._v(" "),e("p",[e("strong",[n._v("5) Blog Gradient / Activation checkpointing")]),n._v("\nhttps://iq.opengenus.org/gradient-checkpointing/")]),n._v(" "),e("p",[e("strong",[n._v("6) Tianqi Chen Gradient Checkpointing Paper")]),n._v("\nTraining Deep Nets with Sublinear Memory Cost")]),n._v(" "),e("p",[e("strong",[n._v("7）UCSD Efficient Finetuning of LLMs")]),n._v("\nhttps://cseweb.ucsd.edu/classes/wi24/cse234-a/slides/CSE234-GuestLecture-SumanthHegde.pdf")])])}),[],!1,null,null,null);e.default=r.exports}}]);